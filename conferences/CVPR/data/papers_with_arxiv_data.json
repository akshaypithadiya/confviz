[{"title": "Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild", "authors": "Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi", "link": "https://arxiv.org/abs/1911.11130", "summary": "We propose a method to learn 3D deformable object categories from raw\nsingle-view images, without external supervision. The method is based on an\nautoencoder that factors each input image into depth, albedo, viewpoint and\nillumination. In order to disentangle these components without supervision, we\nuse the fact that many object categories have, at least in principle, a\nsymmetric structure. We show that reasoning about illumination allows us to\nexploit the underlying object symmetry even if the appearance is not symmetric\ndue to shading. Furthermore, we model objects that are probably, but not\ncertainly, symmetric by predicting a symmetry probability map, learned\nend-to-end with the other components of the model. Our experiments show that\nthis method can recover very accurately the 3D shape of human faces, cat faces\nand cars from single-view images, without any supervision or a prior shape\nmodel. On benchmarks, we demonstrate superior accuracy compared to another\nmethod that uses supervision at the level of 2D image correspondences."}, {"title": "Footprints and Free Space From a Single Color Image", "authors": "Jamie Watson, Michael Firman, Aron Monszpart, Gabriel J. Brostow", "link": "https://arxiv.org/abs/2004.06376", "summary": "Understanding the shape of a scene from a single color image is a formidable\ncomputer vision task. However, most methods aim to predict the geometry of\nsurfaces that are visible to the camera, which is of limited use when planning\npaths for robots or augmented reality agents. Such agents can only move when\ngrounded on a traversable surface, which we define as the set of classes which\nhumans can also walk over, such as grass, footpaths and pavement. Models which\npredict beyond the line of sight often parameterize the scene with voxels or\nmeshes, which can be expensive to use in machine learning frameworks.\n  We introduce a model to predict the geometry of both visible and occluded\ntraversable surfaces, given a single RGB image as input. We learn from stereo\nvideo sequences, using camera poses, per-frame depth and semantic segmentation\nto form training data, which is used to supervise an image-to-image network. We\ntrain models from the KITTI driving dataset, the indoor Matterport dataset, and\nfrom our own casually captured stereo footage. We find that a surprisingly low\nbar for spatial coverage of training scenes is required. We validate our\nalgorithm against a range of strong baselines, and include an assessment of our\npredictions for a path-planning task."}, {"title": "Dynamic Fluid Surface Reconstruction Using Deep Neural Network", "authors": "Simron Thapa, Nianyi Li, Jinwei Ye"}, {"title": "CvxNet: Learnable Convex Decomposition", "authors": "Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, Andrea Tagliasacchi", "link": "https://arxiv.org/abs/1909.05736", "summary": "Any solid object can be decomposed into a collection of convex polytopes (in\nshort, convexes). When a small number of convexes are used, such a\ndecomposition can be thought of as a piece-wise approximation of the geometry.\nThis decomposition is fundamental in computer graphics, where it provides one\nof the most common ways to approximate geometry, for example, in real-time\nphysics simulation. A convex object also has the property of being\nsimultaneously an explicit and implicit representation: one can interpret it\nexplicitly as a mesh derived by computing the vertices of a convex hull, or\nimplicitly as the collection of half-space constraints or support functions.\nTheir implicit representation makes them particularly well suited for neural\nnetwork training, as they abstract away from the topology of the geometry they\nneed to represent. However, at testing time, convexes can also generate\nexplicit representations -- polygonal meshes -- which can then be used in any\ndownstream application. We introduce a network architecture to represent a low\ndimensional family of convexes. This family is automatically derived via an\nauto-encoding process. We investigate the applications of this architecture\nincluding automatic convex decomposition, image to 3D reconstruction, and\npart-based shape retrieval."}, {"title": "BSP-Net: Generating Compact Meshes via Binary Space Partitioning", "authors": "Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang", "link": "http://arxiv.org/abs/1911.06971", "summary": "Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only\nplayed a minor role in the deep learning revolution. Leading methods for\nlearning generative models of shapes rely on implicit functions, and generate\nmeshes only after expensive iso-surfacing routines. To overcome these\nchallenges, we are inspired by a classical spatial data structure from computer\ngraphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core\ningredient of BSP is an operation for recursive subdivision of space to obtain\nconvex sets. By exploiting this property, we devise BSP-Net, a network that\nlearns to represent a 3D shape via convex decomposition. Importantly, BSP-Net\nis unsupervised since no convex shape decompositions are needed for training.\nThe network is trained to reconstruct a shape using a set of convexes obtained\nfrom a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can\nbe easily extracted to form a polygon mesh, without any need for iso-surfacing.\nThe generated meshes are compact (i.e., low-poly) and well suited to represent\nsharp geometry; they are guaranteed to be watertight and can be easily\nparameterized. We also show that the reconstruction quality by BSP-Net is\ncompetitive with state-of-the-art methods while using much fewer primitives.\nCode is available at https://github.com/czq142857/BSP-NET-original."}, {"title": "Total3DUnderstanding: Joint Layout, Object Pose and Mesh Reconstruction for Indoor Scenes From a Single Image", "authors": "Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, Jian Jun Zhang", "link": "https://arxiv.org/abs/2002.12212", "summary": "Semantic reconstruction of indoor scenes refers to both scene understanding\nand object reconstruction. Existing works either address one part of this\nproblem or focus on independent objects. In this paper, we bridge the gap\nbetween understanding and reconstruction, and propose an end-to-end solution to\njointly reconstruct room layout, object bounding boxes and meshes from a single\nimage. Instead of separately resolving scene understanding and object\nreconstruction, our method builds upon a holistic scene context and proposes a\ncoarse-to-fine hierarchy with three components: 1. room layout with camera\npose; 2. 3D object bounding boxes; 3. object meshes. We argue that\nunderstanding the context of each component can assist the task of parsing the\nothers, which enables joint understanding and reconstruction. The experiments\non the SUN RGB-D and Pix3D datasets demonstrate that our method consistently\noutperforms existing methods in indoor layout estimation, 3D object detection\nand mesh reconstruction."}, {"title": "Generating and Exploiting Probabilistic Monocular Depth Estimates", "authors": "Zhihao Xia, Patrick Sullivan, Ayan Chakrabarti", "link": "https://arxiv.org/abs/1906.05739", "summary": "Beyond depth estimation from a single image, the monocular cue is useful in a\nbroader range of depth inference applications and settings---such as when one\ncan leverage other available depth cues for improved accuracy. Currently,\ndifferent applications, with different inference tasks and combinations of\ndepth cues, are solved via different specialized networks---trained separately\nfor each application. Instead, we propose a versatile task-agnostic monocular\nmodel that outputs a probability distribution over scene depth given an input\ncolor image, as a sample approximation of outputs from a patch-wise conditional\nVAE. We show that this distributional output can be used to enable a variety of\ninference tasks in different settings, without needing to retrain for each\napplication. Across a diverse set of applications (depth completion, user\nguided estimation, etc.), our common model yields results with high\naccuracy---comparable to or surpassing that of state-of-the-art methods\ndependent on application-specific networks."}, {"title": "Neural Cages for Detail-Preserving 3D Deformations", "authors": "Wang Yifan, Noam Aigerman, Vladimir G. Kim, Siddhartha Chaudhuri, Olga Sorkine-Hornung", "link": "https://arxiv.org/abs/1912.06395", "summary": "We propose a novel learnable representation for detail-preserving shape\ndeformation. The goal of our method is to warp a source shape to match the\ngeneral structure of a target shape, while preserving the surface details of\nthe source. Our method extends a traditional cage-based deformation technique,\nwhere the source shape is enclosed by a coarse control mesh termed \\emph{cage},\nand translations prescribed on the cage vertices are interpolated to any point\non the source mesh via special weight functions. The use of this sparse cage\nscaffolding enables preserving surface details regardless of the shape's\nintricacy and topology. Our key contribution is a novel neural network\narchitecture for predicting deformations by controlling the cage. We\nincorporate a differentiable cage-based deformation module in our architecture,\nand train our network end-to-end. Our method can be trained with common\ncollections of 3D models in an unsupervised fashion, without any cage-specific\nannotations. We demonstrate the utility of our method for synthesizing shape\nvariations and deformation transfer."}, {"title": "PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization", "authors": "Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo"}, {"title": "A Lighting-Invariant Point Processor for Shading", "authors": "Kathryn Heal, Jialiang Wang, Steven J. Gortler, Todd Zickler"}, {"title": "ActiveMoCap: Optimized Viewpoint Selection for Active Human Motion Capture", "authors": "Sena Kiciroglu, Helge Rhodin, Sudipta N. Sinha, Mathieu Salzmann, Pascal Fua", "link": "", "summary": ""}, {"title": "Peek-a-Boo: Occlusion Reasoning in Indoor Scenes With Plane Representations", "authors": "Ziyu Jiang, Buyu Liu, Samuel Schulter, Zhangyang Wang, Manmohan Chandraker"}, {"title": "Multi-Modal Domain Adaptation for Fine-Grained Action Recognition", "authors": "Jonathan Munro, Dima Damen", "link": "https://arxiv.org/abs/2001.09691", "summary": "Fine-grained action recognition datasets exhibit environmental bias, where\nmultiple video sequences are captured from a limited number of environments.\nTraining a model in one environment and deploying in another results in a drop\nin performance due to an unavoidable domain shift. Unsupervised Domain\nAdaptation (UDA) approaches have frequently utilised adversarial training\nbetween the source and target domains. However, these approaches have not\nexplored the multi-modal nature of video within each domain. In this work we\nexploit the correspondence of modalities as a self-supervised alignment\napproach for UDA in addition to adversarial alignment.\n  We test our approach on three kitchens from our large-scale dataset,\nEPIC-Kitchens, using two modalities commonly employed for action recognition:\nRGB and Optical Flow. We show that multi-modal self-supervision alone improves\nthe performance over source-only training by 2.4% on average. We then combine\nadversarial training with multi-modal self-supervision, showing that our\napproach outperforms other UDA methods by 3%."}, {"title": "Evolving Losses for Unsupervised Video Representation Learning", "authors": "AJ Piergiovanni, Anelia Angelova, Michael S. Ryoo", "link": "https://arxiv.org/abs/2002.12177", "summary": "We present a new method to learn video representations from large-scale\nunlabeled video data. Ideally, this representation will be generic and\ntransferable, directly usable for new tasks such as action recognition and zero\nor few-shot learning. We formulate unsupervised representation learning as a\nmulti-modal, multi-task learning problem, where the representations are shared\nacross different modalities via distillation. Further, we introduce the concept\nof loss function evolution by using an evolutionary search algorithm to\nautomatically find optimal combination of loss functions capturing many\n(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\nrepresentation evaluation metric using distribution matching to a large\nunlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\nconstraint, which is not guided by any labeling, produces similar results to\nweakly-supervised, task-specific ones. The proposed unsupervised representation\nlearning results in a single RGB network and outperforms previous methods.\nNotably, it is also more effective than several label-based methods (e.g.,\nImageNet), with the exception of large, fully labeled video datasets."}, {"title": "Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition", "authors": "Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, Wanli Ouyang", "link": "https://arxiv.org/abs/2003.14111", "summary": "Spatial-temporal graphs have been widely used by skeleton-based action\nrecognition algorithms to model human action dynamics. To capture robust\nmovement patterns from these graphs, long-range and multi-scale context\naggregation and spatial-temporal dependency modeling are critical aspects of a\npowerful feature extractor. However, existing methods have limitations in\nachieving (1) unbiased long-range joint relationship modeling under multi-scale\noperators and (2) unobstructed cross-spacetime information flow for capturing\ncomplex spatial-temporal dependencies. In this work, we present (1) a simple\nmethod to disentangle multi-scale graph convolutions and (2) a unified\nspatial-temporal graph convolutional operator named G3D. The proposed\nmulti-scale aggregation scheme disentangles the importance of nodes in\ndifferent neighborhoods for effective long-range modeling. The proposed G3D\nmodule leverages dense cross-spacetime edges as skip connections for direct\ninformation propagation across the spatial-temporal graph. By coupling these\nproposals, we develop a powerful feature extractor named MS-G3D based on which\nour model outperforms previous state-of-the-art methods on three large-scale\ndatasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400."}, {"title": "A Multigrid Method for Efficiently Training Video Models", "authors": "Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, Philipp Kr\u00e4henb\u00fchl", "link": "https://arxiv.org/abs/1912.00998", "summary": "Training competitive deep video models is an order of magnitude slower than\ntraining their counterpart image models. Slow training causes long research\ncycles, which hinders progress in video understanding research. Following\nstandard practice for training image models, video model training assumes a\nfixed mini-batch shape: a specific number of clips, frames, and spatial size.\nHowever, what is the optimal shape? High resolution models perform well, but\ntrain slowly. Low resolution models train faster, but they are inaccurate.\nInspired by multigrid methods in numerical optimization, we propose to use\nvariable mini-batch shapes with different spatial-temporal resolutions that are\nvaried according to a schedule. The different shapes arise from resampling the\ntraining data on multiple sampling grids. Training is accelerated by scaling up\nthe mini-batch size and learning rate when shrinking the other dimensions. We\nempirically demonstrate a general and robust grid schedule that yields a\nsignificant out-of-the-box training speedup without a loss in accuracy for\ndifferent models (I3D, non-local, SlowFast), datasets (Kinetics,\nSomething-Something, Charades), and training settings (with and without\npre-training, 128 GPUs or 1 GPU). As an illustrative example, the proposed\nmultigrid method trains a ResNet-50 SlowFast network 4.5x faster (wall-clock\ntime, same hardware) while also improving accuracy (+0.8% absolute) on\nKinetics-400 compared to the baseline training method."}, {"title": "Ego-Topo: Environment Affordances From Egocentric Video", "authors": "Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, Kristen Grauman", "link": "https://arxiv.org/abs/2001.04583", "summary": "First-person video naturally brings the use of a physical environment to the\nforefront, since it shows the camera wearer interacting fluidly in a space\nbased on his intentions. However, current methods largely separate the observed\nactions from the persistent space itself. We introduce a model for environment\naffordances that is learned directly from egocentric video. The main idea is to\ngain a human-centric model of a physical space (such as a kitchen) that\ncaptures (1) the primary spatial zones of interaction and (2) the likely\nactivities they support. Our approach decomposes a space into a topological map\nderived from first-person activity, organizing an ego-video into a series of\nvisits to the different zones. Further, we show how to link zones across\nmultiple related environments (e.g., from videos of multiple kitchens) to\nobtain a consolidated representation of environment functionality. On\nEPIC-Kitchens and EGTEA+, we demonstrate our approach for learning scene\naffordances and anticipating future actions in long-form video."}, {"title": "Generative Hybrid Representations for Activity Forecasting With No-Regret Learning", "authors": "Jiaqi Guan, Ye Yuan, Kris M. Kitani, Nicholas Rhinehart", "link": "https://arxiv.org/abs/1904.06250", "summary": "Automatically reasoning about future human behaviors is a difficult problem\nbut has significant practical applications to assistive systems. Part of this\ndifficulty stems from learning systems' inability to represent all kinds of\nbehaviors. Some behaviors, such as motion, are best described with continuous\nrepresentations, whereas others, such as picking up a cup, are best described\nwith discrete representations. Furthermore, human behavior is generally not\nfixed: people can change their habits and routines. This suggests these systems\nmust be able to learn and adapt continuously. In this work, we develop an\nefficient deep generative model to jointly forecast a person's future discrete\nactions and continuous motions. On a large-scale egocentric dataset,\nEPIC-KITCHENS, we observe our method generates high-quality and diverse samples\nwhile exhibiting better generalization than related generative models. Finally,\nwe propose a variant to continually learn our model from streaming data,\nobserve its practical effectiveness, and theoretically justify its learning\nefficiency."}, {"title": "Skeleton-Based Action Recognition With Shift Graph Convolutional Network", "authors": "Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Cheng, Hanqing Lu", "link": "", "summary": ""}, {"title": "Predicting Goal-Directed Human Attention Using Inverse Reinforcement Learning", "authors": "Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory Zelinsky, Dimitris Samaras, Minh Hoai", "link": "https://arxiv.org/abs/2001.11921", "summary": "Understanding how goal states control behavior is a question ripe for\ninterrogation by new methods from machine learning. These methods require large\nand labeled datasets to train models. To annotate a large-scale image dataset\nwith observed search fixations, we collected 16,184 fixations from people\nsearching for either microwaves or clocks in a dataset of 4,366 images\n(MS-COCO). We then used this behaviorally-annotated dataset and the machine\nlearning method of Inverse-Reinforcement Learning (IRL) to learn\ntarget-specific reward functions and policies for these two target goals.\nFinally, we used these learned policies to predict the fixations of 60 new\nbehavioral searchers (clock = 30, microwave = 30) in a disjoint test dataset of\nkitchen scenes depicting both a microwave and a clock (thus controlling for\ndifferences in low-level image contrast). We found that the IRL model predicted\nbehavioral search efficiency and fixation-density maps using multiple metrics.\nMoreover, reward maps from the IRL model revealed target-specific patterns that\nsuggest, not just attention guidance by target features, but also guidance by\nscene context (e.g., fixations along walls in the search of clocks). Using\nmachine learning and the psychologically-meaningful principle of reward, it is\npossible to learn the visual features used in goal-directed attention control."}, {"title": "X3D: Expanding Architectures for Efficient Video Recognition", "authors": "Christoph Feichtenhofer", "link": "https://arxiv.org/abs/2004.04730", "summary": "This paper presents X3D, a family of efficient video networks that\nprogressively expand a tiny 2D image classification architecture along multiple\nnetwork axes, in space, time, width and depth. Inspired by feature selection\nmethods in machine learning, a simple stepwise network expansion approach is\nemployed that expands a single axis in each step, such that good accuracy to\ncomplexity trade-off is achieved. To expand X3D to a specific target\ncomplexity, we perform progressive forward expansion followed by backward\ncontraction. X3D achieves state-of-the-art performance while requiring 4.8x and\n5.5x fewer multiply-adds and parameters for similar accuracy as previous work.\nOur most surprising finding is that networks with high spatiotemporal\nresolution can perform well, while being extremely light in terms of network\nwidth and parameters. We report competitive accuracy at unprecedented\nefficiency on video classification and detection benchmarks. Code will be\navailable at: https://github.com/facebookresearch/SlowFast"}, {"title": "Dynamic Multiscale Graph Neural Networks for 3D Skeleton Based Human Motion Prediction", "authors": "Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, Qi Tian", "link": "https://arxiv.org/abs/2003.08802", "summary": "We propose novel dynamic multiscale graph neural networks (DMGNN) to predict\n3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale\ngraph to comprehensively model the internal relations of a human body for\nmotion feature learning. This multiscale graph is adaptive during training and\ndynamic across network layers. Based on this graph, we propose a multiscale\ngraph computational unit (MGCU) to extract features at individual scales and\nfuse features across scales. The entire model is action-category-agnostic and\nfollows an encoder-decoder framework. The encoder consists of a sequence of\nMGCUs to learn motion features. The decoder uses a proposed graph-based gate\nrecurrent unit to generate future poses. Extensive experiments show that the\nproposed DMGNN outperforms state-of-the-art methods in both short and long-term\npredictions on the datasets of Human 3.6M and CMU Mocap. We further investigate\nthe learned multiscale graphs for the interpretability. The codes could be\ndownloaded from https://github.com/limaosen0/DMGNN."}, {"title": "Use the Force, Luke! Learning to Predict Physical Forces by Simulating Effects", "authors": "Kiana Ehsani, Shubham Tulsiani, Saurabh Gupta, Ali Farhadi, Abhinav Gupta", "link": "https://arxiv.org/abs/2003.12045", "summary": "When we humans look at a video of human-object interaction, we can not only\ninfer what is happening but we can even extract actionable information and\nimitate those interactions. On the other hand, current recognition or geometric\napproaches lack the physicality of action representation. In this paper, we\ntake a step towards a more physical understanding of actions. We address the\nproblem of inferring contact points and the physical forces from videos of\nhumans interacting with objects. One of the main challenges in tackling this\nproblem is obtaining ground-truth labels for forces. We sidestep this problem\nby instead using a physics simulator for supervision. Specifically, we use a\nsimulator to predict effects and enforce that estimated forces must lead to the\nsame effect as depicted in the video. Our quantitative and qualitative results\nshow that (a) we can predict meaningful forces from videos whose effects lead\nto accurate imitation of the motions observed, (b) by jointly optimizing for\ncontact point and force prediction, we can improve the performance on both\ntasks in comparison to independent training, and (c) we can learn a\nrepresentation from this model that generalizes to novel objects using few shot\nexamples."}, {"title": "DaST: Data-Free Substitute Training for Adversarial Attacks", "authors": "Mingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, Ce Zhu", "link": "https://arxiv.org/abs/2003.12703", "summary": "Machine learning models are vulnerable to adversarial examples. For the\nblack-box setting, current substitute attacks need pre-trained models to\ngenerate adversarial examples. However, pre-trained models are hard to obtain\nin real-world tasks. In this paper, we propose a data-free substitute training\nmethod (DaST) to obtain substitute models for adversarial black-box attacks\nwithout the requirement of any real data. To achieve this, DaST utilizes\nspecially designed generative adversarial networks (GANs) to train the\nsubstitute models. In particular, we design a multi-branch architecture and\nlabel-control loss for the generative model to deal with the uneven\ndistribution of synthetic samples. The substitute model is then trained by the\nsynthetic samples generated by the generative model, which are labeled by the\nattacked model subsequently. The experiments demonstrate the substitute models\nproduced by DaST can achieve competitive performance compared with the baseline\nmodels which are trained by the same train set with attacked models.\nAdditionally, to evaluate the practicability of the proposed method on the\nreal-world task, we attack an online machine learning model on the Microsoft\nAzure platform. The remote model misclassifies 98.35% of the adversarial\nexamples crafted by our method. To the best of our knowledge, we are the first\nto train a substitute model for adversarial attacks without any real data."}, {"title": "Towards Verifying Robustness of Neural Networks Against A Family of Semantic Perturbations", "authors": "Jeet Mohapatra, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, Luca Daniel", "link": "https://arxiv.org/abs/1912.09533", "summary": "Verifying robustness of neural networks given a specified threat model is a\nfundamental yet challenging task. While current verification methods mainly\nfocus on the L_p-norm-ball threat model of the input instances, robustness\nverification against semantic adversarial attacks inducing large L_p-norm\nperturbations such as color shifting and lighting adjustment are beyond their\ncapacity. To bridge this gap, we propose Semantify-NN, a model-agnostic and\ngeneric robustness verification approach against semantic perturbations for\nneural networks. By simply inserting our proposed semantic perturbation layers\n(SP-layers) to the input layer of any given model, Semantify-NN is\nmodel-agnostic, and any $L_p$-norm-ball based verification tools can be used to\nverify the model robustness against semantic perturbations. We illustrate the\nprinciples of designing the SP-layers and provide examples including semantic\nperturbations to image classification in the space of hue, saturation,\nlightness, brightness, contrast and rotation, respectively. Experimental\nresults on various network architectures and different datasets demonstrate the\nsuperior verification performance of Semantify-NN over L_p-norm-based\nverification frameworks that naively convert semantic perturbation to L_p-norm.\nTo the best of our knowledge, Semantify-NN is the first framework to support\nrobustness verification against a wide range of semantic perturbations."}, {"title": "The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks", "authors": "Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, Dawn Song", "link": "https://arxiv.org/abs/1911.07135", "summary": "This paper studies model-inversion attacks, in which the access to a model is\nabused to infer information about the training data. Since its first\nintroduction, such attacks have raised serious concerns given that training\ndata usually contain privacy-sensitive information. Thus far, successful\nmodel-inversion attacks have only been demonstrated on simple models, such as\nlinear regression and logistic regression. Previous attempts to invert neural\nnetworks, even the ones with simple architectures, have failed to produce\nconvincing results. We present a novel attack method, termed the generative\nmodel-inversion attack, which can invert deep neural networks with high success\nrates. Rather than reconstructing private training data from scratch, we\nleverage partial public information, which can be very generic, to learn a\ndistributional prior via generative adversarial networks (GANs) and use it to\nguide the inversion process. Moreover, we theoretically prove that a model's\npredictive power and its vulnerability to inversion attacks are indeed two\nsides of the same coin---highly predictive models are able to establish a\nstrong correlation between features and labels, which coincides exactly with\nwhat an adversary exploits to mount the attacks. Our extensive experiments\ndemonstrate that the proposed attack improves identification accuracy over the\nexisting work by about 75\\% for reconstructing face images from a\nstate-of-the-art face recognition classifier. We also show that differential\nprivacy, in its canonical form, is of little avail to defend against our\nattacks."}, {"title": "A Self-supervised Approach for Adversarial Robustness", "authors": "Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih Porikli", "link": "", "summary": ""}, {"title": "Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization", "authors": "Saehyung Lee, Hyungyu Lee, Sungroh Yoon", "link": "https://arxiv.org/abs/2003.02484", "summary": "Adversarial examples cause neural networks to produce incorrect outputs with\nhigh confidence. Although adversarial training is one of the most effective\nforms of defense against adversarial examples, unfortunately, a large gap\nexists between test accuracy and training accuracy in adversarial training. In\nthis paper, we identify Adversarial Feature Overfitting (AFO), which may cause\npoor adversarially robust generalization, and we show that adversarial training\ncan overshoot the optimal point in terms of robust generalization, leading to\nAFO in our simple Gaussian model. Considering these theoretical results, we\npresent soft labeling as a solution to the AFO problem. Furthermore, we propose\nAdversarial Vertex mixup (AVmixup), a soft-labeled data augmentation approach\nfor improving adversarially robust generalization. We complement our\ntheoretical analysis with experiments on CIFAR10, CIFAR100, SVHN, and Tiny\nImageNet, and show that AVmixup significantly improves the robust\ngeneralization performance and that it reduces the trade-off between standard\naccuracy and adversarial robustness."}, {"title": "How Does Noise Help Robustness? Explanation and Exploration under the Neural SDE Framework", "authors": "Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh", "link": "", "summary": ""}, {"title": "Unpaired Image Super-Resolution Using Pseudo-Supervision", "authors": "Shunta Maeda", "link": "https://arxiv.org/abs/2002.11397", "summary": "In most studies on learning-based image super-resolution (SR), the paired\ntraining dataset is created by downscaling high-resolution (HR) images with a\npredetermined operation (e.g., bicubic). However, these methods fail to\nsuper-resolve real-world low-resolution (LR) images, for which the degradation\nprocess is much more complicated and unknown. In this paper, we propose an\nunpaired SR method using a generative adversarial network that does not require\na paired/aligned training dataset. Our network consists of an unpaired\nkernel/noise correction network and a pseudo-paired SR network. The correction\nnetwork removes noise and adjusts the kernel of the inputted LR image; then,\nthe corrected clean LR image is upscaled by the SR network. In the training\nphase, the correction network also produces a pseudo-clean LR image from the\ninputted HR image, and then a mapping from the pseudo-clean LR image to the\ninputted HR image is learned by the SR network in a paired manner. Because our\nSR network is independent of the correction network, well-studied existing\nnetwork architectures and pixel-wise loss functions can be integrated with the\nproposed framework. Experiments on diverse datasets show that the proposed\nmethod is superior to existing solutions to the unpaired SR problem."}, {"title": "Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs", "authors": "Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, Heiko Hoffmann", "link": "https://arxiv.org/abs/1906.10842", "summary": "The unprecedented success of deep neural networks in many applications has\nmade these networks a prime target for adversarial exploitation. In this paper,\nwe introduce a benchmark technique for detecting backdoor attacks (aka Trojan\nattacks) on deep convolutional neural networks (CNNs). We introduce the concept\nof Universal Litmus Patterns (ULPs), which enable one to reveal backdoor\nattacks by feeding these universal patterns to the network and analyzing the\noutput (i.e., classifying the network as `clean' or `corrupted'). This\ndetection is fast because it requires only a few forward passes through a CNN.\nWe demonstrate the effectiveness of ULPs for detecting backdoor attacks on\nthousands of networks with different architectures trained on four benchmark\ndatasets, namely the German Traffic Sign Recognition Benchmark (GTSRB), MNIST,\nCIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper can\nbe found here https://umbcvision.github.io/Universal-Litmus-Patterns/."}, {"title": "Robustness Guarantees for Deep Neural Networks on Videos", "authors": "Min Wu, Marta Kwiatkowska", "link": "https://arxiv.org/abs/1907.00098", "summary": "The widespread adoption of deep learning models places demands on their\nrobustness. In this paper, we consider the robustness of deep neural networks\non videos, which comprise both the spatial features of individual frames\nextracted by a convolutional neural network and the temporal dynamics between\nadjacent frames captured by a recurrent neural network. To measure robustness,\nwe study the maximum safe radius problem, which computes the minimum distance\nfrom the optical flow sequence obtained from a given input to that of an\nadversarial example in the neighbourhood of the input. We demonstrate that,\nunder the assumption of Lipschitz continuity, the problem can be approximated\nusing finite optimisation via discretising the optical flow space, and the\napproximation has provable guarantees. We then show that the finite\noptimisation problem can be solved by utilising a two-player turn-based game in\na cooperative setting, where the first player selects the optical flows and the\nsecond player determines the dimensions to be manipulated in the chosen flow.\nWe employ an anytime approach to solve the game, in the sense of approximating\nthe value of the game by monotonically improving its upper and lower bounds. We\nexploit a gradient-based search algorithm to compute the upper bounds, and the\nadmissible A* algorithm to update the lower bounds. Finally, we evaluate our\nframework on the UCF101 video dataset."}, {"title": "Benchmarking Adversarial Robustness on Image Classification", "authors": "Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, Jun Zhu", "link": "", "summary": ""}, {"title": "What It Thinks Is Important Is Important: Robustness Transfers Through Input Gradients", "authors": "Alvin Chan, Yi Tay, Yew-Soon Ong", "link": "https://arxiv.org/abs/1912.05699", "summary": "Adversarial perturbations are imperceptible changes to input pixels that can\nchange the prediction of deep learning models. Learned weights of models robust\nto such perturbations are previously found to be transferable across different\ntasks but this applies only if the model architecture for the source and target\ntasks is the same. Input gradients characterize how small changes at each input\npixel affect the model output. Using only natural images, we show here that\ntraining a student model's input gradients to match those of a robust teacher\nmodel can gain robustness close to a strong baseline that is robustly trained\nfrom scratch. Through experiments in MNIST, CIFAR-10, CIFAR-100 and\nTiny-ImageNet, we show that our proposed method, input gradient adversarial\nmatching, can transfer robustness across different tasks and even across\ndifferent model architectures. This demonstrates that directly targeting the\nsemantics of input gradients is a feasible way towards adversarial robustness."}, {"title": "Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking", "authors": "Hongjun Wang, Guangrun Wang, Ya Li, Dongyu Zhang, Liang Lin", "link": "https://arxiv.org/abs/2004.04199", "summary": "The success of DNNs has driven the extensive applications of person\nre-identification (ReID) into a new era. However, whether ReID inherits the\nvulnerability of DNNs remains unexplored. To examine the robustness of ReID\nsystems is rather important because the insecurity of ReID systems may cause\nsevere losses, e.g., the criminals may use the adversarial perturbations to\ncheat the CCTV systems. In this work, we examine the insecurity of current\nbest-performing ReID models by proposing a learning-to-mis-rank formulation to\nperturb the ranking of the system output. As the cross-dataset transferability\nis crucial in the ReID domain, we also perform a back-box attack by developing\na novel multi-stage network architecture that pyramids the features of\ndifferent levels to extract general and transferable features for the\nadversarial perturbations. Our method can control the number of malicious\npixels by using differentiable multi-shot sampling. To guarantee the\ninconspicuousness of the attack, we also propose a new perception loss to\nachieve better visual quality. Extensive experiments on four of the largest\nReID benchmarks (i.e., Market1501 [45], CUHK03 [18], DukeMTMC [33], and MSMT17\n[40]) not only show the effectiveness of our method, but also provides\ndirections of the future improvement in the robustness of ReID systems. For\nexample, the accuracy of one of the best-performing ReID systems drops sharply\nfrom 91.8% to 1.4% after being attacked by our method. Some attack results are\nshown in Fig. 1. The code is available at\nhttps://github.com/whj363636/Adversarial-attack-on-Person-ReID-With-Deep-Mis-Ranking."}, {"title": "Video Modeling With Correlation Networks", "authors": "Heng Wang, Du Tran, Lorenzo Torresani, Matt Feiszli", "link": "https://arxiv.org/abs/1906.03349", "summary": "Motion is a salient cue to recognize actions in video. Modern action\nrecognition models leverage motion information either explicitly by using\noptical flow as input or implicitly by means of 3D convolutional filters that\nsimultaneously capture appearance and motion information. This paper proposes\nan alternative approach based on a learnable correlation operator that can be\nused to establish frame-toframe matches over convolutional feature maps in the\ndifferent layers of the network. The proposed architecture enables the fusion\nof this explicit temporal matching information with traditional appearance cues\ncaptured by 2D convolution. Our correlation network compares favorably with\nwidely-used 3D CNNs for video modeling, and achieves competitive results over\nthe prominent two-stream network while being much faster to train. We\nempirically demonstrate that correlation networks produce strong results on a\nvariety of video datasets, and outperform the state of the art on four popular\nbenchmarks for action recognition: Kinetics, Something-Something, Diving48 and\nSports1M."}, {"title": "Projection & Probability-Driven Black-Box Attack", "authors": "Jie Li, Rongrong Ji, Hong Liu, Jianzhuang Liu, Bineng Zhong, Cheng Deng, Qi Tian", "link": "https://arxiv.org/abs/2005.03837", "summary": "Generating adversarial examples in a black-box setting retains a significant\nchallenge with vast practical application prospects. In particular, existing\nblack-box attacks suffer from the need for excessive queries, as it is\nnon-trivial to find an appropriate direction to optimize in the\nhigh-dimensional space. In this paper, we propose Projection &\nProbability-driven Black-box Attack (PPBA) to tackle this problem by reducing\nthe solution space and providing better optimization. For reducing the solution\nspace, we first model the adversarial perturbation optimization problem as a\nprocess of recovering frequency-sparse perturbations with compressed sensing,\nunder the setting that random noise in the low-frequency space is more likely\nto be adversarial. We then propose a simple method to construct a low-frequency\nconstrained sensing matrix, which works as a plug-and-play projection matrix to\nreduce the dimensionality. Such a sensing matrix is shown to be flexible enough\nto be integrated into existing methods like NES and Bandits$_{TD}$. For better\noptimization, we perform a random walk with a probability-driven strategy,\nwhich utilizes all queries over the whole progress to make full use of the\nsensing matrix for a less query budget. Extensive experiments show that our\nmethod requires at most 24% fewer queries with a higher attack success rate\ncompared with state-of-the-art approaches. Finally, the attack method is\nevaluated on the real-world online service, i.e., Google Cloud Vision API,\nwhich further demonstrates our practical potentials."}, {"title": "Auxiliary Training: Towards Accurate and Robust Models", "authors": "Linfeng Zhang, Muzhou Yu, Tong Chen, Zuoqiang Shi, Chenglong Bao, Kaisheng Ma", "link": "", "summary": ""}, {"title": "PaStaNet: Toward Human Activity Knowledge Engine", "authors": "Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Shiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, Cewu Lu", "link": "https://arxiv.org/abs/2004.00945", "summary": "Existing image-based activity understanding methods mainly adopt direct\nmapping, i.e. from image to activity concepts, which may encounter performance\nbottleneck since the huge gap. In light of this, we propose a new path: infer\nhuman part states first and then reason out the activities based on part-level\nsemantics. Human Body Part States (PaSta) are fine-grained action semantic\ntokens, e.g. <hand, hold, something>, which can compose the activities and help\nus step toward human activity knowledge engine. To fully utilize the power of\nPaSta, we build a large-scale knowledge base PaStaNet, which contains 7M+ PaSta\nannotations. And two corresponding models are proposed: first, we design a\nmodel named Activity2Vec to extract PaSta features, which aim to be general\nrepresentations for various activities. Second, we use a PaSta-based Reasoning\nmethod to infer activities. Promoted by PaStaNet, our method achieves\nsignificant improvements, e.g. 6.4 and 13.9 mAP on full and one-shot sets of\nHICO in supervised learning, and 3.2 and 4.2 mAP on V-COCO and images-based AVA\nin transfer learning. Code and data are available at http://hake-mvig.cn/."}, {"title": "A Hierarchical Graph Network for 3D Object Detection on Point Clouds", "authors": "Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z. Chen, Jian Wu", "link": "", "summary": ""}, {"title": "Learning Generative Models of Shape Handles", "authors": "Matheus Gadelha, Giorgio Gori, Duygu Ceylan, Radom\u00edr M\u011bch, Nathan Carr, Tamy Boubekeur, Rui Wang, Subhransu Maji", "link": "https://arxiv.org/abs/2004.03028", "summary": "We present a generative model to synthesize 3D shapes as sets of handles --\nlightweight proxies that approximate the original 3D shape -- for applications\nin interactive editing, shape parsing, and building compact 3D representations.\nOur model can generate handle sets with varying cardinality and different types\nof handles (Figure 1). Key to our approach is a deep architecture that predicts\nboth the parameters and existence of shape handles, and a novel similarity\nmeasure that can easily accommodate different types of handles, such as cuboids\nor sphere-meshes. We leverage the recent advances in semantic 3D annotation as\nwell as automatic shape summarizing techniques to supervise our approach. We\nshow that the resulting shape representations are intuitive and achieve\nsuperior quality than previous state-of-the-art. Finally, we demonstrate how\nour method can be used in applications such as interactive shape editing,\ncompletion, and interpolation, leveraging the latent space learned by our model\nto guide these tasks. Project page: http://mgadelha.me/shapehandles."}, {"title": "One Man\u2019s Trash Is Another Man\u2019s Treasure: Resisting Adversarial Examples by Adversarial Examples", "authors": "Chang Xiao, Changxi Zheng", "link": "https://arxiv.org/abs/1911.11219", "summary": "Modern image classification systems are often built on deep neural networks,\nwhich suffer from adversarial examples--images with deliberately crafted,\nimperceptible noise to mislead the network's classification. To defend against\nadversarial examples, a plausible idea is to obfuscate the network's gradient\nwith respect to the input image. This general idea has inspired a long line of\ndefense methods. Yet, almost all of them have proven vulnerable. We revisit\nthis seemingly flawed idea from a radically different perspective. We embrace\nthe omnipresence of adversarial examples and the numerical procedure of\ncrafting them, and turn this harmful attacking process into a useful defense\nmechanism. Our defense method is conceptually simple: before feeding an input\nimage for classification, transform it by finding an adversarial example on a\npre-trained external model. We evaluate our method against a wide range of\npossible attacks. On both CIFAR-10 and Tiny ImageNet datasets, our method is\nsignificantly more robust than state-of-the-art methods. Particularly, in\ncomparison to adversarial training, our method offers lower training cost as\nwell as stronger robustness."}, {"title": "Toward a Universal Model for Shape From Texture", "authors": "Dor Verbin, Todd Zickler"}, {"title": "HybridPose: 6D Object Pose Estimation Under Hybrid Representations", "authors": "Chen Song, Jiaru Song, Qixing Huang", "link": "https://arxiv.org/abs/2001.01869", "summary": "We introduce HybridPose, a novel 6D object pose estimation approach.\nHybridPose utilizes a hybrid intermediate representation to express different\ngeometric information in the input image, including keypoints, edge vectors,\nand symmetry correspondences. Compared to a unitary representation, our hybrid\nrepresentation allows pose regression to exploit more and diverse features when\none type of predicted representation is inaccurate (e.g., because of\nocclusion). Different intermediate representations used by HybridPose can all\nbe predicted by the same simple neural network, and outliers in predicted\nintermediate representations are filtered by a robust regression module.\nCompared to state-of-the-art pose estimation approaches, HybridPose is\ncomparable in running time and is significantly more accurate. For example, on\nOcclusion Linemod dataset, our method achieves a prediction speed of 30 fps\nwith a mean ADD(-S) accuracy of 79.2%, representing a 67.4% improvement from\nthe current state-of-the-art approach. The implementation of HybridPose is\navailable at https://github.com/chensong1995/HybridPose."}, {"title": "Boundary-Aware 3D Building Reconstruction From a Single Overhead Image", "authors": "Jisan Mahmud, True Price, Akash Bapat, Jan-Michael Frahm"}, {"title": "Articulation-Aware Canonical Surface Mapping", "authors": "Nilesh Kulkarni, Abhinav Gupta, David F. Fouhey, Shubham Tulsiani", "link": "https://arxiv.org/abs/2004.00614", "summary": "We tackle the tasks of: 1) predicting a Canonical Surface Mapping (CSM) that\nindicates the mapping from 2D pixels to corresponding points on a canonical\ntemplate shape, and 2) inferring the articulation and pose of the template\ncorresponding to the input image. While previous approaches rely on keypoint\nsupervision for learning, we present an approach that can learn without such\nannotations. Our key insight is that these tasks are geometrically related, and\nwe can obtain supervisory signal via enforcing consistency among the\npredictions. We present results across a diverse set of animal object\ncategories, showing that our method can learn articulation and CSM prediction\nfrom image collections using only foreground mask labels for training. We\nempirically show that allowing articulation helps learn more accurate CSM\nprediction, and that enforcing the consistency with predicted CSM is similarly\ncritical for learning meaningful articulation."}, {"title": "BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion", "authors": "Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, Yi-Hsuan Tsai"}, {"title": "Transformation GAN for Unsupervised Image Synthesis and Representation Learning", "authors": "Jiayu Wang, Wengang Zhou, Guo-Jun Qi, Zhongqian Fu, Qi Tian, Houqiang Li"}, {"title": "PPDM: Parallel Point Detection and Matching for Real-Time Human-Object Interaction Detection", "authors": "Yue Liao, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, Jiashi Feng", "link": "https://arxiv.org/abs/1912.12898", "summary": "We propose a single-stage Human-Object Interaction (HOI) detection method\nthat has outperformed all existing methods on HICO-DET dataset at 37 fps on a\nsingle Titan XP GPU. It is the first real-time HOI detection method.\nConventional HOI detection methods are composed of two stages, i.e.,\nhuman-object proposals generation, and proposals classification. Their\neffectiveness and efficiency are limited by the sequential and separate\narchitecture. In this paper, we propose a Parallel Point Detection and Matching\n(PPDM) HOI detection framework. In PPDM, an HOI is defined as a point triplet <\nhuman point, interaction point, object point>. Human and object points are the\ncenter of the detection boxes, and the interaction point is the midpoint of the\nhuman and object points. PPDM contains two parallel branches, namely point\ndetection branch and point matching branch. The point detection branch predicts\nthree points. Simultaneously, the point matching branch predicts two\ndisplacements from the interaction point to its corresponding human and object\npoints. The human point and the object point originated from the same\ninteraction point are considered as matched pairs. In our novel parallel\narchitecture, the interaction points implicitly provide context and\nregularization for human and object detection. The isolated detection boxes are\nunlikely to form meaning HOI triplets are suppressed, which increases the\nprecision of HOI detection. Moreover, the matching between human and object\ndetection boxes is only applied around limited numbers of filtered candidate\ninteraction points, which saves much computational cost. Additionally, we build\na new application-oriented database named HOI-A, which severs as a good\nsupplement to the existing datasets. The source code and the dataset will be\nmade publicly available to facilitate the development of HOI detection."}, {"title": "Height and Uprightness Invariance for 3D Prediction From a Single View", "authors": "Manel Baradad, Antonio Torralba"}, {"title": "SCT: Set Constrained Temporal Transformer for Set Supervised Action Segmentation", "authors": "Mohsen Fayyaz, J\u00fcrgen Gall", "link": "https://arxiv.org/abs/2003.14266", "summary": "Temporal action segmentation is a topic of increasing interest, however,\nannotating each frame in a video is cumbersome and costly. Weakly supervised\napproaches therefore aim at learning temporal action segmentation from videos\nthat are only weakly labeled. In this work, we assume that for each training\nvideo only the list of actions is given that occur in the video, but not when,\nhow often, and in which order they occur. In order to address this task, we\npropose an approach that can be trained end-to-end on such data. The approach\ndivides the video into smaller temporal regions and predicts for each region\nthe action label and its length. In addition, the network estimates the action\nlabels for each frame. By measuring how consistent the frame-wise predictions\nare with respect to the temporal regions and the annotated action labels, the\nnetwork learns to divide a video into class-consistent regions. We evaluate our\napproach on three datasets where the approach achieves state-of-the-art\nresults."}, {"title": "3DV: 3D Dynamic Voxel for Action Recognition in Depth Video", "authors": "Yancheng Wang, Yang Xiao, Fu Xiong, Wenxiang Jiang, Zhiguo Cao, Joey Tianyi Zhou, Junsong Yuan", "link": "https://arxiv.org/abs/2005.05501", "summary": "To facilitate depth-based 3D action recognition, 3D dynamic voxel (3DV) is\nproposed as a novel 3D motion representation. With 3D space voxelization, the\nkey idea of 3DV is to encode 3D motion information within depth video into a\nregular voxel set (i.e., 3DV) compactly, via temporal rank pooling. Each\navailable 3DV voxel intrinsically involves 3D spatial and motion feature\njointly. 3DV is then abstracted as a point set and input into PointNet++ for 3D\naction recognition, in the end-to-end learning way. The intuition for\ntransferring 3DV into the point set form is that, PointNet++ is lightweight and\neffective for deep feature learning towards point set. Since 3DV may lose\nappearance clue, a multi-stream 3D action recognition manner is also proposed\nto learn motion and appearance feature jointly. To extract richer temporal\norder information of actions, we also divide the depth video into temporal\nsplits and encode this procedure in 3DV integrally. The extensive experiments\non 4 well-established benchmark datasets demonstrate the superiority of our\nproposition. Impressively, we acquire the accuracy of 82.4% and 93.5% on NTU\nRGB+D 120 [13] with the cross-subject and crosssetup test setting respectively.\n3DV's code is available at https://github.com/3huo/3DV-Action."}, {"title": "Adaptive Interaction Modeling via Graph Operations Search", "authors": "Haoxin Li, Wei-Shi Zheng, Yu Tao, Haifeng Hu, Jian-Huang Lai", "link": "https://arxiv.org/abs/2005.02113", "summary": "Interaction modeling is important for video action analysis. Recently,\nseveral works design specific structures to model interactions in videos.\nHowever, their structures are manually designed and non-adaptive, which require\nstructures design efforts and more importantly could not model interactions\nadaptively. In this paper, we automate the process of structures design to\nlearn adaptive structures for interaction modeling. We propose to search the\nnetwork structures with differentiable architecture search mechanism, which\nlearns to construct adaptive structures for different videos to facilitate\nadaptive interaction modeling. To this end, we first design the search space\nwith several basic graph operations that explicitly capture different relations\nin videos. We experimentally demonstrate that our architecture search framework\nlearns to construct adaptive interaction modeling structures, which provides\nmore understanding about the relations between the structures and some\ninteraction characteristics, and also releases the requirement of structures\ndesign efforts. Additionally, we show that the designed basic graph operations\nin the search space are able to model different interactions in videos. The\nexperiments on two interaction datasets show that our method achieves\ncompetitive performance with state-of-the-arts."}, {"title": "Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction", "authors": "Yuan Yao, Nico Schertler, Enrique Rosales, Helge Rhodin, Leonid Sigal, Alla Sheffer", "link": "https://arxiv.org/abs/1912.10589", "summary": "Reconstruction of a 3D shape from a single 2D image is a classical computer\nvision problem, whose difficulty stems from the inherent ambiguity of\nrecovering occluded or only partially observed surfaces. Recent methods address\nthis challenge through the use of largely unstructured neural networks that\neffectively distill conditional mapping and priors over 3D shape. In this work,\nwe induce structure and geometric constraints by leveraging three core\nobservations: (1) the surface of most everyday objects is often almost entirely\nexposed from pairs of typical opposite views; (2) everyday objects often\nexhibit global reflective symmetries which can be accurately predicted from\nsingle views; (3) opposite orthographic views of a 3D shape share consistent\nsilhouettes. Following these observations, we first predict orthographic 2.5D\nvisible surface maps (depth, normal and silhouette) from perspective 2D images,\nand detect global reflective symmetries in this data; second, we predict the\nback facing depth and normal maps using as input the front maps and, when\navailable, the symmetric reflections of these maps; and finally, we reconstruct\na 3D mesh from the union of these maps using a surface reconstruction method\nbest suited for this data. Our experiments demonstrate that our framework\noutperforms state-of-the art approaches for 3D shape reconstructions from 2D\nand 2.5D data in terms of input fidelity and details preservation.\nSpecifically, we achieve 12% better performance on average in ShapeNet\nbenchmark dataset, and up to 19% for certain classes of objects (e.g., chairs\nand vessels)."}, {"title": "SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth Estimation", "authors": "Lijun Wang, Jianming Zhang, Oliver Wang, Zhe Lin, Huchuan Lu"}, {"title": "Single-View View Synthesis With Multiplane Images", "authors": "Richard Tucker, Noah Snavely", "link": "http://arxiv.org/abs/2004.11364", "summary": "A recent strand of work in view synthesis uses deep learning to generate\nmultiplane images (a camera-centric, layered 3D representation) given two or\nmore input images at known viewpoints. We apply this representation to\nsingle-view view synthesis, a problem which is more challenging but has\npotentially much wider application. Our method learns to predict a multiplane\nimage directly from a single image input, and we introduce scale-invariant view\nsynthesis for supervision, enabling us to train on online video. We show this\napproach is applicable to several different datasets, that it additionally\ngenerates reasonable depth maps, and that it learns to fill in content behind\nthe edges of foreground objects in background layers.\n  Project page at https://single-view-mpi.github.io/."}, {"title": "Deep Parametric Shape Predictions Using Distance Fields", "authors": "Dmitriy Smirnov, Matthew Fisher, Vladimir G. Kim, Richard Zhang, Justin Solomon", "link": "https://arxiv.org/abs/1904.08921", "summary": "Many tasks in graphics and vision demand machinery for converting shapes into\nconsistent representations with sparse sets of parameters; these\nrepresentations facilitate rendering, editing, and storage. When the source\ndata is noisy or ambiguous, however, artists and engineers often manually\nconstruct such representations, a tedious and potentially time-consuming\nprocess. While advances in deep learning have been successfully applied to\nnoisy geometric data, the task of generating parametric shapes has so far been\ndifficult for these methods. Hence, we propose a new framework for predicting\nparametric shape primitives using deep learning. We use distance fields to\ntransition between shape parameters like control points and input data on a\npixel grid. We demonstrate efficacy on 2D and 3D tasks, including font\nvectorization and surface abstraction."}, {"title": "Leveraging Photometric Consistency Over Time for Sparsely Supervised Hand-Object Reconstruction", "authors": "Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev, Marc Pollefeys, Cordelia Schmid", "link": "http://arxiv.org/abs/2004.13449", "summary": "Modeling hand-object manipulations is essential for understanding how humans\ninteract with their environment. While of practical importance, estimating the\npose of hands and objects during interactions is challenging due to the large\nmutual occlusions that occur during manipulation. Recent efforts have been\ndirected towards fully-supervised methods that require large amounts of labeled\ntraining samples. Collecting 3D ground-truth data for hand-object interactions,\nhowever, is costly, tedious, and error-prone. To overcome this challenge we\npresent a method to leverage photometric consistency across time when\nannotations are only available for a sparse subset of frames in a video. Our\nmodel is trained end-to-end on color images to jointly reconstruct hands and\nobjects in 3D by inferring their poses. Given our estimated reconstructions, we\ndifferentiably render the optical flow between pairs of adjacent images and use\nit within the network to warp one frame to another. We then apply a\nself-supervised photometric loss that relies on the visual consistency between\nnearby images. We achieve state-of-the-art results on 3D hand-object\nreconstruction benchmarks and demonstrate that our approach allows us to\nimprove the pose estimation accuracy by leveraging information from neighboring\nframes in low-data regimes."}, {"title": "Ensemble Generative Cleaning With Feedback Loops for Defending Adversarial Attacks", "authors": "Jianhe Yuan, Zhihai He", "link": "https://arxiv.org/abs/2004.11273", "summary": "Effective defense of deep neural networks against adversarial attacks remains\na challenging problem, especially under powerful white-box attacks. In this\npaper, we develop a new method called ensemble generative cleaning with\nfeedback loops (EGC-FL) for effective defense of deep neural networks. The\nproposed EGC-FL method is based on two central ideas. First, we introduce a\ntransformed deadzone layer into the defense network, which consists of an\northonormal transform and a deadzone-based activation function, to destroy the\nsophisticated noise pattern of adversarial attacks. Second, by constructing a\ngenerative cleaning network with a feedback loop, we are able to generate an\nensemble of diverse estimations of the original clean image. We then learn a\nnetwork to fuse this set of diverse estimations together to restore the\noriginal image. Our extensive experimental results demonstrate that our\napproach improves the state-of-art by large margins in both white-box and\nblack-box attacks. It significantly improves the classification accuracy for\nwhite-box PGD attacks upon the second best method by more than 29% on the SVHN\ndataset and more than 39% on the challenging CIFAR-10 dataset."}, {"title": "Temporal Pyramid Network for Action Recognition", "authors": "Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, Bolei Zhou", "link": "https://arxiv.org/abs/2004.03548", "summary": "Visual tempo characterizes the dynamics and the temporal scale of an action.\nModeling such visual tempos of different actions facilitates their recognition.\nPrevious works often capture the visual tempo through sampling raw videos at\nmultiple rates and constructing an input-level frame pyramid, which usually\nrequires a costly multi-branch network to handle. In this work we propose a\ngeneric Temporal Pyramid Network (TPN) at the feature-level, which can be\nflexibly integrated into 2D or 3D backbone networks in a plug-and-play manner.\nTwo essential components of TPN, the source of features and the fusion of\nfeatures, form a feature hierarchy for the backbone so that it can capture\naction instances at various tempos. TPN also shows consistent improvements over\nother challenging baselines on several action recognition datasets.\nSpecifically, when equipped with TPN, the 3D ResNet-50 with dense sampling\nobtains a 2% gain on the validation set of Kinetics-400. A further analysis\nalso reveals that TPN gains most of its improvements on action classes that\nhave large variances in their visual tempos, validating the effectiveness of\nTPN."}, {"title": "FaceScape: A Large-Scale High Quality 3D Face Dataset and Detailed Riggable 3D Face Prediction", "authors": "Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, Xun Cao", "link": "http://arxiv.org/abs/2003.13989", "summary": "In this paper, we present a large-scale detailed 3D face dataset, FaceScape,\nand propose a novel algorithm that is able to predict elaborate riggable 3D\nface models from a single image input. FaceScape dataset provides 18,760\ntextured 3D faces, captured from 938 subjects and each with 20 specific\nexpressions. The 3D models contain the pore-level facial geometry that is also\nprocessed to be topologically uniformed. These fine 3D facial models can be\nrepresented as a 3D morphable model for rough shapes and displacement maps for\ndetailed geometry. Taking advantage of the large-scale and high-accuracy\ndataset, a novel algorithm is further proposed to learn the expression-specific\ndynamic details using a deep neural network. The learned relationship serves as\nthe foundation of our 3D face prediction system from a single image input.\nDifferent than the previous methods, our predicted 3D models are riggable with\nhighly detailed geometry under different expressions. The unprecedented dataset\nand code will be released to public for research purpose."}, {"title": "Structure-Guided Ranking Loss for Single Image Depth Prediction", "authors": "Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, Zhiguo Cao"}, {"title": "In Perfect Shape: Certifiably Optimal 3D Shape Reconstruction From 2D Landmarks", "authors": "Heng Yang, Luca Carlone", "link": "https://arxiv.org/abs/1911.11924", "summary": "We study the problem of 3D shape reconstruction from 2D landmarks extracted\nin a single image. We adopt the 3D deformable shape model and formulate the\nreconstruction as a joint optimization of the camera pose and the linear shape\nparameters. Our first contribution is to apply Lasserre's hierarchy of convex\nSums-of-Squares (SOS) relaxations to solve the shape reconstruction problem and\nshow that the SOS relaxation of minimum order 2 empirically solves the original\nnon-convex problem exactly. Our second contribution is to exploit the structure\nof the polynomial in the objective function and find a reduced set of basis\nmonomials for the SOS relaxation that significantly decreases the size of the\nresulting semidefinite program (SDP) without compromising its accuracy. These\ntwo contributions, to the best of our knowledge, lead to the first certifiably\noptimal solver for 3D shape reconstruction, that we name Shape*. Our third\ncontribution is to add an outlier rejection layer to Shape* using a truncated\nleast squares (TLS) robust cost function and leveraging graduated non-convexity\nto solve TLS without initialization. The result is a robust reconstruction\nalgorithm, named Shape#, that tolerates a large amount of outlier measurements.\nWe evaluate the performance of Shape* and Shape# in both simulated and real\nexperiments, showing that Shape* outperforms local optimization and previous\nconvex relaxation techniques, while Shape# achieves state-of-the-art\nperformance and is robust against 70% outliers in the FG3DCar dataset."}, {"title": "When NAS Meets Robustness: In Search of Robust Architectures Against Adversarial Attacks", "authors": "Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, Dahua Lin", "link": "https://arxiv.org/abs/1911.10695", "summary": "Recent advances in adversarial attacks uncover the intrinsic vulnerability of\nmodern deep neural networks. Since then, extensive efforts have been devoted to\nenhancing the robustness of deep networks via specialized learning algorithms\nand loss functions. In this work, we take an architectural perspective and\ninvestigate the patterns of network architectures that are resilient to\nadversarial attacks. To obtain the large number of networks needed for this\nstudy, we adopt one-shot neural architecture search, training a large network\nfor once and then finetuning the sub-networks sampled therefrom. The sampled\narchitectures together with the accuracies they achieve provide a rich basis\nfor our study. Our \"robust architecture Odyssey\" reveals several valuable\nobservations: 1) densely connected patterns result in improved robustness; 2)\nunder computational budget, adding convolution operations to direct connection\nedge is effective; 3) flow of solution procedure (FSP) matrix is a good\nindicator of network robustness. Based on these observations, we discover a\nfamily of robust architectures (RobNets). On various datasets, including CIFAR,\nSVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness\nperformance to other widely used architectures. Notably, RobNets substantially\nimprove the robust accuracy (~5% absolute gains) under both white-box and\nblack-box attacks, even with fewer parameter numbers. Code is available at\nhttps://github.com/gmh14/RobNets."}, {"title": "Towards Transferable Targeted Attack", "authors": "Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao, Heng Huang", "link": "", "summary": ""}, {"title": "Self-Supervised Human Depth Estimation From Monocular Videos", "authors": "Feitong Tan, Hao Zhu, Zhaopeng Cui, Siyu Zhu, Marc Pollefeys, Ping Tan", "link": "https://arxiv.org/abs/2005.03358", "summary": "Previous methods on estimating detailed human depth often require supervised\ntraining with `ground truth' depth data. This paper presents a self-supervised\nmethod that can be trained on YouTube videos without known depth, which makes\ntraining data collection simple and improves the generalization of the learned\nnetwork. The self-supervised learning is achieved by minimizing a\nphoto-consistency loss, which is evaluated between a video frame and its\nneighboring frames warped according to the estimated depth and the 3D non-rigid\nmotion of the human body. To solve this non-rigid motion, we first estimate a\nrough SMPL model at each video frame and compute the non-rigid body motion\naccordingly, which enables self-supervised learning on estimating the shape\ndetails. Experiments demonstrate that our method enjoys better generalization\nand performs much better on data in the wild."}, {"title": "Recursive Social Behavior Graph for Trajectory Prediction", "authors": "Jianhua Sun, Qinhong Jiang, Cewu Lu", "link": "https://arxiv.org/abs/2004.10402", "summary": "Social interaction is an important topic in human trajectory prediction to\ngenerate plausible paths. In this paper, we present a novel insight of\ngroup-based social interaction model to explore relationships among\npedestrians. We recursively extract social representations supervised by\ngroup-based annotations and formulate them into a social behavior graph, called\nRecursive Social Behavior Graph. Our recursive mechanism explores the\nrepresentation power largely. Graph Convolutional Neural Network then is used\nto propagate social interaction information in such a graph. With the guidance\nof Recursive Social Behavior Graph, we surpass state-of-the-art method on ETH\nand UCY dataset for 11.1% in ADE and 10.8% in FDE in average, and successfully\npredict complex social behaviors."}, {"title": "Context-Aware and Scale-Insensitive Temporal Repetition Counting", "authors": "Huaidong Zhang, Xuemiao Xu, Guoqiang Han, Shengfeng He", "link": "https://arxiv.org/abs/2005.08465", "summary": "Temporal repetition counting aims to estimate the number of cycles of a given\nrepetitive action. Existing deep learning methods assume repetitive actions are\nperformed in a fixed time-scale, which is invalid for the complex repetitive\nactions in real life. In this paper, we tailor a context-aware and\nscale-insensitive framework, to tackle the challenges in repetition counting\ncaused by the unknown and diverse cycle-lengths. Our approach combines two key\ninsights: (1) Cycle lengths from different actions are unpredictable that\nrequire large-scale searching, but, once a coarse cycle length is determined,\nthe variety between repetitions can be overcome by regression. (2) Determining\nthe cycle length cannot only rely on a short fragment of video but a contextual\nunderstanding. The first point is implemented by a coarse-to-fine cycle\nrefinement method. It avoids the heavy computation of exhaustively searching\nall the cycle lengths in the video, and, instead, it propagates the coarse\nprediction for further refinement in a hierarchical manner. We secondly propose\na bidirectional cycle length estimation method for a context-aware prediction.\nIt is a regression network that takes two consecutive coarse cycles as input,\nand predicts the locations of the previous and next repetitive cycles. To\nbenefit the training and evaluation of temporal repetition counting area, we\nconstruct a new and largest benchmark, which contains 526 videos with diverse\nrepetitive actions. Extensive experiments show that the proposed network\ntrained on a single dataset outperforms state-of-the-art methods on several\nbenchmarks, indicating that the proposed framework is general enough to capture\nrepetition patterns across domains."}, {"title": "OASIS: A Large-Scale Dataset for Single Image 3D in the Wild", "authors": "Weifeng Chen, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, Jia Deng"}, {"title": "VPLNet: Deep Single View Normal Estimation With Vanishing Points and Lines", "authors": "Rui Wang, David Geraghty, Kevin Matzen, Richard Szeliski, Jan-Michael Frahm"}, {"title": "Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning", "authors": "Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, Zhangyang Wang", "link": "https://arxiv.org/abs/2003.12862", "summary": "Pretrained models from self-supervision are prevalently used in fine-tuning\ndownstream tasks faster or for better accuracy. However, gaining robustness\nfrom pretraining is left unexplored. We introduce adversarial training into\nself-supervision, to provide general-purpose robust pre-trained models for the\nfirst time. We find these robust pre-trained models can benefit the subsequent\nfine-tuning in two ways: i) boosting final model robustness; ii) saving the\ncomputation cost, if proceeding towards adversarial fine-tuning. We conduct\nextensive experiments to demonstrate that the proposed framework achieves large\nperformance margins (eg, 3.83% on robust accuracy and 1.3% on standard\naccuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end\nadversarial training baseline. Moreover, we find that different self-supervised\npre-trained models have a diverse adversarial vulnerability. It inspires us to\nensemble several pretraining tasks, which boosts robustness more. Our ensemble\nstrategy contributes to a further improvement of 3.59% on robust accuracy,\nwhile maintaining a slightly higher standard accuracy on CIFAR-10. Our codes\nare available at https://github.com/TAMU-VITA/Adv-SS-Pretraining."}, {"title": "Defending Against Universal Attacks Through Selective Feature Regeneration", "authors": "Tejas Borkar, Felix Heide, Lina Karam", "link": "https://arxiv.org/abs/1906.03444", "summary": "Deep neural network (DNN) predictions have been shown to be vulnerable to\ncarefully crafted adversarial perturbations. Specifically, image-agnostic\n(universal adversarial) perturbations added to any image can fool a target\nnetwork into making erroneous predictions. Departing from existing defense\nstrategies that work mostly in the image domain, we present a novel defense\nwhich operates in the DNN feature domain and effectively defends against such\nuniversal perturbations. Our approach identifies pre-trained convolutional\nfeatures that are most vulnerable to adversarial noise and deploys trainable\nfeature regeneration units which transform these DNN filter activations into\nresilient features that are robust to universal perturbations. Regenerating\nonly the top 50% adversarially susceptible activations in at most 6 DNN layers\nand leaving all remaining DNN activations unchanged, we outperform existing\ndefense strategies across different network architectures by more than 10% in\nrestored accuracy. We show that without any additional modification, our\ndefense trained on ImageNet with one type of universal attack examples\neffectively defends against other types of unseen universal attacks."}, {"title": "Universal Physical Camouflage Attacks on Object Detectors", "authors": "Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan L. Yuille, Changqing Zou, Ning Liu", "link": "https://arxiv.org/abs/1909.04326", "summary": "In this paper, we study physical adversarial attacks on object detectors in\nthe wild. Previous works mostly craft instance-dependent perturbations only for\nrigid or planar objects. To this end, we propose to learn an adversarial\npattern to effectively attack all instances belonging to the same object\ncategory, referred to as Universal Physical Camouflage Attack (UPC).\nConcretely, UPC crafts camouflage by jointly fooling the region proposal\nnetwork, as well as misleading the classifier and the regressor to output\nerrors. In order to make UPC effective for non-rigid or non-planar objects, we\nintroduce a set of transformations for mimicking deformable properties. We\nadditionally impose optimization constraint to make generated patterns look\nnatural to human observers. To fairly evaluate the effectiveness of different\nphysical-world attacks, we present the first standardized virtual database,\nAttackScenes, which simulates the real 3D world in a controllable and\nreproducible environment. Extensive experiments suggest the superiority of our\nproposed UPC compared with existing physical adversarial attackers not only in\nvirtual environments (AttackScenes), but also in real-world physical\nenvironments. Code and dataset are available at\nhttps://mesunhlf.github.io/index_physical.html."}, {"title": "Intra- and Inter-Action Understanding via Temporal Action Parsing", "authors": "Dian Shao, Yue Zhao, Bo Dai, Dahua Lin", "link": "https://arxiv.org/abs/2005.10229", "summary": "Current methods for action recognition primarily rely on deep convolutional\nnetworks to derive feature embeddings of visual and motion features. While\nthese methods have demonstrated remarkable performance on standard benchmarks,\nwe are still in need of a better understanding as to how the videos, in\nparticular their internal structures, relate to high-level semantics, which may\nlead to benefits in multiple aspects, e.g. interpretable predictions and even\nnew methods that can take the recognition performances to a next level. Towards\nthis goal, we construct TAPOS, a new dataset developed on sport videos with\nmanual annotations of sub-actions, and conduct a study on temporal action\nparsing on top. Our study shows that a sport activity usually consists of\nmultiple sub-actions and that the awareness of such temporal structures is\nbeneficial to action recognition. We also investigate a number of temporal\nparsing methods, and thereon devise an improved method that is capable of\nmining sub-actions from training data without knowing the labels of them. On\nthe constructed TAPOS, the proposed method is shown to reveal intra-action\ninformation, i.e. how action instances are made of sub-actions, and\ninter-action information, i.e. one specific sub-action may commonly appear in\nvarious actions."}, {"title": "Lightweight Photometric Stereo for Facial Details Recovery", "authors": "Xueying Wang, Yudong Guo, Bailin Deng, Juyong Zhang", "link": "https://arxiv.org/abs/2003.12307", "summary": "Recently, 3D face reconstruction from a single image has achieved great\nsuccess with the help of deep learning and shape prior knowledge, but they\noften fail to produce accurate geometry details. On the other hand, photometric\nstereo methods can recover reliable geometry details, but require dense inputs\nand need to solve a complex optimization problem. In this paper, we present a\nlightweight strategy that only requires sparse inputs or even a single image to\nrecover high-fidelity face shapes with images captured under near-field lights.\nTo this end, we construct a dataset containing 84 different subjects with 29\nexpressions under 3 different lights. Data augmentation is applied to enrich\nthe data in terms of diversity in identity, lighting, expression, etc. With\nthis constructed dataset, we propose a novel neural network specially designed\nfor photometric stereo based 3D face reconstruction. Extensive experiments and\ncomparisons demonstrate that our method can generate high-quality\nreconstruction results with one to three facial images captured under\nnear-field lights. Our full framework is available at\nhttps://github.com/Juyong/FacePSNet."}, {"title": "Bundle Pooling for Polygonal Architecture Segmentation Problem", "authors": "Huayi Zeng, Kevin Joseph, Adam Vest, Yasutaka Furukawa"}, {"title": "AvatarMe: Realistically Renderable 3D Facial Reconstruction \u201cIn-the-Wild\u201d", "authors": "Alexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh, Stefanos Zafeiriou", "link": "https://arxiv.org/abs/2003.13845", "summary": "Over the last years, with the advent of Generative Adversarial Networks\n(GANs), many face analysis tasks have accomplished astounding performance, with\napplications including, but not limited to, face generation and 3D face\nreconstruction from a single \"in-the-wild\" image. Nevertheless, to the best of\nour knowledge, there is no method which can produce high-resolution\nphotorealistic 3D faces from \"in-the-wild\" images and this can be attributed to\nthe: (a) scarcity of available data for training, and (b) lack of robust\nmethodologies that can successfully be applied on very high-resolution data. In\nthis paper, we introduce AvatarMe, the first method that is able to reconstruct\nphotorealistic 3D faces from a single \"in-the-wild\" image with an increasing\nlevel of detail. To achieve this, we capture a large dataset of facial shape\nand reflectance and build on a state-of-the-art 3D texture and shape\nreconstruction method and successively refine its results, while generating the\nper-pixel diffuse and specular components that are required for realistic\nrendering. As we demonstrate in a series of qualitative and quantitative\nexperiments, AvatarMe outperforms the existing arts by a significant margin and\nreconstructs authentic, 4K by 6K-resolution 3D faces from a single\nlow-resolution image that, for the first time, bridges the uncanny valley."}, {"title": "Defending Against Model Stealing Attacks With Adaptive Misinformation", "authors": "Sanjay Kariyappa, Moinuddin K. Qureshi", "link": "https://arxiv.org/abs/1911.07100", "summary": "Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which\nallows a data-limited adversary with no knowledge of the training dataset to\nclone the functionality of a target model, just by using black-box query\naccess. Such attacks are typically carried out by querying the target model\nusing inputs that are synthetically generated or sampled from a surrogate\ndataset to construct a labeled dataset. The adversary can use this labeled\ndataset to train a clone model, which achieves a classification accuracy\ncomparable to that of the target model. We propose \"Adaptive Misinformation\" to\ndefend against such model stealing attacks. We identify that all existing model\nstealing attacks invariably query the target model with Out-Of-Distribution\n(OOD) inputs. By selectively sending incorrect predictions for OOD queries, our\ndefense substantially degrades the accuracy of the attacker's clone model (by\nup to 40%), while minimally impacting the accuracy (<0.5%) for benign users.\nCompared to existing defenses, our defense has a significantly better security\nvs accuracy trade-off and incurs minimal computational overhead."}, {"title": "Learning to Generate 3D Training Data Through Hybrid Gradient", "authors": "Dawei Yang, Jia Deng"}, {"title": "Cascaded Refinement Network for Point Cloud Completion", "authors": "Xiaogang Wang, Marcelo H. Ang Jr., Gim Hee Lee", "link": "https://arxiv.org/abs/2004.03327", "summary": "Point clouds are often sparse and incomplete. Existing shape completion\nmethods are incapable of generating details of objects or learning the complex\npoint distributions. To this end, we propose a cascaded refinement network\ntogether with a coarse-to-fine strategy to synthesize the detailed object\nshapes. Considering the local details of partial input with the global shape\ninformation together, we can preserve the existing details in the incomplete\npoint set and generate the missing parts with high fidelity. We also design a\npatch discriminator that guarantees every local area has the same pattern with\nthe ground truth to learn the complicated point distribution. Quantitative and\nqualitative experiments on different datasets show that our method achieves\nsuperior results compared to existing state-of-the-art approaches on the 3D\npoint cloud completion task. Our source code is available at\nhttps://github.com/xiaogangw/cascaded-point-completion.git."}, {"title": "Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder", "authors": "Guanlin Li, Shuya Ding, Jun Luo, Chang Liu", "link": "https://arxiv.org/abs/2005.02552", "summary": "Whereas adversarial training is employed as the main defence strategy against\nspecific adversarial samples, it has limited generalization capability and\nincurs excessive time complexity. In this paper, we propose an attack-agnostic\ndefence framework to enhance the intrinsic robustness of neural networks,\nwithout jeopardizing the ability of generalizing clean samples. Our Feature\nPyramid Decoder (FPD) framework applies to all block-based convolutional neural\nnetworks (CNNs). It implants denoising and image restoration modules into a\ntargeted CNN, and it also constraints the Lipschitz constant of the\nclassification layer. Moreover, we propose a two-phase strategy to train the\nFPD-enhanced CNN, utilizing $\\epsilon$-neighbourhood noisy images with\nmulti-task and self-supervised learning. Evaluated against a variety of\nwhite-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain\nsufficient robustness against general adversarial samples on MNIST, SVHN and\nCALTECH. In addition, if we further conduct adversarial training, the\nFPD-enhanced CNNs perform better than their non-enhanced versions."}, {"title": "Learning to Discriminate Information for Online Action Detection", "authors": "Hyunjun Eun, Jinyoung Moon, Jongyoul Park, Chanho Jung, Changick Kim", "link": "https://arxiv.org/abs/1912.04461", "summary": "From a streaming video, online action detection aims to identify actions in\nthe present. For this task, previous methods use recurrent networks to model\nthe temporal sequence of current action frames. However, these methods overlook\nthe fact that an input image sequence includes background and irrelevant\nactions as well as the action of interest. For online action detection, in this\npaper, we propose a novel recurrent unit to explicitly discriminate the\ninformation relevant to an ongoing action from others. Our unit, named\nInformation Discrimination Unit (IDU), decides whether to accumulate input\ninformation based on its relevance to the current action. This enables our\nrecurrent network with IDU to learn a more discriminative representation for\nidentifying ongoing actions. In experiments on two benchmark datasets, TVSeries\nand THUMOS-14, the proposed method outperforms state-of-the-art methods by a\nsignificant margin. Moreover, we demonstrate the effectiveness of our recurrent\nunit by conducting comprehensive ablation studies."}, {"title": "Adversarial Examples Improve Image Recognition", "authors": "Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L. Yuille, Quoc V. Le", "link": "https://arxiv.org/abs/1911.09665", "summary": "Adversarial examples are commonly viewed as a threat to ConvNets. Here we\npresent an opposite perspective: adversarial examples can be used to improve\nimage recognition models if harnessed in the right manner. We propose AdvProp,\nan enhanced adversarial training scheme which treats adversarial examples as\nadditional examples, to prevent overfitting. Key to our method is the usage of\na separate auxiliary batch norm for adversarial examples, as they have\ndifferent underlying distributions to normal examples.\n  We show that AdvProp improves a wide range of models on various image\nrecognition tasks and performs better when the models are bigger. For instance,\nby applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve\nsignificant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A\n(+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our\nmethod achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without\nextra data. This result even surpasses the best model in [20] which is trained\nwith 3.5B Instagram images (~3000X more than ImageNet) and ~9.4X more\nparameters. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet."}, {"title": "PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes", "authors": "Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, Baoquan Chen", "link": "", "summary": ""}, {"title": "Actor-Transformers for Group Activity Recognition", "authors": "Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, Cees G. M. Snoek", "link": "https://arxiv.org/abs/2003.12737", "summary": "This paper strives to recognize individual actions and group activities from\nvideos. While existing solutions for this challenging problem explicitly model\nspatial and temporal relationships based on location of individual actors, we\npropose an actor-transformer model able to learn and selectively extract\ninformation relevant for group activity recognition. We feed the transformer\nwith rich actor-specific static and dynamic representations expressed by\nfeatures from a 2D pose network and 3D CNN, respectively. We empirically study\ndifferent ways to combine these representations and show their complementary\nbenefits. Experiments show what is important to transform and how it should be\ntransformed. What is more, actor-transformers achieve state-of-the-art results\non two publicly available benchmarks for group activity recognition,\noutperforming the previous best published results by a considerable margin."}, {"title": "SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans", "authors": "Angela Dai, Christian Diller, Matthias Nie\u00dfner", "link": "https://arxiv.org/abs/1912.00036", "summary": "We present a novel approach that converts partial and noisy RGB-D scans into\nhigh-quality 3D scene reconstructions by inferring unobserved scene geometry.\nOur approach is fully self-supervised and can hence be trained solely on\nreal-world, incomplete scans. To achieve self-supervision, we remove frames\nfrom a given (incomplete) 3D scan in order to make it even more incomplete;\nself-supervision is then formulated by correlating the two levels of\npartialness of the same scan while masking out regions that have never been\nobserved. Through generalization across a large training set, we can then\npredict 3D scene completion without ever seeing any 3D scan of entirely\ncomplete geometry. Combined with a new 3D sparse generative neural network\narchitecture, our method is able to predict highly-detailed surfaces in a\ncoarse-to-fine hierarchical fashion, generating 3D scenes at 2cm resolution,\nmore than twice the resolution of existing state-of-the-art methods as well as\noutperforming them by a significant margin in reconstruction quality."}, {"title": "Geometry-Aware Satellite-to-Ground Image Synthesis for Urban Areas", "authors": "Xiaohu Lu, Zuoyue Li, Zhaopeng Cui, Martin R. Oswald, Marc Pollefeys, Rongjun Qin"}, {"title": "Action Modifiers: Learning From Adverbs in Instructional Videos", "authors": "Hazel Doughty, Ivan Laptev, Walterio Mayol-Cuevas, Dima Damen", "link": "https://arxiv.org/abs/1912.06617", "summary": "We present a method to learn a representation for adverbs from instructional\nvideos using weak supervision from the accompanying narrations. Key to our\nmethod is the fact that the visual representation of the adverb is highly\ndependant on the action to which it applies, although the same adverb will\nmodify multiple actions in a similar way. For instance, while 'spread quickly'\nand 'mix quickly' will look dissimilar, we can learn a common representation\nthat allows us to recognize both, among other actions. We formulate this as an\nembedding problem, and use scaled dot-product attention to learn from\nweakly-supervised video narrations. We jointly learn adverbs as invertible\ntransformations operating on the embedding space, so as to add or remove the\neffect of the adverb. As there is no prior work on weakly supervised learning\nfrom adverbs, we gather paired action-adverb annotations from a subset of the\nHowTo100M dataset for 6 adverbs: quickly/slowly, finely/coarsely, and\npartially/completely. Our method outperforms all baselines for video-to-adverb\nretrieval with a performance of 0.719 mAP. We also demonstrate our model's\nability to attend to the relevant video parts in order to determine the adverb\nfor a given action."}, {"title": "ZSTAD: Zero-Shot Temporal Activity Detection", "authors": "Lingling Zhang, Xiaojun Chang, Jun Liu, Minnan Luo, Sen Wang, Zongyuan Ge, Alexander Hauptmann", "link": "", "summary": ""}, {"title": "Geometric Structure Based and Regularized Depth Estimation From 360 Indoor Imagery", "authors": "Lei Jin, Yanyu Xu, Jia Zheng, Junfei Zhang, Rui Tang, Shugong Xu, Jingyi Yu, Shenghua Gao"}, {"title": "Deep Kinematics Analysis for Monocular 3D Human Pose Estimation", "authors": "Jingwei Xu, Zhenbo Yu, Bingbing Ni, Jiancheng Yang, Xiaokang Yang, Wenjun Zhang"}, {"title": "TEA: Temporal Excitation and Aggregation for Action Recognition", "authors": "Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, Limin Wang", "link": "http://arxiv.org/abs/2004.01398", "summary": "Temporal modeling is key for action recognition in videos. It normally\nconsiders both short-range motions and long-range aggregations. In this paper,\nwe propose a Temporal Excitation and Aggregation (TEA) block, including a\nmotion excitation (ME) module and a multiple temporal aggregation (MTA) module,\nspecifically designed to capture both short- and long-range temporal evolution.\nIn particular, for short-range motion modeling, the ME module calculates the\nfeature-level temporal differences from spatiotemporal features. It then\nutilizes the differences to excite the motion-sensitive channels of the\nfeatures. The long-range temporal aggregations in previous works are typically\nachieved by stacking a large number of local temporal convolutions. Each\nconvolution processes a local temporal window at a time. In contrast, the MTA\nmodule proposes to deform the local convolution to a group of sub-convolutions,\nforming a hierarchical residual architecture. Without introducing additional\nparameters, the features will be processed with a series of sub-convolutions,\nand each frame could complete multiple temporal aggregations with\nneighborhoods. The final equivalent receptive field of temporal dimension is\naccordingly enlarged, which is capable of modeling the long-range temporal\nrelationship over distant frames. The two components of the TEA block are\ncomplementary in temporal modeling. Finally, our approach achieves impressive\nresults at low FLOPs on several action recognition benchmarks, such as\nKinetics, Something-Something, HMDB51, and UCF101, which confirms its\neffectiveness and efficiency."}, {"title": "Oops! Predicting Unintentional Action in Video", "authors": "Dave Epstein, Boyuan Chen, Carl Vondrick", "link": "https://arxiv.org/abs/1911.11206", "summary": "From just a short glance at a video, we can often tell whether a person's\naction is intentional or not. Can we train a model to recognize this? We\nintroduce a dataset of in-the-wild videos of unintentional action, as well as a\nsuite of tasks for recognizing, localizing, and anticipating its onset. We\ntrain a supervised neural network as a baseline and analyze its performance\ncompared to human consistency on the tasks. We also investigate self-supervised\nrepresentations that leverage natural signals in our dataset, and show the\neffectiveness of an approach that uses the intrinsic speed of video to perform\ncompetitively with highly-supervised pretraining. However, a significant gap\nbetween machine and human performance remains. The project website is available\nat https://oops.cs.columbia.edu"}, {"title": "Scene Recomposition by Learning-Based ICP", "authors": "Hamid Izadinia, Steven M. Seitz", "link": "https://arxiv.org/abs/1812.05583", "summary": "By moving a depth sensor around a room, we compute a 3D CAD model of the\nenvironment, capturing the room shape and contents such as chairs, desks,\nsofas, and tables. Rather than reconstructing geometry, we match, place, and\nalign each object in the scene to thousands of CAD models of objects. In\naddition to the fully automatic system, the key technical contribution is a\nnovel approach for aligning CAD models to 3D scans, based on deep reinforcement\nlearning. This approach, which we call Learning-based ICP, outperforms prior\nICP methods in the literature, by learning the best points to match and\nconditioning on object viewpoint. LICP learns to align using only synthetic\ndata and does not require ground truth annotation of object pose or keypoint\npair matching in real scene scans. While LICP is trained on synthetic data and\nwithout 3D real scene annotations, it outperforms both learned local deep\nfeature matching and geometric based alignment methods in real scenes. The\nproposed method is evaluated on real scenes datasets of SceneNN and ScanNet as\nwell as synthetic scenes of SUNCG. High quality results are demonstrated on a\nrange of real world scenes, with robustness to clutter, viewpoint, and\nocclusion."}, {"title": "Enhancing Cross-Task Black-Box Transferability of Adversarial Examples With Dispersion Reduction", "authors": "Yantao Lu, Yunhan Jia, Jianyu Wang, Bai Li, Weiheng Chai, Lawrence Carin, Senem Velipasalar", "link": "https://arxiv.org/abs/1911.11616", "summary": "Neural networks are known to be vulnerable to carefully crafted adversarial\nexamples, and these malicious samples often transfer, i.e., they remain\nadversarial even against other models. Although great efforts have been delved\ninto the transferability across models, surprisingly, less attention has been\npaid to the cross-task transferability, which represents the real-world\ncybercriminal's situation, where an ensemble of different defense/detection\nmechanisms need to be evaded all at once. In this paper, we investigate the\ntransferability of adversarial examples across a wide range of real-world\ncomputer vision tasks, including image classification, object detection,\nsemantic segmentation, explicit content detection, and text detection. Our\nproposed attack minimizes the ``dispersion'' of the internal feature map, which\novercomes existing attacks' limitation of requiring task-specific loss\nfunctions and/or probing a target model. We conduct evaluation on open source\ndetection and segmentation models as well as four different computer vision\ntasks provided by Google Cloud Vision (GCV) APIs, to show how our approach\noutperforms existing attacks by degrading performance of multiple CV tasks by a\nlarge margin with only modest perturbations linf=16."}, {"title": "Single-Step Adversarial Training With Dropout Scheduling", "authors": "Vivek B.S., R. Venkatesh Babu", "link": "https://arxiv.org/abs/2004.08628", "summary": "Deep learning models have shown impressive performance across a spectrum of\ncomputer vision applications including medical diagnosis and autonomous\ndriving. One of the major concerns that these models face is their\nsusceptibility to adversarial attacks. Realizing the importance of this issue,\nmore researchers are working towards developing robust models that are less\naffected by adversarial attacks. Adversarial training method shows promising\nresults in this direction. In adversarial training regime, models are trained\nwith mini-batches augmented with adversarial samples. Fast and simple methods\n(e.g., single-step gradient ascent) are used for generating adversarial\nsamples, in order to reduce computational complexity. It is shown that models\ntrained using single-step adversarial training method (adversarial samples are\ngenerated using non-iterative method) are pseudo robust. Further, this pseudo\nrobustness of models is attributed to the gradient masking effect. However,\nexisting works fail to explain when and why gradient masking effect occurs\nduring single-step adversarial training. In this work, (i) we show that models\ntrained using single-step adversarial training method learn to prevent the\ngeneration of single-step adversaries, and this is due to over-fitting of the\nmodel during the initial stages of training, and (ii) to mitigate this effect,\nwe propose a single-step adversarial training method with dropout scheduling.\nUnlike models trained using existing single-step adversarial training methods,\nmodels trained using the proposed single-step adversarial training method are\nrobust against both single-step and multi-step adversarial attacks, and the\nperformance is on par with models trained using computationally expensive\nmulti-step adversarial training methods, in white-box and black-box settings."}, {"title": "Deep Non-Line-of-Sight Reconstruction", "authors": "Javier Grau Chopite, Matthias B. Hullin, Michael Wand, Julian Iseringhausen", "link": "http://arxiv.org/abs/2001.09067", "summary": "The recent years have seen a surge of interest in methods for imaging beyond\nthe direct line of sight. The most prominent techniques rely on time-resolved\noptical impulse responses, obtained by illuminating a diffuse wall with an\nultrashort light pulse and observing multi-bounce indirect reflections with an\nultrafast time-resolved imager. Reconstruction of geometry from such data,\nhowever, is a complex non-linear inverse problem that comes with substantial\ncomputational demands. In this paper, we employ convolutional feed-forward\nnetworks for solving the reconstruction problem efficiently while maintaining\ngood reconstruction quality. Specifically, we devise a tailored autoencoder\narchitecture, trained end-to-end, that maps transient images directly to a\ndepth map representation. Training is done using an efficient transient\nrenderer for diffuse three-bounce indirect light transport that enables the\nquick generation of large amounts of training data for the network. We examine\nthe performance of our method on a variety of synthetic and experimental\ndatasets and its dependency on the choice of training data and augmentation\nstrategies, as well as architectural features. We demonstrate that our\nfeed-forward network, even though it is trained solely on synthetic data,\ngeneralizes to measured data from SPAD sensors and is able to obtain results\nthat are competitive with model-based reconstruction methods."}, {"title": "SSRNet: Scalable 3D Surface Reconstruction Network", "authors": "Zhenxing Mi, Yiming Luo, Wenbing Tao", "link": "http://arxiv.org/abs/1911.07401", "summary": "Existing learning-based surface reconstruction methods from point clouds are\nstill facing challenges in terms of scalability and preservation of details on\nlarge-scale point clouds. In this paper, we propose the SSRNet, a novel\nscalable learning-based method for surface reconstruction. The proposed SSRNet\nconstructs local geometry-aware features for octree vertices and designs a\nscalable reconstruction pipeline, which not only greatly enhances the\npredication accuracy of the relative position between the vertices and the\nimplicit surface facilitating the surface reconstruction quality, but also\nallows dividing the point cloud and octree vertices and processing different\nparts in parallel for superior scalability on large-scale point clouds with\nmillions of points. Moreover, SSRNet demonstrates outstanding generalization\ncapability and only needs several surface data for training, much less than\nother learning-based reconstruction methods, which can effectively avoid\noverfitting. The trained model of SSRNet on one dataset can be directly used on\nother datasets with superior performance. Finally, the time consumption with\nSSRNet on a large-scale point cloud is acceptable and competitive. To our\nknowledge, the proposed SSRNet is the first to really bring a convincing\nsolution to the scalability issue of the learning-based surface reconstruction\nmethods, and is an important step to make learning-based methods competitive\nwith respect to geometry processing methods on real-world and challenging data.\nExperiments show that our method achieves a breakthrough in scalability and\nquality compared with state-of-the-art learning-based methods."}, {"title": "Progressive Relation Learning for Group Activity Recognition", "authors": "Guyue Hu, Bo Cui, Yuan He, Shan Yu", "link": "https://arxiv.org/abs/1908.02948", "summary": "Group activities usually involve spatiotemporal dynamics among many\ninteractive individuals, while only a few participants at several key frames\nessentially define the activity. Therefore, effectively modeling the\ngroup-relevant and suppressing the irrelevant actions (and interactions) are\nvital for group activity recognition. In this paper, we propose a novel method\nbased on deep reinforcement learning to progressively refine the low-level\nfeatures and high-level relations of group activities. Firstly, we construct a\nsemantic relation graph (SRG) to explicitly model the relations among persons.\nThen, two agents adopting policy according to two Markov decision processes are\napplied to progressively refine the SRG. Specifically, one feature-distilling\n(FD) agent in the discrete action space refines the low-level spatio-temporal\nfeatures by distilling the most informative frames. Another relation-gating\n(RG) agent in continuous action space adjusts the high-level semantic graph to\npay more attention to group-relevant relations. The SRG, FD agent, and RG agent\nare optimized alternately to mutually boost the performance of each other.\nExtensive experiments on two widely used benchmarks demonstrate the\neffectiveness and superiority of the proposed approach."}, {"title": "Cooling-Shrinking Attack: Blinding the Tracker With Imperceptible Noises", "authors": "Bin Yan, Dong Wang, Huchuan Lu, Xiaoyun Yang", "link": "https://arxiv.org/abs/2003.09595", "summary": "Adversarial attack of CNN aims at deceiving models to misbehave by adding\nimperceptible perturbations to images. This feature facilitates to understand\nneural networks deeply and to improve the robustness of deep learning models.\nAlthough several works have focused on attacking image classifiers and object\ndetectors, an effective and efficient method for attacking single object\ntrackers of any target in a model-free way remains lacking. In this paper, a\ncooling-shrinking attack method is proposed to deceive state-of-the-art\nSiameseRPN-based trackers. An effective and efficient perturbation generator is\ntrained with a carefully designed adversarial loss, which can simultaneously\ncool hot regions where the target exists on the heatmaps and force the\npredicted bounding box to shrink, making the tracked target invisible to\ntrackers. Numerous experiments on OTB100, VOT2018, and LaSOT datasets show that\nour method can effectively fool the state-of-the-art SiameseRPN++ tracker by\nadding small perturbations to the template or the search regions. Besides, our\nmethod has good transferability and is able to deceive other top-performance\ntrackers such as DaSiamRPN, DaSiamRPN-UpdateNet, and DiMP. The source codes are\navailable at https://github.com/MasterBin-IIAU/CSA."}, {"title": "Adversarial Camouflage: Hiding Physical-World Attacks With Natural Styles", "authors": "Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A. K. Qin, Yun Yang", "link": "https://arxiv.org/abs/2003.08757", "summary": "Deep neural networks (DNNs) are known to be vulnerable to adversarial\nexamples. Existing works have mostly focused on either digital adversarial\nexamples created via small and imperceptible perturbations, or physical-world\nadversarial examples created with large and less realistic distortions that are\neasily identified by human observers. In this paper, we propose a novel\napproach, called Adversarial Camouflage (\\emph{AdvCam}), to craft and\ncamouflage physical-world adversarial examples into natural styles that appear\nlegitimate to human observers. Specifically, \\emph{AdvCam} transfers large\nadversarial perturbations into customized styles, which are then \"hidden\"\non-target object or off-target background. Experimental evaluation shows that,\nin both digital and physical-world scenarios, adversarial examples crafted by\n\\emph{AdvCam} are well camouflaged and highly stealthy, while remaining\neffective in fooling state-of-the-art DNN image classifiers. Hence,\n\\emph{AdvCam} is a flexible approach that can help craft stealthy attacks to\nevaluate the robustness of DNNs. \\emph{AdvCam} can also be used to protect\nprivate information from being detected by deep learning systems."}, {"title": "Weakly-Supervised Action Localization by Generative Attention Modeling", "authors": "Baifeng Shi, Qi Dai, Yadong Mu, Jingdong Wang", "link": "https://arxiv.org/abs/2003.12424", "summary": "Weakly-supervised temporal action localization is a problem of learning an\naction localization model with only video-level action labeling available. The\ngeneral framework largely relies on the classification activation, which\nemploys an attention model to identify the action-related frames and then\ncategorizes them into different classes. Such method results in the\naction-context confusion issue: context frames near action clips tend to be\nrecognized as action frames themselves, since they are closely related to the\nspecific classes. To solve the problem, in this paper we propose to model the\nclass-agnostic frame-wise probability conditioned on the frame attention using\nconditional Variational Auto-Encoder (VAE). With the observation that the\ncontext exhibits notable difference from the action at representation level, a\nprobabilistic model, i.e., conditional VAE, is learned to model the likelihood\nof each frame given the attention. By maximizing the conditional probability\nwith respect to the attention, the action and non-action frames are well\nseparated. Experiments on THUMOS14 and ActivityNet1.2 demonstrate advantage of\nour method and effectiveness in handling action-context confusion problem. Code\nis now available on GitHub."}, {"title": "Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes", "authors": "Sravanti Addepalli, Vivek B.S., Arya Baburaj, Gaurang Sriramanan, R. Venkatesh Babu", "link": "https://arxiv.org/abs/2004.00306", "summary": "As humans, we inherently perceive images based on their predominant features,\nand ignore noise embedded within lower bit planes. On the contrary, Deep Neural\nNetworks are known to confidently misclassify images corrupted with\nmeticulously crafted perturbations that are nearly imperceptible to the human\neye. In this work, we attempt to address this problem by training networks to\nform coarse impressions based on the information in higher bit planes, and use\nthe lower bit planes only to refine their prediction. We demonstrate that, by\nimposing consistency on the representations learned across differently\nquantized images, the adversarial robustness of networks improves significantly\nwhen compared to a normally trained model. Present state-of-the-art defenses\nagainst adversarial attacks require the networks to be explicitly trained using\nadversarial samples that are computationally expensive to generate. While such\nmethods that use adversarial training continue to achieve the best results,\nthis work paves the way towards achieving robustness without having to\nexplicitly train on adversarial samples. The proposed approach is therefore\nfaster, and also closer to the natural learning process in humans."}, {"title": "Polishing Decision-Based Adversarial Noise With a Customized Sampling", "authors": "Yucheng Shi, Yahong Han, Qi Tian"}, {"title": "Towards Large Yet Imperceptible Adversarial Image Perturbations With Perceptual Color Distance", "authors": "Zhengyu Zhao, Zhuoran Liu, Martha Larson", "link": "https://arxiv.org/abs/1911.02466", "summary": "The success of image perturbations that are designed to fool image classifier\nis assessed in terms of both adversarial effect and visual imperceptibility.\nThe conventional assumption on imperceptibility is that perturbations should\nstrive for tight $L_p$-norm bounds in RGB space. In this work, we drop this\nassumption by pursuing an approach that exploits human color perception, and\nmore specifically, minimizing perturbation size with respect to perceptual\ncolor distance. Our first approach, Perceptual Color distance C&W (PerC-C&W),\nextends the widely-used C&W approach and produces larger RGB perturbations.\nPerC-C&W is able to maintain adversarial strength, while contributing to\nimperceptibility. Our second approach, Perceptual Color distance Alternating\nLoss (PerC-AL), achieves the same outcome, but does so more efficiently by\nalternating between the classification loss and perceptual color difference\nwhen updating perturbations. Experimental evaluation shows PerC approaches\noutperform conventional $L_p$ approaches in terms of robustness and\ntransferability, and also demonstrates that the PerC distance can provide added\nvalue on top of existing structure-based methods to creating image\nperturbations."}, {"title": "Something-Else: Compositional Action Recognition With Spatial-Temporal Interaction Networks", "authors": "Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, Trevor Darrell", "link": "https://arxiv.org/abs/1912.09930", "summary": "Human action is naturally compositional: humans can easily recognize and\nperform actions with objects that are different from those used in training\ndemonstrations. In this paper, we study the compositionality of action by\nlooking into the dynamics of subject-object interactions. We propose a novel\nmodel which can explicitly reason about the geometric relations between\nconstituent objects and an agent performing an action. To train our model, we\ncollect dense object box annotations on the Something-Something dataset. We\npropose a novel compositional action recognition task where the training\ncombinations of verbs and nouns do not overlap with the test set. The novel\naspects of our model are applicable to activities with prominent object\ninteraction dynamics and to objects which can be tracked using state-of-the-art\napproaches; for activities without clearly defined spatial object-agent\ninteractions, we rely on baseline scene-level spatio-temporal representations.\nWe show the effectiveness of our approach not only on the proposed\ncompositional action recognition task, but also in a few-shot compositional\nsetting which requires the model to generalize across both object appearance\nand action category."}, {"title": "Learning Unsupervised Hierarchical Part Decomposition of 3D Objects From a Single RGB Image", "authors": "Despoina Paschalidou, Luc Van Gool, Andreas Geiger", "link": "https://arxiv.org/abs/2004.01176", "summary": "Humans perceive the 3D world as a set of distinct objects that are\ncharacterized by various low-level (geometry, reflectance) and high-level\n(connectivity, adjacency, symmetry) properties. Recent methods based on\nconvolutional neural networks (CNNs) demonstrated impressive progress in 3D\nreconstruction, even when using a single 2D image as input. However, the\nmajority of these methods focuses on recovering the local 3D geometry of an\nobject without considering its part-based decomposition or relations between\nparts. We address this challenging problem by proposing a novel formulation\nthat allows to jointly recover the geometry of a 3D object as a set of\nprimitives as well as their latent hierarchical structure without part-level\nsupervision. Our model recovers the higher level structural decomposition of\nvarious objects in the form of a binary tree of primitives, where simple parts\nare represented with fewer primitives and more complex parts are modeled with\nmore components. Our experiments on the ShapeNet and D-FAUST datasets\ndemonstrate that considering the organization of parts indeed facilitates\nreasoning about 3D geometry."}, {"title": "Focus on Defocus: Bridging the Synthetic to Real Domain Gap for Depth Estimation", "authors": "Maxim Maximov, Kevin Galim, Laura Leal-Taix\u00e9", "link": "https://arxiv.org/abs/2005.09623", "summary": "Data-driven depth estimation methods struggle with the generalization outside\ntheir training scenes due to the immense variability of the real-world scenes.\nThis problem can be partially addressed by utilising synthetically generated\nimages, but closing the synthetic-real domain gap is far from trivial. In this\npaper, we tackle this issue by using domain invariant defocus blur as direct\nsupervision. We leverage defocus cues by using a permutation invariant\nconvolutional neural network that encourages the network to learn from the\ndifferences between images with a different point of focus. Our proposed\nnetwork uses the defocus map as an intermediate supervisory signal. We are able\nto train our model completely on synthetic data and directly apply it to a wide\nrange of real-world images. We evaluate our model on synthetic and real\ndatasets, showing compelling generalization results and state-of-the-art depth\nprediction."}, {"title": "Active Vision for Early Recognition of Human Actions", "authors": "Boyu Wang, Lihan Huang, Minh Hoai"}, {"title": "SmallBigNet: Integrating Core and Contextual Views for Video Classification", "authors": "Xianhang Li, Yali Wang, Zhipeng Zhou, Yu Qiao"}, {"title": "Gate-Shift Networks for Video Action Recognition", "authors": "Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz", "link": "https://arxiv.org/abs/1912.00381", "summary": "Deep 3D CNNs for video action recognition are designed to learn powerful\nrepresentations in the joint spatio-temporal feature space. In practice\nhowever, because of the large number of parameters and computations involved,\nthey may under-perform in the lack of sufficiently large datasets for training\nthem at scale. In this paper we introduce spatial gating in spatial-temporal\ndecomposition of 3D kernels. We implement this concept with Gate-Shift Module\n(GSM). GSM is lightweight and turns a 2D-CNN into a highly efficient\nspatio-temporal feature extractor. With GSM plugged in, a 2D-CNN learns to\nadaptively route features through time and combine them, at almost no\nadditional parameters and computational overhead. We perform an extensive\nevaluation of the proposed module to study its effectiveness in video action\nrecognition, achieving state-of-the-art results on Something Something-V1 and\nDiving48 datasets, and obtaining competitive results on EPIC-Kitchens with far\nless model complexity."}, {"title": "Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition", "authors": "Pengfei Zhang, Cuiling Lan, Wenjun Zeng, Junliang Xing, Jianru Xue, Nanning Zheng", "link": "https://arxiv.org/abs/1904.01189", "summary": "Skeleton-based human action recognition has attracted great interest thanks\nto the easy accessibility of the human skeleton data. Recently, there is a\ntrend of using very deep feedforward neural networks to model the 3D\ncoordinates of joints without considering the computational efficiency. In this\npaper, we propose a simple yet effective semantics-guided neural network (SGN)\nfor skeleton-based action recognition. We explicitly introduce the high level\nsemantics of joints (joint type and frame index) into the network to enhance\nthe feature representation capability. In addition, we exploit the relationship\nof joints hierarchically through two modules, i.e., a joint-level module for\nmodeling the correlations of joints in the same frame and a framelevel module\nfor modeling the dependencies of frames by taking the joints in the same frame\nas a whole. A strong baseline is proposed to facilitate the study of this\nfield. With an order of magnitude smaller model size than most previous works,\nSGN achieves the state-of-the-art performance on the NTU60, NTU120, and SYSU\ndatasets. The source code is available at https://github.com/microsoft/SGN."}, {"title": "Exploiting Joint Robustness to Adversarial Perturbations", "authors": "Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Jeremy Dawson, Nasser M. Nasrabadi", "link": "", "summary": ""}, {"title": "From Image Collections to Point Clouds With Self-Supervised Shape and Pose Networks", "authors": "K L Navaneet, Ansu Mathew, Shashank Kashyap, Wei-Chih Hung, Varun Jampani, R. Venkatesh Babu", "link": "http://arxiv.org/abs/2005.01939", "summary": "Reconstructing 3D models from 2D images is one of the fundamental problems in\ncomputer vision. In this work, we propose a deep learning technique for 3D\nobject reconstruction from a single image. Contrary to recent works that either\nuse 3D supervision or multi-view supervision, we use only single view images\nwith no pose information during training as well. This makes our approach more\npractical requiring only an image collection of an object category and the\ncorresponding silhouettes. We learn both 3D point cloud reconstruction and pose\nestimation networks in a self-supervised manner, making use of differentiable\npoint cloud renderer to train with 2D supervision. A key novelty of the\nproposed technique is to impose 3D geometric reasoning into predicted 3D point\nclouds by rotating them with randomly sampled poses and then enforcing cycle\nconsistency on both 3D reconstructions and poses. In addition, using\nsingle-view supervision allows us to do test-time optimization on a given test\nimage. Experiments on the synthetic ShapeNet and real-world Pix3D datasets\ndemonstrate that our approach, despite using less supervision, can achieve\ncompetitive performance compared to pose-supervised and multi-view supervised\napproaches."}, {"title": "Searching for Actions on the Hyperbole", "authors": "Teng Long, Pascal Mettes, Heng Tao Shen, Cees G. M. Snoek"}, {"title": "ColorFool: Semantic Adversarial Colorization", "authors": "Ali Shahin Shamsabadi, Ricardo S\u00e1nchez-Matilla, Andrea Cavallaro", "link": "https://arxiv.org/abs/1911.10891", "summary": "Adversarial attacks that generate small L_p-norm perturbations to mislead\nclassifiers have limited success in black-box settings and with unseen\nclassifiers. These attacks are also not robust to defenses that use denoising\nfilters and to adversarial training procedures. Instead, adversarial attacks\nthat generate unrestricted perturbations are more robust to defenses, are\ngenerally more successful in black-box settings and are more transferable to\nunseen classifiers. However, unrestricted perturbations may be noticeable to\nhumans. In this paper, we propose a content-based black-box adversarial attack\nthat generates unrestricted perturbations by exploiting image semantics to\nselectively modify colors within chosen ranges that are perceived as natural by\nhumans. We show that the proposed approach, ColorFool, outperforms in terms of\nsuccess rate, robustness to defense frameworks and transferability, five\nstate-of-the-art adversarial attacks on two different tasks, scene and object\nclassification, when attacking three state-of-the-art deep neural networks\nusing three standard datasets. The source code is available at\nhttps://github.com/smartcameras/ColorFool."}, {"title": "Boosting the Transferability of Adversarial Samples via Attention", "authors": "Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R. Lyu, Yu-Wing Tai"}, {"title": "ActionBytes: Learning From Trimmed Videos to Localize Actions", "authors": "Mihir Jain, Amir Ghodrati, Cees G. M. Snoek"}, {"title": "Efficient Adversarial Training With Transferable Adversarial Examples", "authors": "Haizhong Zheng, Ziqi Zhang, Juncheng Gu, Honglak Lee, Atul Prakash", "link": "https://arxiv.org/abs/1912.11969", "summary": "Adversarial training is an effective defense method to protect classification\nmodels against adversarial attacks. However, one limitation of this approach is\nthat it can require orders of magnitude additional training time due to high\ncost of generating strong adversarial examples during training. In this paper,\nwe first show that there is high transferability between models from\nneighboring epochs in the same training process, i.e., adversarial examples\nfrom one epoch continue to be adversarial in subsequent epochs. Leveraging this\nproperty, we propose a novel method, Adversarial Training with Transferable\nAdversarial Examples (ATTA), that can enhance the robustness of trained models\nand greatly improve the training efficiency by accumulating adversarial\nperturbations through epochs. Compared to state-of-the-art adversarial training\nmethods, ATTA enhances adversarial accuracy by up to 7.2% on CIFAR10 and\nrequires 12~14x less training time on MNIST and CIFAR10 datasets with\ncomparable model robustness."}, {"title": "Alleviation of Gradient Exploding in GANs: Fake Can Be Real", "authors": "Song Tao, Jia Wang", "link": "https://arxiv.org/abs/1912.12485", "summary": "In order to alleviate the notorious mode collapse phenomenon in generative\nadversarial networks (GANs), we propose a novel training method of GANs in\nwhich certain fake samples are considered as real ones during the training\nprocess. This strategy can reduce the gradient value that generator receives in\nthe region where gradient exploding happens. We show the process of an\nunbalanced generation and a vicious circle issue resulted from gradient\nexploding in practical training, which explains the instability of GANs. We\nalso theoretically prove that gradient exploding can be alleviated by\npenalizing the difference between discriminator outputs and fake-as-real\nconsideration for very close real and fake samples. Accordingly, Fake-As-Real\nGAN (FARGAN) is proposed with a more stable training process and a more\nfaithful generated distribution. Experiments on different datasets verify our\ntheoretical analysis."}, {"title": "On Isometry Robustness of Deep 3D Point Cloud Models Under Adversarial Attacks", "authors": "Yue Zhao, Yuwei Wu, Caihua Chen, Andrew Lim", "link": "https://arxiv.org/abs/2002.12222", "summary": "While deep learning in 3D domain has achieved revolutionary performance in\nmany tasks, the robustness of these models has not been sufficiently studied or\nexplored. Regarding the 3D adversarial samples, most existing works focus on\nmanipulation of local points, which may fail to invoke the global geometry\nproperties, like robustness under linear projection that preserves the\nEuclidean distance, i.e., isometry. In this work, we show that existing\nstate-of-the-art deep 3D models are extremely vulnerable to isometry\ntransformations. Armed with the Thompson Sampling, we develop a black-box\nattack with success rate over 95% on ModelNet40 data set. Incorporating with\nthe Restricted Isometry Property, we propose a novel framework of white-box\nattack on top of spectral norm based perturbation. In contrast to previous\nworks, our adversarial samples are experimentally shown to be strongly\ntransferable. Evaluated on a sequence of prevailing 3D models, our white-box\nattack achieves success rates from 98.88% to 100%. It maintains a successful\nattack rate over 95% even within an imperceptible rotation range $[\\pm\n2.81^{\\circ}]$."}, {"title": "Achieving Robustness in the Wild via Adversarial Mixing With Disentangled Representations", "authors": "Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham, Timothy Mann, Pushmeet Kohli", "link": "https://arxiv.org/abs/1912.03192", "summary": "Recent research has made the surprising finding that state-of-the-art deep\nlearning models sometimes fail to generalize to small variations of the input.\nAdversarial training has been shown to be an effective approach to overcome\nthis problem. However, its application has been limited to enforcing invariance\nto analytically defined transformations like $\\ell_p$-norm bounded\nperturbations. Such perturbations do not necessarily cover plausible real-world\nvariations that preserve the semantics of the input (such as a change in\nlighting conditions). In this paper, we propose a novel approach to express and\nformalize robustness to these kinds of real-world transformations of the input.\nThe two key ideas underlying our formulation are (1) leveraging disentangled\nrepresentations of the input to define different factors of variations, and (2)\ngenerating new input images by adversarially composing the representations of\ndifferent images. We use a StyleGAN model to demonstrate the efficacy of this\nframework. Specifically, we leverage the disentangled latent representations\ncomputed by a StyleGAN model to generate perturbations of an image that are\nsimilar to real-world variations (like adding make-up, or changing the\nskin-tone of a person) and train models to be invariant to these perturbations.\nExtensive experiments show that our method improves generalization and reduces\nthe effect of spurious correlations (reducing the error rate of a \"smile\"\ndetector by 21% for example)."}, {"title": "QEBA: Query-Efficient Boundary-Based Blackbox Attack", "authors": "Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, Bo Li", "link": "https://arxiv.org/abs/2005.14137", "summary": "Machine learning (ML), especially deep neural networks (DNNs) have been\nwidely used in various applications, including several safety-critical ones\n(e.g. autonomous driving). As a result, recent research about adversarial\nexamples has raised great concerns. Such adversarial attacks can be achieved by\nadding a small magnitude of perturbation to the input to mislead model\nprediction. While several whitebox attacks have demonstrated their\neffectiveness, which assume that the attackers have full access to the machine\nlearning models; blackbox attacks are more realistic in practice. In this\npaper, we propose a Query-Efficient Boundary-based blackbox Attack (QEBA) based\nonly on model's final prediction labels. We theoretically show why previous\nboundary-based attack with gradient estimation on the whole gradient space is\nnot efficient in terms of query numbers, and provide optimality analysis for\nour dimension reduction-based gradient estimation. On the other hand, we\nconducted extensive experiments on ImageNet and CelebA datasets to evaluate\nQEBA. We show that compared with the state-of-the-art blackbox attacks, QEBA is\nable to use a smaller number of queries to achieve a lower magnitude of\nperturbation with 100% attack success rate. We also show case studies of\nattacks on real-world APIs including MEGVII Face++ and Microsoft Azure."}, {"title": "Learning to Simulate Dynamic Environments With GameGAN", "authors": "Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler"}, {"title": "Learn2Perturb: An End-to-End Feature Perturbation Learning to Improve Adversarial Robustness", "authors": "Ahmadreza Jeddi, Mohammad Javad Shafiee, Michelle Karg, Christian Scharfenberger, Alexander Wong", "link": "https://arxiv.org/abs/2003.01090", "summary": "While deep neural networks have been achieving state-of-the-art performance\nacross a wide variety of applications, their vulnerability to adversarial\nattacks limits their widespread deployment for safety-critical applications.\nAlongside other adversarial defense approaches being investigated, there has\nbeen a very recent interest in improving adversarial robustness in deep neural\nnetworks through the introduction of perturbations during the training process.\nHowever, such methods leverage fixed, pre-defined perturbations and require\nsignificant hyper-parameter tuning that makes them very difficult to leverage\nin a general fashion. In this study, we introduce Learn2Perturb, an end-to-end\nfeature perturbation learning approach for improving the adversarial robustness\nof deep neural networks. More specifically, we introduce novel\nperturbation-injection modules that are incorporated at each layer to perturb\nthe feature space and increase uncertainty in the network. This feature\nperturbation is performed at both the training and the inference stages.\nFurthermore, inspired by the Expectation-Maximization, an alternating\nback-propagation training algorithm is introduced to train the network and\nnoise parameters consecutively. Experimental results on CIFAR-10 and CIFAR-100\ndatasets show that the proposed Learn2Perturb method can result in deep neural\nnetworks which are $4-7\\%$ more robust on $l_{\\infty}$ FGSM and PDG adversarial\nattacks and significantly outperforms the state-of-the-art against $l_2$ $C\\&W$\nattack and a wide range of well-known black-box attacks."}, {"title": "SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization", "authors": "Yue Jiang, Dantong Ji, Zhizhong Han, Matthias Zwicker", "link": "https://arxiv.org/abs/1912.07109", "summary": "We propose SDFDiff, a novel approach for image-based shape optimization using\ndifferentiable rendering of 3D shapes represented by signed distance functions\n(SDF). Compared to other representations, SDFs have the advantage that they can\nrepresent shapes with arbitrary topology, and that they guarantee watertight\nsurfaces. We apply our approach to the problem of multi-view 3D reconstruction,\nwhere we achieve high reconstruction quality and can capture complex topology\nof 3D objects. In addition, we employ a multi-resolution strategy to obtain a\nrobust optimization algorithm. We further demonstrate that our SDF-based\ndifferentiable renderer can be integrated with deep learning models, which\nopens up options for learning approaches on 3D objects without 3D supervision.\nIn particular, we apply our method to single-view 3D reconstruction and achieve\nstate-of-the-art results."}, {"title": "Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes", "authors": "Zhengqin Li, Yu-Ying Yeh, Manmohan Chandraker", "link": "https://arxiv.org/abs/2004.10904", "summary": "Recovering the 3D shape of transparent objects using a small number of\nunconstrained natural images is an ill-posed problem. Complex light paths\ninduced by refraction and reflection have prevented both traditional and deep\nmultiview stereo from solving this challenge. We propose a physically-based\nnetwork to recover 3D shape of transparent objects using a few images acquired\nwith a mobile phone camera, under a known but arbitrary environment map. Our\nnovel contributions include a normal representation that enables the network to\nmodel complex light transport through local computation, a rendering layer that\nmodels refractions and reflections, a cost volume specifically designed for\nnormal refinement of transparent shapes and a feature mapping based on\npredicted normals for 3D point cloud reconstruction. We render a synthetic\ndataset to encourage the model to learn refractive light transport across\ndifferent views. Our experiments show successful recovery of high-quality 3D\ngeometry for complex transparent shapes using as few as 5-12 natural images.\nCode and data are publicly released."}, {"title": "TextureFusion: High-Quality Texture Acquisition for Real-Time RGB-D Scanning", "authors": "Joo Ho Lee, Hyunho Ha, Yue Dong, Xin Tong, Min H. Kim"}, {"title": "D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry", "authors": "Nan Yang, Lukas von Stumberg, Rui Wang, Daniel Cremers", "link": "https://arxiv.org/abs/2003.01060", "summary": "We propose D3VO as a novel framework for monocular visual odometry that\nexploits deep networks on three levels -- deep depth, pose and uncertainty\nestimation. We first propose a novel self-supervised monocular depth estimation\nnetwork trained on stereo videos without any external supervision. In\nparticular, it aligns the training image pairs into similar lighting condition\nwith predictive brightness transformation parameters. Besides, we model the\nphotometric uncertainties of pixels on the input images, which improves the\ndepth estimation accuracy and provides a learned weighting function for the\nphotometric residuals in direct (feature-less) visual odometry. Evaluation\nresults show that the proposed network outperforms state-of-the-art\nself-supervised depth estimation networks. D3VO tightly incorporates the\npredicted depth, pose and uncertainty into a direct visual odometry method to\nboost both the front-end tracking as well as the back-end non-linear\noptimization. We evaluate D3VO in terms of monocular visual odometry on both\nthe KITTI odometry benchmark and the EuRoC MAV dataset.The results show that\nD3VO outperforms state-of-the-art traditional monocular VO methods by a large\nmargin. It also achieves comparable results to state-of-the-art stereo/LiDAR\nodometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC\nMAV, while using only a single camera."}, {"title": "Deep Implicit Volume Compression", "authors": "Danhang Tang, Saurabh Singh, Philip A. Chou, Christian H\u00e4ne, Mingsong Dou, Sean Fanello, Jonathan Taylor, Philip Davidson, Onur G. Guleryuz, Yinda Zhang, Shahram Izadi, Andrea Tagliasacchi, Sofien Bouaziz, Cem Keskin", "link": "https://arxiv.org/abs/2005.08877", "summary": "We describe a novel approach for compressing truncated signed distance fields\n(TSDF) stored in 3D voxel grids, and their corresponding textures. To compress\nthe TSDF, our method relies on a block-based neural network architecture\ntrained end-to-end, achieving state-of-the-art rate-distortion trade-off. To\nprevent topological errors, we losslessly compress the signs of the TSDF, which\nalso upper bounds the reconstruction error by the voxel size. To compress the\ncorresponding texture, we designed a fast block-based UV parameterization,\ngenerating coherent texture maps that can be effectively compressed using\nexisting video compression algorithms. We demonstrate the performance of our\nalgorithms on two 4D performance capture datasets, reducing bitrate by 66% for\nthe same distortion, or alternatively reducing the distortion by 50% for the\nsame bitrate, compared to the state-of-the-art."}, {"title": "MAGSAC++, a Fast, Reliable and Accurate Robust Estimator", "authors": "D\u00e1niel Bar\u00e1th, Jana Noskova, Maksym Ivashechkin, Ji\u0159\u00ed Matas", "link": "https://arxiv.org/abs/1912.05909", "summary": "A new method for robust estimation, MAGSAC++, is proposed. It introduces a\nnew model quality (scoring) function that does not require the inlier-outlier\ndecision, and a novel marginalization procedure formulated as an iteratively\nre-weighted least-squares approach. We also propose a new sampler, Progressive\nNAPSAC, for RANSAC-like robust estimators. Exploiting the fact that nearby\npoints often originate from the same model in real-world data, it finds local\nstructures earlier than global samplers. The progressive transition from local\nto global sampling does not suffer from the weaknesses of purely localized\nsamplers. On six publicly available real-world datasets for homography and\nfundamental matrix fitting, MAGSAC++ produces results superior to\nstate-of-the-art robust methods. It is faster, more geometrically accurate and\nfails less often."}, {"title": "OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression", "authors": "Lila Huang, Shenlong Wang, Kelvin Wong, Jerry Liu, Raquel Urtasun", "link": "https://arxiv.org/abs/2005.07178", "summary": "We present a novel deep compression algorithm to reduce the memory footprint\nof LiDAR point clouds. Our method exploits the sparsity and structural\nredundancy between points to reduce the bitrate. Towards this goal, we first\nencode the LiDAR points into an octree, a data-efficient structure suitable for\nsparse point clouds. We then design a tree-structured conditional entropy model\nthat models the probabilities of the octree symbols to encode the octree into a\ncompact bitstream. We validate the effectiveness of our method over two\nlarge-scale datasets. The results demonstrate that our approach reduces the\nbitrate by 10-20% at the same reconstruction quality, compared to the previous\nstate-of-the-art. Importantly, we also show that for the same bitrate, our\napproach outperforms other compression algorithms when performing downstream 3D\nsegmentation and detection tasks using compressed representations. Our\nalgorithm can be used to reduce the onboard and offboard storage of LiDAR\npoints for applications such as self-driving cars, where a single vehicle\ncaptures 84 billion points per day"}, {"title": "4D Association Graph for Realtime Multi-Person Motion Capture Using Multiple Video Cameras", "authors": "Yuxiang Zhang, Liang An, Tao Yu, Xiu Li, Kun Li, Yebin Liu", "link": "https://arxiv.org/abs/2002.12625", "summary": "This paper contributes a novel realtime multi-person motion capture algorithm\nusing multiview video inputs. Due to the heavy occlusions in each view, joint\noptimization on the multiview images and multiple temporal frames is\nindispensable, which brings up the essential challenge of realtime efficiency.\nTo this end, for the first time, we unify per-view parsing, cross-view\nmatching, and temporal tracking into a single optimization framework, i.e., a\n4D association graph that each dimension (image space, viewpoint and time) can\nbe treated equally and simultaneously. To solve the 4D association graph\nefficiently, we further contribute the idea of 4D limb bundle parsing based on\nheuristic searching, followed with limb bundle assembling by proposing a bundle\nKruskal's algorithm. Our method enables a realtime online motion capture system\nrunning at 30fps using 5 cameras on a 5-person scene. Benefiting from the\nunified parsing, matching and tracking constraints, our method is robust to\nnoisy detection, and achieves high-quality online pose reconstruction quality.\nThe proposed method outperforms the state-of-the-art method quantitatively\nwithout using high-level appearance information. We also contribute a multiview\nvideo dataset synchronized with a marker-based motion capture system for\nscientific evaluation."}, {"title": "Upgrading Optical Flow to 3D Scene Flow Through Optical Expansion", "authors": "Gengshan Yang, Deva Ramanan"}, {"title": "Robust 3D Self-Portraits in Seconds", "authors": "Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu", "link": "https://arxiv.org/abs/2004.02460", "summary": "In this paper, we propose an efficient method for robust 3D self-portraits\nusing a single RGBD camera. Benefiting from the proposed PIFusion and\nlightweight bundle adjustment algorithm, our method can generate detailed 3D\nself-portraits in seconds and shows the ability to handle subjects wearing\nextremely loose clothes. To achieve highly efficient and robust reconstruction,\nwe propose PIFusion, which combines learning-based 3D recovery with volumetric\nnon-rigid fusion to generate accurate sparse partial scans of the subject.\nMoreover, a non-rigid volumetric deformation method is proposed to continuously\nrefine the learned shape prior. Finally, a lightweight bundle adjustment\nalgorithm is proposed to guarantee that all the partial scans can not only\n\"loop\" with each other but also remain consistent with the selected live key\nobservations. The results and experiments show that the proposed method\nachieves more robust and efficient 3D self-portraits compared with\nstate-of-the-art methods."}, {"title": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow Estimation", "authors": "Matias Tassano, Julie Delon, Thomas Veit", "link": "https://arxiv.org/abs/1907.01361", "summary": "In this paper, we propose a state-of-the-art video denoising algorithm based\non a convolutional neural network architecture. Until recently, video denoising\nwith neural networks had been a largely under explored domain, and existing\nmethods could not compete with the performance of the best patch-based methods.\nThe approach we introduce in this paper, called FastDVDnet, shows similar or\nbetter performance than other state-of-the-art competitors with significantly\nlower computing times. In contrast to other existing neural network denoisers,\nour algorithm exhibits several desirable properties such as fast runtimes, and\nthe ability to handle a wide range of noise levels with a single network model.\nThe characteristics of its architecture make it possible to avoid using a\ncostly motion compensation stage while achieving excellent performance. The\ncombination between its denoising performance and lower computational load\nmakes this algorithm attractive for practical denoising applications. We\ncompare our method with different state-of-art algorithms, both visually and\nwith respect to objective quality metrics."}, {"title": "Learning to Have an Ear for Face Super-Resolution", "authors": "Givi Meishvili, Simon Jenni, Paolo Favaro", "link": "https://arxiv.org/abs/1909.12780", "summary": "We propose a novel method to use both audio and a low-resolution image to\nperform extreme face super-resolution (a 16x increase of the input size). When\nthe resolution of the input image is very low (e.g., 8x8 pixels), the loss of\ninformation is so dire that important details of the original identity have\nbeen lost and audio can aid the recovery of a plausible high-resolution image.\nIn fact, audio carries information about facial attributes, such as gender and\nage. To combine the aural and visual modalities, we propose a method to first\nbuild the latent representations of a face from the lone audio track and then\nfrom the lone low-resolution image. We then train a network to fuse these two\nrepresentations. We show experimentally that audio can assist in recovering\nattributes such as the gender, the age and the identity, and thus improve the\ncorrectness of the high-resolution image reconstruction process. Our procedure\ndoes not make use of human annotation and thus can be easily trained with\nexisting video datasets. Moreover, we show that our model builds a factorized\nrepresentation of images and audio as it allows one to mix low-resolution\nimages and audio from different videos and to generate realistic faces with\nsemantically meaningful combinations."}, {"title": "Deep Optics for Single-Shot High-Dynamic-Range Imaging", "authors": "Christopher A. Metzler, Hayato Ikoma, Yifan Peng, Gordon Wetzstein", "link": "https://arxiv.org/abs/1908.00620", "summary": "High-dynamic-range (HDR) imaging is crucial for many computer graphics and\nvision applications. Yet, acquiring HDR images with a single shot remains a\nchallenging problem. Whereas modern deep learning approaches are successful at\nhallucinating plausible HDR content from a single low-dynamic-range (LDR)\nimage, saturated scene details often cannot be faithfully recovered. Inspired\nby recent deep optical imaging approaches, we interpret this problem as jointly\ntraining an optical encoder and electronic decoder where the encoder is\nparameterized by the point spread function (PSF) of the lens, the bottleneck is\nthe sensor with a limited dynamic range, and the decoder is a convolutional\nneural network (CNN). The lens surface is then jointly optimized with the CNN\nin a training phase; we fabricate this optimized optical element and attach it\nas a hardware add-on to a conventional camera during inference. In extensive\nsimulations and with a physical prototype, we demonstrate that this end-to-end\ndeep optical imaging approach to single-shot HDR imaging outperforms both\npurely CNN-based approaches and other PSF engineering approaches."}, {"title": "Learning Rank-1 Diffractive Optics for Single-Shot High Dynamic Range Imaging", "authors": "Qilin Sun, Ethan Tseng, Qiang Fu, Wolfgang Heidrich, Felix Heide"}, {"title": "Deep White-Balance Editing", "authors": "Mahmoud Afifi, Michael S. Brown", "link": "https://arxiv.org/abs/2004.01354", "summary": "We introduce a deep learning approach to realistically edit an sRGB image's\nwhite balance. Cameras capture sensor images that are rendered by their\nintegrated signal processor (ISP) to a standard RGB (sRGB) color space\nencoding. The ISP rendering begins with a white-balance procedure that is used\nto remove the color cast of the scene's illumination. The ISP then applies a\nseries of nonlinear color manipulations to enhance the visual quality of the\nfinal sRGB image. Recent work by [3] showed that sRGB images that were rendered\nwith the incorrect white balance cannot be easily corrected due to the ISP's\nnonlinear rendering. The work in [3] proposed a k-nearest neighbor (KNN)\nsolution based on tens of thousands of image pairs. We propose to solve this\nproblem with a deep neural network (DNN) architecture trained in an end-to-end\nmanner to learn the correct white balance. Our DNN maps an input image to two\nadditional white-balance settings corresponding to indoor and outdoor\nilluminations. Our solution not only is more accurate than the KNN approach in\nterms of correcting a wrong white-balance setting but also provides the user\nthe freedom to edit the white balance in the sRGB image to other illumination\nsettings."}, {"title": "Non-Line-of-Sight Surface Reconstruction Using the Directional Light-Cone Transform", "authors": "Sean I. Young, David B. Lindell, Bernd Girod, David Taubman, Gordon Wetzstein"}, {"title": "Seeing the World in a Bag of Chips", "authors": "Jeong Joon Park, Aleksander Holynski, Steven M. Seitz", "link": "https://arxiv.org/abs/2001.04642", "summary": "We address the dual problems of novel view synthesis and environment\nreconstruction from hand-held RGBD sensors. Our contributions include 1)\nmodeling highly specular objects, 2) modeling inter-reflections and Fresnel\neffects, and 3) enabling surface light field reconstruction with the same input\nneeded to reconstruct shape alone. In cases where scene surface has a strong\nmirror-like material component, we generate highly detailed environment images,\nrevealing room composition, objects, people, buildings, and trees visible\nthrough windows. Our approach yields state of the art view synthesis\ntechniques, operates on low dynamic range imagery, and is robust to geometric\nand calibration errors."}, {"title": "Correction Filter for Single Image Super-Resolution: Robustifying Off-the-Shelf Deep Super-Resolvers", "authors": "Shady Abu Hussein, Tom Tirer, Raja Giryes", "link": "https://arxiv.org/abs/1912.00157", "summary": "The single image super-resolution task is one of the most examined inverse\nproblems in the past decade. In the recent years, Deep Neural Networks (DNNs)\nhave shown superior performance over alternative methods when the acquisition\nprocess uses a fixed known downsampling kernel-typically a bicubic kernel.\nHowever, several recent works have shown that in practical scenarios, where the\ntest data mismatch the training data (e.g. when the downsampling kernel is not\nthe bicubic kernel or is not available at training), the leading DNN methods\nsuffer from a huge performance drop. Inspired by the literature on generalized\nsampling, in this work we propose a method for improving the performance of\nDNNs that have been trained with a fixed kernel on observations acquired by\nother kernels. For a known kernel, we design a closed-form correction filter\nthat modifies the low-resolution image to match one which is obtained by\nanother kernel (e.g. bicubic), and thus improves the results of existing\npre-trained DNNs. For an unknown kernel, we extend this idea and propose an\nalgorithm for blind estimation of the required correction filter. We show that\nour approach outperforms other super-resolution methods, which are designed for\ngeneral downsampling kernels."}, {"title": "Retina-Like Visual Image Reconstruction via Spiking Neural Model", "authors": "Lin Zhu, Siwei Dong, Jianing Li, Tiejun Huang, Yonghong Tian"}, {"title": "Plug-and-Play Algorithms for Large-Scale Snapshot Compressive Imaging", "authors": "Xin Yuan, Yang Liu, Jinli Suo, Qionghai Dai", "link": "https://arxiv.org/abs/2003.13654", "summary": "Snapshot compressive imaging (SCI) aims to capture the high-dimensional\n(usually 3D) images using a 2D sensor (detector) in a single snapshot. Though\nenjoying the advantages of low-bandwidth, low-power and low-cost, applying SCI\nto large-scale problems (HD or UHD videos) in our daily life is still\nchallenging. The bottleneck lies in the reconstruction algorithms; they are\neither too slow (iterative optimization algorithms) or not flexible to the\nencoding process (deep learning based end-to-end networks). In this paper, we\ndevelop fast and flexible algorithms for SCI based on the plug-and-play (PnP)\nframework. In addition to the widely used PnP-ADMM method, we further propose\nthe PnP-GAP (generalized alternating projection) algorithm with a lower\ncomputational workload and prove the {global convergence} of PnP-GAP under the\nSCI hardware constraints. By employing deep denoising priors, we first time\nshow that PnP can recover a UHD color video ($3840\\times 1644\\times 48$ with\nPNSR above 30dB) from a snapshot 2D measurement. Extensive results on both\nsimulation and real datasets verify the superiority of our proposed algorithm.\nThe code is available at https://github.com/liuyang12/PnP-SCI."}, {"title": "Neural Network Pruning With Residual-Connections and Limited-Data", "authors": "Jian-Hao Luo, Jianxin Wu", "link": "http://arxiv.org/abs/1911.08114", "summary": "Filter level pruning is an effective method to accelerate the inference speed\nof deep CNN models. Although numerous pruning algorithms have been proposed,\nthere are still two open issues. The first problem is how to prune residual\nconnections. We propose to prune both channels inside and outside the residual\nconnections via a KL-divergence based criterion. The second issue is pruning\nwith limited data. We observe an interesting phenomenon: directly pruning on a\nsmall dataset is usually worse than fine-tuning a small model which is pruned\nor trained from scratch on the large dataset. Knowledge distillation is an\neffective approach to compensate for the weakness of limited data. However, the\nlogits of a teacher model may be noisy. In order to avoid the influence of\nlabel noise, we propose a label refinement approach to solve this problem.\nExperiments have demonstrated the effectiveness of our method (CURL,\nCompression Using Residual-connections and Limited-data). CURL significantly\noutperforms previous state-of-the-art methods on ImageNet. More importantly,\nwhen pruning on small datasets, CURL achieves comparable or much better\nperformance than fine-tuning a pretrained small model."}, {"title": "AdderNet: Do We Really Need Multiplications in Deep Learning?", "authors": "Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, Chang Xu", "link": "https://arxiv.org/abs/1912.13200", "summary": "Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. The influence of this new similarity measure on\nthe optimization of neural network have been thoroughly analyzed. To achieve a\nbetter performance, we develop a special back-propagation approach for\nAdderNets by investigating the full-precision gradient. We then propose an\nadaptive learning rate strategy to enhance the training procedure of AdderNets\naccording to the magnitude of each neuron's gradient. As a result, the proposed\nAdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50\non the ImageNet dataset without any multiplication in convolution layer."}, {"title": "NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks", "authors": "Eugene Lee, Chen-Yi Lee"}, {"title": "Training Quantized Neural Networks With a Full-Precision Auxiliary Module", "authors": "Bohan Zhuang, Lingqiao Liu, Mingkui Tan, Chunhua Shen, Ian Reid", "link": "https://arxiv.org/abs/1903.11236", "summary": "In this paper, we seek to tackle a challenge in training low-precision\nnetworks: the notorious difficulty in propagating gradient through a\nlow-precision network due to the non-differentiable quantization function. We\npropose a solution by training the low-precision network with a fullprecision\nauxiliary module. Specifically, during training, we construct a mix-precision\nnetwork by augmenting the original low-precision network with the full\nprecision auxiliary module. Then the augmented mix-precision network and the\nlow-precision network are jointly optimized. This strategy creates additional\nfull-precision routes to update the parameters of the low-precision model, thus\nmaking the gradient back-propagates more easily. At the inference time, we\ndiscard the auxiliary module without introducing any computational complexity\nto the low-precision network. We evaluate the proposed method on image\nclassification and object detection over various quantization approaches and\nshow consistent performance increase. In particular, we achieve near lossless\nperformance to the full-precision model by using a 4-bit detector, which is of\ngreat practical value."}, {"title": "Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation From a Blackbox Model", "authors": "Dongdong Wang, Yandong Li, Liqiang Wang, Boqing Gong", "link": "https://arxiv.org/abs/2003.13960", "summary": "We study how to train a student deep neural network for visual recognition by\ndistilling knowledge from a blackbox teacher model in a data-efficient manner.\nProgress on this problem can significantly reduce the dependence on large-scale\ndatasets for learning high-performing visual recognition models. There are two\nmajor challenges. One is that the number of queries into the teacher model\nshould be minimized to save computational and/or financial costs. The other is\nthat the number of images used for the knowledge distillation should be small;\notherwise, it violates our expectation of reducing the dependence on\nlarge-scale datasets. To tackle these challenges, we propose an approach that\nblends mixup and active learning. The former effectively augments the few\nunlabeled images by a big pool of synthetic images sampled from the convex hull\nof the original images, and the latter actively chooses from the pool hard\nexamples for the student neural network and query their labels from the teacher\nmodel. We validate our approach with extensive experiments."}, {"title": "Multi-Dimensional Pruning: A Unified Framework for Model Compression", "authors": "Jinyang Guo, Wanli Ouyang, Dong Xu", "link": "", "summary": ""}, {"title": "Towards Efficient Model Compression via Learned Global Ranking", "authors": "Ting-Wu Chin, Ruizhou Ding, Cha Zhang, Diana Marculescu", "link": "https://arxiv.org/abs/1904.12368", "summary": "Pruning convolutional filters has demonstrated its effectiveness in\ncompressing ConvNets. Prior art in filter pruning requires users to specify a\ntarget model complexity (e.g., model size or FLOP count) for the resulting\narchitecture. However, determining a target model complexity can be difficult\nfor optimizing various embodied AI applications such as autonomous robots,\ndrones, and user-facing applications. First, both the accuracy and the speed of\nConvNets can affect the performance of the application. Second, the performance\nof the application can be hard to assess without evaluating ConvNets during\ninference. As a consequence, finding a sweet-spot between the accuracy and\nspeed via filter pruning, which needs to be done in a trial-and-error fashion,\ncan be time-consuming. This work takes a first step toward making this process\nmore efficient by altering the goal of model compression to producing a set of\nConvNets with various accuracy and latency trade-offs instead of producing one\nConvNet targeting some pre-defined latency constraint. To this end, we propose\nto learn a global ranking of the filters across different layers of the\nConvNet, which is used to obtain a set of ConvNet architectures that have\ndifferent accuracy/latency trade-offs by pruning the bottom-ranked filters. Our\nproposed algorithm, LeGR, is shown to be 2x to 3x faster than prior work while\nhaving comparable or better performance when targeting seven pruned ResNet-56\nwith different accuracy/FLOPs profiles on the CIFAR-100 dataset. Additionally,\nwe have evaluated LeGR on ImageNet and Bird-200 with ResNet-50 and MobileNetV2\nto demonstrate its effectiveness. Code available at\nhttps://github.com/cmu-enyac/LeGR."}, {"title": "HRank: Filter Pruning Using High-Rank Feature Map", "authors": "Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, Ling Shao", "link": "https://arxiv.org/abs/2002.10179", "summary": "Neural network pruning offers a promising prospect to facilitate deploying\ndeep neural networks on resource-limited devices. However, existing methods are\nstill challenged by the training inefficiency and labor cost in pruning\ndesigns, due to missing theoretical guidance of non-salient network components.\nIn this paper, we propose a novel filter pruning method by exploring the High\nRank of feature maps (HRank). Our HRank is inspired by the discovery that the\naverage rank of multiple feature maps generated by a single filter is always\nthe same, regardless of the number of image batches CNNs receive. Based on\nHRank, we develop a method that is mathematically formulated to prune filters\nwith low-rank feature maps. The principle behind our pruning is that low-rank\nfeature maps contain less information, and thus pruned results can be easily\nreproduced. Besides, we experimentally show that weights with high-rank feature\nmaps contain more important information, such that even when a portion is not\nupdated, very little damage would be done to the model performance. Without\nintroducing any additional constraints, HRank leads to significant improvements\nover the state-of-the-arts in terms of FLOPs and parameters reduction, with\nsimilar accuracies. For example, with ResNet-110, we achieve a 58.2%-FLOPs\nreduction by removing 59.2% of the parameters, with only a small loss of 0.14%\nin top-1 accuracy on CIFAR-10. With Res-50, we achieve a 43.8%-FLOPs reduction\nby removing 36.7% of the parameters, with only a loss of 1.17% in the top-1\naccuracy on ImageNet. The codes can be available at\nhttps://github.com/lmbxmu/HRank."}, {"title": "DMCP: Differentiable Markov Channel Pruning for Neural Networks", "authors": "Shaopeng Guo, Yujie Wang, Quanquan Li, Junjie Yan", "link": "http://arxiv.org/abs/2005.03354", "summary": "Recent works imply that the channel pruning can be regarded as searching\noptimal sub-structure from unpruned networks. However, existing works based on\nthis observation require training and evaluating a large number of structures,\nwhich limits their application. In this paper, we propose a novel\ndifferentiable method for channel pruning, named Differentiable Markov Channel\nPruning (DMCP), to efficiently search the optimal sub-structure. Our method is\ndifferentiable and can be directly optimized by gradient descent with respect\nto standard task loss and budget regularization (e.g. FLOPs constraint). In\nDMCP, we model the channel pruning as a Markov process, in which each state\nrepresents for retaining the corresponding channel during pruning, and\ntransitions between states denote the pruning process. In the end, our method\nis able to implicitly select the proper number of channels in each layer by the\nMarkov process with optimized transitions. To validate the effectiveness of our\nmethod, we perform extensive experiments on Imagenet with ResNet and\nMobilenetV2. Results show our method can achieve consistent improvement than\nstate-of-the-art pruning methods in various FLOPs settings. The code is\navailable at https://github.com/zx55/dmcp"}, {"title": "ReSprop: Reuse Sparsified Backpropagation", "authors": "Negar Goli, Tor M. Aamodt", "link": "", "summary": ""}, {"title": "Adversarial Texture Optimization From RGB-D Scans", "authors": "Jingwei Huang, Justus Thies, Angela Dai, Abhijit Kundu, Chiyu \"Max\" Jiang, Leonidas J. Guibas, Matthias Nie\u00dfner, Thomas Funkhouser", "link": "https://arxiv.org/abs/2003.08400", "summary": "Realistic color texture generation is an important step in RGB-D surface\nreconstruction, but remains challenging in practice due to inaccuracies in\nreconstructed geometry, misaligned camera poses, and view-dependent imaging\nartifacts.\n  In this work, we present a novel approach for color texture generation using\na conditional adversarial loss obtained from weakly-supervised views.\n  Specifically, we propose an approach to produce photorealistic textures for\napproximate surfaces, even from misaligned images, by learning an objective\nfunction that is robust to these errors.\n  The key idea of our approach is to learn a patch-based conditional\ndiscriminator which guides the texture optimization to be tolerant to\nmisalignments.\n  Our discriminator takes a synthesized view and a real image, and evaluates\nwhether the synthesized one is realistic, under a broadened definition of\nrealism.\n  We train the discriminator by providing as `real' examples pairs of input\nviews and their misaligned versions -- so that the learned adversarial loss\nwill tolerate errors from the scans.\n  Experiments on synthetic and real data under quantitative or qualitative\nevaluation demonstrate the advantage of our approach in comparison to state of\nthe art. Our code is publicly available with video demonstration."}, {"title": "Synchronizing Probability Measures on Rotations via Optimal Transport", "authors": "Tolga Birdal, Michael Arbel, Umut \u015eim\u015fekli, Leonidas J. Guibas", "link": "http://arxiv.org/abs/2004.00663", "summary": "We introduce a new paradigm, $\\textit{measure synchronization}$, for\nsynchronizing graphs with measure-valued edges. We formulate this problem as\nmaximization of the cycle-consistency in the space of probability measures over\nrelative rotations. In particular, we aim at estimating marginal distributions\nof absolute orientations by synchronizing the $\\textit{conditional}$ ones,\nwhich are defined on the Riemannian manifold of quaternions. Such graph\noptimization on distributions-on-manifolds enables a natural treatment of\nmultimodal hypotheses, ambiguities and uncertainties arising in many computer\nvision applications such as SLAM, SfM, and object pose estimation. We first\nformally define the problem as a generalization of the classical rotation graph\nsynchronization, where in our case the vertices denote probability measures\nover rotations. We then measure the quality of the synchronization by using\nSinkhorn divergences, which reduces to other popular metrics such as\nWasserstein distance or the maximum mean discrepancy as limit cases. We propose\na nonparametric Riemannian particle optimization approach to solve the problem.\nEven though the problem is non-convex, by drawing a connection to the recently\nproposed sparse optimization methods, we show that the proposed algorithm\nconverges to the global optimum in a special case of the problem under certain\nconditions. Our qualitative and quantitative experiments show the validity of\nour approach and we bring in new perspectives to the study of synchronization."}, {"title": "GhostNet: More Features From Cheap Operations", "authors": "Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu", "link": "https://arxiv.org/abs/1911.11907", "summary": "Deploying convolutional neural networks (CNNs) on embedded devices is\ndifficult due to the limited memory and computation resources. The redundancy\nin feature maps is an important characteristic of those successful CNNs, but\nhas rarely been investigated in neural architecture design. This paper proposes\na novel Ghost module to generate more feature maps from cheap operations. Based\non a set of intrinsic feature maps, we apply a series of linear transformations\nwith cheap cost to generate many ghost feature maps that could fully reveal\ninformation underlying intrinsic features. The proposed Ghost module can be\ntaken as a plug-and-play component to upgrade existing convolutional neural\nnetworks. Ghost bottlenecks are designed to stack Ghost modules, and then the\nlightweight GhostNet can be easily established. Experiments conducted on\nbenchmarks demonstrate that the proposed Ghost module is an impressive\nalternative of convolution layers in baseline models, and our GhostNet can\nachieve higher recognition performance (e.g. $75.7\\%$ top-1 accuracy) than\nMobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012\nclassification dataset. Code is available at\nhttps://github.com/huawei-noah/ghostnet"}, {"title": "Attention-Aware Multi-View Stereo", "authors": "Keyang Luo, Tao Guan, Lili Ju, Yuesong Wang, Zhuo Chen, Yawei Luo"}, {"title": "Bi3D: Stereo Depth Estimation via Binary Classifications", "authors": "Abhishek Badki, Alejandro Troccoli, Kihwan Kim, Jan Kautz, Pradeep Sen, Orazio Gallo", "link": "https://arxiv.org/abs/2005.07274", "summary": "Stereo-based depth estimation is a cornerstone of computer vision, with\nstate-of-the-art methods delivering accurate results in real time. For several\napplications such as autonomous navigation, however, it may be useful to trade\naccuracy for lower latency. We present Bi3D, a method that estimates depth via\na series of binary classifications. Rather than testing if objects are at a\nparticular depth $D$, as existing stereo methods do, it classifies them as\nbeing closer or farther than $D$. This property offers a powerful mechanism to\nbalance accuracy and latency. Given a strict time budget, Bi3D can detect\nobjects closer than a given distance in as little as a few milliseconds, or\nestimate depth with arbitrarily coarse quantization, with complexity linear\nwith the number of quantization levels. Bi3D can also use the allotted\nquantization levels to get continuous depth, but in a specific depth range. For\nstandard stereo (i.e., continuous depth on the whole range), our method is\nclose to or on par with state-of-the-art, finely tuned stereo methods."}, {"title": "Joint Filtering of Intensity Images and Neuromorphic Events for High-Resolution Noise-Robust Imaging", "authors": "Zihao W. Wang, Peiqi Duan, Oliver Cossairt, Aggelos Katsaggelos, Tiejun Huang, Boxin Shi"}, {"title": "SGAS: Sequential Greedy Architecture Search", "authors": "Guohao Li, Guocheng Qian, Itzel C. Delgadillo, Matthias M\u00fcller, Ali Thabet, Bernard Ghanem", "link": "https://arxiv.org/abs/1912.00195", "summary": "Architecture design has become a crucial component of successful deep\nlearning. Recent progress in automatic neural architecture search (NAS) shows a\nlot of promise. However, discovered architectures often fail to generalize in\nthe final evaluation. Architectures with a higher validation accuracy during\nthe search phase may perform worse in the evaluation. Aiming to alleviate this\ncommon issue, we introduce sequential greedy architecture search (SGAS), an\nefficient method for neural architecture search. By dividing the search\nprocedure into sub-problems, SGAS chooses and prunes candidate operations in a\ngreedy fashion. We apply SGAS to search architectures for Convolutional Neural\nNetworks (CNN) and Graph Convolutional Networks (GCN). Extensive experiments\nshow that SGAS is able to find state-of-the-art architectures for tasks such as\nimage classification, point cloud classification and node classification in\nprotein-protein interaction graphs with minimal computational cost. Please\nvisit https://www.deepgcns.org/auto/sgas for more information about SGAS."}, {"title": "HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection", "authors": "Maosheng Ye, Shuangjie Xu, Tongyi Cao", "link": "", "summary": ""}, {"title": "Frequency Domain Compact 3D Convolutional Neural Networks", "authors": "Hanting Chen, Yunhe Wang, Han Shu, Yehui Tang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, Chang Xu"}, {"title": "Single-Image HDR Reconstruction by Learning to Reverse the Camera Pipeline", "authors": "Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang", "link": "https://arxiv.org/abs/2004.01179", "summary": "Recovering a high dynamic range (HDR) image from a single low dynamic range\n(LDR) input image is challenging due to missing details in under-/over-exposed\nregions caused by quantization and saturation of camera sensors. In contrast to\nexisting learning-based methods, our core idea is to incorporate the domain\nknowledge of the LDR image formation pipeline into our model. We model the\nHDRto-LDR image formation pipeline as the (1) dynamic range clipping, (2)\nnon-linear mapping from a camera response function, and (3) quantization. We\nthen propose to learn three specialized CNNs to reverse these steps. By\ndecomposing the problem into specific sub-tasks, we impose effective physical\nconstraints to facilitate the training of individual sub-networks. Finally, we\njointly fine-tune the entire model end-to-end to reduce error accumulation.\nWith extensive quantitative and qualitative experiments on diverse image\ndatasets, we demonstrate that the proposed method performs favorably against\nstate-of-the-art single-image HDR reconstruction algorithms."}, {"title": "DNU: Deep Non-Local Unrolling for Computational Spectral Imaging", "authors": "Lizhi Wang, Chen Sun, Maoqing Zhang, Ying Fu, Hua Huang"}, {"title": "Single Image Optical Flow Estimation With an Event Camera", "authors": "Liyuan Pan, Miaomiao Liu, Richard Hartley", "link": "https://arxiv.org/abs/2004.00347", "summary": "Event cameras are bio-inspired sensors that asynchronously report intensity\nchanges in microsecond resolution. DAVIS can capture high dynamics of a scene\nand simultaneously output high temporal resolution events and low frame-rate\nintensity images. In this paper, we propose a single image (potentially\nblurred) and events based optical flow estimation approach. First, we\ndemonstrate how events can be used to improve flow estimates. To this end, we\nencode the relation between flow and events effectively by presenting an\nevent-based photometric consistency formulation. Then, we consider the special\ncase of image blur caused by high dynamics in the visual environments and show\nthat including the blur formation in our model further constrains flow\nestimation. This is in sharp contrast to existing works that ignore the blurred\nimages while our formulation can naturally handle either blurred or sharp\nimages to achieve accurate flow estimation. Finally, we reduce flow estimation,\nas well as image deblurring, to an alternative optimization problem of an\nobjective function using the primal-dual algorithm. Experimental results on\nboth synthetic and real data (with blurred and non-blurred images) show the\nsuperiority of our model in comparison to state-of-the-art approaches."}, {"title": "Multi-View Neural Human Rendering", "authors": "Minye Wu, Yuehao Wang, Qiang Hu, Jingyi Yu", "link": "", "summary": ""}, {"title": "Depth Sensing Beyond LiDAR Range", "authors": "Kai Zhang, Jiaxin Xie, Noah Snavely, Qifeng Chen", "link": "", "summary": ""}, {"title": "Event Probability Mask (EPM) and Event Denoising Convolutional Neural Network (EDnCNN) for Neuromorphic Cameras", "authors": "R. Wes Baldwin, Mohammed Almatrafi, Vijayan Asari, Keigo Hirakawa", "link": "https://arxiv.org/abs/2003.08282", "summary": "This paper presents a novel method for labeling real-world neuromorphic\ncamera sensor data by calculating the likelihood of generating an event at each\npixel within a short time window, which we refer to as \"event probability mask\"\nor EPM. Its applications include (i) objective benchmarking of event denoising\nperformance, (ii) training convolutional neural networks for noise removal\ncalled \"event denoising convolutional neural network\" (EDnCNN), and (iii)\nestimating internal neuromorphic camera parameters. We provide the first\ndataset (DVSNOISE20) of real-world labeled neuromorphic camera events for noise\nremoval."}, {"title": "Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud", "authors": "Weijing Shi, Raj Rajkumar", "link": "https://arxiv.org/abs/2003.01251", "summary": "In this paper, we propose a graph neural network to detect objects from a\nLiDAR point cloud. Towards this end, we encode the point cloud efficiently in a\nfixed radius near-neighbors graph. We design a graph neural network, named\nPoint-GNN, to predict the category and shape of the object that each vertex in\nthe graph belongs to. In Point-GNN, we propose an auto-registration mechanism\nto reduce translation variance, and also design a box merging and scoring\noperation to combine detections from multiple vertices accurately. Our\nexperiments on the KITTI benchmark show the proposed approach achieves leading\naccuracy using the point cloud alone and can even surpass fusion-based\nalgorithms. Our results demonstrate the potential of using the graph neural\nnetwork as a new approach for 3D object detection. The code is available\nhttps://github.com/WeijingShi/Point-GNN."}, {"title": "Self-Learning Video Rain Streak Removal: When Cyclic Consistency Meets Temporal Correspondence", "authors": "Wenhan Yang, Robby T. Tan, Shiqi Wang, Jiaying Liu"}, {"title": "Neuromorphic Camera Guided High Dynamic Range Imaging", "authors": "Jin Han, Chu Zhou, Peiqi Duan, Yehui Tang, Chang Xu, Chao Xu, Tiejun Huang, Boxin Shi"}, {"title": "Learning in the Frequency Domain", "authors": "Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, Fengbo Ren", "link": "https://arxiv.org/abs/2002.12416", "summary": "Deep neural networks have achieved remarkable success in computer vision\ntasks. Existing neural networks mainly operate in the spatial domain with fixed\ninput sizes. For practical applications, images are usually large and have to\nbe downsampled to the predetermined input size of neural networks. Even though\nthe downsampling operations reduce computation and the required communication\nbandwidth, it removes both redundant and salient information obliviously, which\nresults in accuracy degradation. Inspired by digital signal processing\ntheories, we analyze the spectral bias from the frequency perspective and\npropose a learning-based frequency selection method to identify the trivial\nfrequency components which can be removed without accuracy loss. The proposed\nmethod of learning in the frequency domain leverages identical structures of\nthe well-known neural networks, such as ResNet-50, MobileNetV2, and Mask R-CNN,\nwhile accepting the frequency-domain information as the input. Experiment\nresults show that learning in the frequency domain with static channel\nselection can achieve higher accuracy than the conventional spatial\ndownsampling approach and meanwhile further reduce the input data size.\nSpecifically for ImageNet classification with the same input size, the proposed\nmethod achieves 1.41% and 0.66% top-1 accuracy improvements on ResNet-50 and\nMobileNetV2, respectively. Even with half input size, the proposed method still\nimproves the top-1 accuracy on ResNet-50 by 1%. In addition, we observe a 0.8%\naverage precision improvement on Mask R-CNN for instance segmentation on the\nCOCO dataset."}, {"title": "Polarized Reflection Removal With Perfect Alignment in the Wild", "authors": "Chenyang Lei, Xuhua Huang, Mengdi Zhang, Qiong Yan, Wenxiu Sun, Qifeng Chen", "link": "https://arxiv.org/abs/2003.12789", "summary": "We present a novel formulation to removing reflection from polarized images\nin the wild. We first identify the misalignment issues of existing reflection\nremoval datasets where the collected reflection-free images are not perfectly\naligned with input mixed images due to glass refraction. Then we build a new\ndataset with more than 100 types of glass in which obtained transmission images\nare perfectly aligned with input mixed images. Second, capitalizing on the\nspecial relationship between reflection and polarized light, we propose a\npolarized reflection removal model with a two-stage architecture. In addition,\nwe design a novel perceptual NCC loss that can improve the performance of\nreflection removal and general image decomposition tasks. We conduct extensive\nexperiments, and results suggest that our model outperforms state-of-the-art\nmethods on reflection removal."}, {"title": "Learning Multiview 3D Point Cloud Registration", "authors": "Zan Gojcic, Caifa Zhou, Jan D. Wegner, Leonidas J. Guibas, Tolga Birdal", "link": "https://arxiv.org/abs/2001.05119", "summary": "We present a novel, end-to-end learnable, multiview 3D point cloud\nregistration algorithm. Registration of multiple scans typically follows a\ntwo-stage pipeline: the initial pairwise alignment and the globally consistent\nrefinement. The former is often ambiguous due to the low overlap of neighboring\npoint clouds, symmetries and repetitive scene parts. Therefore, the latter\nglobal refinement aims at establishing the cyclic consistency across multiple\nscans and helps in resolving the ambiguous cases. In this paper we propose, to\nthe best of our knowledge, the first end-to-end algorithm for joint learning of\nboth parts of this two-stage problem. Experimental evaluation on well accepted\nbenchmark datasets shows that our approach outperforms the state-of-the-art by\na significant margin, while being end-to-end trainable and computationally less\ncostly. Moreover, we present detailed analysis and an ablation study that\nvalidate the novel components of our approach. The source code and pretrained\nmodels are publicly available under\nhttps://github.com/zgojcic/3D_multiview_reg."}, {"title": "A Sparse Resultant Based Method for Efficient Minimal Solvers", "authors": "Snehal Bhayani, Zuzana Kukelova, Janne Heikkil\u00e4", "link": "https://arxiv.org/abs/1912.10268", "summary": "Many computer vision applications require robust and efficient estimation of\ncamera geometry. The robust estimation is usually based on solving camera\ngeometry problems from a minimal number of input data measurements, i.e.\nsolving minimal problems in a RANSAC framework. Minimal problems often result\nin complex systems of polynomial equations. Many state-of-the-art efficient\npolynomial solvers to these problems are based on Gr\\\"obner bases and the\naction-matrix method that has been automatized and highly optimized in recent\nyears. In this paper we study an alternative algebraic method for solving\nsystems of polynomial equations, i.e., the sparse resultant-based method and\npropose a novel approach to convert the resultant constraint to an eigenvalue\nproblem. This technique can significantly improve the efficiency and stability\nof existing resultant-based solvers. We applied our new resultant-based method\nto a large variety of computer vision problems and show that for most of the\nconsidered problems, the new method leads to solvers that are the same size as\nthe the best available Gr\\\"obner basis solvers and of similar accuracy. For\nsome problems the new sparse-resultant based method leads to even smaller and\nmore stable solvers than the state-of-the-art Gr\\\"obner basis solvers. Our new\nmethod can be fully automatized and incorporated into existing tools for\nautomatic generation of efficient polynomial solvers and as such it represents\na competitive alternative to popular Gr\\\"obner basis methods for minimal\nproblems in computer vision."}, {"title": "Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement", "authors": "Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, Runmin Cong", "link": "https://arxiv.org/abs/2001.06826", "summary": "The paper presents a novel method, Zero-Reference Deep Curve Estimation\n(Zero-DCE), which formulates light enhancement as a task of image-specific\ncurve estimation with a deep network. Our method trains a lightweight deep\nnetwork, DCE-Net, to estimate pixel-wise and high-order curves for dynamic\nrange adjustment of a given image. The curve estimation is specially designed,\nconsidering pixel value range, monotonicity, and differentiability. Zero-DCE is\nappealing in its relaxed assumption on reference images, i.e., it does not\nrequire any paired or unpaired data during training. This is achieved through a\nset of carefully formulated non-reference loss functions, which implicitly\nmeasure the enhancement quality and drive the learning of the network. Our\nmethod is efficient as image enhancement can be achieved by an intuitive and\nsimple nonlinear curve mapping. Despite its simplicity, we show that it\ngeneralizes well to diverse lighting conditions. Extensive experiments on\nvarious benchmarks demonstrate the advantages of our method over\nstate-of-the-art methods qualitatively and quantitatively. Furthermore, the\npotential benefits of our Zero-DCE to face detection in the dark are discussed.\nCode and model will be available at https://github.com/Li-Chongyi/Zero-DCE."}, {"title": "BlendedMVS: A Large-Scale Dataset for Generalized Multi-View Stereo Networks", "authors": "Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, Long Quan", "link": "https://arxiv.org/abs/1911.10127", "summary": "While deep learning has recently achieved great success on multi-view stereo\n(MVS), limited training data makes the trained model hard to be generalized to\nunseen scenarios. Compared with other computer vision tasks, it is rather\ndifficult to collect a large-scale MVS dataset as it requires expensive active\nscanners and labor-intensive process to obtain ground truth 3D structures. In\nthis paper, we introduce BlendedMVS, a novel large-scale dataset, to provide\nsufficient training ground truth for learning-based MVS. To create the dataset,\nwe apply a 3D reconstruction pipeline to recover high-quality textured meshes\nfrom images of well-selected scenes. Then, we render these mesh models to color\nimages and depth maps. To introduce the ambient lighting information during\ntraining, the rendered color images are further blended with the input images\nto generate the training input. Our dataset contains over 17k high-resolution\nimages covering a variety of scenes, including cities, architectures,\nsculptures and small objects. Extensive experiments demonstrate that BlendedMVS\nendows the trained model with significantly better generalization ability\ncompared with other MVS datasets. The dataset and pretrained models are\navailable at \\url{https://github.com/YoYo000/BlendedMVS}."}, {"title": "Convolution in the Cloud: Learning Deformable Kernels in 3D Graph Convolution Networks for Point Cloud Analysis", "authors": "Zhi-Hao Lin, Sheng-Yu Huang, Yu-Chiang Frank Wang"}, {"title": "A Semi-Supervised Assessor of Neural Architectures", "authors": "Yehui Tang, Yunhe Wang, Yixing Xu, Hanting Chen, Boxin Shi, Chao Xu, Chunjing Xu, Qi Tian, Chang Xu", "link": "https://arxiv.org/abs/2005.06821", "summary": "Neural architecture search (NAS) aims to automatically design deep neural\nnetworks of satisfactory performance. Wherein, architecture performance\npredictor is critical to efficiently value an intermediate neural architecture.\nBut for the training of this predictor, a number of neural architectures and\ntheir corresponding real performance often have to be collected. In contrast\nwith classical performance predictor optimized in a fully supervised way, this\npaper suggests a semi-supervised assessor of neural architectures. We employ an\nauto-encoder to discover meaningful representations of neural architectures.\nTaking each neural architecture as an individual instance in the search space,\nwe construct a graph to capture their intrinsic similarities, where both\nlabeled and unlabeled architectures are involved. A graph convolutional neural\nnetwork is introduced to predict the performance of architectures based on the\nlearned representations and their relation modeled by the graph. Extensive\nexperimental results on the NAS-Benchmark-101 dataset demonstrated that our\nmethod is able to make a significant reduction on the required fully trained\narchitectures for finding efficient architectures."}, {"title": "Learning a Reinforced Agent for Flexible Exposure Bracketing Selection", "authors": "Zhouxia Wang, Jiawei Zhang, Mude Lin, Jiong Wang, Ping Luo, Jimmy Ren", "link": "https://arxiv.org/abs/2005.12536", "summary": "Automatically selecting exposure bracketing (images exposed differently) is\nimportant to obtain a high dynamic range image by using multi-exposure fusion.\nUnlike previous methods that have many restrictions such as requiring camera\nresponse function, sensor noise model, and a stream of preview images with\ndifferent exposures (not accessible in some scenarios e.g. some mobile\napplications), we propose a novel deep neural network to automatically select\nexposure bracketing, named EBSNet, which is sufficiently flexible without\nhaving the above restrictions. EBSNet is formulated as a reinforced agent that\nis trained by maximizing rewards provided by a multi-exposure fusion network\n(MEFNet). By utilizing the illumination and semantic information extracted from\njust a single auto-exposure preview image, EBSNet can select an optimal\nexposure bracketing for multi-exposure fusion. EBSNet and MEFNet can be jointly\ntrained to produce favorable results against recent state-of-the-art\napproaches. To facilitate future research, we provide a new benchmark dataset\nfor multi-exposure selection and fusion."}, {"title": "CARS: Continuous Evolution for Efficient Neural Architecture Search", "authors": "Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing Xu, Qi Tian, Chang Xu", "link": "http://arxiv.org/abs/1909.04977", "summary": "Searching techniques in most of existing neural architecture search (NAS)\nalgorithms are mainly dominated by differentiable methods for the efficiency\nreason. In contrast, we develop an efficient continuous evolutionary approach\nfor searching neural networks. Architectures in the population that share\nparameters within one SuperNet in the latest generation will be tuned over the\ntraining dataset with a few epochs. The searching in the next evolution\ngeneration will directly inherit both the SuperNet and the population, which\naccelerates the optimal network generation. The non-dominated sorting strategy\nis further applied to preserve only results on the Pareto front for accurately\nupdating the SuperNet. Several neural networks with different model sizes and\nperformances will be produced after the continuous search with only 0.4 GPU\ndays. As a result, our framework provides a series of networks with the number\nof parameters ranging from 3.7M to 5.1M under mobile settings. These networks\nsurpass those produced by the state-of-the-art methods on the benchmark\nImageNet dataset."}, {"title": "Joint 3D Instance Segmentation and Object Detection for Autonomous Driving", "authors": "Dingfu Zhou, Jin Fang, Xibin Song, Liu Liu, Junbo Yin, Yuchao Dai, Hongdong Li, Ruigang Yang"}, {"title": "View-GCN: View-Based Graph Convolutional Network for 3D Shape Analysis", "authors": "Xin Wei, Ruixuan Yu, Jian Sun"}, {"title": "Collaborative Distillation for Ultra-Resolution Universal Style Transfer", "authors": "Huan Wang, Yijun Li, Yuehai Wang, Haoji Hu, Ming-Hsuan Yang", "link": "https://arxiv.org/abs/2003.08436", "summary": "Universal style transfer methods typically leverage rich representations from\ndeep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on\nlarge collections of images. Despite the effectiveness, its application is\nheavily constrained by the large model size to handle ultra-resolution images\ngiven limited memory. In this work, we present a new knowledge distillation\nmethod (named Collaborative Distillation) for encoder-decoder based neural\nstyle transfer to reduce the convolutional filters. The main idea is\nunderpinned by a finding that the encoder-decoder pairs construct an exclusive\ncollaborative relationship, which is regarded as a new kind of knowledge for\nstyle transfer models. Moreover, to overcome the feature size mismatch when\napplying collaborative distillation, a linear embedding loss is introduced to\ndrive the student network to learn a linear embedding of the teacher's\nfeatures. Extensive experiments show the effectiveness of our method when\napplied to different universal style transfer approaches (WCT and AdaIN), even\nif the model size is reduced by 15.5 times. Especially, on WCT with the\ncompressed models, we achieve ultra-resolution (over 40 megapixels) universal\nstyle transfer on a 12GB GPU for the first time. Further experiments on\noptimization-based stylization scheme show the generality of our algorithm on\ndifferent stylization paradigms. Our code and trained models are available at\nhttps://github.com/mingsun-tse/collaborative-distillation."}, {"title": "TomoFluid: Reconstructing Dynamic Fluid From Sparse View Videos", "authors": "Guangming Zang, Ramzi Idoughi, Congli Wang, Anthony Bennett, Jianguo Du, Scott Skeen, William L. Roberts, Peter Wonka, Wolfgang Heidrich"}, {"title": "Instance Shadow Detection", "authors": "Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, Chi-Wing Fu", "link": "https://arxiv.org/abs/1911.07034", "summary": "Instance shadow detection is a brand new problem, aiming to find shadow\ninstances paired with object instances. To approach it, we first prepare a new\ndataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of\nshadow and object instances in 1,000 photos, each with individual labeled\nmasks. Second, we design LISA, named after Light-guided Instance Shadow-object\nAssociation, an end-to-end framework to automatically predict the shadow and\nobject instances, together with the shadow-object associations and light\ndirection. Then, we pair up the predicted shadow and object instances, and\nmatch them with the predicted shadow-object associations to generate the final\nresults. In our evaluations, we formulate a new metric named the shadow-object\naverage precision to measure the performance of our results. Further, we\nconducted various experiments and demonstrate our method's applicability on\nlight direction estimation and photo editing."}, {"title": "Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image", "authors": "Yuhui Quan, Mingqin Chen, Tongyao Pang, Hui Ji", "link": "", "summary": ""}, {"title": "Discrete Model Compression With Resource Constraint for Deep Neural Networks", "authors": "Shangqian Gao, Feihu Huang, Jian Pei, Heng Huang"}, {"title": "Structured Compression by Weight Encryption for Unstructured Pruning and Quantization", "authors": "Se Jung Kwon, Dongsoo Lee, Byeongwook Kim, Parichay Kapoor, Baeseong Park, Gu-Yeon Wei", "link": "https://arxiv.org/abs/1905.10138", "summary": "Model compression techniques, such as pruning and quantization, are becoming\nincreasingly important to reduce the memory footprints and the amount of\ncomputations. Despite model size reduction, achieving performance enhancement\non devices is, however, still challenging mainly due to the irregular\nrepresentations of sparse matrix formats. This paper proposes a new weight\nrepresentation scheme for Sparse Quantized Neural Networks, specifically\nachieved by fine-grained and unstructured pruning method. The representation is\nencrypted in a structured regular format, which can be efficiently decoded\nthrough XOR-gate network during inference in a parallel manner. We demonstrate\nvarious deep learning models that can be compressed and represented by our\nproposed format with fixed and high compression ratio. For example, for\nfully-connected layers of AlexNet on ImageNet dataset, we can represent the\nsparse weights by only 0.28 bits/weight for 1-bit quantization and 91% pruning\nrate with a fixed decoding rate and full memory bandwidth usage. Decoding\nthrough XOR-gate network can be performed without any model accuracy\ndegradation with additional patch data associated with small overhead."}, {"title": "End-to-End Learning Local Multi-View Descriptors for 3D Point Clouds", "authors": "Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, Chiew-Lan Tai", "link": "https://arxiv.org/abs/2003.05855", "summary": "In this work, we propose an end-to-end framework to learn local multi-view\ndescriptors for 3D point clouds. To adopt a similar multi-view representation,\nexisting studies use hand-crafted viewpoints for rendering in a preprocessing\nstage, which is detached from the subsequent descriptor learning stage. In our\nframework, we integrate the multi-view rendering into neural networks by using\na differentiable renderer, which allows the viewpoints to be optimizable\nparameters for capturing more informative local context of interest points. To\nobtain discriminative descriptors, we also design a soft-view pooling module to\nattentively fuse convolutional features across views. Extensive experiments on\nexisting 3D registration benchmarks show that our method outperforms existing\nlocal descriptors both quantitatively and qualitatively."}, {"title": "Minimal Solutions for Relative Pose With a Single Affine Correspondence", "authors": "Banglei Guan, Ji Zhao, Zhang Li, Fang Sun, Friedrich Fraundorfer", "link": "https://arxiv.org/abs/1912.10776", "summary": "In this paper we present four cases of minimal solutions for two-view\nrelative pose estimation by exploiting the affine transformation between\nfeature points and we demonstrate efficient solvers for these cases. It is\nshown, that under the planar motion assumption or with knowledge of a vertical\ndirection, a single affine correspondence is sufficient to recover the relative\ncamera pose. The four cases considered are two-view planar relative motion for\ncalibrated cameras as a closed-form and a least-squares solution, a closed-form\nsolution for unknown focal length and the case of a known vertical direction.\nThese algorithms can be used efficiently for outlier detection within a RANSAC\nloop and for initial motion estimation. All the methods are evaluated on both\nsynthetic data and real-world datasets from the KITTI benchmark. The\nexperimental results demonstrate that our methods outperform comparable\nstate-of-the-art methods in accuracy with the benefit of a reduced number of\nneeded RANSAC iterations."}, {"title": "Point Cloud Completion by Skip-Attention Network With Hierarchical Folding", "authors": "Xin Wen, Tianyang Li, Zhizhong Han, Yu-Shen Liu", "link": "https://arxiv.org/abs/2005.03871", "summary": "Point cloud completion aims to infer the complete geometries for missing\nregions of 3D objects from incomplete ones. Previous methods usually predict\nthe complete point cloud based on the global shape representation extracted\nfrom the incomplete input. However, the global representation often suffers\nfrom the information loss of structure details on local regions of incomplete\npoint cloud. To address this problem, we propose Skip-Attention Network\n(SA-Net) for 3D point cloud completion. Our main contributions lie in the\nfollowing two-folds. First, we propose a skip-attention mechanism to\neffectively exploit the local structure details of incomplete point clouds\nduring the inference of missing parts. The skip-attention mechanism selectively\nconveys geometric information from the local regions of incomplete point clouds\nfor the generation of complete ones at different resolutions, where the\nskip-attention reveals the completion process in an interpretable way. Second,\nin order to fully utilize the selected geometric information encoded by\nskip-attention mechanism at different resolutions, we propose a novel\nstructure-preserving decoder with hierarchical folding for complete shape\ngeneration. The hierarchical folding preserves the structure of complete point\ncloud generated in upper layer by progressively detailing the local regions,\nusing the skip-attentioned geometry at the same resolution. We conduct\ncomprehensive experiments on ShapeNet and KITTI datasets, which demonstrate\nthat the proposed SA-Net outperforms the state-of-the-art point cloud\ncompletion methods."}, {"title": "Fast-MVSNet: Sparse-to-Dense Multi-View Stereo With Learned Propagation and Gauss-Newton Refinement", "authors": "Zehao Yu, Shenghua Gao", "link": "https://arxiv.org/abs/2003.13017", "summary": "Almost all previous deep learning-based multi-view stereo (MVS) approaches\nfocus on improving reconstruction quality. Besides quality, efficiency is also\na desirable feature for MVS in real scenarios. Towards this end, this paper\npresents a Fast-MVSNet, a novel sparse-to-dense coarse-to-fine framework, for\nfast and accurate depth estimation in MVS. Specifically, in our Fast-MVSNet, we\nfirst construct a sparse cost volume for learning a sparse and high-resolution\ndepth map. Then we leverage a small-scale convolutional neural network to\nencode the depth dependencies for pixels within a local region to densify the\nsparse high-resolution depth map. At last, a simple but efficient Gauss-Newton\nlayer is proposed to further optimize the depth map. On one hand, the\nhigh-resolution depth map, the data-adaptive propagation method and the\nGauss-Newton layer jointly guarantee the effectiveness of our method. On the\nother hand, all modules in our Fast-MVSNet are lightweight and thus guarantee\nthe efficiency of our approach. Besides, our approach is also memory-friendly\nbecause of the sparse depth representation. Extensive experimental results show\nthat our method is 5$\\times$ and 14$\\times$ faster than Point-MVSNet and\nR-MVSNet, respectively, while achieving comparable or even better results on\nthe challenging Tanks and Temples dataset as well as the DTU dataset. Code is\navailable at https://github.com/svip-lab/FastMVSNet."}, {"title": "AANet: Adaptive Aggregation Network for Efficient Stereo Matching", "authors": "Haofei Xu, Juyong Zhang", "link": "https://arxiv.org/abs/2004.09548", "summary": "Despite the remarkable progress made by learning based stereo matching\nalgorithms, one key challenge remains unsolved. Current state-of-the-art stereo\nmodels are mostly based on costly 3D convolutions, the cubic computational\ncomplexity and high memory consumption make it quite expensive to deploy in\nreal-world applications. In this paper, we aim at completely replacing the\ncommonly used 3D convolutions to achieve fast inference speed while maintaining\ncomparable accuracy. To this end, we first propose a sparse points based\nintra-scale cost aggregation method to alleviate the well-known edge-fattening\nissue at disparity discontinuities. Further, we approximate traditional\ncross-scale cost aggregation algorithm with neural network layers to handle\nlarge textureless regions. Both modules are simple, lightweight, and\ncomplementary, leading to an effective and efficient architecture for cost\naggregation. With these two modules, we can not only significantly speed up\nexisting top-performing models (e.g., $41\\times$ than GC-Net, $4\\times$ than\nPSMNet and $38\\times$ than GA-Net), but also improve the performance of fast\nstereo models (e.g., StereoNet). We also achieve competitive results on Scene\nFlow and KITTI datasets while running at 62ms, demonstrating the versatility\nand high efficiency of the proposed method. Our full framework is available at\nhttps://github.com/haofeixu/aanet ."}, {"title": "Towards Unified INT8 Training for Convolutional Neural Network", "authors": "Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, Junjie Yan", "link": "https://arxiv.org/abs/1912.12607", "summary": "Recently low-bit (e.g., 8-bit) network quantization has been extensively\nstudied to accelerate the inference. Besides inference, low-bit training with\nquantized gradients can further bring more considerable acceleration, since the\nbackward process is often computation-intensive. Unfortunately, the\ninappropriate quantization of backward propagation usually makes the training\nunstable and even crash. There lacks a successful unified low-bit training\nframework that can support diverse networks on various tasks. In this paper, we\ngive an attempt to build a unified 8-bit (INT8) training framework for common\nconvolutional neural networks from the aspects of both accuracy and speed.\nFirst, we empirically find the four distinctive characteristics of gradients,\nwhich provide us insightful clues for gradient quantization. Then, we\ntheoretically give an in-depth analysis of the convergence bound and derive two\nprinciples for stable INT8 training. Finally, we propose two universal\ntechniques, including Direction Sensitive Gradient Clipping that reduces the\ndirection deviation of gradients and Deviation Counteractive Learning Rate\nScaling that avoids illegal gradient update along the wrong direction. The\nexperiments show that our unified solution promises accurate and efficient INT8\ntraining for a variety of networks and tasks, including MobileNetV2,\nInceptionV3 and object detection that prior studies have never succeeded.\nMoreover, it enjoys a strong flexibility to run on off-the-shelf hardware, and\nreduces the training time by 22% on Pascal GPU without too much optimization\neffort. We believe that this pioneering study will help lead the community\ntowards a fully unified INT8 training for convolutional neural networks."}, {"title": "Active 3D Motion Visualization Based on Spatiotemporal Light-Ray Integration", "authors": "Fumihiko Sakaue, Jun Sato"}, {"title": "Block-Wisely Supervised Neural Architecture Search With Knowledge Distillation", "authors": "Changlin Li, Jiefeng Peng, Liuchun Yuan, Guangrun Wang, Xiaodan Liang, Liang Lin, Xiaojun Chang", "link": "https://arxiv.org/abs/1911.13053", "summary": "Neural Architecture Search (NAS), aiming at automatically designing network\narchitectures by machines, is hoped and expected to bring about a new\nrevolution in machine learning. Despite these high expectation, the\neffectiveness and efficiency of existing NAS solutions are unclear, with some\nrecent works going so far as to suggest that many existing NAS solutions are no\nbetter than random architecture selection. The inefficiency of NAS solutions\nmay be attributed to inaccurate architecture evaluation. Specifically, to speed\nup NAS, recent works have proposed under-training different candidate\narchitectures in a large search space concurrently by using shared network\nparameters; however, this has resulted in incorrect architecture ratings and\nfurthered the ineffectiveness of NAS.\n  In this work, we propose to modularize the large search space of NAS into\nblocks to ensure that the potential candidate architectures are fully trained;\nthis reduces the representation shift caused by the shared parameters and leads\nto the correct rating of the candidates. Thanks to the block-wise search, we\ncan also evaluate all of the candidate architectures within a block. Moreover,\nwe find that the knowledge of a network model lies not only in the network\nparameters but also in the network architecture. Therefore, we propose to\ndistill the neural architecture (DNA) knowledge from a teacher model as the\nsupervision to guide our block-wise architecture search, which significantly\nimproves the effectiveness of NAS. Remarkably, the capacity of our searched\narchitecture has exceeded the teacher model, demonstrating the practicability\nand scalability of our method. Finally, our method achieves a state-of-the-art\n78.4\\% top-1 accuracy on ImageNet in a mobile setting, which is about a 2.1\\%\ngain over EfficientNet-B0. All of our searched models along with the evaluation\ncode are available online."}, {"title": "GreedyNAS: Towards Fast One-Shot NAS With Greedy Supernet", "authors": "Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, Changshui Zhang", "link": "", "summary": ""}, {"title": "Learning Filter Pruning Criteria for Deep Convolutional Neural Networks Acceleration", "authors": "Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, Yi Yang", "link": "", "summary": ""}, {"title": "DIST: Rendering Deep Implicit Signed Distance Function With Differentiable Sphere Tracing", "authors": "Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui", "link": "https://arxiv.org/abs/1911.13225", "summary": "We propose a differentiable sphere tracing algorithm to bridge the gap\nbetween inverse graphics methods and the recently proposed deep learning based\nimplicit signed distance function. Due to the nature of the implicit function,\nthe rendering process requires tremendous function queries, which is\nparticularly problematic when the function is represented as a neural network.\nWe optimize both the forward and backward pass of our rendering layer to make\nit run efficiently with affordable memory consumption on a commodity graphics\ncard. Our rendering method is fully differentiable such that losses can be\ndirectly computed on the rendered 2D observations, and the gradients can be\npropagated backward to optimize the 3D geometry. We show that our rendering\nmethod can effectively reconstruct accurate 3D shapes from various inputs, such\nas sparse depth and multi-view images, through inverse optimization. With the\ngeometry based reasoning, our 3D shape prediction methods show excellent\ngeneralization capability and robustness against various noise."}, {"title": "Visually Imbalanced Stereo Matching", "authors": "Yicun Liu, Jimmy Ren, Jiawei Zhang, Jianbo Liu, Mude Lin"}, {"title": "Mesh-Guided Multi-View Stereo With Pyramid Architecture", "authors": "Yuesong Wang, Tao Guan, Zhuo Chen, Yawei Luo, Keyang Luo, Lili Ju"}, {"title": "BiDet: An Efficient Binarized Object Detector", "authors": "Ziwei Wang, Ziyi Wu, Jiwen Lu, Jie Zhou", "link": "https://arxiv.org/abs/2003.03961", "summary": "In this paper, we propose a binarized neural network learning method called\nBiDet for efficient object detection. Conventional network binarization methods\ndirectly quantize the weights and activations in one-stage or two-stage\ndetectors with constrained representational capacity, so that the information\nredundancy in the networks causes numerous false positives and degrades the\nperformance significantly. On the contrary, our BiDet fully utilizes the\nrepresentational capacity of the binary neural networks for object detection by\nredundancy removal, through which the detection precision is enhanced with\nalleviated false positives. Specifically, we generalize the information\nbottleneck (IB) principle to object detection, where the amount of information\nin the high-level feature maps is constrained and the mutual information\nbetween the feature maps and object detection is maximized. Meanwhile, we learn\nsparse object priors so that the posteriors are concentrated on informative\ndetection prediction with false positive elimination. Extensive experiments on\nthe PASCAL VOC and COCO datasets show that our method outperforms the\nstate-of-the-art binary neural networks by a sizable margin."}, {"title": "Local Non-Rigid Structure-From-Motion From Diffeomorphic Mappings", "authors": "Shaifali Parashar, Mathieu Salzmann, Pascal Fua"}, {"title": "Seeing Around Street Corners: Non-Line-of-Sight Detection and Tracking In-the-Wild Using Doppler Radar", "authors": "Nicolas Scheiner, Florian Kraus, Fangyin Wei, Buu Phan, Fahim Mannan, Nils Appenrodt, Werner Ritter, J\u00fcrgen Dickmann, Klaus Dietmayer, Bernhard Sick, Felix Heide", "link": "https://arxiv.org/abs/1912.06613", "summary": "Conventional sensor systems record information about directly visible\nobjects, whereas occluded scene components are considered lost in the\nmeasurement process. Non-line-of-sight (NLOS) methods try to recover such\nhidden objects from their indirect reflections - faint signal components,\ntraditionally treated as measurement noise. Existing NLOS approaches struggle\nto record these low-signal components outside the lab, and do not scale to\nlarge-scale outdoor scenes and high-speed motion, typical in automotive\nscenarios. In particular, optical NLOS capture is fundamentally limited by the\nquartic intensity falloff of diffuse indirect reflections. In this work, we\ndepart from visible-wavelength approaches and demonstrate detection,\nclassification, and tracking of hidden objects in large-scale dynamic\nenvironments using Doppler radars that can be manufactured at low-cost in\nseries production. To untangle noisy indirect and direct reflections, we learn\nfrom temporal sequences of Doppler velocity and position measurements, which we\nfuse in a joint NLOS detection and tracking network over time. We validate the\napproach on in-the-wild automotive scenes, including sequences of parked cars\nor house facades as relay surfaces, and demonstrate low-cost, real-time NLOS in\ndynamic automotive environments."}, {"title": "APQ: Joint Search for Network Architecture, Pruning and Quantization Policy", "authors": "Tianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, Song Han"}, {"title": "On the Acceleration of Deep Learning Model Parallelism With Staleness", "authors": "An Xu, Zhouyuan Huo, Heng Huang"}, {"title": "RevealNet: Seeing Behind Objects in RGB-D Scans", "authors": "Ji Hou, Angela Dai, Matthias Nie\u00dfner", "link": "", "summary": ""}, {"title": "MemNAS: Memory-Efficient Neural Architecture Search With Grow-Trim Learning", "authors": "Peiye Liu, Bo Wu, Huadong Ma, Mingoo Seok", "link": "", "summary": ""}, {"title": "StegaStamp: Invisible Hyperlinks in Physical Photographs", "authors": "Matthew Tancik, Ben Mildenhall, Ren Ng", "link": "http://arxiv.org/abs/1904.05343", "summary": "Printed and digitally displayed photos have the ability to hide imperceptible\ndigital data that can be accessed through internet-connected imaging systems.\nAnother way to think about this is physical photographs that have unique QR\ncodes invisibly embedded within them. This paper presents an architecture,\nalgorithms, and a prototype implementation addressing this vision. Our key\ntechnical contribution is StegaStamp, a learned steganographic algorithm to\nenable robust encoding and decoding of arbitrary hyperlink bitstrings into\nphotos in a manner that approaches perceptual invisibility. StegaStamp\ncomprises a deep neural network that learns an encoding/decoding algorithm\nrobust to image perturbations approximating the space of distortions resulting\nfrom real printing and photography. We demonstrates real-time decoding of\nhyperlinks in photos from in-the-wild videos that contain variation in\nlighting, shadows, perspective, occlusion and viewing distance. Our prototype\nsystem robustly retrieves 56 bit hyperlinks after error correction - sufficient\nto embed a unique code within every photo on the internet."}, {"title": "L2-GCN: Layer-Wise and Learned Efficient Training of Graph Convolutional Networks", "authors": "Yuning You, Tianlong Chen, Zhangyang Wang, Yang Shen", "link": "http://arxiv.org/abs/2003.13606", "summary": "Graph convolution networks (GCN) are increasingly popular in many\napplications, yet remain notoriously hard to train over large graph datasets.\nThey need to compute node representations recursively from their neighbors.\nCurrent GCN training algorithms suffer from either high computational costs\nthat grow exponentially with the number of layers, or high memory usage for\nloading the entire graph and node embeddings. In this paper, we propose a novel\nefficient layer-wise training framework for GCN (L-GCN), that disentangles\nfeature aggregation and feature transformation during training, hence greatly\nreducing time and memory complexities. We present theoretical analysis for\nL-GCN under the graph isomorphism framework, that L-GCN leads to as powerful\nGCNs as the more costly conventional training algorithm does, under mild\nconditions. We further propose L^2-GCN, which learns a controller for each\nlayer that can automatically adjust the training epochs per layer in L-GCN.\nExperiments show that L-GCN is faster than state-of-the-arts by at least an\norder of magnitude, with a consistent of memory usage not dependent on dataset\nsize, while maintaining comparable prediction performance. With the learned\ncontroller, L^2-GCN can further cut the training time in half. Our codes are\navailable at https://github.com/Shen-Lab/L2-GCN and supplementary materials at\nhttps://yyou1996.github.io/papers/cvpr2020_l2gcn/supplement.pdf."}, {"title": "Polarized Non-Line-of-Sight Imaging", "authors": "Kenichiro Tanaka, Yasuhiro Mukaigawa, Achuta Kadambi", "link": "", "summary": ""}, {"title": "AdaBits: Neural Network Quantization With Adaptive Bit-Widths", "authors": "Qing Jin, Linjie Yang, Zhenyu Liao", "link": "https://arxiv.org/abs/1912.09666", "summary": "Deep neural networks with adaptive configurations have gained increasing\nattention due to the instant and flexible deployment of these models on\nplatforms with different resource budgets. In this paper, we investigate a\nnovel option to achieve this goal by enabling adaptive bit-widths of weights\nand activations in the model. We first examine the benefits and challenges of\ntraining quantized model with adaptive bit-widths, and then experiment with\nseveral approaches including direct adaptation, progressive training and joint\ntraining. We discover that joint training is able to produce comparable\nperformance on the adaptive model as individual models. We further propose a\nnew technique named Switchable Clipping Level (S-CL) to further improve\nquantized models at the lowest bit-width. With our proposed techniques applied\non a bunch of models including MobileNet-V1/V2 and ResNet-50, we demonstrate\nthat bit-width of weights and activations is a new option for adaptively\nexecutable deep neural networks, offering a distinct opportunity for improved\naccuracy-efficiency trade-off as well as instant adaptation according to the\nplatform constraints in real-world applications."}, {"title": "Multi-Scale Boosted Dehazing Network With Dense Feature Fusion", "authors": "Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang, Fei Wang, Ming-Hsuan Yang", "link": "http://arxiv.org/abs/2004.13388", "summary": "In this paper, we propose a Multi-Scale Boosted Dehazing Network with Dense\nFeature Fusion based on the U-Net architecture. The proposed method is designed\nbased on two principles, boosting and error feedback, and we show that they are\nsuitable for the dehazing problem. By incorporating the\nStrengthen-Operate-Subtract boosting strategy in the decoder of the proposed\nmodel, we develop a simple yet effective boosted decoder to progressively\nrestore the haze-free image. To address the issue of preserving spatial\ninformation in the U-Net architecture, we design a dense feature fusion module\nusing the back-projection feedback scheme. We show that the dense feature\nfusion module can simultaneously remedy the missing spatial information from\nhigh-resolution features and exploit the non-adjacent features. Extensive\nevaluations demonstrate that the proposed model performs favorably against the\nstate-of-the-art approaches on the benchmark datasets as well as real-world\nhazy images."}, {"title": "ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings", "authors": "Jiahui Huang, Sheng Yang, Tai-Jiang Mu, Shi-Min Hu", "link": "https://arxiv.org/abs/2003.12980", "summary": "We present ClusterVO, a stereo Visual Odometry which simultaneously clusters\nand estimates the motion of both ego and surrounding rigid clusters/objects.\nUnlike previous solutions relying on batch input or imposing priors on scene\nstructure or dynamic object models, ClusterVO is online, general and thus can\nbe used in various scenarios including indoor scene understanding and\nautonomous driving. At the core of our system lies a multi-level probabilistic\nassociation mechanism and a heterogeneous Conditional Random Field (CRF)\nclustering approach combining semantic, spatial and motion information to\njointly infer cluster segmentations online for every frame. The poses of camera\nand dynamic objects are instantly solved through a sliding-window optimization.\nOur system is evaluated on Oxford Multimotion and KITTI dataset both\nquantitatively and qualitatively, reaching comparable results to\nstate-of-the-art solutions on both odometry and dynamic trajectory recovery."}, {"title": "Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach", "authors": "Haichuan Yang, Shupeng Gui, Yuhao Zhu, Ji Liu", "link": "https://arxiv.org/abs/1910.05897", "summary": "Deep Neural Networks (DNNs) are applied in a wide range of usecases. There is\nan increased demand for deploying DNNs on devices that do not have abundant\nresources such as memory and computation units. Recently, network compression\nthrough a variety of techniques such as pruning and quantization have been\nproposed to reduce the resource requirement. A key parameter that all existing\ncompression techniques are sensitive to is the compression ratio (e.g., pruning\nsparsity, quantization bitwidth) of each layer. Traditional solutions treat the\ncompression ratios of each layer as hyper-parameters, and tune them using human\nheuristic. Recent researchers start using black-box hyper-parameter\noptimizations, but they will introduce new hyper-parameters and have efficiency\nissue. In this paper, we propose a framework to jointly prune and quantize the\nDNNs automatically according to a target model size without using any\nhyper-parameters to manually set the compression ratio for each layer. In the\nexperiments, we show that our framework can compress the weights data of\nResNet-50 to be 836$\\times$ smaller without accuracy loss on CIFAR-10, and\ncompress AlexNet to be 205$\\times$ smaller without accuracy loss on ImageNet\nclassification."}, {"title": "Normal Assisted Stereo Depth Estimation", "authors": "Uday Kusupati, Shuo Cheng, Rui Chen, Hao Su", "link": "https://arxiv.org/abs/1911.10444", "summary": "Accurate stereo depth estimation plays a critical role in various 3D tasks in\nboth indoor and outdoor environments. Recently, learning-based multi-view\nstereo methods have demonstrated competitive performance with a limited number\nof views. However, in challenging scenarios, especially when building\ncross-view correspondences is hard, these methods still cannot produce\nsatisfying results. In this paper, we study how to leverage a normal estimation\nmodel and the predicted normal maps to improve the depth quality. We couple the\nlearning of a multi-view normal estimation module and a multi-view depth\nestimation module. In addition, we propose a novel consistency loss to train an\nindependent consistency module that refines the depths from depth/normal pairs.\nWe find that the joint learning can improve both the prediction of normal and\ndepth, and the accuracy & smoothness can be further improved by enforcing the\nconsistency. Experiments on MVS, SUN3D, RGBD, and Scenes11 demonstrate the\neffectiveness of our method and state-of-the-art performance."}, {"title": "Fusing Wearable IMUs With Multi-View Images for Human Pose Estimation: A Geometric Approach", "authors": "Zhe Zhang, Chunyu Wang, Wenhu Qin, Wenjun Zeng", "link": "https://arxiv.org/abs/2003.11163", "summary": "We propose to estimate 3D human pose from multi-view images and a few IMUs\nattached at person's limbs. It operates by firstly detecting 2D poses from the\ntwo signals, and then lifting them to the 3D space. We present a geometric\napproach to reinforce the visual features of each pair of joints based on the\nIMUs. This notably improves 2D pose estimation accuracy especially when one\njoint is occluded. We call this approach Orientation Regularized Network (ORN).\nThen we lift the multi-view 2D poses to the 3D space by an Orientation\nRegularized Pictorial Structure Model (ORPSM) which jointly minimizes the\nprojection error between the 3D and 2D poses, along with the discrepancy\nbetween the 3D pose and IMU orientations. The simple two-step approach reduces\nthe error of the state-of-the-art by a large margin on a public dataset. Our\ncode will be released at https://github.com/CHUNYUWANG/imu-human-pose-pytorch."}, {"title": "gDLS*: Generalized Pose-and-Scale Estimation Given Scale and Gravity Priors", "authors": "Victor Fragoso, Joseph DeGol, Gang Hua", "link": "https://arxiv.org/abs/2004.02052", "summary": "Many real-world applications in augmented reality (AR), 3D mapping, and\nrobotics require both fast and accurate estimation of camera poses and scales\nfrom multiple images captured by multiple cameras or a single moving camera.\nAchieving high speed and maintaining high accuracy in a pose-and-scale\nestimator are often conflicting goals. To simultaneously achieve both, we\nexploit a priori knowledge about the solution space. We present gDLS*, a\ngeneralized-camera-model pose-and-scale estimator that utilizes rotation and\nscale priors. gDLS* allows an application to flexibly weigh the contribution of\neach prior, which is important since priors often come from noisy sensors.\nCompared to state-of-the-art generalized-pose-and-scale estimators (e.g.,\ngDLS), our experiments on both synthetic and real data consistently demonstrate\nthat gDLS* accelerates the estimation process and improves scale and pose\naccuracy."}, {"title": "Embodied Language Grounding With 3D Visual Feature Representations", "authors": "Mihir Prabhudesai, Hsiao-Yu Fish Tung, Syed Ashar Javed, Maximilian Sieb, Adam W. Harley, Katerina Fragkiadaki", "link": "https://arxiv.org/abs/1910.01210", "summary": "Consider the utterance \"the tomato is to the left of the pot.\" Humans can\nanswer numerous questions about the situation described, as well as reason\nthrough counterfactuals and alternatives, such as, \"is the pot larger than the\ntomato ?\", \"can we move to a viewpoint from which the tomato is completely\nhidden behind the pot ?\", \"can we have an object that is both to the left of\nthe tomato and to the right of the pot ?\", \"would the tomato fit inside the pot\n?\", and so on. Such reasoning capability remains elusive from current\ncomputational models of language understanding. To link language processing\nwith spatial reasoning, we propose associating natural language utterances to a\nmental workspace of their meaning, encoded as 3-dimensional visual feature\nrepresentations of the world scenes they describe. We learn such 3-dimensional\nvisual representations---we call them visual imaginations--- by predicting\nimages a mobile agent sees while moving around in the 3D world. The input image\nstreams the agent collects are unprojected into egomotion-stable 3D scene\nfeature maps of the scene, and projected from novel viewpoints to match the\nobserved RGB image views in an end-to-end differentiable manner. We then train\nmodular neural models to generate such 3D feature representations given\nlanguage utterances, to localize the objects an utterance mentions in the 3D\nfeature representation inferred from an image, and to predict the desired 3D\nobject locations given a manipulation instruction. We empirically show the\nproposed models outperform by a large margin existing 2D models in spatial\nreasoning, referential object detection and instruction following, and\ngeneralize better across camera viewpoints and object arrangements."}, {"title": "Learning to Autofocus", "authors": "Charles Herrmann, Richard Strong Bowen, Neal Wadhwa, Rahul Garg, Qiurui He, Jonathan T. Barron, Ramin Zabih", "link": "https://arxiv.org/abs/2004.12260", "summary": "Autofocus is an important task for digital cameras, yet current approaches\noften exhibit poor performance. We propose a learning-based approach to this\nproblem, and provide a realistic dataset of sufficient size for effective\nlearning. Our dataset is labeled with per-pixel depths obtained from multi-view\nstereo, following \"Learning single camera depth estimation using dual-pixels\".\nUsing this dataset, we apply modern deep classification models and an ordinal\nregression loss to obtain an efficient learning-based autofocus technique. We\ndemonstrate that our approach provides a significant improvement compared with\nprevious learned and non-learned methods: our model reduces the mean absolute\nerror by a factor of 3.6 over the best comparable baseline algorithm. Our\ndataset and code are publicly available."}, {"title": "Joint Demosaicing and Denoising With Self Guidance", "authors": "Lin Liu, Xu Jia, Jianzhuang Liu, Qi Tian"}, {"title": "Forward and Backward Information Retention for Accurate Binary Neural Networks", "authors": "Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, Jingkuan Song", "link": "https://arxiv.org/abs/1909.10788", "summary": "Weight and activation binarization is an effective approach to deep neural\nnetwork compression and can accelerate the inference by leveraging bitwise\noperations. Although many binarization methods have improved the accuracy of\nthe model by minimizing the quantization error in forward propagation, there\nremains a noticeable performance gap between the binarized model and the\nfull-precision one. Our empirical study indicates that the quantization brings\ninformation loss in both forward and backward propagation, which is the\nbottleneck of training accurate binary neural networks. To address these\nissues, we propose an Information Retention Network (IR-Net) to retain the\ninformation that consists in the forward activations and backward gradients.\nIR-Net mainly relies on two technical contributions: (1) Libra Parameter\nBinarization (Libra-PB): simultaneously minimizing both quantization error and\ninformation loss of parameters by balanced and standardized weights in forward\npropagation; (2) Error Decay Estimator (EDE): minimizing the information loss\nof gradients by gradually approximating the sign function in backward\npropagation, jointly considering the updating ability and accurate gradients.\nWe are the first to investigate both forward and backward processes of binary\nnetworks from the unified information perspective, which provides new insight\ninto the mechanism of network binarization. Comprehensive experiments with\nvarious network structures on CIFAR-10 and ImageNet datasets manifest that the\nproposed IR-Net can consistently outperform state-of-the-art quantization\nmethods."}, {"title": "Light Field Spatial Super-Resolution via Deep Combinatorial Geometry Embedding and Structural Consistency Regularization", "authors": "Jing Jin, Junhui Hou, Jie Chen, Sam Kwong", "link": "https://arxiv.org/abs/2004.02215", "summary": "Light field (LF) images acquired by hand-held devices usually suffer from low\nspatial resolution as the limited sampling resources have to be shared with the\nangular dimension. LF spatial super-resolution (SR) thus becomes an\nindispensable part of the LF camera processing pipeline. The\nhigh-dimensionality characteristic and complex geometrical structure of LF\nimages make the problem more challenging than traditional single-image SR. The\nperformance of existing methods is still limited as they fail to thoroughly\nexplore the coherence among LF views and are insufficient in accurately\npreserving the parallax structure of the scene. In this paper, we propose a\nnovel learning-based LF spatial SR framework, in which each view of an LF image\nis first individually super-resolved by exploring the complementary information\namong views with combinatorial geometry embedding. For accurate preservation of\nthe parallax structure among the reconstructed views, a regularization network\ntrained over a structure-aware loss function is subsequently appended to\nenforce correct parallax relationships over the intermediate estimation. Our\nproposed approach is evaluated over datasets with a large number of testing\nimages including both synthetic and real-world scenes. Experimental results\ndemonstrate the advantage of our approach over state-of-the-art methods, i.e.,\nour method not only improves the average PSNR by more than 1.0 dB but also\npreserves more accurate parallax details, at a lower computational cost."}, {"title": "A Multi-Hypothesis Approach to Color Constancy", "authors": "Daniel Hernandez-Juarez, Sarah Parisot, Benjamin Busam, Ale\u0161 Leonardis, Gregory Slabaugh, Steven McDonagh", "link": "https://arxiv.org/abs/2002.12896", "summary": "Contemporary approaches frame the color constancy problem as learning camera\nspecific illuminant mappings. While high accuracy can be achieved on camera\nspecific data, these models depend on camera spectral sensitivity and typically\nexhibit poor generalisation to new devices. Additionally, regression methods\nproduce point estimates that do not explicitly account for potential\nambiguities among plausible illuminant solutions, due to the ill-posed nature\nof the problem. We propose a Bayesian framework that naturally handles color\nconstancy ambiguity via a multi-hypothesis strategy. Firstly, we select a set\nof candidate scene illuminants in a data-driven fashion and apply them to a\ntarget image to generate of set of corrected images. Secondly, we estimate, for\neach corrected image, the likelihood of the light source being achromatic using\na camera-agnostic CNN. Finally, our method explicitly learns a final\nillumination estimate from the generated posterior probability distribution.\nOur likelihood estimator learns to answer a camera-agnostic question and thus\nenables effective multi-camera training by disentangling illuminant estimation\nfrom the supervised learning task. We extensively evaluate our proposed\napproach and additionally set a benchmark for novel sensor generalisation\nwithout re-training. Our method provides state-of-the-art accuracy on multiple\npublic datasets (up to 11% median angular error improvement) while maintaining\nreal-time execution."}, {"title": "Learning to Restore Low-Light Images via Decomposition-and-Enhancement", "authors": "Ke Xu, Xin Yang, Baocai Yin, Rynson W.H. Lau", "link": "", "summary": ""}, {"title": "Background Matting: The World Is Your Green Screen", "authors": "Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M. Seitz, Ira Kemelmacher-Shlizerman"}, {"title": "Supervised Raw Video Denoising With a Benchmark Dataset on Dynamic Scenes", "authors": "Huanjing Yue, Cong Cao, Lei Liao, Ronghe Chu, Jingyu Yang", "link": "https://arxiv.org/abs/2003.14013", "summary": "In recent years, the supervised learning strategy for real noisy image\ndenoising has been emerging and has achieved promising results. In contrast,\nrealistic noise removal for raw noisy videos is rarely studied due to the lack\nof noisy-clean pairs for dynamic scenes. Clean video frames for dynamic scenes\ncannot be captured with a long-exposure shutter or averaging multi-shots as was\ndone for static images. In this paper, we solve this problem by creating\nmotions for controllable objects, such as toys, and capturing each static\nmoment for multiple times to generate clean video frames. In this way, we\nconstruct a dataset with 55 groups of noisy-clean videos with ISO values\nranging from 1600 to 25600. To our knowledge, this is the first dynamic video\ndataset with noisy-clean pairs. Correspondingly, we propose a raw video\ndenoising network (RViDeNet) by exploring the temporal, spatial, and channel\ncorrelations of video frames. Since the raw video has Bayer patterns, we pack\nit into four sub-sequences, i.e RGBG sequences, which are denoised by the\nproposed RViDeNet separately and finally fused into a clean video. In addition,\nour network not only outputs a raw denoising result, but also the sRGB result\nby going through an image signal processing (ISP) module, which enables users\nto generate the sRGB result with their favourite ISPs. Experimental results\ndemonstrate that our method outperforms state-of-the-art video and raw image\ndenoising algorithms on both indoor and outdoor videos."}, {"title": "Photometric Stereo via Discrete Hypothesis-and-Test Search", "authors": "Kenji Enomoto, Michael Waechter, Kiriakos N. Kutulakos, Yasuyuki Matsushita"}, {"title": "Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference", "authors": "Thomas Verelst, Tinne Tuytelaars", "link": "https://arxiv.org/abs/1912.03203", "summary": "Modern convolutional neural networks apply the same operations on every pixel\nin an image. However, not all image regions are equally important. To address\nthis inefficiency, we propose a method to dynamically apply convolutions\nconditioned on the input image. We introduce a residual block where a small\ngating branch learns which spatial positions should be evaluated. These\ndiscrete gating decisions are trained end-to-end using the Gumbel-Softmax\ntrick, in combination with a sparsity criterion. Our experiments on CIFAR,\nImageNet and MPII show that our method has better focus on the region of\ninterest and better accuracy than existing methods, at a lower computational\ncomplexity. Moreover, we provide an efficient CUDA implementation of our\ndynamic convolutions using a gather-scatter approach, achieving a significant\nimprovement in inference speed with MobileNetV2 residual blocks. On human pose\nestimation, a task that is inherently spatially sparse, the processing speed is\nincreased by 60% with no loss in accuracy."}, {"title": "Fixed-Point Back-Propagation Training", "authors": "Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo, Zidong Du, Tian Zhi, Yunji Chen", "link": "", "summary": ""}, {"title": "Heterogeneous Knowledge Distillation Using Information Flow Modeling", "authors": "Nikolaos Passalis, Maria Tzelepi, Anastasios Tefas", "link": "https://arxiv.org/abs/2005.00727", "summary": "Knowledge Distillation (KD) methods are capable of transferring the knowledge\nencoded in a large and complex teacher into a smaller and faster student. Early\nmethods were usually limited to transferring the knowledge only between the\nlast layers of the networks, while latter approaches were capable of performing\nmulti-layer KD, further increasing the accuracy of the student. However,\ndespite their improved performance, these methods still suffer from several\nlimitations that restrict both their efficiency and flexibility. First,\nexisting KD methods typically ignore that neural networks undergo through\ndifferent learning phases during the training process, which often requires\ndifferent types of supervision for each one. Furthermore, existing multi-layer\nKD methods are usually unable to effectively handle networks with significantly\ndifferent architectures (heterogeneous KD). In this paper we propose a novel KD\nmethod that works by modeling the information flow through the various layers\nof the teacher model and then train a student model to mimic this information\nflow. The proposed method is capable of overcoming the aforementioned\nlimitations by using an appropriate supervision scheme during the different\nphases of the training process, as well as by designing and training an\nappropriate auxiliary teacher model that acts as a proxy model capable of\n\"explaining\" the way the teacher works to the student. The effectiveness of the\nproposed method is demonstrated using four image datasets and several different\nevaluation setups."}, {"title": "Rethinking Differentiable Search for Mixed-Precision Neural Networks", "authors": "Zhaowei Cai, Nuno Vasconcelos", "link": "https://arxiv.org/abs/2004.05795", "summary": "Low-precision networks, with weights and activations quantized to low\nbit-width, are widely used to accelerate inference on edge devices. However,\ncurrent solutions are uniform, using identical bit-width for all filters. This\nfails to account for the different sensitivities of different filters and is\nsuboptimal. Mixed-precision networks address this problem, by tuning the\nbit-width to individual filter requirements. In this work, the problem of\noptimal mixed-precision network search (MPS) is considered. To circumvent its\ndifficulties of discrete search space and combinatorial optimization, a new\ndifferentiable search architecture is proposed, with several novel\ncontributions to advance the efficiency by leveraging the unique properties of\nthe MPS problem. The resulting Efficient differentiable MIxed-Precision network\nSearch (EdMIPS) method is effective at finding the optimal bit allocation for\nmultiple popular networks, and can search a large model, e.g. Inception-V3,\ndirectly on ImageNet without proxy task in a reasonable amount of time. The\nlearned mixed-precision networks significantly outperform their uniform\ncounterparts."}, {"title": "Residual Feature Aggregation Network for Image Super-Resolution", "authors": "Jie Liu, Wenjie Zhang, Yuting Tang, Jie Tang, Gangshan Wu"}, {"title": "Resolution Adaptive Networks for Efficient Inference", "authors": "Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, Gao Huang", "link": "https://arxiv.org/abs/2003.07326", "summary": "Adaptive inference is an effective mechanism to achieve a dynamic tradeoff\nbetween accuracy and computational cost in deep networks. Existing works mainly\nexploit architecture redundancy in network depth or width. In this paper, we\nfocus on spatial redundancy of input samples and propose a novel Resolution\nAdaptive Network (RANet), which is inspired by the intuition that\nlow-resolution representations are sufficient for classifying \"easy\" inputs\ncontaining large objects with prototypical features, while only some \"hard\"\nsamples need spatially detailed information. In RANet, the input images are\nfirst routed to a lightweight sub-network that efficiently extracts\nlow-resolution representations, and those samples with high prediction\nconfidence will exit early from the network without being further processed.\nMeanwhile, high-resolution paths in the network maintain the capability to\nrecognize the \"hard\" samples. Therefore, RANet can effectively reduce the\nspatial redundancy involved in inferring high-resolution inputs. Empirically,\nwe demonstrate the effectiveness of the proposed RANet on the CIFAR-10,\nCIFAR-100 and ImageNet datasets in both the anytime prediction setting and the\nbudgeted batch classification setting."}, {"title": "Learning to Forget for Meta-Learning", "authors": "Sungyong Baik, Seokil Hong, Kyoung Mu Lee", "link": "https://arxiv.org/abs/1906.05895", "summary": "Few-shot learning is a challenging problem where the system is required to\nachieve generalization from only few examples. Meta-learning tackles the\nproblem by learning prior knowledge shared across a distribution of tasks,\nwhich is then used to quickly adapt to unseen tasks. Model-agnostic\nmeta-learning (MAML) algorithm formulates prior knowledge as a common\ninitialization across tasks. However, forcibly sharing an initialization brings\nabout conflicts between tasks and thus compromises the quality of the\ninitialization. In this work, by observing that the extent of compromise\ndiffers among tasks and between layers of a neural network, we propose a new\ninitialization idea that employs task-dependent layer-wise attenuation, which\nwe call selective forgetting. The proposed attenuation scheme dynamically\ncontrols how much of prior knowledge each layer will exploit for a given task.\nThe experimental results demonstrate that the proposed method mitigates the\nconflicts and provides outstanding performance as a result. We further show\nthat the proposed method, named L2F, can be applied and improve other\nstate-of-the-art MAML-based frameworks, illustrating its generalizability."}, {"title": "Deep Learning for Handling Kernel/model Uncertainty in Image Deconvolution", "authors": "Yuesong Nan, Hui Ji"}, {"title": "Reflection Scene Separation From a Single Image", "authors": "Renjie Wan, Boxin Shi, Haoliang Li, Ling-Yu Duan, Alex C. Kot", "link": "", "summary": ""}, {"title": "Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre Bokeh Effect on Smartphones", "authors": "Chenchi Luo, Yingmao Li, Kaimo Lin, George Chen, Seok-Jun Lee, Jihwan Choi, Youngjun Francis Yoo, Michael O. Polley"}, {"title": "Bundle Adjustment on a Graph Processor", "authors": "Joseph Ortiz, Mark Pupilli, Stefan Leutenegger, Andrew J. Davison", "link": "https://arxiv.org/abs/2003.03134", "summary": "Graph processors such as Graphcore's Intelligence Processing Unit (IPU) are\npart of the major new wave of novel computer architecture for AI, and have a\ngeneral design with massively parallel computation, distributed on-chip memory\nand very high inter-core communication bandwidth which allows breakthrough\nperformance for message passing algorithms on arbitrary graphs. We show for the\nfirst time that the classical computer vision problem of bundle adjustment (BA)\ncan be solved extremely fast on a graph processor using Gaussian Belief\nPropagation. Our simple but fully parallel implementation uses the 1216 cores\non a single IPU chip to, for instance, solve a real BA problem with 125\nkeyframes and 1919 points in under 40ms, compared to 1450ms for the Ceres CPU\nlibrary. Further code optimisation will surely increase this difference on\nstatic problems, but we argue that the real promise of graph processing is for\nflexible in-place optimisation of general, dynamically changing factor graphs\nrepresenting Spatial AI problems. We give indications of this with experiments\nshowing the ability of GBP to efficiently solve incremental SLAM problems, and\ndeal with robust cost functions and different types of factors."}, {"title": "3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset", "authors": "Malte Pedersen, Joakim Bruslund Haurum, Stefan Hein Bengtson, Thomas B. Moeslund"}, {"title": "PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models", "authors": "Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin", "link": "https://arxiv.org/abs/2003.03808", "summary": "The primary aim of single-image super-resolution is to construct a\nhigh-resolution (HR) image from a corresponding low-resolution (LR) input. In\nprevious approaches, which have generally been supervised, the training\nobjective typically measures a pixel-wise average distance between the\nsuper-resolved (SR) and HR images. Optimizing such metrics often leads to\nblurring, especially in high variance (detailed) regions. We propose an\nalternative formulation of the super-resolution problem based on creating\nrealistic SR images that downscale correctly. We present a novel\nsuper-resolution algorithm addressing this problem, PULSE (Photo Upsampling via\nLatent Space Exploration), which generates high-resolution, realistic images at\nresolutions previously unseen in the literature. It accomplishes this in an\nentirely self-supervised fashion and is not confined to a specific degradation\noperator used during training, unlike previous methods (which require training\non databases of LR-HR image pairs for supervised learning). Instead of starting\nwith the LR image and slowly adding detail, PULSE traverses the high-resolution\nnatural image manifold, searching for images that downscale to the original LR\nimage. This is formalized through the \"downscaling loss,\" which guides\nexploration through the latent space of a generative model. By leveraging\nproperties of high-dimensional Gaussians, we restrict the search space to\nguarantee that our outputs are realistic. PULSE thereby generates\nsuper-resolved images that both are realistic and downscale correctly. We show\nextensive experimental results demonstrating the efficacy of our approach in\nthe domain of face super-resolution (also known as face hallucination). Our\nmethod outperforms state-of-the-art methods in perceptual quality at higher\nresolutions and scale factors than previously possible."}, {"title": "Scalability in Perception for Autonomous Driving: Waymo Open Dataset", "authors": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aur\u00e9lien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, Dragomir Anguelov", "link": "https://arxiv.org/abs/1912.04838", "summary": "The research community has increasing interest in autonomous driving\nresearch, despite the resource intensity of obtaining representative real world\ndata. Existing self-driving datasets are limited in the scale and variation of\nthe environments they capture, even though generalization within and between\noperating regions is crucial to the overall viability of the technology. In an\neffort to help align the research community's contributions with real-world\nself-driving problems, we introduce a new large scale, high quality, diverse\ndataset. Our new dataset consists of 1150 scenes that each span 20 seconds,\nconsisting of well synchronized and calibrated high quality LiDAR and camera\ndata captured across a range of urban and suburban geographies. It is 15x more\ndiverse than the largest camera+LiDAR dataset available based on our proposed\ndiversity metric. We exhaustively annotated this data with 2D (camera image)\nand 3D (LiDAR) bounding boxes, with consistent identifiers across frames.\nFinally, we provide strong baselines for 2D as well as 3D detection and\ntracking tasks. We further study the effects of dataset size and generalization\nacross geographies on 3D detection methods. Find data, code and more up-to-date\ninformation at http://www.waymo.com/open."}, {"title": "Extreme Relative Pose Network Under Hybrid Representations", "authors": "Zhenpei Yang, Siming Yan, Qixing Huang", "link": "https://arxiv.org/abs/1912.11695", "summary": "In this paper, we introduce a novel RGB-D based relative pose estimation\napproach that is suitable for small-overlapping or non-overlapping scans and\ncan output multiple relative poses. Our method performs scene completion and\nmatches the completed scans. However, instead of using a fixed representation\nfor completion, the key idea is to utilize hybrid representations that combine\n360-image, 2D image-based layout, and planar patches. This approach offers\nadaptively feature representations for relative pose estimation. Besides, we\nintroduce a global-2-local matching procedure, which utilizes initial relative\nposes obtained during the global phase to detect and then integrate geometric\nrelations for pose refinement. Experimental results justify the potential of\nthis approach across a wide range of benchmark datasets. For example, on\nScanNet, the rotation translation errors of the top-1/top-5 predictions of our\napproach are 28.6/0.90m and 16.8/0.76m, respectively. Our approach also\nconsiderably boosts the performance of multi-scan reconstruction in few-view\nreconstruction settings."}, {"title": "Single-Shot Monocular RGB-D Imaging Using Uneven Double Refraction", "authors": "Andreas Meuleman, Seung-Hwan Baek, Felix Heide, Min H. Kim"}, {"title": "Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying Lighting and SVBRDF From a Single Image", "authors": "Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, Manmohan Chandraker", "link": "https://arxiv.org/abs/1905.02722", "summary": "We propose a deep inverse rendering framework for indoor scenes. From a\nsingle RGB image of an arbitrary indoor scene, we create a complete scene\nreconstruction, estimating shape, spatially-varying lighting, and\nspatially-varying, non-Lambertian surface reflectance. To train this network,\nwe augment the SUNCG indoor scene dataset with real-world materials and render\nthem with a fast, high-quality, physically-based GPU renderer to create a\nlarge-scale, photorealistic indoor dataset. Our inverse rendering network\nincorporates physical insights -- including a spatially-varying spherical\nGaussian lighting representation, a differentiable rendering layer to model\nscene appearance, a cascade structure to iteratively refine the predictions and\na bilateral solver for refinement -- allowing us to jointly reason about shape,\nlighting, and reflectance. Experiments show that our framework outperforms\nprevious methods for estimating individual scene components, which also enables\nvarious novel applications for augmented reality, such as photorealistic object\ninsertion and material editing. Code and data will be made publicly available."}, {"title": "3D Packing for Self-Supervised Monocular Depth Estimation", "authors": "Vitor Guizilini, Rare\u0219 Ambru\u0219, Sudeep Pillai, Allan Raventos, Adrien Gaidon", "link": "https://arxiv.org/abs/1905.02693", "summary": "Although cameras are ubiquitous, robotic platforms typically rely on active\nsensors like LiDAR for direct 3D perception. In this work, we propose a novel\nself-supervised monocular depth estimation method combining geometry with a new\ndeep network, PackNet, learned only from unlabeled monocular videos. Our\narchitecture leverages novel symmetrical packing and unpacking blocks to\njointly learn to compress and decompress detail-preserving representations\nusing 3D convolutions. Although self-supervised, our method outperforms other\nself, semi, and fully supervised methods on the KITTI benchmark. The 3D\ninductive bias in PackNet enables it to scale with input resolution and number\nof parameters without overfitting, generalizing better on out-of-domain data\nsuch as the NuScenes dataset. Furthermore, it does not require large-scale\nsupervised pretraining on ImageNet and can run in real-time. Finally, we\nrelease DDAD (Dense Depth for Automated Driving), a new urban driving dataset\nwith more challenging and accurate depth evaluation, thanks to longer-range and\ndenser ground-truth depth generated from high-density LiDARs mounted on a fleet\nof self-driving cars operating world-wide."}, {"title": "Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching", "authors": "Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, Ping Tan", "link": "https://arxiv.org/abs/1912.06378", "summary": "The deep multi-view stereo (MVS) and stereo matching approaches generally\nconstruct 3D cost volumes to regularize and regress the output depth or\ndisparity. These methods are limited when high-resolution outputs are needed\nsince the memory and time costs grow cubically as the volume resolution\nincreases. In this paper, we propose a both memory and time efficient cost\nvolume formulation that is complementary to existing multi-view stereo and\nstereo matching approaches based on 3D cost volumes. First, the proposed cost\nvolume is built upon a standard feature pyramid encoding geometry and context\nat gradually finer scales. Then, we can narrow the depth (or disparity) range\nof each stage by the depth (or disparity) map from the previous stage. With\ngradually higher cost volume resolution and adaptive adjustment of depth (or\ndisparity) intervals, the output is recovered in a coarser to fine manner.\n  We apply the cascade cost volume to the representative MVS-Net, and obtain a\n23.1% improvement on DTU benchmark (1st place), with 50.6% and 74.2% reduction\nin GPU memory and run-time. It is also the state-of-the-art learning-based\nmethod on Tanks and Temples benchmark. The statistics of accuracy, run-time and\nGPU memory on other representative stereo CNNs also validate the effectiveness\nof our proposed method."}, {"title": "From Two Rolling Shutters to One Global Shutter", "authors": "Cenek Albl, Zuzana Kukelova, Viktor Larsson, Michal Polic, Tomas Pajdla, Konrad Schindler"}, {"title": "Deep Global Registration", "authors": "Christopher Choy, Wei Dong, Vladlen Koltun", "link": "https://arxiv.org/abs/2004.11540", "summary": "We present Deep Global Registration, a differentiable framework for pairwise\nregistration of real-world 3D scans. Deep global registration is based on three\nmodules: a 6-dimensional convolutional network for correspondence confidence\nprediction, a differentiable Weighted Procrustes algorithm for closed-form pose\nestimation, and a robust gradient-based SE(3) optimizer for pose refinement.\nExperiments demonstrate that our approach outperforms state-of-the-art methods,\nboth learning-based and classical, on real-world data."}, {"title": "Deep Stereo Using Adaptive Thin Volume Representation With Uncertainty Awareness", "authors": "Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, Hao Su", "link": "https://arxiv.org/abs/1911.12012", "summary": "We present Uncertainty-aware Cascaded Stereo Network (UCS-Net) for 3D\nreconstruction from multiple RGB images. Multi-view stereo (MVS) aims to\nreconstruct fine-grained scene geometry from multi-view images. Previous\nlearning-based MVS methods estimate per-view depth using plane sweep volumes\nwith a fixed depth hypothesis at each plane; this generally requires densely\nsampled planes for desired accuracy, and it is very hard to achieve\nhigh-resolution depth. In contrast, we propose adaptive thin volumes (ATVs); in\nan ATV, the depth hypothesis of each plane is spatially varying, which adapts\nto the uncertainties of previous per-pixel depth predictions. Our UCS-Net has\nthree stages: the first stage processes a small standard plane sweep volume to\npredict low-resolution depth; two ATVs are then used in the following stages to\nrefine the depth with higher resolution and higher accuracy. Our ATV consists\nof only a small number of planes; yet, it efficiently partitions local depth\nranges within learned small intervals. In particular, we propose to use\nvariance-based uncertainty estimates to adaptively construct ATVs; this\ndifferentiable process introduces reasonable and fine-grained spatial\npartitioning. Our multi-stage framework progressively subdivides the vast scene\nspace with increasing depth resolution and precision, which enables scene\nreconstruction with high completeness and accuracy in a coarse-to-fine fashion.\nWe demonstrate that our method achieves superior performance compared with\nstate-of-the-art benchmarks on various challenging datasets."}, {"title": "Why Having 10,000 Parameters in Your Camera Model Is Better Than Twelve", "authors": "Thomas Sch\u00f6ps, Viktor Larsson, Marc Pollefeys, Torsten Sattler", "link": "https://arxiv.org/abs/1912.02908", "summary": "Camera calibration is an essential first step in setting up 3D Computer\nVision systems. Commonly used parametric camera models are limited to a few\ndegrees of freedom and thus often do not optimally fit to complex real lens\ndistortion. In contrast, generic camera models allow for very accurate\ncalibration due to their flexibility. Despite this, they have seen little use\nin practice. In this paper, we argue that this should change. We propose a\ncalibration pipeline for generic models that is fully automated, easy to use,\nand can act as a drop-in replacement for parametric calibration, with a focus\non accuracy. We compare our results to parametric calibrations. Considering\nstereo depth estimation and camera pose estimation as examples, we show that\nthe calibration error acts as a bias on the results. We thus argue that in\ncontrast to current common practice, generic models should be preferred over\nparametric ones whenever possible. To facilitate this, we released our\ncalibration pipeline at https://github.com/puzzlepaint/camera_calibration,\nmaking both easy-to-use and accurate camera calibration available to everyone."}, {"title": "Blur Aware Calibration of Multi-Focus Plenoptic Camera", "authors": "Mathieu Labussi\u00e8re, C\u00e9line Teuli\u00e8re, Fr\u00e9d\u00e9ric Bernardin, Omar Ait-Aider", "link": "http://arxiv.org/abs/2004.07745", "summary": "This paper presents a novel calibration algorithm for Multi-Focus Plenoptic\nCameras (MFPCs) using raw images only. The design of such cameras is usually\ncomplex and relies on precise placement of optic elements. Several calibration\nprocedures have been proposed to retrieve the camera parameters but relying on\nsimplified models, reconstructed images to extract features, or multiple\ncalibrations when several types of micro-lens are used. Considering blur\ninformation, we propose a new Blur Aware Plenoptic (BAP) feature. It is first\nexploited in a pre-calibration step that retrieves initial camera parameters,\nand secondly to express a new cost function for our single optimization\nprocess. The effectiveness of our calibration method is validated by\nquantitative and qualitative experiments."}, {"title": "Learning Fused Pixel and Feature-Based View Reconstructions for Light Fields", "authors": "Jinglei Shi, Xiaoran Jiang, Christine Guillemot"}, {"title": "SAL: Sign Agnostic Learning of Shapes From Raw Data", "authors": "Matan Atzmon, Yaron Lipman", "link": "https://arxiv.org/abs/1911.10414", "summary": "Recently, neural networks have been used as implicit representations for\nsurface reconstruction, modelling, learning, and generation. So far, training\nneural networks to be implicit representations of surfaces required training\ndata sampled from a ground-truth signed implicit functions such as signed\ndistance or occupancy functions, which are notoriously hard to compute.\n  In this paper we introduce Sign Agnostic Learning (SAL), a deep learning\napproach for learning implicit shape representations directly from raw,\nunsigned geometric data, such as point clouds and triangle soups.\n  We have tested SAL on the challenging problem of surface reconstruction from\nan un-oriented point cloud, as well as end-to-end human shape space learning\ndirectly from raw scans dataset, and achieved state of the art reconstructions\ncompared to current approaches. We believe SAL opens the door to many geometric\ndeep learning applications with real-world data, alleviating the usual\npainstaking, often manual pre-process."}, {"title": "Google Landmarks Dataset v2 \u2013 A Large-Scale Benchmark for Instance-Level Recognition and Retrieval", "authors": "Tobias Weyand, Andr\u00e9 Araujo, Bingyi Cao, Jack Sim", "link": "https://arxiv.org/abs/2004.01804", "summary": "While image retrieval and instance recognition techniques are progressing\nrapidly, there is a need for challenging datasets to accurately measure their\nperformance -- while posing novel challenges that are relevant for practical\napplications. We introduce the Google Landmarks Dataset v2 (GLDv2), a new\nbenchmark for large-scale, fine-grained instance recognition and image\nretrieval in the domain of human-made and natural landmarks. GLDv2 is the\nlargest such dataset to date by a large margin, including over 5M images and\n200k distinct instance labels. Its test set consists of 118k images with ground\ntruth annotations for both the retrieval and recognition tasks. The ground\ntruth construction involved over 800 hours of human annotator work. Our new\ndataset has several challenging properties inspired by real world applications\nthat previous datasets did not consider: An extremely long-tailed class\ndistribution, a large fraction of out-of-domain test photos and large\nintra-class variability. The dataset is sourced from Wikimedia Commons, the\nworld's largest crowdsourced collection of landmark photos. We provide baseline\nresults for both recognition and retrieval tasks based on state-of-the-art\nmethods as well as competitive results from a public challenge. We further\ndemonstrate the suitability of the dataset for transfer learning by showing\nthat image embeddings trained on it achieve competitive retrieval performance\non independent datasets. The dataset images, ground-truth and metric scoring\ncode are available at https://github.com/cvdfoundation/google-landmark."}, {"title": "Instance Guided Proposal Network for Person Search", "authors": "Wenkai Dong, Zhaoxiang Zhang, Chunfeng Song, Tieniu Tan"}, {"title": "Which Is Plagiarism: Fashion Image Retrieval Based on Regional Representation for Design Protection", "authors": "Yining Lang, Yuan He, Fan Yang, Jianfeng Dong, Hui Xue"}, {"title": "Inter-Task Association Critic for Cross-Resolution Person Re-Identification", "authors": "Zhiyi Cheng, Qi Dong, Shaogang Gong, Xiatian Zhu"}, {"title": "FineGym: A Hierarchical Video Dataset for Fine-Grained Action Understanding", "authors": "Dian Shao, Yue Zhao, Bo Dai, Dahua Lin", "link": "https://arxiv.org/abs/2004.06704", "summary": "On public benchmarks, current action recognition techniques have achieved\ngreat success. However, when used in real-world applications, e.g. sport\nanalysis, which requires the capability of parsing an activity into phases and\ndifferentiating between subtly different actions, their performances remain far\nfrom being satisfactory. To take action recognition to a new level, we develop\nFineGym, a new dataset built on top of gymnastic videos. Compared to existing\naction recognition datasets, FineGym is distinguished in richness, quality, and\ndiversity. In particular, it provides temporal annotations at both action and\nsub-action levels with a three-level semantic hierarchy. For example, a\n\"balance beam\" event will be annotated as a sequence of elementary sub-actions\nderived from five sets: \"leap-jump-hop\", \"beam-turns\", \"flight-salto\",\n\"flight-handspring\", and \"dismount\", where the sub-action in each set will be\nfurther annotated with finely defined class labels. This new level of\ngranularity presents significant challenges for action recognition, e.g. how to\nparse the temporal structures from a coherent action, and how to distinguish\nbetween subtly different action classes. We systematically investigate\nrepresentative methods on this dataset and obtain a number of interesting\nfindings. We hope this dataset could advance research towards action\nunderstanding."}, {"title": "Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition", "authors": "Frederik Warburg, S\u00f8ren Hauberg, Manuel L\u00f3pez-Antequera, Pau Gargallo, Yubin Kuang, Javier Civera"}, {"title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning", "authors": "Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, Trevor Darrell", "link": "https://arxiv.org/abs/1805.04687", "summary": "Datasets drive vision progress, yet existing driving datasets are\nimpoverished in terms of visual content and supported tasks to study multitask\nlearning for autonomous driving. Researchers are usually constrained to study a\nsmall set of problems on one dataset, while real-world computer vision\napplications require performing tasks of various complexities. We construct\nBDD100K, the largest driving video dataset with 100K videos and 10 tasks to\nevaluate the exciting progress of image recognition algorithms on autonomous\ndriving. The dataset possesses geographic, environmental, and weather\ndiversity, which is useful for training models that are less likely to be\nsurprised by new conditions. Based on this diverse dataset, we build a\nbenchmark for heterogeneous multitask learning and study how to solve the tasks\ntogether. Our experiments show that special training strategies are needed for\nexisting models to perform such heterogeneous tasks. BDD100K opens the door for\nfuture studies in this important venue."}, {"title": "Rethinking Computer-Aided Tuberculosis Diagnosis", "authors": "Yun Liu, Yu-Huan Wu, Yunfeng Ban, Huifang Wang, Ming-Ming Cheng"}, {"title": "IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning", "authors": "Xi Yang, Ding Xia, Taichi Kin, Takeo Igarashi", "link": "https://arxiv.org/abs/2003.02920", "summary": "Medicine is an important application area for deep learning models. Research\nin this field is a combination of medical expertise and data science knowledge.\nIn this paper, instead of 2D medical images, we introduce an open-access 3D\nintracranial aneurysm dataset, IntrA, that makes the application of\npoints-based and mesh-based classification and segmentation models available.\nOur dataset can be used to diagnose intracranial aneurysms and to extract the\nneck for a clipping operation in medicine and other areas of deep learning,\nsuch as normal estimation and surface reconstruction. We provide a large-scale\nbenchmark of classification and part segmentation by testing state-of-the-art\nnetworks. We also discuss the performance of each method and demonstrate the\nchallenges of our dataset. The published dataset can be accessed here:\nhttps://github.com/intra3d2019/IntrA."}, {"title": "Revisiting Saliency Metrics: Farthest-Neighbor Area Under Curve", "authors": "Sen Jia, Neil D. B. Bruce", "link": "https://arxiv.org/abs/2002.10540", "summary": "Saliency detection has been widely studied because it plays an important role\nin various vision applications, but it is difficult to evaluate saliency\nsystems because each measure has its own bias. In this paper, we first revisit\nthe problem of applying the widely used saliency metrics on modern\nConvolutional Neural Networks(CNNs). Our investigation shows the saliency\ndatasets have been built based on different choices of parameters and CNNs are\ndesigned to fit a dataset-specific distribution. Secondly, we show that the\nShuffled Area Under Curve(S-AUC) metric still suffers from spatial biases. We\npropose a new saliency metric based on the AUC property, which aims at sampling\na more directional negative set for evaluation, denoted as Farthest-Neighbor\nAUC(FN-AUC). We also propose a strategy to measure the quality of the sampled\nnegative set. Our experiment shows FN-AUC can measure spatial biases, central\nand peripheral, more effectively than S-AUC without penalizing the fixation\nlocations. Thirdly, we propose a global smoothing function to overcome the\nproblem of few value degrees (output quantization) in computing AUC metrics.\nComparing with random noise, our smooth function can create unique values\nwithout losing the relative saliency relationship."}, {"title": "Computing the Testing Error Without a Testing Set", "authors": "Ciprian A. Corneanu, Sergio Escalera, Aleix M. Martinez", "link": "https://arxiv.org/abs/2005.00450", "summary": "Deep Neural Networks (DNNs) have revolutionized computer vision. We now have\nDNNs that achieve top (performance) results in many problems, including object\nrecognition, facial expression analysis, and semantic segmentation, to name but\na few. The design of the DNNs that achieve top results is, however, non-trivial\nand mostly done by trail-and-error. That is, typically, researchers will derive\nmany DNN architectures (i.e., topologies) and then test them on multiple\ndatasets. However, there are no guarantees that the selected DNN will perform\nwell in the real world. One can use a testing set to estimate the performance\ngap between the training and testing sets, but avoiding\noverfitting-to-the-testing-data is almost impossible. Using a sequestered\ntesting dataset may address this problem, but this requires a constant update\nof the dataset, a very expensive venture. Here, we derive an algorithm to\nestimate the performance gap between training and testing that does not require\nany testing dataset. Specifically, we derive a number of persistent topology\nmeasures that identify when a DNN is learning to generalize to unseen samples.\nThis allows us to compute the DNN's testing error on unseen samples, even when\nwe do not have access to them. We provide extensive experimental validation on\nmultiple networks and datasets to demonstrate the feasibility of the proposed\napproach."}, {"title": "Improving Confidence Estimates for Unfamiliar Examples", "authors": "Zhizhong Li, Derek Hoiem", "link": "https://arxiv.org/abs/1804.03166", "summary": "Intuitively, unfamiliarity should lead to lack of confidence. In reality,\ncurrent algorithms often make highly confident yet wrong predictions when faced\nwith relevant but unfamiliar examples. A classifier we trained to recognize\ngender is 12 times more likely to be wrong with a 99% confident prediction if\npresented with a subject from a different age group than those seen during\ntraining. In this paper, we compare and evaluate several methods to improve\nconfidence estimates for unfamiliar and familiar samples. We propose a testing\nmethodology of splitting unfamiliar and familiar samples by attribute (age,\nbreed, subcategory) or sampling (similar datasets collected by different people\nat different times). We evaluate methods including confidence calibration,\nensembles, distillation, and a Bayesian model and use several metrics to\nanalyze label, likelihood, and calibration error. While all methods reduce\nover-confident errors, the ensemble of calibrated models performs best overall,\nand T-scaling performs best among the approaches with fastest inference.\n  $\\color{red}{\\text{Please see ERRATA.}}$"}, {"title": "CycleISP: Real Image Restoration via Improved Data Synthesis", "authors": "Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, Ling Shao", "link": "https://arxiv.org/abs/2003.07761", "summary": "The availability of large-scale datasets has helped unleash the true\npotential of deep convolutional neural networks (CNNs). However, for the\nsingle-image denoising problem, capturing a real dataset is an unacceptably\nexpensive and cumbersome procedure. Consequently, image denoising algorithms\nare mostly developed and evaluated on synthetic data that is usually generated\nwith a widespread assumption of additive white Gaussian noise (AWGN). While the\nCNNs achieve impressive results on these synthetic datasets, they do not\nperform well when applied on real camera images, as reported in recent\nbenchmark datasets. This is mainly because the AWGN is not adequate for\nmodeling the real camera noise which is signal-dependent and heavily\ntransformed by the camera imaging pipeline. In this paper, we present a\nframework that models camera imaging pipeline in forward and reverse\ndirections. It allows us to produce any number of realistic image pairs for\ndenoising both in RAW and sRGB spaces. By training a new image denoising\nnetwork on realistic synthetic data, we achieve the state-of-the-art\nperformance on real camera benchmark datasets. The parameters in our model are\n~5 times lesser than the previous best method for RAW denoising. Furthermore,\nwe demonstrate that the proposed framework generalizes beyond image denoising\nproblem e.g., for color matching in stereoscopic cinema. The source code and\npre-trained models are available at https://github.com/swz30/CycleISP."}, {"title": "Enhanced Blind Face Restoration With Multi-Exemplar Images and Adaptive Spatial Feature Fusion", "authors": "Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, Wangmeng Zuo"}, {"title": "Explorable Super Resolution", "authors": "Yuval Bahat, Tomer Michaeli", "link": "https://arxiv.org/abs/1912.01839", "summary": "Single image super resolution (SR) has seen major performance leaps in recent\nyears. However, existing methods do not allow exploring the infinitely many\nplausible reconstructions that might have given rise to the observed\nlow-resolution (LR) image. These different explanations to the LR image may\ndramatically vary in their textures and fine details, and may often encode\ncompletely different semantic information. In this paper, we introduce the task\nof explorable super resolution. We propose a framework comprising a graphical\nuser interface with a neural network backend, allowing editing the SR output so\nas to explore the abundance of plausible HR explanations to the LR input. At\nthe heart of our method is a novel module that can wrap any existing SR\nnetwork, analytically guaranteeing that its SR outputs would precisely match\nthe LR input, when downsampled. Besides its importance in our setting, this\nmodule is guaranteed to decrease the reconstruction error of any SR network it\nwraps, and can be used to cope with blur kernels that are different from the\none the network was trained for. We illustrate our approach in a variety of use\ncases, ranging from medical imaging and forensics, to graphics."}, {"title": "Syn2Real Transfer Learning for Image Deraining Using Gaussian Processes", "authors": "Rajeev Yasarla, Vishwanath A. Sindagi, Vishal M. Patel"}, {"title": "Deblurring by Realistic Blurring", "authors": "Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bj\u00f6rn Stenger, Wei Liu, Hongdong Li", "link": "http://arxiv.org/abs/2004.01860", "summary": "Existing deep learning methods for image deblurring typically train models\nusing pairs of sharp images and their blurred counterparts. However,\nsynthetically blurring images do not necessarily model the genuine blurring\nprocess in real-world scenarios with sufficient accuracy. To address this\nproblem, we propose a new method which combines two GAN models, i.e., a\nlearning-to-Blur GAN (BGAN) and learning-to-DeBlur GAN (DBGAN), in order to\nlearn a better model for image deblurring by primarily learning how to blur\nimages. The first model, BGAN, learns how to blur sharp images with unpaired\nsharp and blurry image sets, and then guides the second model, DBGAN, to learn\nhow to correctly deblur such images. In order to reduce the discrepancy between\nreal blur and synthesized blur, a relativistic blur loss is leveraged. As an\nadditional contribution, this paper also introduces a Real-World Blurred Image\n(RWBI) dataset including diverse blurry images. Our experiments show that the\nproposed method achieves consistently superior quantitative performance as well\nas higher perceptual quality on both the newly proposed dataset and the public\nGOPRO dataset."}, {"title": "Bringing Old Photos Back to Life", "authors": "Ziyu Wan, Bo Zhang, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao, Fang Wen", "link": "https://arxiv.org/abs/2004.09484", "summary": "We propose to restore old photos that suffer from severe degradation through\na deep learning approach. Unlike conventional restoration tasks that can be\nsolved through supervised learning, the degradation in real photos is complex\nand the domain gap between synthetic images and real old photos makes the\nnetwork fail to generalize. Therefore, we propose a novel triplet domain\ntranslation network by leveraging real photos along with massive synthetic\nimage pairs. Specifically, we train two variational autoencoders (VAEs) to\nrespectively transform old photos and clean photos into two latent spaces. And\nthe translation between these two latent spaces is learned with synthetic\npaired data. This translation generalizes well to real photos because the\ndomain gap is closed in the compact latent space. Besides, to address multiple\ndegradations mixed in one old photo, we design a global branch with a partial\nnonlocal block targeting to the structured defects, such as scratches and dust\nspots, and a local branch targeting to the unstructured defects, such as noises\nand blurriness. Two branches are fused in the latent space, leading to improved\ncapability to restore old photos from multiple defects. The proposed method\noutperforms state-of-the-art methods in terms of visual quality for old photos\nrestoration."}, {"title": "A Physics-Based Noise Formation Model for Extreme Low-Light Raw Denoising", "authors": "Kaixuan Wei, Ying Fu, Jiaolong Yang, Hua Huang", "link": "https://arxiv.org/abs/2003.12751", "summary": "Lacking rich and realistic data, learned single image denoising algorithms\ngeneralize poorly to real raw images that do not resemble the data used for\ntraining. Although the problem can be alleviated by the heteroscedastic\nGaussian model for noise synthesis, the noise sources caused by digital camera\nelectronics are still largely overlooked, despite their significant effect on\nraw measurement, especially under extremely low-light condition. To address\nthis issue, we present a highly accurate noise formation model based on the\ncharacteristics of CMOS photosensors, thereby enabling us to synthesize\nrealistic samples that better match the physics of image formation process.\nGiven the proposed noise model, we additionally propose a method to calibrate\nthe noise parameters for available modern digital cameras, which is simple and\nreproducible for any new device. We systematically study the generalizability\nof a neural network trained with existing schemes, by introducing a new\nlow-light denoising dataset that covers many modern digital cameras from\ndiverse brands. Extensive empirical results collectively show that by utilizing\nour proposed noise formation model, a network can reach the capability as if it\nhad been trained with rich real data, which demonstrates the effectiveness of\nour noise formation model."}, {"title": "Learning to Super Resolve Intensity Images From Events", "authors": "S. Mohammad Mostafavi I., Jonghyun Choi, Kuk-Jin Yoon", "link": "https://arxiv.org/abs/1912.01196", "summary": "An event camera detects per-pixel intensity difference and produces\nasynchronous event stream with low latency, high dynamic range, and low power\nconsumption. As a trade-off, the event camera has low spatial resolution. We\npropose an end-to-end network to reconstruct high resolution, high dynamic\nrange (HDR) images directly from the event stream. We evaluate our algorithm on\nboth simulated and real-world sequences and verify that it captures fine\ndetails of a scene and outperforms the combination of the state-of-the-art\nevent to image algorithms with the state-of-the-art super resolution schemes in\nmany quantitative measures by large margins. We further extend our method by\nusing the active sensor pixel (APS) frames or reconstructing images\niteratively."}, {"title": "Camouflaged Object Detection", "authors": "Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, Ling Shao"}, {"title": "Holistically-Attracted Wireframe Parsing", "authors": "Nan Xue, Tianfu Wu, Song Bai, Fudong Wang, Gui-Song Xia, Liangpei Zhang, Philip H.S. Torr", "link": "https://arxiv.org/abs/2003.01663", "summary": "This paper presents a fast and parsimonious parsing method to accurately and\nrobustly detect a vectorized wireframe in an input image with a single forward\npass. The proposed method is end-to-end trainable, consisting of three\ncomponents: (i) line segment and junction proposal generation, (ii) line\nsegment and junction matching, and (iii) line segment and junction\nverification. For computing line segment proposals, a novel exact dual\nrepresentation is proposed which exploits a parsimonious geometric\nreparameterization for line segments and forms a holistic 4-dimensional\nattraction field map for an input image. Junctions can be treated as the\n\"basins\" in the attraction field. The proposed method is thus called\nHolistically-Attracted Wireframe Parser (HAWP). In experiments, the proposed\nmethod is tested on two benchmarks, the Wireframe dataset, and the YorkUrban\ndataset. On both benchmarks, it obtains state-of-the-art performance in terms\nof accuracy and efficiency. For example, on the Wireframe dataset, compared to\nthe previous state-of-the-art method L-CNN, it improves the challenging mean\nstructural average precision (msAP) by a large margin ($2.8\\%$ absolute\nimprovements) and achieves 29.5 FPS on single GPU ($89\\%$ relative\nimprovement). A systematic ablation study is performed to further justify the\nproposed method."}, {"title": "Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction", "authors": "Fuyang Zhang, Nelson Nauata, Yasutaka Furukawa", "link": "https://arxiv.org/abs/1912.01756", "summary": "This paper proposes a novel message passing neural (MPN) architecture\nConv-MPN, which reconstructs an outdoor building as a planar graph from a\nsingle RGB image. Conv-MPN is specifically designed for cases where nodes of a\ngraph have explicit spatial embedding. In our problem, nodes correspond to\nbuilding edges in an image. Conv-MPN is different from MPN in that 1) the\nfeature associated with a node is represented as a feature volume instead of a\n1D vector; and 2) convolutions encode messages instead of fully connected\nlayers. Conv-MPN learns to select a true subset of nodes (i.e., building edges)\nto reconstruct a building planar graph. Our qualitative and quantitative\nevaluations over 2,000 buildings show that Conv-MPN makes significant\nimprovements over the existing fully neural solutions. We believe that the\npaper has a potential to open a new line of graph neural network research for\nstructured geometry reconstruction."}, {"title": "Domain Adaptation for Image Dehazing", "authors": "Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao, Nong Sang", "link": "https://arxiv.org/abs/2005.04668", "summary": "Image dehazing using learning-based methods has achieved state-of-the-art\nperformance in recent years. However, most existing methods train a dehazing\nmodel on synthetic hazy images, which are less able to generalize well to real\nhazy images due to domain shift. To address this issue, we propose a domain\nadaptation paradigm, which consists of an image translation module and two\nimage dehazing modules. Specifically, we first apply a bidirectional\ntranslation network to bridge the gap between the synthetic and real domains by\ntranslating images from one domain to another. And then, we use images before\nand after translation to train the proposed two image dehazing networks with a\nconsistency constraint. In this phase, we incorporate the real hazy image into\nthe dehazing training via exploiting the properties of the clear image (e.g.,\ndark channel prior and image gradient smoothing) to further improve the domain\nadaptivity. By training image translation and dehazing network in an end-to-end\nmanner, we can obtain better effects of both image translation and dehazing.\nExperimental results on both synthetic and real-world images demonstrate that\nour model performs favorably against the state-of-the-art dehazing algorithms."}, {"title": "Auto-Encoding Twin-Bottleneck Hashing", "authors": "Yuming Shen, Jie Qin, Jiaxin Chen, Mengyang Yu, Li Liu, Fan Zhu, Fumin Shen, Ling Shao", "link": "http://arxiv.org/abs/2002.11930", "summary": "Conventional unsupervised hashing methods usually take advantage of\nsimilarity graphs, which are either pre-computed in the high-dimensional space\nor obtained from random anchor points. On the one hand, existing methods\nuncouple the procedures of hash function learning and graph construction. On\nthe other hand, graphs empirically built upon original data could introduce\nbiased prior knowledge of data relevance, leading to sub-optimal retrieval\nperformance. In this paper, we tackle the above problems by proposing an\nefficient and adaptive code-driven graph, which is updated by decoding in the\ncontext of an auto-encoder. Specifically, we introduce into our framework twin\nbottlenecks (i.e., latent variables) that exchange crucial information\ncollaboratively. One bottleneck (i.e., binary codes) conveys the high-level\nintrinsic data structure captured by the code-driven graph to the other (i.e.,\ncontinuous variables for low-level detail information), which in turn\npropagates the updated network feedback for the encoder to learn more\ndiscriminative binary codes. The auto-encoding learning objective literally\nrewards the code-driven graph to learn an optimal encoder. Moreover, the\nproposed model can be simply optimized by gradient descent without violating\nthe binary constraints. Experiments on benchmarked datasets clearly show the\nsuperiority of our framework over the state-of-the-art hashing methods. Our\nsource code can be found at https://github.com/ymcidence/TBH."}, {"title": "Agriculture-Vision: A Large Aerial Image Database for Agricultural Pattern Analysis", "authors": "Mang Tik Chiu, Xingqian Xu, Yunchao Wei, Zilong Huang, Alexander G. Schwing, Robert Brunner, Hrant Khachatrian, Hovnatan Karapetyan, Ivan Dozier, Greg Rose, David Wilson, Adrian Tudor, Naira Hovakimyan, Thomas S. Huang, Honghui Shi", "link": "https://arxiv.org/abs/2001.01306", "summary": "The success of deep learning in visual recognition tasks has driven\nadvancements in multiple fields of research. Particularly, increasing attention\nhas been drawn towards its application in agriculture. Nevertheless, while\nvisual pattern recognition on farmlands carries enormous economic values,\nlittle progress has been made to merge computer vision and crop sciences due to\nthe lack of suitable agricultural image datasets. Meanwhile, problems in\nagriculture also pose new challenges in computer vision. For example, semantic\nsegmentation of aerial farmland images requires inference over extremely\nlarge-size images with extreme annotation sparsity. These challenges are not\npresent in most of the common object datasets, and we show that they are more\nchallenging than many other aerial image datasets. To encourage research in\ncomputer vision for agriculture, we present Agriculture-Vision: a large-scale\naerial farmland image dataset for semantic segmentation of agricultural\npatterns. We collected 94,986 high-quality aerial images from 3,432 farmlands\nacross the US, where each image consists of RGB and Near-infrared (NIR)\nchannels with resolution as high as 10 cm per pixel. We annotate nine types of\nfield anomaly patterns that are most important to farmers. As a pilot study of\naerial agricultural semantic segmentation, we perform comprehensive experiments\nusing popular semantic segmentation models; we also propose an effective model\ndesigned for aerial agricultural pattern recognition. Our experiments\ndemonstrate several challenges Agriculture-Vision poses to both the computer\nvision and agriculture communities. Future versions of this dataset will\ninclude even more aerial images, anomaly patterns and image channels. More\ninformation at https://www.agriculture-vision.com."}, {"title": "Bi-Directional Interaction Network for Person Search", "authors": "Wenkai Dong, Zhaoxiang Zhang, Chunfeng Song, Tieniu Tan"}, {"title": "Meshlet Priors for 3D Mesh Reconstruction", "authors": "Abhishek Badki, Orazio Gallo, Jan Kautz, Pradeep Sen", "link": "https://arxiv.org/abs/2001.01744", "summary": "Estimating a mesh from an unordered set of sparse, noisy 3D points is a\nchallenging problem that requires carefully selected priors. Existing\nhand-crafted priors, such as smoothness regularizers, impose an undesirable\ntrade-off between attenuating noise and preserving local detail. Recent\ndeep-learning approaches produce impressive results by learning priors directly\nfrom the data. However, the priors are learned at the object level, which makes\nthese algorithms class-specific and even sensitive to the pose of the object.\nWe introduce meshlets, small patches of mesh that we use to learn local shape\npriors. Meshlets act as a dictionary of local features and thus allow to use\nlearned priors to reconstruct object meshes in any pose and from unseen\nclasses, even when the noise is large and the samples sparse."}, {"title": "Space-Time-Aware Multi-Resolution Video Enhancement", "authors": "Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita", "link": "https://arxiv.org/abs/2003.13170", "summary": "We consider the problem of space-time super-resolution (ST-SR): increasing\nspatial resolution of video frames and simultaneously interpolating frames to\nincrease the frame rate. Modern approaches handle these axes one at a time. In\ncontrast, our proposed model called STARnet super-resolves jointly in space and\ntime. This allows us to leverage mutually informative relationships between\ntime and space: higher resolution can provide more detailed information about\nmotion, and higher frame-rate can provide better pixel alignment. The\ncomponents of our model that generate latent low- and high-resolution\nrepresentations during ST-SR can be used to finetune a specialized mechanism\nfor just spatial or just temporal super-resolution. Experimental results\ndemonstrate that STARnet improves the performances of space-time, spatial, and\ntemporal video super-resolution by substantial margins on publicly available\ndatasets."}, {"title": "FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation", "authors": "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang", "link": "https://arxiv.org/abs/1907.12347", "summary": "Over the past few years, we have witnessed the success of deep learning in\nimage recognition thanks to the availability of large-scale human-annotated\ndatasets such as PASCAL VOC, ImageNet, and COCO. Although these datasets have\ncovered a wide range of object categories, there are still a significant number\nof objects that are not included. Can we perform the same task without a lot of\nhuman annotations? In this paper, we are interested in few-shot object\nsegmentation where the number of annotated training examples are limited to 5\nonly. To evaluate and validate the performance of our approach, we have built a\nfew-shot segmentation dataset, FSS-1000, which consists of 1000 object classes\nwith pixelwise annotation of ground-truth segmentation. Unique in FSS-1000, our\ndataset contains significant number of objects that have never been seen or\nannotated in previous datasets, such as tiny daily objects, merchandise,\ncartoon characters, logos, etc. We build our baseline model using standard\nbackbone networks such as VGG-16, ResNet-101, and Inception. To our surprise,\nwe found that training our model from scratch using FSS-1000 achieves\ncomparable and even better results than training with weights pre-trained by\nImageNet which is more than 100 times larger than FSS-1000. Both our approach\nand dataset are simple, effective, and easily extensible to learn segmentation\nof new object classes given very few annotated training examples. Dataset is\navailable at https://github.com/HKUSTCV/FSS-1000."}, {"title": "MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation", "authors": "John Lambert, Zhuang Liu, Ozan Sener, James Hays, Vladlen Koltun"}, {"title": "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection", "authors": "Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy", "link": "https://arxiv.org/abs/2001.03024", "summary": "In this paper, we present our on-going effort of constructing a large-scale\nbenchmark, DeeperForensics-1.0, for face forgery detection. Our benchmark\nrepresents the largest face forgery detection dataset by far, with 60, 000\nvideos constituted by a total of 17.6 million frames, 10 times larger than\nexisting datasets of the same kind. Extensive real-world perturbations are\napplied to obtain a more challenging benchmark of larger scale and higher\ndiversity. All source videos in DeeperForensics-1.0 are carefully collected,\nand fake videos are generated by a newly proposed end-to-end face swapping\nframework. The quality of generated videos outperforms those in existing\ndatasets, validated by user studies. The benchmark features a hidden test set,\nwhich contains manipulated videos achieving high deceptive scores in human\nevaluations. We further contribute a comprehensive study that evaluates five\nrepresentative detection baselines and make a thorough analysis of different\nsettings. We believe this dataset will contribute to real-world face forgery\ndetection research."}, {"title": "Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identification", "authors": "Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, Ling Shao", "link": "", "summary": ""}, {"title": "Online Joint Multi-Metric Adaptation From Frequent Sharing-Subset Mining for Person Re-Identification", "authors": "Jiahuan Zhou, Bing Su, Ying Wu"}, {"title": "Taking a Deeper Look at Co-Salient Object Detection", "authors": "Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, Ming-Ming Cheng"}, {"title": "Single-Stage 6D Object Pose Estimation", "authors": "Yinlin Hu, Pascal Fua, Wei Wang, Mathieu Salzmann", "link": "https://arxiv.org/abs/1911.08324", "summary": "Most recent 6D pose estimation frameworks first rely on a deep network to\nestablish correspondences between 3D object keypoints and 2D image locations\nand then use a variant of a RANSAC-based Perspective-n-Point (PnP) algorithm.\nThis two-stage process, however, is suboptimal: First, it is not end-to-end\ntrainable. Second, training the deep network relies on a surrogate loss that\ndoes not directly reflect the final 6D pose estimation task.\n  In this work, we introduce a deep architecture that directly regresses 6D\nposes from correspondences. It takes as input a group of candidate\ncorrespondences for each 3D keypoint and accounts for the fact that the order\nof the correspondences within each group is irrelevant, while the order of the\ngroups, that is, of the 3D keypoints, is fixed. Our architecture is generic and\ncan thus be exploited in conjunction with existing correspondence-extraction\nnetworks so as to yield single-stage 6D pose estimation frameworks. Our\nexperiments demonstrate that these single-stage frameworks consistently\noutperform their two-stage counterparts in terms of both accuracy and speed."}, {"title": "OccuSeg: Occupancy-Aware 3D Instance Segmentation", "authors": "Lei Han, Tian Zheng, Lan Xu, Lu Fang", "link": "https://arxiv.org/abs/2003.06537", "summary": "3D instance segmentation, with a variety of applications in robotics and\naugmented reality, is in large demands these days. Unlike 2D images that are\nprojective observations of the environment, 3D models provide metric\nreconstruction of the scenes without occlusion or scale ambiguity. In this\npaper, we define \"3D occupancy size\", as the number of voxels occupied by each\ninstance. It owns advantages of robustness in prediction, on which basis,\nOccuSeg, an occupancy-aware 3D instance segmentation scheme is proposed. Our\nmulti-task learning produces both occupancy signal and embedding\nrepresentations, where the training of spatial and feature embeddings varies\nwith their difference in scale-aware. Our clustering scheme benefits from the\nreliable comparison between the predicted occupancy size and the clustered\noccupancy size, which encourages hard samples being correctly clustered and\navoids over segmentation. The proposed approach achieves state-of-the-art\nperformance on 3 real-world datasets, i.e. ScanNetV2, S3DIS and SceneNN, while\nmaintaining high efficiency."}, {"title": "Camera Trace Erasing", "authors": "Chang Chen, Zhiwei Xiong, Xiaoming Liu, Feng Wu", "link": "https://arxiv.org/abs/2003.06951", "summary": "Camera trace is a unique noise produced in digital imaging process. Most\nexisting forensic methods analyze camera trace to identify image origins. In\nthis paper, we address a new low-level vision problem, camera trace erasing, to\nreveal the weakness of trace-based forensic methods. A comprehensive\ninvestigation on existing anti-forensic methods reveals that it is non-trivial\nto effectively erase camera trace while avoiding the destruction of content\nsignal. To reconcile these two demands, we propose Siamese Trace Erasing\n(SiamTE), in which a novel hybrid loss is designed on the basis of Siamese\narchitecture for network training. Specifically, we propose embedded\nsimilarity, truncated fidelity, and cross identity to form the hybrid loss.\nCompared with existing anti-forensic methods, SiamTE has a clear advantage for\ncamera trace erasing, which is demonstrated in three representative tasks. Code\nand dataset are available at https://github.com/ngchc/CameraTE."}, {"title": "Deep Metric Learning via Adaptive Learnable Assessment", "authors": "Wenzhao Zheng, Jiwen Lu, Jie Zhou"}, {"title": "Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective", "authors": "Jialun Liu, Yifan Sun, Chuchu Han, Zhaopeng Dou, Wenhui Li", "link": "https://arxiv.org/abs/2002.10826", "summary": "This paper considers learning deep features from long-tailed data. We observe\nthat in the deep feature space, the head classes and the tail classes present\ndifferent distribution patterns. The head classes have a relatively large\nspatial span, while the tail classes have significantly small spatial span, due\nto the lack of intra-class diversity. This uneven distribution between head and\ntail classes distorts the overall feature space, which compromises the\ndiscriminative ability of the learned features. Intuitively, we seek to expand\nthe distribution of the tail classes by transferring from the head classes, so\nas to alleviate the distortion of the feature space. To this end, we propose to\nconstruct each feature into a \"feature cloud\". If a sample belongs to a tail\nclass, the corresponding feature cloud will have relatively large distribution\nrange, in compensation to its lack of diversity. It allows each tail sample to\npush the samples from other classes far away, recovering the intra-class\ndiversity of tail classes. Extensive experimental evaluations on person\nre-identification and face recognition tasks confirm the effectiveness of our\nmethod."}, {"title": "Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention", "authors": "Ming Jiang, Shi Chen, Jinhui Yang, Qi Zhao"}, {"title": "HUMBI: A Large Multiview Dataset of Human Body Expressions", "authors": "Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth Venkatesh, Jaesik Park, Jihun Yu, Hyun Soo Park", "link": "", "summary": ""}, {"title": "Image Search With Text Feedback by Visiolinguistic Attention Learning", "authors": "Yanbei Chen, Shaogang Gong, Loris Bazzani"}, {"title": "Image Processing Using Multi-Code GAN Prior", "authors": "Jinjin Gu, Yujun Shen, Bolei Zhou", "link": "", "summary": ""}, {"title": "What Does Plate Glass Reveal About Camera Calibration?", "authors": "Qian Zheng, Jinnan Chen, Zhan Lu, Boxin Shi, Xudong Jiang, Kim-Hui Yap, Ling-Yu Duan, Alex C. Kot"}, {"title": "Zero-Assignment Constraint for Graph Matching With Outliers", "authors": "Fudong Wang, Nan Xue, Jin-Gang Yu, Gui-Song Xia", "link": "https://arxiv.org/abs/2003.11928", "summary": "Graph matching (GM), as a longstanding problem in computer vision and pattern\nrecognition, still suffers from numerous cluttered outliers in practical\napplications. To address this issue, we present the zero-assignment constraint\n(ZAC) for approaching the graph matching problem in the presence of outliers.\nThe underlying idea is to suppress the matchings of outliers by assigning\nzero-valued vectors to the potential outliers in the obtained optimal\ncorrespondence matrix. We provide elaborate theoretical analysis to the\nproblem, i.e., GM with ZAC, and figure out that the GM problem with and without\noutliers are intrinsically different, which enables us to put forward a\nsufficient condition to construct valid and reasonable objective function.\nConsequently, we design an efficient outlier-robust algorithm to significantly\nreduce the incorrect or redundant matchings caused by numerous outliers.\nExtensive experiments demonstrate that our method can achieve the\nstate-of-the-art performance in terms of accuracy and efficiency, especially in\nthe presence of numerous outliers."}, {"title": "Cascaded Deep Video Deblurring Using Temporal Sharpness Prior", "authors": "Jinshan Pan, Haoran Bai, Jinhui Tang", "link": "https://arxiv.org/abs/2004.02501", "summary": "We present a simple and effective deep convolutional neural network (CNN)\nmodel for video deblurring. The proposed algorithm mainly consists of optical\nflow estimation from intermediate latent frames and latent frame restoration\nsteps. It first develops a deep CNN model to estimate optical flow from\nintermediate latent frames and then restores the latent frames based on the\nestimated optical flow. To better explore the temporal information from videos,\nwe develop a temporal sharpness prior to constrain the deep CNN model to help\nthe latent frame restoration. We develop an effective cascaded training\napproach and jointly train the proposed CNN model in an end-to-end manner. We\nshow that exploring the domain knowledge of video deblurring is able to make\nthe deep CNN model more compact and efficient. Extensive experimental results\nshow that the proposed algorithm performs favorably against state-of-the-art\nmethods on the benchmark datasets as well as real-world videos."}, {"title": "JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection", "authors": "Keren Fu, Deng-Ping Fan, Ge-Peng Ji, Qijun Zhao", "link": "https://arxiv.org/abs/2004.08515", "summary": "This paper proposes a novel joint learning and densely-cooperative fusion\n(JL-DCF) architecture for RGB-D salient object detection. Existing models\nusually treat RGB and depth as independent information and design separate\nnetworks for feature extraction from each. Such schemes can easily be\nconstrained by a limited amount of training data or over-reliance on an\nelaborately-designed training process. In contrast, our JL-DCF learns from both\nRGB and depth inputs through a Siamese network. To this end, we propose two\neffective components: joint learning (JL), and densely-cooperative fusion\n(DCF). The JL module provides robust saliency feature learning, while the\nlatter is introduced for complementary feature discovery. Comprehensive\nexperiments on four popular metrics show that the designed framework yields a\nrobust RGB-D saliency detector with good generalization. As a result, JL-DCF\nsignificantly advances the top-1 D3Net model by an average of ~1.9% (S-measure)\nacross six challenging datasets, showing that the proposed framework offers a\npotential solution for real-world applications and could provide more insight\ninto the cross-modality complementarity task. The code will be available at\nhttps://github.com/kerenfu/JLDCF/."}, {"title": "From Fidelity to Perceptual Quality: A Semi-Supervised Approach for Low-Light Image Enhancement", "authors": "Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, Jiaying Liu"}, {"title": "Unsupervised Adaptation Learning for Hyperspectral Imagery Super-Resolution", "authors": "Lei Zhang, Jiangtao Nie, Wei Wei, Yanning Zhang, Shengcai Liao, Ling Shao", "link": "", "summary": ""}, {"title": "Central Similarity Quantization for Efficient Image and Video Retrieval", "authors": "Li Yuan, Tao Wang, Xiaopeng Zhang, Francis EH Tay, Zequn Jie, Wei Liu, Jiashi Feng", "link": "https://arxiv.org/abs/1908.00347", "summary": "Existing data-dependent hashing methods usually learn hash functions from\npairwise or triplet data relationships, which only capture the data similarity\nlocally, and often suffer from low learning efficiency and low collision rate.\nIn this work, we propose a new \\emph{global} similarity metric, termed as\n\\emph{central similarity}, with which the hash codes of similar data pairs are\nencouraged to approach a common center and those for dissimilar pairs to\nconverge to different centers, to improve hash learning efficiency and\nretrieval accuracy. We principally formulate the computation of the proposed\ncentral similarity metric by introducing a new concept, i.e., \\emph{hash\ncenter} that refers to a set of data points scattered in the Hamming space with\na sufficient mutual distance between each other. We then provide an efficient\nmethod to construct well separated hash centers by leveraging the Hadamard\nmatrix and Bernoulli distributions. Finally, we propose the Central Similarity\nQuantization (CSQ) that optimizes the central similarity between data points\nw.r.t.\\ their hash centers instead of optimizing the local similarity. CSQ is\ngeneric and applicable to both image and video hashing scenarios. Extensive\nexperiments on large-scale image and video retrieval tasks demonstrate that CSQ\ncan generate cohesive hash codes for similar data pairs and dispersed hash\ncodes for dissimilar pairs, achieving a noticeable boost in retrieval\nperformance, i.e. 3\\%-20\\% in mAP over the previous state-of-the-arts. The code\nis at: \\url{https://github.com/yuanli2333/Hadamard-Matrix-for-hashing}"}, {"title": "ARCH: Animatable Reconstruction of Clothed Humans", "authors": "Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung", "link": "https://arxiv.org/abs/2004.04572", "summary": "In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans),\na novel end-to-end framework for accurate reconstruction of animation-ready 3D\nclothed humans from a monocular image. Existing approaches to digitize 3D\nhumans struggle to handle pose variations and recover details. Also, they do\nnot produce models that are animation ready. In contrast, ARCH is a learned\npose-aware model that produces detailed 3D rigged full-body human avatars from\na single unconstrained RGB image. A Semantic Space and a Semantic Deformation\nField are created using a parametric 3D body estimator. They allow the\ntransformation of 2D/3D clothed humans into a canonical space, reducing\nambiguities in geometry caused by pose variations and occlusions in training\ndata. Detailed surface geometry and appearance are learned using an implicit\nfunction representation with spatial local features. Furthermore, we propose\nadditional per-pixel supervision on the 3D reconstruction using opacity-aware\ndifferentiable rendering. Our experiments indicate that ARCH increases the\nfidelity of the reconstructed humans. We obtain more than 50% lower\nreconstruction errors for standard metrics compared to state-of-the-art methods\non public datasets. We also show numerous qualitative examples of animated,\nhigh-quality reconstructed avatars unseen in the literature so far."}, {"title": "A Model-Driven Deep Neural Network for Single Image Rain Removal", "authors": "Hong Wang, Qi Xie, Qian Zhao, Deyu Meng", "link": "https://arxiv.org/abs/2005.01333", "summary": "Deep learning (DL) methods have achieved state-of-the-art performance in the\ntask of single image rain removal. Most of current DL architectures, however,\nare still lack of sufficient interpretability and not fully integrated with\nphysical structures inside general rain streaks. To this issue, in this paper,\nwe propose a model-driven deep neural network for the task, with fully\ninterpretable network structures. Specifically, based on the convolutional\ndictionary learning mechanism for representing rain, we propose a novel single\nimage deraining model and utilize the proximal gradient descent technique to\ndesign an iterative algorithm only containing simple operators for solving the\nmodel. Such a simple implementation scheme facilitates us to unfold it into a\nnew deep network architecture, called rain convolutional dictionary network\n(RCDNet), with almost every network module one-to-one corresponding to each\noperation involved in the algorithm. By end-to-end training the proposed\nRCDNet, all the rain kernels and proximal operators can be automatically\nextracted, faithfully characterizing the features of both rain and clean\nbackground layers, and thus naturally lead to its better deraining performance,\nespecially in real scenarios. Comprehensive experiments substantiate the\nsuperiority of the proposed network, especially its well generality to diverse\ntesting scenarios and good interpretability for all its modules, as compared\nwith state-of-the-arts both visually and quantitatively. The source codes are\navailable at \\url{https://github.com/hongwang01/RCDNet}."}, {"title": "Novel Object Viewpoint Estimation Through Reconstruction Alignment", "authors": "Mohamed El Banani, Jason J. Corso, David F. Fouhey"}, {"title": "Creating Something From Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing", "authors": "Hengtong Hu, Lingxi Xie, Richang Hong, Qi Tian", "link": "https://arxiv.org/abs/2004.00280", "summary": "In recent years, cross-modal hashing (CMH) has attracted increasing\nattentions, mainly because its potential ability of mapping contents from\ndifferent modalities, especially in vision and language, into the same space,\nso that it becomes efficient in cross-modal data retrieval. There are two main\nframeworks for CMH, differing from each other in whether semantic supervision\nis required. Compared to the unsupervised methods, the supervised methods often\nenjoy more accurate results, but require much heavier labors in data\nannotation. In this paper, we propose a novel approach that enables guiding a\nsupervised method using outputs produced by an unsupervised method.\nSpecifically, we make use of teacher-student optimization for propagating\nknowledge. Experiments are performed on two popular CMH benchmarks, i.e., the\nMIRFlickr and NUS-WIDE datasets. Our approach outperforms all existing\nunsupervised methods by a large margin."}, {"title": "Evaluating Weakly Supervised Object Localization Methods Right", "authors": "Junsuk Choe, Seong Joon Oh, Seungho Lee, Sanghyuk Chun, Zeynep Akata, Hyunjung Shim", "link": "https://arxiv.org/abs/2001.07437", "summary": "Weakly-supervised object localization (WSOL) has gained popularity over the\nlast years for its promise to train localization models with only image-level\nlabels. Since the seminal WSOL work of class activation mapping (CAM), the\nfield has focused on how to expand the attention regions to cover objects more\nbroadly and localize them better. However, these strategies rely on full\nlocalization supervision to validate hyperparameters and for model selection,\nwhich is in principle prohibited under the WSOL setup. In this paper, we argue\nthat WSOL task is ill-posed with only image-level labels, and propose a new\nevaluation protocol where full supervision is limited to only a small held-out\nset not overlapping with the test set. We observe that, under our protocol, the\nfive most recent WSOL methods have not made a major improvement over the CAM\nbaseline. Moreover, we report that existing WSOL methods have not reached the\nfew-shot learning baseline, where the full-supervision at validation time is\nused for model training instead. Based on our findings, we discuss some future\ndirections for WSOL."}, {"title": "Style Normalization and Restitution for Generalizable Person Re-Identification", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Li Zhang"}, {"title": "Reconstruct Locally, Localize Globally: A Model Free Method for Object Pose Estimation", "authors": "Ming Cai, Ian Reid"}, {"title": "RoboTHOR: An Open Simulation-to-Real Embodied AI Platform", "authors": "Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli VanderBilt, Matthew Wallingford, Luca Weihs, Mark Yatskar, Ali Farhadi", "link": "", "summary": ""}, {"title": "All in One Bad Weather Removal Using Architectural Search", "authors": "Ruoteng Li, Robby T. Tan, Loong-Fah Cheong"}, {"title": "Relation-Aware Global Attention for Person Re-Identification", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, Zhibo Chen", "link": "https://arxiv.org/abs/1904.02998", "summary": "For person re-identification (re-id), attention mechanisms have become\nattractive as they aim at strengthening discriminative features and suppressing\nirrelevant ones, which matches well the key of re-id, i.e., discriminative\nfeature learning. Previous approaches typically learn attention using local\nconvolutions, ignoring the mining of knowledge from global structure patterns.\nIntuitively, the affinities among spatial positions/nodes in the feature map\nprovide clustering-like information and are helpful for inferring semantics and\nthus attention, especially for person images where the feasible human poses are\nconstrained. In this work, we propose an effective Relation-Aware Global\nAttention (RGA) module which captures the global structural information for\nbetter attention learning. Specifically, for each feature position, in order to\ncompactly grasp the structural information of global scope and local appearance\ninformation, we propose to stack the relations, i.e., its pairwise\ncorrelations/affinities with all the feature positions (e.g., in raster scan\norder), and the feature itself together to learn the attention with a shallow\nconvolutional model. Extensive ablation studies demonstrate that our RGA can\nsignificantly enhance the feature representation power and help achieve the\nstate-of-the-art performance on several popular benchmarks. The source code is\navailable at\nhttps://github.com/microsoft/Relation-Aware-Global-Attention-Networks."}, {"title": "HOnnotate: A Method for 3D Annotation of Hand and Object Poses", "authors": "Shreyas Hampali, Mahdi Rad, Markus Oberweger, Vincent Lepetit", "link": "https://arxiv.org/abs/1907.01481", "summary": "We propose a method for annotating images of a hand manipulating an object\nwith the 3D poses of both the hand and the object, together with a dataset\ncreated using this method. Our motivation is the current lack of annotated real\nimages for this problem, as estimating the 3D poses is challenging, mostly\nbecause of the mutual occlusions between the hand and the object. To tackle\nthis challenge, we capture sequences with one or several RGB-D cameras and\njointly optimize the 3D hand and object poses over all the frames\nsimultaneously. This method allows us to automatically annotate each frame with\naccurate estimates of the poses, despite large mutual occlusions. With this\nmethod, we created HO-3D, the first markerless dataset of color images with 3D\nannotations for both the hand and object. This dataset is currently made of\n77,558 frames, 68 sequences, 10 persons, and 10 objects. Using our dataset, we\ndevelop a single RGB image-based method to predict the hand pose when\ninteracting with objects under severe occlusions and show it generalizes to\nobjects not seen in the dataset."}, {"title": "Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics", "authors": "Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu", "link": "https://arxiv.org/abs/1909.12962", "summary": "AI-synthesized face-swapping videos, commonly known as DeepFakes, is an\nemerging problem threatening the trustworthiness of online information. The\nneed to develop and evaluate DeepFake detection algorithms calls for\nlarge-scale datasets. However, current DeepFake datasets suffer from low visual\nquality and do not resemble DeepFake videos circulated on the Internet. We\npresent a new large-scale challenging DeepFake video dataset, Celeb-DF, which\ncontains 5,639 high-quality DeepFake videos of celebrities generated using\nimproved synthesis process. We conduct a comprehensive evaluation of DeepFake\ndetection methods and datasets to demonstrate the escalated level of challenges\nposed by Celeb-DF."}, {"title": "Deep Unfolding Network for Image Super-Resolution", "authors": "Kai Zhang, Luc Van Gool, Radu Timofte", "link": "https://arxiv.org/abs/2003.10428", "summary": "Learning-based single image super-resolution (SISR) methods are continuously\nshowing superior effectiveness and efficiency over traditional model-based\nmethods, largely due to the end-to-end training. However, different from\nmodel-based methods that can handle the SISR problem with different scale\nfactors, blur kernels and noise levels under a unified MAP (maximum a\nposteriori) framework, learning-based methods generally lack such flexibility.\nTo address this issue, this paper proposes an end-to-end trainable unfolding\nnetwork which leverages both learning-based methods and model-based methods.\nSpecifically, by unfolding the MAP inference via a half-quadratic splitting\nalgorithm, a fixed number of iterations consisting of alternately solving a\ndata subproblem and a prior subproblem can be obtained. The two subproblems\nthen can be solved with neural modules, resulting in an end-to-end trainable,\niterative network. As a result, the proposed network inherits the flexibility\nof model-based methods to super-resolve blurry, noisy images for different\nscale factors via a single model, while maintaining the advantages of\nlearning-based methods. Extensive experiments demonstrate the superiority of\nthe proposed deep unfolding network in terms of flexibility, effectiveness and\nalso generalizability."}, {"title": "On the Uncertainty of Self-Supervised Monocular Depth Estimation", "authors": "Matteo Poggi, Filippo Aleotti, Fabio Tosi, Stefano Mattoccia", "link": "https://arxiv.org/abs/2005.06209", "summary": "Self-supervised paradigms for monocular depth estimation are very appealing\nsince they do not require ground truth annotations at all. Despite the\nastonishing results yielded by such methodologies, learning to reason about the\nuncertainty of the estimated depth maps is of paramount importance for\npractical applications, yet uncharted in the literature. Purposely, we explore\nfor the first time how to estimate the uncertainty for this task and how this\naffects depth accuracy, proposing a novel peculiar technique specifically\ndesigned for self-supervised approaches. On the standard KITTI dataset, we\nexhaustively assess the performance of each method with different\nself-supervised paradigms. Such evaluation highlights that our proposal i)\nalways improves depth accuracy significantly and ii) yields state-of-the-art\nresults concerning uncertainty estimation when training on sequences and\ncompetitive results uniquely deploying stereo pairs."}, {"title": "Proxy Anchor Loss for Deep Metric Learning", "authors": "Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak", "link": "https://arxiv.org/abs/2003.13911", "summary": "Existing metric learning losses can be categorized into two classes:\npair-based and proxy-based losses. The former class can leverage fine-grained\nsemantic relations between data points, but slows convergence in general due to\nits high training complexity. In contrast, the latter class enables fast and\nreliable convergence, but cannot consider the rich data-to-data relations. This\npaper presents a new proxy-based loss that takes advantages of both pair- and\nproxy-based methods and overcomes their limitations. Thanks to the use of\nproxies, our loss boosts the speed of convergence and is robust against noisy\nlabels and outliers. At the same time, it allows embedding vectors of data to\ninteract with each other in its gradients to exploit data-to-data relations.\nOur method is evaluated on four public benchmarks, where a standard network\ntrained with our loss achieves state-of-the-art performance and most quickly\nconverges."}, {"title": "Unsupervised Learning for Intrinsic Image Decomposition From a Single Image", "authors": "Yunfei Liu, Yu Li, Shaodi You, Feng Lu", "link": "https://arxiv.org/abs/1911.09930", "summary": "Intrinsic image decomposition, which is an essential task in computer vision,\naims to infer the reflectance and shading of the scene. It is challenging since\nit needs to separate one image into two components. To tackle this,\nconventional methods introduce various priors to constrain the solution, yet\nwith limited performance. Meanwhile, the problem is typically solved by\nsupervised learning methods, which is actually not an ideal solution since\nobtaining ground truth reflectance and shading for massive general natural\nscenes is challenging and even impossible. In this paper, we propose a novel\nunsupervised intrinsic image decomposition framework, which relies on neither\nlabeled training data nor hand-crafted priors. Instead, it directly learns the\nlatent feature of reflectance and shading from unsupervised and uncorrelated\ndata. To enable this, we explore the independence between reflectance and\nshading, the domain invariant content constraint and the physical constraint.\nExtensive experiments on both synthetic and real image datasets demonstrate\nconsistently superior performance of the proposed method."}, {"title": "Multi-Domain Learning for Accurate and Few-Shot Color Constancy", "authors": "Jin Xiao, Shuhang Gu, Lei Zhang"}, {"title": "PANDA: A Gigapixel-Level Human-Centric Video Dataset", "authors": "Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo, Xiaoyun Yuan, Liuyu Xiang, Zerun Wang, Guiguang Ding, David Brady, Qionghai Dai, Lu Fang"}, {"title": "Cross-View Tracking for Multi-Human 3D Pose Estimation at Over 100 FPS", "authors": "Long Chen, Haizhou Ai, Rui Chen, Zijie Zhuang, Shuang Liu", "link": "https://arxiv.org/abs/2003.03972", "summary": "Estimating 3D poses of multiple humans in real-time is a classic but still\nchallenging task in computer vision. Its major difficulty lies in the ambiguity\nin cross-view association of 2D poses and the huge state space when there are\nmultiple people in multiple views. In this paper, we present a novel solution\nfor multi-human 3D pose estimation from multiple calibrated camera views. It\ntakes 2D poses in different camera coordinates as inputs and aims for the\naccurate 3D poses in the global coordinate. Unlike previous methods that\nassociate 2D poses among all pairs of views from scratch at every frame, we\nexploit the temporal consistency in videos to match the 2D inputs with 3D poses\ndirectly in 3-space. More specifically, we propose to retain the 3D pose for\neach person and update them iteratively via the cross-view multi-human\ntracking. This novel formulation improves both accuracy and efficiency, as we\ndemonstrated on widely-used public datasets. To further verify the scalability\nof our method, we propose a new large-scale multi-human dataset with 12 to 28\ncamera views. Without bells and whistles, our solution achieves 154 FPS on 12\ncameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale\nreal-world applications. The proposed dataset will be released soon."}, {"title": "Spatial-Temporal Graph Convolutional Network for Video-Based Person Re-Identification", "authors": "Jinrui Yang, Wei-Shi Zheng, Qize Yang, Ying-Cong Chen, Qi Tian"}, {"title": "Salience-Guided Cascaded Suppression Network for Person Re-Identification", "authors": "Xuesong Chen, Canmiao Fu, Yong Zhao, Feng Zheng, Jingkuan Song, Rongrong Ji, Yi Yang", "link": "", "summary": ""}, {"title": "Fashion Outfit Complementary Item Retrieval", "authors": "Yen-Liang Lin, Son Tran, Larry S. Davis"}, {"title": "Learning Event-Based Motion Deblurring", "authors": "Zhe Jiang, Yu Zhang, Dongqing Zou, Jimmy Ren, Jiancheng Lv, Yebin Liu", "link": "https://arxiv.org/abs/2004.05794", "summary": "Recovering sharp video sequence from a motion-blurred image is highly\nill-posed due to the significant loss of motion information in the blurring\nprocess. For event-based cameras, however, fast motion can be captured as\nevents at high time rate, raising new opportunities to exploring effective\nsolutions. In this paper, we start from a sequential formulation of event-based\nmotion deblurring, then show how its optimization can be unfolded with a novel\nend-to-end deep architecture. The proposed architecture is a convolutional\nrecurrent neural network that integrates visual and temporal knowledge of both\nglobal and local scales in principled manner. To further improve the\nreconstruction, we propose a differentiable directional event filtering module\nto effectively extract rich boundary prior from the stream of events. We\nconduct extensive experiments on the synthetic GoPro dataset and a large newly\nintroduced dataset captured by a DAVIS240C camera. The proposed approach\nachieves state-of-the-art reconstruction quality, and generalizes better to\nhandling real-world motion blur."}, {"title": "Domain Decluttering: Simplifying Images to Mitigate Synthetic-Real Domain Shift and Improve Depth Estimation", "authors": "Yunhan Zhao, Shu Kong, Daeyun Shin, Charless Fowlkes", "link": "https://arxiv.org/abs/2002.12114", "summary": "Leveraging synthetically rendered data offers great potential to improve\nmonocular depth estimation, but closing the synthetic-real domain gap is a\nnon-trivial and important task. While much recent work has focused on\nunsupervised domain adaptation, we consider a more realistic scenario where a\nlarge amount of synthetic training data is supplemented by a small set of real\nimages with ground-truth. In this setting we find that existing domain\ntranslation approaches are difficult to train and offer little advantage over\nsimple baselines that use a mix of real and synthetic data. A key failure mode\nis that real-world images contain novel objects and clutter not present in\nsynthetic training. This high-level domain shift isn't handled by existing\nimage translation models.\n  Based on these observations, we develop an attentional module that learns to\nidentify and remove (hard) out-of-domain regions in real images in order to\nimprove depth prediction for a model trained primarily on synthetic data. We\ncarry out extensive experiments to validate our attend-remove-complete approach\n(ARC) and find that it significantly outperforms state-of-the-art domain\nadaptation methods for depth prediction. Visualizing the removed regions\nprovides interpretable insights into the synthetic-real domain gap."}, {"title": "Neural Blind Deconvolution Using Deep Priors", "authors": "Dongwei Ren, Kai Zhang, Qilong Wang, Qinghua Hu, Wangmeng Zuo", "link": "https://arxiv.org/abs/1908.02197", "summary": "Blind deconvolution is a classical yet challenging low-level vision problem\nwith many real-world applications. Traditional maximum a posterior (MAP) based\nmethods rely heavily on fixed and handcrafted priors that certainly are\ninsufficient in characterizing clean images and blur kernels, and usually adopt\nspecially designed alternating minimization to avoid trivial solution. In\ncontrast, existing deep motion deblurring networks learn from massive training\nimages the mapping to clean image or blur kernel, but are limited in handling\nvarious complex and large size blur kernels. To connect MAP and deep models, we\nin this paper present two generative networks for respectively modeling the\ndeep priors of clean image and blur kernel, and propose an unconstrained neural\noptimization solution to blind deconvolution. In particular, we adopt an\nasymmetric Autoencoder with skip connections for generating latent clean image,\nand a fully-connected network (FCN) for generating blur kernel. Moreover, the\nSoftMax nonlinearity is applied to the output layer of FCN to meet the\nnon-negative and equality constraints. The process of neural optimization can\nbe explained as a kind of \"zero-shot\" self-supervised learning of the\ngenerative networks, and thus our proposed method is dubbed SelfDeblur.\nExperimental results show that our SelfDeblur can achieve notable quantitative\ngains as well as more visually plausible deblurring results in comparison to\nstate-of-the-art blind deconvolution methods on benchmark datasets and\nreal-world blurry images. The source code is available at\nhttps://github.com/csdwren/SelfDeblur"}, {"title": "Anisotropic Convolutional Networks for 3D Semantic Scene Completion", "authors": "Jie Li, Kai Han, Peng Wang, Yu Liu, Xia Yuan", "link": "https://arxiv.org/abs/2004.02122", "summary": "As a voxel-wise labeling task, semantic scene completion (SSC) tries to\nsimultaneously infer the occupancy and semantic labels for a scene from a\nsingle depth and/or RGB image. The key challenge for SSC is how to effectively\ntake advantage of the 3D context to model various objects or stuffs with severe\nvariations in shapes, layouts and visibility. To handle such variations, we\npropose a novel module called anisotropic convolution, which properties with\nflexibility and power impossible for the competing methods such as standard 3D\nconvolution and some of its variations. In contrast to the standard 3D\nconvolution that is limited to a fixed 3D receptive field, our module is\ncapable of modeling the dimensional anisotropy voxel-wisely. The basic idea is\nto enable anisotropic 3D receptive field by decomposing a 3D convolution into\nthree consecutive 1D convolutions, and the kernel size for each such 1D\nconvolution is adaptively determined on the fly. By stacking multiple such\nanisotropic convolution modules, the voxel-wise modeling capability can be\nfurther enhanced while maintaining a controllable amount of model parameters.\nExtensive experiments on two SSC benchmarks, NYU-Depth-v2 and NYUCAD, show the\nsuperior performance of the proposed method. Our code is available at\nhttps://waterljwant.github.io/SSC/"}, {"title": "TDAN: Temporally-Deformable Alignment Network for Video Super-Resolution", "authors": "Yapeng Tian, Yulun Zhang, Yun Fu, Chenliang Xu", "link": "https://arxiv.org/abs/1812.02898", "summary": "Video super-resolution (VSR) aims to restore a photo-realistic\nhigh-resolution (HR) video frame from both its corresponding low-resolution\n(LR) frame (reference frame) and multiple neighboring frames (supporting\nframes). Due to varying motion of cameras or objects, the reference frame and\neach support frame are not aligned. Therefore, temporal alignment is a\nchallenging yet important problem for VSR. Previous VSR methods usually utilize\noptical flow between the reference frame and each supporting frame to wrap the\nsupporting frame for temporal alignment. Therefore, the performance of these\nimage-level wrapping-based models will highly depend on the prediction accuracy\nof optical flow, and inaccurate optical flow will lead to artifacts in the\nwrapped supporting frames, which also will be propagated into the reconstructed\nHR video frame. To overcome the limitation, in this paper, we propose a\ntemporal deformable alignment network (TDAN) to adaptively align the reference\nframe and each supporting frame at the feature level without computing optical\nflow. The TDAN uses features from both the reference frame and each supporting\nframe to dynamically predict offsets of sampling convolution kernels. By using\nthe corresponding kernels, TDAN transforms supporting frames to align with the\nreference frame. To predict the HR video frame, a reconstruction network taking\naligned frames and the reference frame is utilized. Experimental results\ndemonstrate the effectiveness of the proposed TDAN-based VSR model."}, {"title": "Zooming Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution", "authors": "Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach, Chenliang Xu", "link": "https://arxiv.org/abs/2002.11616", "summary": "In this paper, we explore the space-time video super-resolution task, which\naims to generate a high-resolution (HR) slow-motion video from a low frame rate\n(LFR), low-resolution (LR) video. A simple solution is to split it into two\nsub-tasks: video frame interpolation (VFI) and video super-resolution (VSR).\nHowever, temporal interpolation and spatial super-resolution are intra-related\nin this task. Two-stage methods cannot fully take advantage of the natural\nproperty. In addition, state-of-the-art VFI or VSR networks require a large\nframe-synthesis or reconstruction module for predicting high-quality video\nframes, which makes the two-stage methods have large model sizes and thus be\ntime-consuming. To overcome the problems, we propose a one-stage space-time\nvideo super-resolution framework, which directly synthesizes an HR slow-motion\nvideo from an LFR, LR video. Rather than synthesizing missing LR video frames\nas VFI networks do, we firstly temporally interpolate LR frame features in\nmissing LR video frames capturing local temporal contexts by the proposed\nfeature temporal interpolation network. Then, we propose a deformable ConvLSTM\nto align and aggregate temporal information simultaneously for better\nleveraging global temporal contexts. Finally, a deep reconstruction network is\nadopted to predict HR slow-motion video frames. Extensive experiments on\nbenchmark datasets demonstrate that the proposed method not only achieves\nbetter quantitative and qualitative performance but also is more than three\ntimes faster than recent two-stage state-of-the-art methods, e.g., DAIN+EDVR\nand DAIN+RBPN."}, {"title": "Fast MSER", "authors": "Hailiang Xu, Siqi Xie, Fan Chen"}, {"title": "Unsupervised Person Re-Identification via Softened Similarity Learning", "authors": "Yutian Lin, Lingxi Xie, Yu Wu, Chenggang Yan, Qi Tian", "link": "https://arxiv.org/abs/2004.03547", "summary": "Person re-identification (re-ID) is an important topic in computer vision.\nThis paper studies the unsupervised setting of re-ID, which does not require\nany labeled information and thus is freely deployed to new scenarios. There are\nvery few studies under this setting, and one of the best approach till now used\niterative clustering and classification, so that unlabeled images are clustered\ninto pseudo classes for a classifier to get trained, and the updated features\nare used for clustering and so on. This approach suffers two problems, namely,\nthe difficulty of determining the number of clusters, and the hard quantization\nloss in clustering. In this paper, we follow the iterative training mechanism\nbut discard clustering, since it incurs loss from hard quantization, yet its\nonly product, image-level similarity, can be easily replaced by pairwise\ncomputation and a softened classification task. With these improvements, our\napproach becomes more elegant and is more robust to hyper-parameter changes.\nExperiments on two image-based and video-based datasets demonstrate\nstate-of-the-art performance under the unsupervised re-ID setting."}, {"title": "COCAS: A Large-Scale Clothes Changing Person Dataset for Re-Identification", "authors": "Shijie Yu, Shihua Li, Dapeng Chen, Rui Zhao, Junjie Yan, Yu Qiao", "link": "https://arxiv.org/abs/2005.07862", "summary": "Recent years have witnessed great progress in person re-identification\n(re-id). Several academic benchmarks such as Market1501, CUHK03 and DukeMTMC\nplay important roles to promote the re-id research. To our best knowledge, all\nthe existing benchmarks assume the same person will have the same clothes.\nWhile in real-world scenarios, it is very often for a person to change clothes.\nTo address the clothes changing person re-id problem, we construct a novel\nlarge-scale re-id benchmark named ClOthes ChAnging Person Set (COCAS), which\nprovides multiple images of the same identity with different clothes. COCAS\ntotally contains 62,382 body images from 5,266 persons. Based on COCAS, we\nintroduce a new person re-id setting for clothes changing problem, where the\nquery includes both a clothes template and a person image taking another\nclothes. Moreover, we propose a two-branch network named Biometric-Clothes\nNetwork (BC-Net) which can effectively integrate biometric and clothes feature\nfor re-id under our setting. Experiments show that it is feasible for clothes\nchanging re-id with clothes templates."}, {"title": "Learning Formation of Physically-Based Face Attributes", "authors": "Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara, Owen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha Prasad, Bipin Kishore, Jun Xing, Hao Li", "link": "https://arxiv.org/abs/2004.03458", "summary": "Based on a combined data set of 4000 high resolution facial scans, we\nintroduce a non-linear morphable face model, capable of producing multifarious\nface geometry of pore-level resolution, coupled with material attributes for\nuse in physically-based rendering. We aim to maximize the variety of face\nidentities, while increasing the robustness of correspondence between unique\ncomponents, including middle-frequency geometry, albedo maps, specular\nintensity maps and high-frequency displacement details. Our deep learning based\ngenerative model learns to correlate albedo and geometry, which ensures the\nanatomical correctness of the generated assets. We demonstrate potential use of\nour generative model for novel identity generation, model fitting,\ninterpolation, animation, high fidelity data visualization, and low-to-high\nresolution data domain transferring. We hope the release of this generative\nmodel will encourage further cooperation between all graphics, vision, and data\nfocused professionals while demonstrating the cumulative value of every\nindividual's complete biometric profile."}, {"title": "Generalized Product Quantization Network for Semi-Supervised Image Retrieval", "authors": "Young Kyun Jang, Nam Ik Cho", "link": "https://arxiv.org/abs/2002.11281", "summary": "Image retrieval methods that employ hashing or vector quantization have\nachieved great success by taking advantage of deep learning. However, these\napproaches do not meet expectations unless expensive label information is\nsufficient. To resolve this issue, we propose the first quantization-based\nsemi-supervised image retrieval scheme: Generalized Product Quantization (GPQ)\nnetwork. We design a novel metric learning strategy that preserves semantic\nsimilarity between labeled data, and employ entropy regularization term to\nfully exploit inherent potentials of unlabeled data. Our solution increases the\ngeneralization capacity of the quantization network, which allows overcoming\nprevious limitations in the retrieval community. Extensive experimental results\ndemonstrate that GPQ yields state-of-the-art performance on large-scale real\nimage benchmark datasets."}, {"title": "Stereoscopic Flash and No-Flash Photography for Shape and Albedo Recovery", "authors": "Xu Cao, Michael Waechter, Boxin Shi, Ye Gao, Bo Zheng, Yasuyuki Matsushita"}, {"title": "Context-Aware Group Captioning via Self-Attention and Contrastive Features", "authors": "Zhuowan Li, Quan Tran, Long Mai, Zhe Lin, Alan L. Yuille", "link": "https://arxiv.org/abs/2004.03708", "summary": "While image captioning has progressed rapidly, existing works focus mainly on\ndescribing single images. In this paper, we introduce a new task, context-aware\ngroup captioning, which aims to describe a group of target images in the\ncontext of another group of related reference images. Context-aware group\ncaptioning requires not only summarizing information from both the target and\nreference image group but also contrasting between them. To solve this problem,\nwe propose a framework combining self-attention mechanism with contrastive\nfeature construction to effectively summarize common information from each\nimage group while capturing discriminative information between them. To build\nthe dataset for this task, we propose to group the images and generate the\ngroup captions based on single image captions using scene graphs matching. Our\ndatasets are constructed on top of the public Conceptual Captions dataset and\nour new Stock Captions dataset. Experiments on the two datasets show the\neffectiveness of our method on this new task. Related Datasets and code are\nreleased at https://lizw14.github.io/project/groupcap ."}, {"title": "MEBOW: Monocular Estimation of Body Orientation in the Wild", "authors": "Chenyan Wu, Yukun Chen, Jiajia Luo, Che-Chun Su, Anuja Dawane, Bikramjot Hanzra, Zhuo Deng, Bilan Liu, James Z. Wang, Cheng-hao Kuo"}, {"title": "Distilling Image Dehazing With Heterogeneous Task Imitation", "authors": "Ming Hong, Yuan Xie, Cuihua Li, Yanyun Qu"}, {"title": "Select, Supplement and Focus for RGB-D Saliency Detection", "authors": "Miao Zhang, Weisong Ren, Yongri Piao, Zhengkun Rong, Huchuan Lu"}, {"title": "Transfer Learning From Synthetic to Real-Noise Denoising With Adaptive Instance Normalization", "authors": "Yoonsik Kim, Jae Woong Soh, Gu Yong Park, Nam Ik Cho", "link": "https://arxiv.org/abs/2002.11244", "summary": "Real-noise denoising is a challenging task because the statistics of\nreal-noise do not follow the normal distribution, and they are also spatially\nand temporally changing. In order to cope with various and complex real-noise,\nwe propose a well-generalized denoising architecture and a transfer learning\nscheme. Specifically, we adopt an adaptive instance normalization to build a\ndenoiser, which can regularize the feature map and prevent the network from\noverfitting to the training set. We also introduce a transfer learning scheme\nthat transfers knowledge learned from synthetic-noise data to the real-noise\ndenoiser. From the proposed transfer learning, the synthetic-noise denoiser can\nlearn general features from various synthetic-noise data, and the real-noise\ndenoiser can learn the real-noise characteristics from real data. From the\nexperiments, we find that the proposed denoising method has great\ngeneralization ability, such that our network trained with synthetic-noise\nachieves the best performance for Darmstadt Noise Dataset (DND) among the\nmethods from published papers. We can also see that the proposed transfer\nlearning scheme robustly works for real-noise images through the learning with\na very small number of labeled data."}, {"title": "On Joint Estimation of Pose, Geometry and svBRDF From a Handheld Scanner", "authors": "Carolin Schmitt, Simon Donn\u00e9, Gernot Riegler, Vladlen Koltun, Andreas Geiger"}, {"title": "Differentiable Volumetric Rendering: Learning Implicit 3D Representations Without 3D Supervision", "authors": "Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger", "link": "https://arxiv.org/abs/1912.07372", "summary": "Learning-based 3D reconstruction methods have shown impressive results.\nHowever, most methods require 3D supervision which is often hard to obtain for\nreal-world datasets. Recently, several works have proposed differentiable\nrendering techniques to train reconstruction models from RGB images.\nUnfortunately, these approaches are currently restricted to voxel- and\nmesh-based representations, suffering from discretization or low resolution. In\nthis work, we propose a differentiable rendering formulation for implicit shape\nand texture representations. Implicit representations have recently gained\npopularity as they represent shape and texture continuously. Our key insight is\nthat depth gradients can be derived analytically using the concept of implicit\ndifferentiation. This allows us to learn implicit shape and texture\nrepresentations directly from RGB images. We experimentally show that our\nsingle-view reconstructions rival those learned with full 3D supervision.\nMoreover, we find that our method can be used for multi-view 3D reconstruction,\ndirectly resulting in watertight meshes."}, {"title": "Meta-Transfer Learning for Zero-Shot Super-Resolution", "authors": "Jae Woong Soh, Sunwoo Cho, Nam Ik Cho", "link": "https://arxiv.org/abs/2002.12213", "summary": "Convolutional neural networks (CNNs) have shown dramatic improvements in\nsingle image super-resolution (SISR) by using large-scale external samples.\nDespite their remarkable performance based on the external dataset, they cannot\nexploit internal information within a specific image. Another problem is that\nthey are applicable only to the specific condition of data that they are\nsupervised. For instance, the low-resolution (LR) image should be a \"bicubic\"\ndownsampled noise-free image from a high-resolution (HR) one. To address both\nissues, zero-shot super-resolution (ZSSR) has been proposed for flexible\ninternal learning. However, they require thousands of gradient updates, i.e.,\nlong inference time. In this paper, we present Meta-Transfer Learning for\nZero-Shot Super-Resolution (MZSR), which leverages ZSSR. Precisely, it is based\non finding a generic initial parameter that is suitable for internal learning.\nThus, we can exploit both external and internal information, where one single\ngradient update can yield quite considerable results. (See Figure 1). With our\nmethod, the network can quickly adapt to a given image condition. In this\nrespect, our method can be applied to a large spectrum of image conditions\nwithin a fast adaptation process."}, {"title": "Solving Jigsaw Puzzles With Eroded Boundaries", "authors": "Dov Bridger, Dov Danon, Ayellet Tal", "link": "https://arxiv.org/abs/1912.00755", "summary": "Jigsaw puzzle solving is an intriguing problem which has been explored in\ncomputer vision for decades. This paper focuses on a specific variant of the\nproblem - solving puzzles with eroded boundaries. Such erosion makes the\nproblem extremely difficult, since most existing solvers utilize solely the\ninformation at the boundaries. Nevertheless, this variant is important since\nerosion and missing data often occur at the boundaries. The key idea of our\nproposed approach is to inpaint the eroded boundaries between puzzle pieces and\nlater leverage the quality of the inpainted area to classify a pair of pieces\nas 'neighbors or not'. An interesting feature of our architecture is that the\nsame GAN discriminator is used for both inpainting and classification; Training\nof the second task is simply a continuation of the training of the first,\nbeginning from the point it left off. We show that our approach outperforms\nother SOTA methods"}, {"title": "Context-Aware Attention Network for Image-Text Retrieval", "authors": "Qi Zhang, Zhen Lei, Zhaoxiang Zhang, Stan Z. Li", "link": "", "summary": ""}, {"title": "M-LVC: Multiple Frames Prediction for Learned Video Compression", "authors": "Jianping Lin, Dong Liu, Houqiang Li, Feng Wu", "link": "https://arxiv.org/abs/2004.10290", "summary": "We propose an end-to-end learned video compression scheme for low-latency\nscenarios. Previous methods are limited in using the previous one frame as\nreference. Our method introduces the usage of the previous multiple frames as\nreferences. In our scheme, the motion vector (MV) field is calculated between\nthe current frame and the previous one. With multiple reference frames and\nassociated multiple MV fields, our designed network can generate more accurate\nprediction of the current frame, yielding less residual. Multiple reference\nframes also help generate MV prediction, which reduces the coding cost of MV\nfield. We use two deep auto-encoders to compress the residual and the MV,\nrespectively. To compensate for the compression error of the auto-encoders, we\nfurther design a MV refinement network and a residual refinement network,\ntaking use of the multiple reference frames as well. All the modules in our\nscheme are jointly optimized through a single rate-distortion loss function. We\nuse a step-by-step training strategy to optimize the entire scheme.\nExperimental results show that the proposed method outperforms the existing\nlearned video compression methods for low-latency mode. Our method also\nperforms better than H.265 in both PSNR and MS-SSIM. Our code and models are\npublicly available."}, {"title": "Efficient Dynamic Scene Deblurring Using Spatially Variant Deconvolution Network With Optical Flow Guided Training", "authors": "Yuan Yuan, Wei Su, Dandan Ma"}, {"title": "Single Image Reflection Removal Through Cascaded Refinement", "authors": "Chao Li, Yixiao Yang, Kun He, Stephen Lin, John E. Hopcroft", "link": "https://arxiv.org/abs/1911.06634", "summary": "We address the problem of removing undesirable reflections from a single\nimage captured through a glass surface, which is an ill-posed, challenging but\npractically important problem for photo enhancement. Inspired by iterative\nstructure reduction for hidden community detection in social networks, we\npropose an Iterative Boost Convolutional LSTM Network (IBCLN) that enables\ncascaded prediction for reflection removal. IBCLN is a cascaded network that\niteratively refines the estimates of transmission and reflection layers in a\nmanner that they can boost the prediction quality to each other, and\ninformation across steps of the cascade is transferred using an LSTM. The\nintuition is that the transmission is the strong, dominant structure while the\nreflection is the weak, hidden structure. They are complementary to each other\nin a single image and thus a better estimate and reduction on one side from the\noriginal image leads to a more accurate estimate on the other side. To\nfacilitate training over multiple cascade steps, we employ LSTM to address the\nvanishing gradient problem, and propose residual reconstruction loss as further\ntraining guidance. Besides, we create a dataset of real-world images with\nreflection and ground-truth transmission layers to mitigate the problem of\ninsufficient data. Comprehensive experiments demonstrate that the proposed\nmethod can effectively remove reflections in real and synthetic images compared\nwith state-of-the-art reflection removal methods."}, {"title": "From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality", "authors": "Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, Alan Bovik", "link": "https://arxiv.org/abs/1912.10088", "summary": "Blind or no-reference (NR) perceptual picture quality prediction is a\ndifficult, unsolved problem of great consequence to the social and streaming\nmedia industries that impacts billions of viewers daily. Unfortunately, popular\nNR prediction models perform poorly on real-world distorted pictures. To\nadvance progress on this problem, we introduce the largest (by far) subjective\npicture quality database, containing about 40000 real-world distorted pictures\nand 120000 patches, on which we collected about 4M human judgments of picture\nquality. Using these picture and patch quality labels, we built deep\nregion-based architectures that learn to produce state-of-the-art global\npicture quality predictions as well as useful local picture quality maps. Our\ninnovations include picture quality prediction architectures that produce\nglobal-to-local inferences as well as local-to-global inferences (via\nfeedback)."}, {"title": "Video to Events: Recycling Video Datasets for Event Cameras", "authors": "Daniel Gehrig, Mathias Gehrig, Javier Hidalgo-Carri\u00f3, Davide Scaramuzza", "link": "https://arxiv.org/abs/1912.03095", "summary": "Event cameras are novel sensors that output brightness changes in the form of\na stream of asynchronous \"events\" instead of intensity frames. They offer\nsignificant advantages with respect to conventional cameras: high dynamic range\n(HDR), high temporal resolution, and no motion blur. Recently, novel learning\napproaches operating on event data have achieved impressive results. Yet, these\nmethods require a large amount of event data for training, which is hardly\navailable due the novelty of event sensors in computer vision research. In this\npaper, we present a method that addresses these needs by converting any\nexisting video dataset recorded with conventional cameras to synthetic event\ndata. This unlocks the use of a virtually unlimited number of existing video\ndatasets for training networks designed for real event data. We evaluate our\nmethod on two relevant vision tasks, i.e., object recognition and semantic\nsegmentation, and show that models trained on synthetic events have several\nbenefits: (i) they generalize well to real event data, even in scenarios where\nstandard-camera images are blurry or overexposed, by inheriting the outstanding\nproperties of event cameras; (ii) they can be used for fine-tuning on real data\nto improve over state-of-the-art for both classification and semantic\nsegmentation."}, {"title": "Composed Query Image Retrieval Using Locally Bounded Features", "authors": "Mehrdad Hosseinzadeh, Yang Wang"}, {"title": "Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring", "authors": "Maitreya Suin, Kuldeep Purohit, A. N. Rajagopalan", "link": "https://arxiv.org/abs/2004.05343", "summary": "This paper tackles the problem of motion deblurring of dynamic scenes.\nAlthough end-to-end fully convolutional designs have recently advanced the\nstate-of-the-art in non-uniform motion deblurring, their performance-complexity\ntrade-off is still sub-optimal. Existing approaches achieve a large receptive\nfield by increasing the number of generic convolution layers and kernel-size,\nbut this comes at the expense of of the increase in model size and inference\nspeed. In this work, we propose an efficient pixel adaptive and feature\nattentive design for handling large blur variations across different spatial\nlocations and process each test image adaptively. We also propose an effective\ncontent-aware global-local filtering module that significantly improves\nperformance by considering not only global dependencies but also by dynamically\nexploiting neighbouring pixel information. We use a patch-hierarchical\nattentive architecture composed of the above module that implicitly discovers\nthe spatial variations in the blur present in the input image and in turn,\nperforms local and global modulation of intermediate features. Extensive\nqualitative and quantitative comparisons with prior art on deblurring\nbenchmarks demonstrate that our design offers significant improvements over the\nstate-of-the-art in accuracy as well as speed."}, {"title": "End-to-End Illuminant Estimation Based on Deep Metric Learning", "authors": "Bolei Xu, Jingxin Liu, Xianxu Hou, Bozhi Liu, Guoping Qiu"}, {"title": "Variational-EM-Based Deep Learning for Noise-Blind Image Deblurring", "authors": "Yuesong Nan, Yuhui Quan, Hui Ji"}, {"title": "Image Demoireing with Learnable Bandpass Filters", "authors": "Bolun Zheng, Shanxin Yuan, Gregory Slabaugh, Ale\u0161 Leonardis", "link": "https://arxiv.org/abs/2004.00406", "summary": "Image demoireing is a multi-faceted image restoration task involving both\ntexture and color restoration. In this paper, we propose a novel multiscale\nbandpass convolutional neural network (MBCNN) to address this problem. As an\nend-to-end solution, MBCNN respectively solves the two sub-problems. For\ntexture restoration, we propose a learnable bandpass filter (LBF) to learn the\nfrequency prior for moire texture removal. For color restoration, we propose a\ntwo-step tone mapping strategy, which first applies a global tone mapping to\ncorrect for a global color shift, and then performs local fine tuning of the\ncolor per pixel. Through an ablation study, we demonstrate the effectiveness of\nthe different components of MBCNN. Experimental results on two public datasets\nshow that our method outperforms state-of-the-art methods by a large margin\n(more than 2dB in terms of PSNR)."}, {"title": "Assessing Image Quality Issues for Real-World Problems", "authors": "Tai-Yin Chiu, Yinan Zhao, Danna Gurari", "link": "https://arxiv.org/abs/2003.12511", "summary": "We introduce a new large-scale dataset that links the assessment of image\nquality issues to two practical vision tasks: image captioning and visual\nquestion answering. First, we identify for 39,181 images taken by people who\nare blind whether each is sufficient quality to recognize the content as well\nas what quality flaws are observed from six options. These labels serve as a\ncritical foundation for us to make the following contributions: (1) a new\nproblem and algorithms for deciding whether an image is insufficient quality to\nrecognize the content and so not captionable, (2) a new problem and algorithms\nfor deciding which of six quality flaws an image contains, (3) a new problem\nand algorithms for deciding whether a visual question is unanswerable due to\nunrecognizable content versus the content of interest being missing from the\nfield of view, and (4) a novel application of more efficiently creating a\nlarge-scale image captioning dataset by automatically deciding whether an image\nis insufficient quality and so should not be captioned. We publicly-share our\ndatasets and code to facilitate future extensions of this work:\nhttps://vizwiz.org."}, {"title": "Memory-Efficient Hierarchical Neural Architecture Search for Image Denoising", "authors": "Haokui Zhang, Ying Li, Hao Chen, Chunhua Shen", "link": "https://arxiv.org/abs/1909.08228", "summary": "Recently, neural architecture search (NAS) methods have attracted much\nattention and outperformed manually designed architectures on a few high-level\nvision tasks. In this paper, we propose HiNAS (Hierarchical NAS), an effort\ntowards employing NAS to automatically design effective neural network\narchitectures for image denoising. HiNAS adopts gradient based search\nstrategies and employs operations with adaptive receptive field to build an\nflexible hierarchical search space. During the search stage, HiNAS shares cells\nacross different feature levels to save memory and employ an early stopping\nstrategy to avoid the collapse issue in NAS, and considerably accelerate the\nsearch speed. The proposed HiNAS is both memory and computation efficient,\nwhich takes only about 4.5 hours for searching using a single GPU. We evaluate\nthe effectiveness of our proposed HiNAS on two different datasets, namely an\nadditive white Gaussian noise dataset BSD500, and a realistic noise dataset\nSIM1800. Experimental results show that the architecture found by HiNAS has\nfewer parameters and enjoys a faster inference speed, while achieving highly\ncompetitive performance compared with state-of-the-art methods. We also present\nanalysis on the architectures found by NAS. HiNAS also shows good performance\non experiments for image de-raining."}, {"title": "Blindly Assess Image Quality in the Wild Guided by a Self-Adaptive Hyper Network", "authors": "Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, Yanning Zhang", "link": "", "summary": ""}, {"title": "Perceptual Quality Assessment of Smartphone Photography", "authors": "Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, Zhou Wang"}, {"title": "Don\u2019t Hit Me! Glass Detection in Real-World Scenes", "authors": "Haiyang Mei, Xin Yang, Yang Wang, Yuanyuan Liu, Shengfeng He, Qiang Zhang, Xiaopeng Wei, Rynson W.H. Lau"}, {"title": "Progressive Mirror Detection", "authors": "Jiaying Lin, Guodong Wang, Rynson W.H. Lau"}, {"title": "Category-Level Articulated Object Pose Estimation", "authors": "Xiaolong Li, He Wang, Li Yi, Leonidas J. Guibas, A. Lynn Abbott, Shuran Song", "link": "https://arxiv.org/abs/1912.11913", "summary": "This project addresses the task of category-level pose estimation for\narticulated objects from a single depth image. We present a novel\ncategory-level approach that correctly accommodates object instances previously\nunseen during training. We introduce Articulation-aware Normalized Coordinate\nSpace Hierarchy (ANCSH) - a canonical representation for different articulated\nobjects in a given category. As the key to achieve intra-category\ngeneralization, the representation constructs a canonical object space as well\nas a set of canonical part spaces. The canonical object space normalizes the\nobject orientation,scales and articulations (e.g. joint parameters and states)\nwhile each canonical part space further normalizes its part pose and scale. We\ndevelop a deep network based on PointNet++ that predicts ANCSH from a single\ndepth point cloud, including part segmentation, normalized coordinates, and\njoint parameters in the canonical object space. By leveraging the canonicalized\njoints, we demonstrate: 1) improved performance in part pose and scale\nestimations using the induced kinematic constraints from joints; 2) high\naccuracy for joint parameter estimation in camera space."}, {"title": "Unbiased Scene Graph Generation From Biased Training", "authors": "Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, Hanwang Zhang", "link": "https://arxiv.org/abs/2002.11949", "summary": "Today's scene graph generation (SGG) task is still far from practical, mainly\ndue to the severe training bias, e.g., collapsing diverse \"human walk on / sit\non / lay on beach\" into \"human on beach\". Given such SGG, the down-stream tasks\nsuch as VQA can hardly infer better scene structures than merely a bag of\nobjects. However, debiasing in SGG is not trivial because traditional debiasing\nmethods cannot distinguish between the good and bad bias, e.g., good context\nprior (e.g., \"person read book\" rather than \"eat\") and bad long-tailed bias\n(e.g., \"near\" dominating \"behind / in front of\"). In this paper, we present a\nnovel SGG framework based on causal inference but not the conventional\nlikelihood. We first build a causal graph for SGG, and perform traditional\nbiased training with the graph. Then, we propose to draw the counterfactual\ncausality from the trained graph to infer the effect from the bad bias, which\nshould be removed. In particular, we use Total Direct Effect (TDE) as the\nproposed final predicate score for unbiased SGG. Note that our framework is\nagnostic to any SGG model and thus can be widely applied in the community who\nseeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit\non the SGG benchmark Visual Genome and several prevailing models, we observed\nsignificant improvements over the previous state-of-the-art methods."}, {"title": "Dynamic Graph Message Passing Networks", "authors": "Li Zhang, Dan Xu, Anurag Arnab, Philip H.S. Torr", "link": "https://arxiv.org/abs/1908.06955", "summary": "Modelling long-range dependencies is critical for complex scene understanding\ntasks such as semantic segmentation and object detection. Although CNNs have\nexcelled in many computer vision tasks, they are still limited in capturing\nlong-range structured relationships as they typically consist of layers of\nlocal kernels. A fully-connected graph is beneficial for such modelling,\nhowever, its computational overhead is prohibitive. We propose a dynamic graph\nmessage passing network, based on the message passing neural network framework,\nthat significantly reduces the computational complexity compared to related\nworks modelling a fully-connected graph. This is achieved by adaptively\nsampling nodes in the graph, conditioned on the input, for message passing.\nBased on the sampled nodes, we then dynamically predict node-dependent filter\nweights and the affinity matrix for propagating information between them. Using\nthis model, we show significant improvements with respect to strong,\nstate-of-the-art baselines on three different tasks and backbone architectures.\nOur approach also outperforms fully-connected graphs while using substantially\nfewer floating point operations and parameters."}, {"title": "Weakly Supervised Visual Semantic Parsing", "authors": "Alireza Zareian, Svebor Karaman, Shih-Fu Chang", "link": "https://arxiv.org/abs/2001.02359", "summary": "Scene Graph Generation (SGG) aims to extract entities, predicates and their\nsemantic structure from images, enabling deep understanding of visual content,\nwith many applications such as visual reasoning and image retrieval.\nNevertheless, existing SGG methods require millions of manually annotated\nbounding boxes for training, and are computationally inefficient, as they\nexhaustively process all pairs of object proposals to detect predicates. In\nthis paper, we address those two limitations by first proposing a generalized\nformulation of SGG, namely Visual Semantic Parsing, which disentangles entity\nand predicate recognition, and enables sub-quadratic performance. Then we\npropose the Visual Semantic Parsing Network, VSPNet, based on a dynamic,\nattention-based, bipartite message passing framework that jointly infers graph\nnodes and edges through an iterative process. Additionally, we propose the\nfirst graph-based weakly supervised learning framework, based on a novel graph\nalignment algorithm, which enables training without bounding box annotations.\nThrough extensive experiments, we show that VSPNet outperforms weakly\nsupervised baselines significantly and approaches fully supervised performance,\nwhile being several times faster. We publicly release the source code of our\nmethod."}, {"title": "GPS-Net: Graph Property Sensing Network for Scene Graph Generation", "authors": "Xin Lin, Changxing Ding, Jinquan Zeng, Dacheng Tao", "link": "http://arxiv.org/abs/2003.12962", "summary": "Scene graph generation (SGG) aims to detect objects in an image along with\ntheir pairwise relationships. There are three key properties of scene graph\nthat have been underexplored in recent works: namely, the edge direction\ninformation, the difference in priority between nodes, and the long-tailed\ndistribution of relationships. Accordingly, in this paper, we propose a Graph\nProperty Sensing Network (GPS-Net) that fully explores these three properties\nfor SGG. First, we propose a novel message passing module that augments the\nnode feature with node-specific contextual information and encodes the edge\ndirection information via a tri-linear model. Second, we introduce a node\npriority sensitive loss to reflect the difference in priority between nodes\nduring training. This is achieved by designing a mapping function that adjusts\nthe focusing parameter in the focal loss. Third, since the frequency of\nrelationships is affected by the long-tailed distribution problem, we mitigate\nthis issue by first softening the distribution and then enabling it to be\nadjusted for each subject-object pair according to their visual appearance.\nSystematic experiments demonstrate the effectiveness of the proposed\ntechniques. Moreover, GPS-Net achieves state-of-the-art performance on three\npopular databases: VG, OI, and VRD by significant gains under various settings\nand metrics. The code and models are available at\n\\url{https://github.com/taksau/GPS-Net}."}, {"title": "End-to-End Optimization of Scene Layout", "authors": "Andrew Luo, Zhoutong Zhang, Jiajun Wu, Joshua B. Tenenbaum"}, {"title": "Unsupervised Intra-Domain Adaptation for Semantic Segmentation Through Self-Supervision", "authors": "Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, In So Kweon", "link": "https://arxiv.org/abs/2004.07703", "summary": "Convolutional neural network-based approaches have achieved remarkable\nprogress in semantic segmentation. However, these approaches heavily rely on\nannotated data which are labor intensive. To cope with this limitation,\nautomatically annotated data generated from graphic engines are used to train\nsegmentation models. However, the models trained from synthetic data are\ndifficult to transfer to real images. To tackle this issue, previous works have\nconsidered directly adapting models from the source data to the unlabeled\ntarget data (to reduce the inter-domain gap). Nonetheless, these techniques do\nnot consider the large distribution gap among the target data itself\n(intra-domain gap). In this work, we propose a two-step self-supervised domain\nadaptation approach to minimize the inter-domain and intra-domain gap together.\nFirst, we conduct the inter-domain adaptation of the model; from this\nadaptation, we separate the target domain into an easy and hard split using an\nentropy-based ranking function. Finally, to decrease the intra-domain gap, we\npropose to employ a self-supervised adaptation technique from the easy to the\nhard split. Experimental results on numerous benchmark datasets highlight the\neffectiveness of our method against existing state-of-the-art approaches. The\nsource code is available at https://github.com/feipan664/IntraDA.git."}, {"title": "Dual Super-Resolution Learning for Semantic Segmentation", "authors": "Li Wang, Dong Li, Yousong Zhu, Lu Tian, Yi Shan"}, {"title": "Self-Supervised Scene De-Occlusion", "authors": "Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, Chen Change Loy", "link": "https://arxiv.org/abs/2004.02788", "summary": "Natural scene understanding is a challenging task, particularly when\nencountering images of multiple objects that are partially occluded. This\nobstacle is given rise by varying object ordering and positioning. Existing\nscene understanding paradigms are able to parse only the visible parts,\nresulting in incomplete and unstructured scene interpretation. In this paper,\nwe investigate the problem of scene de-occlusion, which aims to recover the\nunderlying occlusion ordering and complete the invisible parts of occluded\nobjects. We make the first attempt to address the problem through a novel and\nunified framework that recovers hidden scene structures without ordering and\namodal annotations as supervisions. This is achieved via Partial Completion\nNetwork (PCNet)-mask (M) and -content (C), that learn to recover fractions of\nobject masks and contents, respectively, in a self-supervised manner. Based on\nPCNet-M and PCNet-C, we devise a novel inference scheme to accomplish scene\nde-occlusion, via progressive ordering recovery, amodal completion and content\ncompletion. Extensive experiments on real-world scenes demonstrate the superior\nperformance of our approach to other alternatives. Remarkably, our approach\nthat is trained in a self-supervised manner achieves comparable results to\nfully-supervised methods. The proposed scene de-occlusion framework benefits\nmany applications, including high-quality and controllable image manipulation\nand scene recomposition (see Fig. 1), as well as the conversion of existing\nmodal mask annotations to amodal mask annotations."}, {"title": "BANet: Bidirectional Aggregation Network With Occlusion Handling for Panoptic Segmentation", "authors": "Yifeng Chen, Guangchen Lin, Songyuan Li, Omar Bourahla, Yiming Wu, Fangfang Wang, Junyi Feng, Mingliang Xu, Xi Li", "link": "https://arxiv.org/abs/2003.14031", "summary": "Panoptic segmentation aims to perform instance segmentation for foreground\ninstances and semantic segmentation for background stuff simultaneously. The\ntypical top-down pipeline concentrates on two key issues: 1) how to effectively\nmodel the intrinsic interaction between semantic segmentation and instance\nsegmentation, and 2) how to properly handle occlusion for panoptic\nsegmentation. Intuitively, the complementarity between semantic segmentation\nand instance segmentation can be leveraged to improve the performance. Besides,\nwe notice that using detection/mask scores is insufficient for resolving the\nocclusion problem. Motivated by these observations, we propose a novel deep\npanoptic segmentation scheme based on a bidirectional learning pipeline.\nMoreover, we introduce a plug-and-play occlusion handling algorithm to deal\nwith the occlusion between different object instances. The experimental results\non COCO panoptic benchmark validate the effectiveness of our proposed method.\nCodes will be released soon at https://github.com/Mooonside/BANet."}, {"title": "CPR-GCN: Conditional Partial-Residual Graph Convolutional Network in Automated Anatomical Labeling of Coronary Arteries", "authors": "Han Yang, Xingjian Zhen, Ying Chi, Lei Zhang, Xian-Sheng Hua", "link": "http://arxiv.org/abs/2003.08560", "summary": "Automated anatomical labeling plays a vital role in coronary artery disease\ndiagnosing procedure. The main challenge in this problem is the large\nindividual variability inherited in human anatomy. Existing methods usually\nrely on the position information and the prior knowledge of the topology of the\ncoronary artery tree, which may lead to unsatisfactory performance when the\nmain branches are confusing. Motivated by the wide application of the graph\nneural network in structured data, in this paper, we propose a conditional\npartial-residual graph convolutional network (CPR-GCN), which takes both\nposition and CT image into consideration, since CT image contains abundant\ninformation such as branch size and spanning direction. Two majority parts, a\nPartial-Residual GCN and a conditions extractor, are included in CPR-GCN. The\nconditions extractor is a hybrid model containing the 3D CNN and the LSTM,\nwhich can extract 3D spatial image features along the branches. On the\ntechnical side, the Partial-Residual GCN takes the position features of the\nbranches, with the 3D spatial image features as conditions, to predict the\nlabel for each branches. While on the mathematical side, our approach twists\nthe partial differential equation (PDE) into the graph modeling. A dataset with\n511 subjects is collected from the clinic and annotated by two experts with a\ntwo-phase annotation process. According to the five-fold cross-validation, our\nCPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which\noutperforms state-of-the-art approaches."}, {"title": "Cross-View Correspondence Reasoning Based on Bipartite Graph Convolutional Network for Mammogram Mass Detection", "authors": "Yuhang Liu, Fandong Zhang, Qianyi Zhang, Siwen Wang, Yizhou Wang, Yizhou Yu", "link": "", "summary": ""}, {"title": "MPM: Joint Representation of Motion and Position Map for Cell Tracking", "authors": "Junya Hayashida, Kazuya Nishimura, Ryoma Bise", "link": "https://arxiv.org/abs/2002.10749", "summary": "Conventional cell tracking methods detect multiple cells in each frame\n(detection) and then associate the detection results in successive time-frames\n(association). Most cell tracking methods perform the association task\nindependently from the detection task. However, there is no guarantee of\npreserving coherence between these tasks, and lack of coherence may adversely\naffect tracking performance. In this paper, we propose the Motion and Position\nMap (MPM) that jointly represents both detection and association for not only\nmigration but also cell division. It guarantees coherence such that if a cell\nis detected, the corresponding motion flow can always be obtained. It is a\nsimple but powerful method for multi-object tracking in dense environments. We\ncompared the proposed method with current tracking methods under various\nconditions in real biological images and found that it outperformed the\nstate-of-the-art (+5.2\\% improvement compared to the second-best)."}, {"title": "Deep Distance Transform for Tubular Structure Segmentation in CT Scans", "authors": "Yan Wang, Xu Wei, Fengze Liu, Jieneng Chen, Yuyin Zhou, Wei Shen, Elliot K. Fishman, Alan L. Yuille", "link": "https://arxiv.org/abs/1912.03383", "summary": "Tubular structure segmentation in medical images, e.g., segmenting vessels in\nCT scans, serves as a vital step in the use of computers to aid in screening\nearly stages of related diseases. But automatic tubular structure segmentation\nin CT scans is a challenging problem, due to issues such as poor contrast,\nnoise and complicated background. A tubular structure usually has a\ncylinder-like shape which can be well represented by its skeleton and\ncross-sectional radii (scales). Inspired by this, we propose a geometry-aware\ntubular structure segmentation method, Deep Distance Transform (DDT), which\ncombines intuitions from the classical distance transform for skeletonization\nand modern deep segmentation networks. DDT first learns a multi-task network to\npredict a segmentation mask for a tubular structure and a distance map. Each\nvalue in the map represents the distance from each tubular structure voxel to\nthe tubular structure surface. Then the segmentation mask is refined by\nleveraging the shape prior reconstructed from the distance map. We apply our\nDDT on six medical image datasets. The experiments show that (1) DDT can boost\ntubular structure segmentation performance significantly (e.g., over 13%\nimprovement measured by DSC for pancreatic duct segmentation), and (2) DDT\nadditionally provides a geometrical measurement for a tubular structure, which\nis important for clinical diagnosis (e.g., the cross-sectional scale of a\npancreatic duct can be an indicator for pancreatic cancer)."}, {"title": "Instance Segmentation of Biological Images Using Harmonic Embeddings", "authors": "Victor Kulikov, Victor Lempitsky", "link": "https://arxiv.org/abs/1904.05257", "summary": "We present a new instance segmentation approach tailored to biological\nimages, where instances may correspond to individual cells, organisms or plant\nparts. Unlike instance segmentation for user photographs or road scenes, in\nbiological data object instances may be particularly densely packed, the\nappearance variation may be particularly low, the processing power may be\nrestricted, while, on the other hand, the variability of sizes of individual\ninstances may be limited. The proposed approach successfully addresses these\npeculiarities.\n  Our approach describes each object instance using an expectation of a limited\nnumber of sine waves with frequencies and phases adjusted to particular object\nsizes and densities. At train time, a fully-convolutional network is learned to\npredict the object embeddings at each pixel using a simple pixelwise regression\nloss, while at test time the instances are recovered using clustering in the\nembedding space. In the experiments, we show that our approach outperforms\nprevious embedding-based instance segmentation approaches on a number of\nbiological datasets, achieving state-of-the-art on a popular CVPPP benchmark.\nThis excellent performance is combined with computational efficiency that is\nneeded for deployment to domain specialists.\n  The source code of the approach is available at\nhttps://github.com/kulikovv/harmonic"}, {"title": "Multi-scale Domain-adversarial Multiple-instance CNN for Cancer Subtype Classification with Unannotated Histopathological Images", "authors": "Noriaki Hashimoto, Daisuke Fukushima, Ryoichi Koga, Yusuke Takagi, Kaho Ko, Kei Kohno, Masato Nakaguro, Shigeo Nakamura, Hidekata Hontani, Ichiro Takeuchi", "link": "https://arxiv.org/abs/2001.01599", "summary": "We propose a new method for cancer subtype classification from\nhistopathological images, which can automatically detect tumor-specific\nfeatures in a given whole slide image (WSI). The cancer subtype should be\nclassified by referring to a WSI, i.e., a large-sized image (typically\n40,000x40,000 pixels) of an entire pathological tissue slide, which consists of\ncancer and non-cancer portions. One difficulty arises from the high cost\nassociated with annotating tumor regions in WSIs. Furthermore, both global and\nlocal image features must be extracted from the WSI by changing the\nmagnifications of the image. In addition, the image features should be stably\ndetected against the differences of staining conditions among the\nhospitals/specimens. In this paper, we develop a new CNN-based cancer subtype\nclassification method by effectively combining multiple-instance, domain\nadversarial, and multi-scale learning frameworks in order to overcome these\npractical difficulties. When the proposed method was applied to malignant\nlymphoma subtype classifications of 196 cases collected from multiple\nhospitals, the classification performance was significantly better than the\nstandard CNN or other conventional methods, and the accuracy compared favorably\nwith that of standard pathologists."}, {"title": "SOS: Selective Objective Switch for Rapid Immunofluorescence Whole Slide Image Classification", "authors": "Sam Maksoud, Kun Zhao, Peter Hobson, Anthony Jennings, Brian C. Lovell", "link": "https://arxiv.org/abs/2003.05080", "summary": "The difficulty of processing gigapixel whole slide images (WSIs) in clinical\nmicroscopy has been a long-standing barrier to implementing computer aided\ndiagnostic systems. Since modern computing resources are unable to perform\ncomputations at this extremely large scale, current state of the art methods\nutilize patch-based processing to preserve the resolution of WSIs. However,\nthese methods are often resource intensive and make significant compromises on\nprocessing time. In this paper, we demonstrate that conventional patch-based\nprocessing is redundant for certain WSI classification tasks where high\nresolution is only required in a minority of cases. This reflects what is\nobserved in clinical practice; where a pathologist may screen slides using a\nlow power objective and only switch to a high power in cases where they are\nuncertain about their findings. To eliminate these redundancies, we propose a\nmethod for the selective use of high resolution processing based on the\nconfidence of predictions on downscaled WSIs --- we call this the Selective\nObjective Switch (SOS). Our method is validated on a novel dataset of 684\nLiver-Kidney-Stomach immunofluorescence WSIs routinely used in the\ninvestigation of autoimmune liver disease. By limiting high resolution\nprocessing to cases which cannot be classified confidently at low resolution,\nwe maintain the accuracy of patch-level analysis whilst reducing the inference\ntime by a factor of 7.74."}, {"title": "Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks", "authors": "Sungjoon Choi, Sanghoon Hong, Kyungjae Lee, Sungbin Lim", "link": "https://arxiv.org/abs/1805.06431", "summary": "In this paper, we focus on weakly supervised learning with noisy training\ndata for both classification and regression problems.We assume that the\ntraining outputs are collected from a mixture of a target and correlated noise\ndistributions.Our proposed method simultaneously estimates the target\ndistribution and the quality of each data which is defined as the correlation\nbetween the target and data generating distributions.The cornerstone of the\nproposed method is a Cholesky Block that enables modeling dependencies among\nmixture distributions in a differentiable manner where we maintain the\ndistribution over the network weights.We first provide illustrative examples in\nboth regression and classification tasks to show the effectiveness of the\nproposed method.Then, the proposed method is extensively evaluated in a number\nof experiments where we show that it constantly shows comparable or superior\nperformances compared to existing baseline methods in the handling of noisy\ndata."}, {"title": "METAL: Minimum Effort Temporal Activity Localization in Untrimmed Videos", "authors": "Da Zhang, Xiyang Dai, Yuan-Fang Wang"}, {"title": "Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data", "authors": "Xi Yan, David Acuna, Sanja Fidler", "link": "https://arxiv.org/abs/2001.02799", "summary": "Transfer learning has proven to be a successful technique to train deep\nlearning models in the domains where little training data is available. The\ndominant approach is to pretrain a model on a large generic dataset such as\nImageNet and finetune its weights on the target domain. However, in the new era\nof an ever-increasing number of massive datasets, selecting the relevant data\nfor pretraining is a critical issue. We introduce Neural Data Server (NDS), a\nlarge-scale search engine for finding the most useful transfer learning data to\nthe target domain. NDS consists of a dataserver which indexes several large\npopular image datasets, and aims to recommend data to a client, an end-user\nwith a target application with its own small labeled dataset. The dataserver\nrepresents large datasets with a much more compact mixture-of-experts model,\nand employs it to perform data search in a series of dataserver-client\ntransactions at a low computational cost. We show the effectiveness of NDS in\nvarious transfer learning scenarios, demonstrating state-of-the-art performance\non several target datasets and tasks such as image classification, object\ndetection and instance segmentation. Neural Data Server is available as a\nweb-service at http://aidemos.cs.toronto.edu/nds/."}, {"title": "Revisiting Knowledge Distillation via Label Smoothing Regularization", "authors": "Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, Jiashi Feng", "link": "", "summary": ""}, {"title": "WCP: Worst-Case Perturbations for Semi-Supervised Deep Learning", "authors": "Liheng Zhang, Guo-Jun Qi"}, {"title": "DEPARA: Deep Attribution Graph for Deep Knowledge Transferability", "authors": "Jie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, Chengchao Shen, Feng Mao, Mingli Song", "link": "https://arxiv.org/abs/2003.07496", "summary": "Exploring the intrinsic interconnections between the knowledge encoded in\nPRe-trained Deep Neural Networks (PR-DNNs) of heterogeneous tasks sheds light\non their mutual transferability, and consequently enables knowledge transfer\nfrom one task to another so as to reduce the training effort of the latter. In\nthis paper, we propose the DEeP Attribution gRAph (DEPARA) to investigate the\ntransferability of knowledge learned from PR-DNNs. In DEPARA, nodes correspond\nto the inputs and are represented by their vectorized attribution maps with\nregards to the outputs of the PR-DNN. Edges denote the relatedness between\ninputs and are measured by the similarity of their features extracted from the\nPR-DNN. The knowledge transferability of two PR-DNNs is measured by the\nsimilarity of their corresponding DEPARAs. We apply DEPARA to two important yet\nunder-studied problems in transfer learning: pre-trained model selection and\nlayer selection. Extensive experiments are conducted to demonstrate the\neffectiveness and superiority of the proposed method in solving both these\nproblems. Code, data and models reproducing the results in this paper are\navailable at \\url{https://github.com/zju-vipa/DEPARA}."}, {"title": "Conditional Channel Gated Networks for Task-Aware Continual Learning", "authors": "Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, Babak Ehteshami Bejnordi", "link": "https://arxiv.org/abs/2004.00070", "summary": "Convolutional Neural Networks experience catastrophic forgetting when\noptimized on a sequence of learning problems: as they meet the objective of the\ncurrent training examples, their performance on previous tasks drops\ndrastically. In this work, we introduce a novel framework to tackle this\nproblem with conditional computation. We equip each convolutional layer with\ntask-specific gating modules, selecting which filters to apply on the given\ninput. This way, we achieve two appealing properties. Firstly, the execution\npatterns of the gates allow to identify and protect important filters, ensuring\nno loss in the performance of the model for previously learned tasks. Secondly,\nby using a sparsity objective, we can promote the selection of a limited set of\nkernels, allowing to retain sufficient model capacity to digest new\ntasks.Existing solutions require, at test time, awareness of the task to which\neach example belongs to. This knowledge, however, may not be available in many\npractical scenarios. Therefore, we additionally introduce a task classifier\nthat predicts the task label of each example, to deal with settings in which a\ntask oracle is not available. We validate our proposal on four continual\nlearning datasets. Results show that our model consistently outperforms\nexisting methods both in the presence and the absence of a task oracle.\nNotably, on Split SVHN and Imagenet-50 datasets, our model yields up to 23.98%\nand 17.42% improvement in accuracy w.r.t. competing methods."}, {"title": "Towards Discriminability and Diversity: Batch Nuclear-Norm Maximization Under Label Insufficient Situations", "authors": "Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, Qi Tian", "link": "https://arxiv.org/abs/2003.12237", "summary": "The learning of the deep networks largely relies on the data with\nhuman-annotated labels. In some label insufficient situations, the performance\ndegrades on the decision boundary with high data density. A common solution is\nto directly minimize the Shannon Entropy, but the side effect caused by entropy\nminimization, i.e., reduction of the prediction diversity, is mostly ignored.\nTo address this issue, we reinvestigate the structure of classification output\nmatrix of a randomly selected data batch. We find by theoretical analysis that\nthe prediction discriminability and diversity could be separately measured by\nthe Frobenius-norm and rank of the batch output matrix. Besides, the\nnuclear-norm is an upperbound of the Frobenius-norm, and a convex approximation\nof the matrix rank. Accordingly, to improve both discriminability and\ndiversity, we propose Batch Nuclear-norm Maximization (BNM) on the output\nmatrix. BNM could boost the learning under typical label insufficient learning\nscenarios, such as semi-supervised learning, domain adaptation and open domain\nrecognition. On these tasks, extensive experimental results show that BNM\noutperforms competitors and works well with existing well-known methods. The\ncode is available at https://github.com/cuishuhao/BNM."}, {"title": "FocalMix: Semi-Supervised Learning for 3D Medical Image Detection", "authors": "Dong Wang, Yuan Zhang, Kexin Zhang, Liwei Wang", "link": "https://arxiv.org/abs/2003.09108", "summary": "Applying artificial intelligence techniques in medical imaging is one of the\nmost promising areas in medicine. However, most of the recent success in this\narea highly relies on large amounts of carefully annotated data, whereas\nannotating medical images is a costly process. In this paper, we propose a\nnovel method, called FocalMix, which, to the best of our knowledge, is the\nfirst to leverage recent advances in semi-supervised learning (SSL) for 3D\nmedical image detection. We conducted extensive experiments on two widely used\ndatasets for lung nodule detection, LUNA16 and NLST. Results show that our\nproposed SSL methods can achieve a substantial improvement of up to 17.3% over\nstate-of-the-art supervised learning approaches with 400 unlabeled CT scans."}, {"title": "Learning 3D Semantic Scene Graphs From 3D Indoor Reconstructions", "authors": "Johanna Wald, Helisa Dhamo, Nassir Navab, Federico Tombari", "link": "https://arxiv.org/abs/2004.03967", "summary": "Scene understanding has been of high interest in computer vision. It\nencompasses not only identifying objects in a scene, but also their\nrelationships within the given context. With this goal, a recent line of works\ntackles 3D semantic segmentation and scene layout prediction. In our work we\nfocus on scene graphs, a data structure that organizes the entities of a scene\nin a graph, where objects are nodes and their relationships modeled as edges.\nWe leverage inference on scene graphs as a way to carry out 3D scene\nunderstanding, mapping objects and their relationships. In particular, we\npropose a learned method that regresses a scene graph from the point cloud of a\nscene. Our novel architecture is based on PointNet and Graph Convolutional\nNetworks (GCN). In addition, we introduce 3DSSG, a semi-automatically generated\ndataset, that contains semantically rich scene graphs of 3D scenes. We show the\napplication of our method in a domain-agnostic retrieval task, where graphs\nserve as an intermediate representation for 3D-3D and 2D-3D matching."}, {"title": "Self-Supervised Viewpoint Learning From Image Collections", "authors": "Siva Karthik Mustikovela, Varun Jampani, Shalini De Mello, Sifei Liu, Umar Iqbal, Carsten Rother, Jan Kautz"}, {"title": "Two-Shot Spatially-Varying BRDF and Shape Estimation", "authors": "Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, Jan Kautz", "link": "http://arxiv.org/abs/2004.00403", "summary": "Capturing the shape and spatially-varying appearance (SVBRDF) of an object\nfrom images is a challenging task that has applications in both computer vision\nand graphics. Traditional optimization-based approaches often need a large\nnumber of images taken from multiple views in a controlled environment. Newer\ndeep learning-based approaches require only a few input images, but the\nreconstruction quality is not on par with optimization techniques. We propose a\nnovel deep learning architecture with a stage-wise estimation of shape and\nSVBRDF. The previous predictions guide each estimation, and a joint refinement\nnetwork later refines both SVBRDF and shape. We follow a practical mobile image\ncapture setting and use unaligned two-shot flash and no-flash images as input.\nBoth our two-shot image capture and network inference can run on mobile\nhardware. We also create a large-scale synthetic training dataset with\ndomain-randomized geometry and realistic materials. Extensive experiments on\nboth synthetic and real-world datasets show that our network trained on a\nsynthetic dataset can generalize well to real-world images. Comparisons with\nrecent approaches demonstrate the superior performance of the proposed\napproach."}, {"title": "Variational Context-Deformable ConvNets for Indoor Scene Parsing", "authors": "Zhitong Xiong, Yuan Yuan, Nianhui Guo, Qi Wang"}, {"title": "Strip Pooling: Rethinking Spatial Pooling for Scene Parsing", "authors": "Qibin Hou, Li Zhang, Ming-Ming Cheng, Jiashi Feng", "link": "https://arxiv.org/abs/2003.13328", "summary": "Spatial pooling has been proven highly effective in capturing long-range\ncontextual information for pixel-wise prediction tasks, such as scene parsing.\nIn this paper, beyond conventional spatial pooling that usually has a regular\nshape of NxN, we rethink the formulation of spatial pooling by introducing a\nnew pooling strategy, called strip pooling, which considers a long but narrow\nkernel, i.e., 1xN or Nx1. Based on strip pooling, we further investigate\nspatial pooling architecture design by 1) introducing a new strip pooling\nmodule that enables backbone networks to efficiently model long-range\ndependencies, 2) presenting a novel building block with diverse spatial pooling\nas a core, and 3) systematically comparing the performance of the proposed\nstrip pooling and conventional spatial pooling techniques. Both novel\npooling-based designs are lightweight and can serve as an efficient\nplug-and-play module in existing scene parsing networks. Extensive experiments\non popular benchmarks (e.g., ADE20K and Cityscapes) demonstrate that our simple\napproach establishes new state-of-the-art results. Code is made available at\nhttps://github.com/Andrew-Qibin/SPNet."}, {"title": "Few-Shot Object Detection With Attention-RPN and Multi-Relation Detector", "authors": "Qi Fan, Wei Zhuo, Chi-Keung Tang, Yu-Wing Tai", "link": "https://arxiv.org/abs/1908.01998", "summary": "Conventional methods for object detection typically require a substantial\namount of training data and preparing such high-quality training data is very\nlabor-intensive. In this paper, we propose a novel few-shot object detection\nnetwork that aims at detecting objects of unseen categories with only a few\nannotated examples. Central to our method are our Attention-RPN, Multi-Relation\nDetector and Contrastive Training strategy, which exploit the similarity\nbetween the few shot support set and query set to detect novel objects while\nsuppressing false detection in the background. To train our network, we\ncontribute a new dataset that contains 1000 categories of various objects with\nhigh-quality annotations. To the best of our knowledge, this is one of the\nfirst datasets specifically designed for few-shot object detection. Once our\nfew-shot network is trained, it can detect objects of unseen categories without\nfurther training or fine-tuning. Our method is general and has a wide range of\npotential applications. We produce a new state-of-the-art performance on\ndifferent datasets in the few-shot setting. The dataset link is\nhttps://github.com/fanq15/Few-Shot-Object-Detection-Dataset."}, {"title": "What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation", "authors": "Jiahua Dong, Yang Cong, Gan Sun, Bineng Zhong, Xiaowei Xu", "link": "https://arxiv.org/abs/2004.11500", "summary": "Unsupervised domain adaptation has attracted growing research attention on\nsemantic segmentation. However, 1) most existing models cannot be directly\napplied into lesions transfer of medical images, due to the diverse appearances\nof same lesion among different datasets; 2) equal attention has been paid into\nall semantic representations instead of neglecting irrelevant knowledge, which\nleads to negative transfer of untransferable knowledge. To address these\nchallenges, we develop a new unsupervised semantic transfer model including two\ncomplementary modules (i.e., T_D and T_F ) for endoscopic lesions segmentation,\nwhich can alternatively determine where and how to explore transferable\ndomain-invariant knowledge between labeled source lesions dataset (e.g.,\ngastroscope) and unlabeled target diseases dataset (e.g., enteroscopy).\nSpecifically, T_D focuses on where to translate transferable visual information\nof medical lesions via residual transferability-aware bottleneck, while\nneglecting untransferable visual characterizations. Furthermore, T_F highlights\nhow to augment transferable semantic features of various lesions and\nautomatically ignore untransferable representations, which explores\ndomain-invariant knowledge and in return improves the performance of T_D. To\nthe end, theoretical analysis and extensive experiments on medical endoscopic\ndataset and several non-medical public datasets well demonstrate the\nsuperiority of our proposed model."}, {"title": "ADINet: Attribute Driven Incremental Network for Retinal Image Classification", "authors": "Qier Meng, Satoh Shin'ichi"}, {"title": "Unsupervised Domain Adaptation With Hierarchical Gradient Synchronization", "authors": "Lanqing Hu, Meina Kan, Shiguang Shan, Xilin Chen", "link": "", "summary": ""}, {"title": "Deep Grouping Model for Unified Perceptual Parsing", "authors": "Zhiheng Li, Wenxuan Bao, Jiayang Zheng, Chenliang Xu", "link": "https://arxiv.org/abs/2003.11647", "summary": "The perceptual-based grouping process produces a hierarchical and\ncompositional image representation that helps both human and machine vision\nsystems recognize heterogeneous visual concepts. Examples can be found in the\nclassical hierarchical superpixel segmentation or image parsing works. However,\nthe grouping process is largely overlooked in modern CNN-based image\nsegmentation networks due to many challenges, including the inherent\nincompatibility between the grid-shaped CNN feature map and the\nirregular-shaped perceptual grouping hierarchy. Overcoming these challenges, we\npropose a deep grouping model (DGM) that tightly marries the two types of\nrepresentations and defines a bottom-up and a top-down process for feature\nexchanging. When evaluating the model on the recent Broden+ dataset for the\nunified perceptual parsing task, it achieves state-of-the-art results while\nhaving a small computational overhead compared to other contextual-based\nsegmentation models. Furthermore, the DGM has better interpretability compared\nwith modern CNN methods."}, {"title": "Where Am I Looking At? Joint Location and Orientation Estimation by Cross-View Matching", "authors": "Yujiao Shi, Xin Yu, Dylan Campbell, Hongdong Li", "link": "https://arxiv.org/abs/2005.03860", "summary": "Cross-view geo-localization is the problem of estimating the position and\norientation (latitude, longitude and azimuth angle) of a camera at ground level\ngiven a large-scale database of geo-tagged aerial (e.g., satellite) images.\nExisting approaches treat the task as a pure location estimation problem by\nlearning discriminative feature descriptors, but neglect orientation alignment.\nIt is well-recognized that knowing the orientation between ground and aerial\nimages can significantly reduce matching ambiguity between these two views,\nespecially when the ground-level images have a limited Field of View (FoV)\ninstead of a full field-of-view panorama. Therefore, we design a Dynamic\nSimilarity Matching network to estimate cross-view orientation alignment during\nlocalization. In particular, we address the cross-view domain gap by applying a\npolar transform to the aerial images to approximately align the images up to an\nunknown azimuth angle. Then, a two-stream convolutional network is used to\nlearn deep features from the ground and polar-transformed aerial images.\nFinally, we obtain the orientation by computing the correlation between\ncross-view features, which also provides a more accurate measure of feature\nsimilarity, improving location recall. Experiments on standard datasets\ndemonstrate that our method significantly improves state-of-the-art\nperformance. Remarkably, we improve the top-1 location recall rate on the CVUSA\ndataset by a factor of 1.5x for panoramas with known orientation, by a factor\nof 3.3x for panoramas with unknown orientation, and by a factor of 6x for\n180-degree FoV images with unknown orientation."}, {"title": "Gum-Net: Unsupervised Geometric Matching for Fast and Accurate 3D Subtomogram Image Alignment and Averaging", "authors": "Xiangrui Zeng, Min Xu"}, {"title": "FDA: Fourier Domain Adaptation for Semantic Segmentation", "authors": "Yanchao Yang, Stefano Soatto", "link": "https://arxiv.org/abs/2004.05498", "summary": "We describe a simple method for unsupervised domain adaptation, whereby the\ndiscrepancy between the source and target distributions is reduced by swapping\nthe low-frequency spectrum of one with the other. We illustrate the method in\nsemantic segmentation, where densely annotated images are aplenty in one domain\n(synthetic data), but difficult to obtain in another (real images). Current\nstate-of-the-art methods are complex, some requiring adversarial optimization\nto render the backbone of a neural network invariant to the discrete domain\nselection variable. Our method does not require any training to perform the\ndomain alignment, just a simple Fourier Transform and its inverse. Despite its\nsimplicity, it achieves state-of-the-art performance in the current benchmarks,\nwhen integrated into a relatively standard semantic segmentation model. Our\nresults indicate that even simple procedures can discount nuisance variability\nin the data that more sophisticated methods struggle to learn away."}, {"title": "Foreground-Aware Relation Network for Geospatial Object Segmentation in High Spatial Resolution Remote Sensing Imagery", "authors": "Zhuo Zheng, Yanfei Zhong, Junjue Wang, Ailong Ma"}, {"title": "When2com: Multi-Agent Perception via Communication Graph Grouping", "authors": "Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, Zsolt Kira", "link": "https://arxiv.org/abs/2006.00176", "summary": "While significant advances have been made for single-agent perception, many\napplications require multiple sensing agents and cross-agent communication due\nto benefits such as coverage and robustness. It is therefore critical to\ndevelop frameworks which support multi-agent collaborative perception in a\ndistributed and bandwidth-efficient manner. In this paper, we address the\ncollaborative perception problem, where one agent is required to perform a\nperception task and can communicate and share information with other agents on\nthe same task. Specifically, we propose a communication framework by learning\nboth to construct communication groups and decide when to communicate. We\ndemonstrate the generalizability of our framework on two different perception\ntasks and show that it significantly reduces communication bandwidth while\nmaintaining superior performance."}, {"title": "Learning Human-Object Interaction Detection Using Interaction Points", "authors": "Tiancai Wang, Tong Yang, Martin Danelljan, Fahad Shahbaz Khan, Xiangyu Zhang, Jian Sun", "link": "https://arxiv.org/abs/2003.14023", "summary": "Understanding interactions between humans and objects is one of the\nfundamental problems in visual classification and an essential step towards\ndetailed scene understanding. Human-object interaction (HOI) detection strives\nto localize both the human and an object as well as the identification of\ncomplex interactions between them. Most existing HOI detection approaches are\ninstance-centric where interactions between all possible human-object pairs are\npredicted based on appearance features and coarse spatial information. We argue\nthat appearance features alone are insufficient to capture complex human-object\ninteractions. In this paper, we therefore propose a novel fully-convolutional\napproach that directly detects the interactions between human-object pairs. Our\nnetwork predicts interaction points, which directly localize and classify the\ninter-action. Paired with the densely predicted interaction vectors, the\ninteractions are associated with human and object detections to obtain final\npredictions. To the best of our knowledge, we are the first to propose an\napproach where HOI detection is posed as a keypoint detection and grouping\nproblem. Experiments are performed on two popular benchmarks: V-COCO and\nHICO-DET. Our approach sets a new state-of-the-art on both datasets. Code is\navailable at https://github.com/vaesl/IP-Net."}, {"title": "C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation", "authors": "Qihang Yu, Dong Yang, Holger Roth, Yutong Bai, Yixiao Zhang, Alan L. Yuille, Daguang Xu", "link": "https://arxiv.org/abs/1912.09628", "summary": "3D convolution neural networks (CNN) have been proved very successful in\nparsing organs or tumours in 3D medical images, but it remains sophisticated\nand time-consuming to choose or design proper 3D networks given different task\ncontexts. Recently, Neural Architecture Search (NAS) is proposed to solve this\nproblem by searching for the best network architecture automatically. However,\nthe inconsistency between search stage and deployment stage often exists in NAS\nalgorithms due to memory constraints and large search space, which could become\nmore serious when applying NAS to some memory and time consuming tasks, such as\n3D medical image segmentation. In this paper, we propose coarse-to-fine neural\narchitecture search (C2FNAS) to automatically search a 3D segmentation network\nfrom scratch without inconsistency on network size or input size. Specifically,\nwe divide the search procedure into two stages: 1) the coarse stage, where we\nsearch the macro-level topology of the network, i.e. how each convolution\nmodule is connected to other modules; 2) the fine stage, where we search at\nmicro-level for operations in each cell based on previous searched macro-level\ntopology. The coarse-to-fine manner divides the search procedure into two\nconsecutive stages and meanwhile resolves the inconsistency. We evaluate our\nmethod on 10 public datasets from Medical Segmentation Decalthon (MSD)\nchallenge, and achieve state-of-the-art performance with the network searched\nusing one dataset, which demonstrates the effectiveness and generalization of\nour searched models."}, {"title": "Adaptive Subspaces for Few-Shot Learning", "authors": "Christian Simon, Piotr Koniusz, Richard Nock, Mehrtash Harandi"}, {"title": "Learning to Detect Important People in Unlabelled Images for Semi-Supervised Important People Detection", "authors": "Fa-Ting Hong, Wei-Hong Li, Wei-Shi Zheng", "link": "http://arxiv.org/abs/2004.07568", "summary": "Important people detection is to automatically detect the individuals who\nplay the most important roles in a social event image, which requires the\ndesigned model to understand a high-level pattern. However, existing methods\nrely heavily on supervised learning using large quantities of annotated image\nsamples, which are more costly to collect for important people detection than\nfor individual entity recognition (eg, object recognition). To overcome this\nproblem, we propose learning important people detection on partially annotated\nimages. Our approach iteratively learns to assign pseudo-labels to individuals\nin un-annotated images and learns to update the important people detection\nmodel based on data with both labels and pseudo-labels. To alleviate the\npseudo-labelling imbalance problem, we introduce a ranking strategy for\npseudo-label estimation, and also introduce two weighting strategies: one for\nweighting the confidence that individuals are important people to strengthen\nthe learning on important people and the other for neglecting noisy unlabelled\nimages (ie, images without any important people). We have collected two\nlarge-scale datasets for evaluation. The extensive experimental results clearly\nconfirm the efficacy of our method attained by leveraging unlabelled images for\nimproving the performance of important people detection."}, {"title": "Stochastic Sparse Subspace Clustering", "authors": "Ying Chen, Chun-Guang Li, Chong You", "link": "https://arxiv.org/abs/2005.01449", "summary": "State-of-the-art subspace clustering methods are based on self-expressive\nmodel, which represents each data point as a linear combination of other data\npoints. By enforcing such representation to be sparse, sparse subspace\nclustering is guaranteed to produce a subspace-preserving data affinity where\ntwo points are connected only if they are from the same subspace. On the other\nhand, however, data points from the same subspace may not be well-connected,\nleading to the issue of over-segmentation. We introduce dropout to address the\nissue of over-segmentation, which is based on randomly dropping out data points\nin self-expressive model. In particular, we show that dropout is equivalent to\nadding a squared $\\ell_2$ norm regularization on the representation\ncoefficients, therefore induces denser solutions. Then, we reformulate the\noptimization problem as a consensus problem over a set of small-scale\nsubproblems. This leads to a scalable and flexible sparse subspace clustering\napproach, termed Stochastic Sparse Subspace Clustering, which can effectively\nhandle large scale datasets. Extensive experiments on synthetic data and real\nworld datasets validate the efficiency and effectiveness of our proposal."}, {"title": "CRNet: Cross-Reference Networks for Few-Shot Segmentation", "authors": "Weide Liu, Chi Zhang, Guosheng Lin, Fayao Liu", "link": "https://arxiv.org/abs/2003.10658", "summary": "Over the past few years, state-of-the-art image segmentation algorithms are\nbased on deep convolutional neural networks. To render a deep network with the\nability to understand a concept, humans need to collect a large amount of\npixel-level annotated data to train the models, which is time-consuming and\ntedious. Recently, few-shot segmentation is proposed to solve this problem.\nFew-shot segmentation aims to learn a segmentation model that can be\ngeneralized to novel classes with only a few training images. In this paper, we\npropose a cross-reference network (CRNet) for few-shot segmentation. Unlike\nprevious works which only predict the mask in the query image, our proposed\nmodel concurrently make predictions for both the support image and the query\nimage. With a cross-reference mechanism, our network can better find the\nco-occurrent objects in the two images, thus helping the few-shot segmentation\ntask. We also develop a mask refinement module to recurrently refine the\nprediction of the foreground regions. For the $k$-shot learning, we propose to\nfinetune parts of networks to take advantage of multiple labeled support\nimages. Experiments on the PASCAL VOC 2012 dataset show that our network\nachieves state-of-the-art performance."}, {"title": "Shoestring: Graph-Based Semi-Supervised Classification With Severely Limited Labeled Data", "authors": "Wanyu Lin, Zhaolin Gao, Baochun Li", "link": "https://arxiv.org/abs/1910.12976", "summary": "Graph-based semi-supervised learning has been shown to be one of the most\neffective approaches for classification tasks from a wide range of domains,\nsuch as image classification and text classification, as they can exploit the\nconnectivity patterns between labeled and unlabeled samples to improve learning\nperformance. In this work, we advance this effective learning paradigm towards\na scenario where labeled data are severely limited. More specifically, we\naddress the problem of graph-based semi-supervised learning in the presence of\nseverely limited labeled samples, and propose a new framework, called {\\em\nShoestring}, that improves the learning performance through semantic transfer\nfrom these very few labeled samples to large numbers of unlabeled samples.\n  In particular, our framework learns a metric space in which classification\ncan be performed by computing the similarity to centroid embedding of each\nclass. {\\em Shoestring} is trained in an end-to-end fashion to learn to\nleverage the semantic knowledge of limited labeled samples as well as their\nconnectivity patterns with large numbers of unlabeled samples simultaneously.\nBy combining {\\em Shoestring} with graph convolutional networks, label\npropagation and their recent label-efficient variations (IGCN and GLP), we are\nable to achieve state-of-the-art node classification performance in the\npresence of very few labeled samples. In addition, we demonstrate the\neffectiveness of our framework on image classification tasks in the few-shot\nlearning regime, with significant gains on miniImageNet ($2.57\\%\\sim3.59\\%$)\nand tieredImageNet ($1.05\\%\\sim2.70\\%$)."}, {"title": "Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings", "authors": "Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger", "link": "https://arxiv.org/abs/1911.02357", "summary": "We introduce a powerful student-teacher framework for the challenging problem\nof unsupervised anomaly detection and pixel-precise anomaly segmentation in\nhigh-resolution images. Student networks are trained to regress the output of a\ndescriptive teacher network that was pretrained on a large dataset of patches\nfrom natural images. This circumvents the need for prior data annotation.\nAnomalies are detected when the outputs of the student networks differ from\nthat of the teacher network. This happens when they fail to generalize outside\nthe manifold of anomaly-free training data. The intrinsic uncertainty in the\nstudent networks is used as an additional scoring function that indicates\nanomalies. We compare our method to a large number of existing deep learning\nbased methods for unsupervised anomaly detection. Our experiments demonstrate\nimprovements over state-of-the-art methods on a number of real-world datasets,\nincluding the recently introduced MVTec Anomaly Detection dataset that was\nspecifically designed to benchmark anomaly segmentation algorithms."}, {"title": "3D Sketch-Aware Semantic Scene Completion via Semi-Supervised Structure Prior", "authors": "Xiaokang Chen, Kwan-Yee Lin, Chen Qian, Gang Zeng, Hongsheng Li", "link": "https://arxiv.org/abs/2003.14052", "summary": "The goal of the Semantic Scene Completion (SSC) task is to simultaneously\npredict a completed 3D voxel representation of volumetric occupancy and\nsemantic labels of objects in the scene from a single-view observation. Since\nthe computational cost generally increases explosively along with the growth of\nvoxel resolution, most current state-of-the-arts have to tailor their framework\ninto a low-resolution representation with the sacrifice of detail prediction.\nThus, voxel resolution becomes one of the crucial difficulties that lead to the\nperformance bottleneck.\n  In this paper, we propose to devise a new geometry-based strategy to embed\ndepth information with low-resolution voxel representation, which could still\nbe able to encode sufficient geometric information, e.g., room layout, object's\nsizes and shapes, to infer the invisible areas of the scene with well\nstructure-preserving details. To this end, we first propose a novel 3D\nsketch-aware feature embedding to explicitly encode geometric information\neffectively and efficiently. With the 3D sketch in hand, we further devise a\nsimple yet effective semantic scene completion framework that incorporates a\nlight-weight 3D Sketch Hallucination module to guide the inference of occupancy\nand the semantic labels via a semi-supervised structure prior learning\nstrategy. We demonstrate that our proposed geometric embedding works better\nthan the depth feature learning from habitual SSC frameworks. Our final model\nsurpasses state-of-the-arts consistently on three public benchmarks, which only\nrequires 3D volumes of 60 x 36 x 60 resolution for both input and output. The\ncode and the supplementary material will be available at\nhttps://charlesCXK.github.io."}, {"title": "Graph-Guided Architecture Search for Real-Time Semantic Segmentation", "authors": "Peiwen Lin, Peng Sun, Guangliang Cheng, Sirui Xie, Xi Li, Jianping Shi", "link": "https://arxiv.org/abs/1909.06793", "summary": "Designing a lightweight semantic segmentation network often requires\nresearchers to find a trade-off between performance and speed, which is always\nempirical due to the limited interpretability of neural networks. In order to\nrelease researchers from these tedious mechanical trials, we propose a\nGraph-guided Architecture Search (GAS) pipeline to automatically search\nreal-time semantic segmentation networks. Unlike previous works that use a\nsimplified search space and stack a repeatable cell to form a network, we\nintroduce a novel search mechanism with new search space where a lightweight\nmodel can be effectively explored through the cell-level diversity and\nlatencyoriented constraint. Specifically, to produce the cell-level diversity,\nthe cell-sharing constraint is eliminated through the cell-independent manner.\nThen a graph convolution network (GCN) is seamlessly integrated as a\ncommunication mechanism between cells. Finally, a latency-oriented constraint\nis endowed into the search process to balance the speed and performance.\nExtensive experiments on Cityscapes and CamVid datasets demonstrate that GAS\nachieves the new state-of-the-art trade-off between accuracy and speed. In\nparticular, on Cityscapes dataset, GAS achieves the new best performance of\n73.5% mIoU with speed of 108.4 FPS on Titan Xp."}, {"title": "Composing Good Shots by Exploiting Mutual Relations", "authors": "Debang Li, Junge Zhang, Kaiqi Huang, Ming-Hsuan Yang"}, {"title": "Organ at Risk Segmentation for Head and Neck Cancer Using Stratified Learning and Neural Architecture Search", "authors": "Dazhou Guo, Dakai Jin, Zhuotun Zhu, Tsung-Ying Ho, Adam P. Harrison, Chun-Hung Chao, Jing Xiao, Le Lu", "link": "http://arxiv.org/abs/2004.08426", "summary": "OAR segmentation is a critical step in radiotherapy of head and neck (H&N)\ncancer, where inconsistencies across radiation oncologists and prohibitive\nlabor costs motivate automated approaches. However, leading methods using\nstandard fully convolutional network workflows that are challenged when the\nnumber of OARs becomes large, e.g. > 40. For such scenarios, insights can be\ngained from the stratification approaches seen in manual clinical OAR\ndelineation. This is the goal of our work, where we introduce stratified organ\nat risk segmentation (SOARS), an approach that stratifies OARs into anchor,\nmid-level, and small & hard (S&H) categories. SOARS stratifies across two\ndimensions. The first dimension is that distinct processing pipelines are used\nfor each OAR category. In particular, inspired by clinical practices, anchor\nOARs are used to guide the mid-level and S&H categories. The second dimension\nis that distinct network architectures are used to manage the significant\ncontrast, size, and anatomy variations between different OARs. We use\ndifferentiable neural architecture search (NAS), allowing the network to choose\namong 2D, 3D or Pseudo-3D convolutions. Extensive 4-fold cross-validation on\n142 H&N cancer patients with 42 manually labeled OARs, the most comprehensive\nOAR dataset to date, demonstrates that both pipeline- and NAS-stratification\nsignificantly improves quantitative performance over the state-of-the-art (from\n69.52% to 73.68% in absolute Dice scores). Thus, SOARS provides a powerful and\nprincipled means to manage the highly complex segmentation space of OARs."}, {"title": "G2L-Net: Global to Local Network for Real-Time 6D Pose Estimation With Embedding Vector Features", "authors": "Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Ale\u0161 Leonardis", "link": "http://arxiv.org/abs/2003.11089", "summary": "In this paper, we propose a novel real-time 6D object pose estimation\nframework, named G2L-Net. Our network operates on point clouds from RGB-D\ndetection in a divide-and-conquer fashion. Specifically, our network consists\nof three steps. First, we extract the coarse object point cloud from the RGB-D\nimage by 2D detection. Second, we feed the coarse object point cloud to a\ntranslation localization network to perform 3D segmentation and object\ntranslation prediction. Third, via the predicted segmentation and translation,\nwe transfer the fine object point cloud into a local canonical coordinate, in\nwhich we train a rotation localization network to estimate initial object\nrotation. In the third step, we define point-wise embedding vector features to\ncapture viewpoint-aware information. To calculate more accurate rotation, we\nadopt a rotation residual estimator to estimate the residual between initial\nrotation and ground truth, which can boost initial pose estimation performance.\nOur proposed G2L-Net is real-time despite the fact multiple steps are stacked\nvia the proposed coarse-to-fine framework. Extensive experiments on two\nbenchmark datasets show that G2L-Net achieves state-of-the-art performance in\nterms of both accuracy and speed."}, {"title": "Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-Weighting", "authors": "Dongnan Liu, Donghao Zhang, Yang Song, Fan Zhang, Lauren O\u2019Donnell, Heng Huang, Mei Chen, Weidong Cai", "link": "http://arxiv.org/abs/2005.02066", "summary": "Unsupervised domain adaptation (UDA) for nuclei instance segmentation is\nimportant for digital pathology, as it alleviates the burden of labor-intensive\nannotation and domain shift across datasets. In this work, we propose a Cycle\nConsistency Panoptic Domain Adaptive Mask R-CNN (CyC-PDAM) architecture for\nunsupervised nuclei segmentation in histopathology images, by learning from\nfluorescence microscopy images. More specifically, we first propose a nuclei\ninpainting mechanism to remove the auxiliary generated objects in the\nsynthesized images. Secondly, a semantic branch with a domain discriminator is\ndesigned to achieve panoptic-level domain adaptation. Thirdly, in order to\navoid the influence of the source-biased features, we propose a task\nre-weighting mechanism to dynamically add trade-off weights for the\ntask-specific loss functions. Experimental results on three datasets indicate\nthat our proposed method outperforms state-of-the-art UDA methods\nsignificantly, and demonstrates a similar performance as fully supervised\nmethods."}, {"title": "Single-Stage Semantic Segmentation From Image Labels", "authors": "Nikita Araslanov, Stefan Roth", "link": "http://arxiv.org/abs/2005.08104", "summary": "Recent years have seen a rapid growth in new approaches improving the\naccuracy of semantic segmentation in a weakly supervised setting, i.e. with\nonly image-level labels available for training. However, this has come at the\ncost of increased model complexity and sophisticated multi-stage training\nprocedures. This is in contrast to earlier work that used only a single stage\n$-$ training one segmentation network on image labels $-$ which was abandoned\ndue to inferior segmentation accuracy. In this work, we first define three\ndesirable properties of a weakly supervised method: local consistency, semantic\nfidelity, and completeness. Using these properties as guidelines, we then\ndevelop a segmentation-based network model and a self-supervised training\nscheme to train for semantic masks from image-level annotations in a single\nstage. We show that despite its simplicity, our method achieves results that\nare competitive with significantly more complex pipelines, substantially\noutperforming earlier single-stage methods."}, {"title": "Cascaded Human-Object Interaction Recognition", "authors": "Tianfei Zhou, Wenguan Wang, Siyuan Qi, Haibin Ling, Jianbing Shen", "link": "http://arxiv.org/abs/2003.04262", "summary": "Rapid progress has been witnessed for human-object interaction (HOI)\nrecognition, but most existing models are confined to single-stage reasoning\npipelines. Considering the intrinsic complexity of the task, we introduce a\ncascade architecture for a multi-stage, coarse-to-fine HOI understanding. At\neach stage, an instance localization network progressively refines HOI\nproposals and feeds them into an interaction recognition network. Each of the\ntwo networks is also connected to its predecessor at the previous stage,\nenabling cross-stage information propagation. The interaction recognition\nnetwork has two crucial parts: a relation ranking module for high-quality HOI\nproposal selection and a triple-stream classifier for relation prediction. With\nour carefully-designed human-centric relation features, these two modules work\ncollaboratively towards effective interaction understanding. Further beyond\nrelation detection on a bounding-box level, we make our framework flexible to\nperform fine-grained pixel-wise relation segmentation; this provides a new\nglimpse into better relation modeling. Our approach reached the $1^{st}$ place\nin the ICCV2019 Person in Context Challenge, on both relation detection and\nsegmentation tasks. It also shows promising results on V-COCO."}, {"title": "DuDoRNet: Learning a Dual-Domain Recurrent Network for Fast MRI Reconstruction With Deep T1 Prior", "authors": "Bo Zhou, S. Kevin Zhou", "link": "https://arxiv.org/abs/2001.03799", "summary": "MRI with multiple protocols is commonly used for diagnosis, but it suffers\nfrom a long acquisition time, which yields the image quality vulnerable to say\nmotion artifacts. To accelerate, various methods have been proposed to\nreconstruct full images from under-sampled k-space data. However, these\nalgorithms are inadequate for two main reasons. Firstly, aliasing artifacts\ngenerated in the image domain are structural and non-local, so that sole image\ndomain restoration is insufficient. Secondly, though MRI comprises multiple\nprotocols during one exam, almost all previous studies only employ the\nreconstruction of an individual protocol using a highly distorted undersampled\nimage as input, leaving the use of fully-sampled short protocol (say T1) as\ncomplementary information highly underexplored. In this work, we address the\nabove two limitations by proposing a Dual Domain Recurrent Network (DuDoRNet)\nwith deep T1 prior embedded to simultaneously recover k-space and images for\naccelerating the acquisition of MRI with a long imaging protocol. Specifically,\na Dilated Residual Dense Network (DRDNet) is customized for dual domain\nrestorations from undersampled MRI data. Extensive experiments on different\nsampling patterns and acceleration rates demonstrate that our method\nconsistently outperforms state-of-the-art methods, and can reconstruct\nhigh-quality MRI."}, {"title": "Learning Integral Objects With Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation", "authors": "Junsong Fan, Zhaoxiang Zhang, Chunfeng Song, Tieniu Tan"}, {"title": "FPConv: Learning Local Flattening for Point Convolution", "authors": "Yiqun Lin, Zizheng Yan, Haibin Huang, Dong Du, Ligang Liu, Shuguang Cui, Xiaoguang Han", "link": "https://arxiv.org/abs/2002.10701", "summary": "We introduce FPConv, a novel surface-style convolution operator designed for\n3D point cloud analysis. Unlike previous methods, FPConv doesn't require\ntransforming to intermediate representation like 3D grid or graph and directly\nworks on surface geometry of point cloud. To be more specific, for each point,\nFPConv performs a local flattening by automatically learning a weight map to\nsoftly project surrounding points onto a 2D grid. Regular 2D convolution can\nthus be applied for efficient feature learning. FPConv can be easily integrated\ninto various network architectures for tasks like 3D object classification and\n3D scene segmentation, and achieve comparable performance with existing\nvolumetric-type convolutions. More importantly, our experiments also show that\nFPConv can be a complementary of volumetric convolutions and jointly training\nthem can further boost overall performance into state-of-the-art results."}, {"title": "Rotation Equivariant Graph Convolutional Network for Spherical Image Classification", "authors": "Qin Yang, Chenglin Li, Wenrui Dai, Junni Zou, Guo-Jun Qi, Hongkai Xiong"}, {"title": "FOAL: Fast Online Adaptive Learning for Cardiac Motion Estimation", "authors": "Hanchao Yu, Shanhui Sun, Haichao Yu, Xiao Chen, Honghui Shi, Thomas S. Huang, Terrence Chen", "link": "https://arxiv.org/abs/2003.04492", "summary": "Motion estimation of cardiac MRI videos is crucial for the evaluation of\nhuman heart anatomy and function. Recent researches show promising results with\ndeep learning-based methods. In clinical deployment, however, they suffer\ndramatic performance drops due to mismatched distributions between training and\ntesting datasets, commonly encountered in the clinical environment. On the\nother hand, it is arguably impossible to collect all representative datasets\nand to train a universal tracker before deployment. In this context, we\nproposed a novel fast online adaptive learning (FOAL) framework: an online\ngradient descent based optimizer that is optimized by a meta-learner. The\nmeta-learner enables the online optimizer to perform a fast and robust\nadaptation. We evaluated our method through extensive experiments on two public\nclinical datasets. The results showed the superior performance of FOAL in\naccuracy compared to the offline-trained tracking method. On average, the FOAL\ntook only $0.4$ second per video for online optimization."}, {"title": "ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation", "authors": "Sharon Fogel, Hadar Averbuch-Elor, Sarel Cohen, Shai Mazor, Roee Litman", "link": "https://arxiv.org/abs/2003.10557", "summary": "Optical character recognition (OCR) systems performance have improved\nsignificantly in the deep learning era. This is especially true for handwritten\ntext recognition (HTR), where each author has a unique style, unlike printed\ntext, where the variation is smaller by design. That said, deep learning based\nHTR is limited, as in every other task, by the number of training examples.\nGathering data is a challenging and costly task, and even more so, the labeling\ntask that follows, of which we focus here. One possible approach to reduce the\nburden of data annotation is semi-supervised learning. Semi supervised methods\nuse, in addition to labeled data, some unlabeled samples to improve\nperformance, compared to fully supervised ones. Consequently, such methods may\nadapt to unseen images during test time.\n  We present ScrabbleGAN, a semi-supervised approach to synthesize handwritten\ntext images that are versatile both in style and lexicon. ScrabbleGAN relies on\na novel generative model which can generate images of words with an arbitrary\nlength. We show how to operate our approach in a semi-supervised manner,\nenjoying the aforementioned benefits such as performance boost over state of\nthe art supervised HTR. Furthermore, our generator can manipulate the resulting\ntext style. This allows us to change, for instance, whether the text is\ncursive, or how thin is the pen stroke."}, {"title": "Cross-Domain Semantic Segmentation via Domain-Invariant Interactive Relation Transfer", "authors": "Fengmao Lv, Tao Liang, Xiang Chen, Guosheng Lin"}, {"title": "Inflated Episodic Memory With Region Self-Attention for Long-Tailed Visual Recognition", "authors": "Linchao Zhu, Yi Yang"}, {"title": "Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View With a Reachability Prior", "authors": "Osama Makansi, \u00d6zg\u00fcn \u00c7i\u00e7ek, Kevin Buchicchio, Thomas Brox"}, {"title": "Structure Preserving Generative Cross-Domain Learning", "authors": "Haifeng Xia, Zhengming Ding"}, {"title": "Reverse Perspective Network for Perspective-Aware Object Counting", "authors": "Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, Nicu Sebe"}, {"title": "Multi-Path Region Mining for Weakly Supervised 3D Semantic Segmentation on Point Clouds", "authors": "Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Tzu-Yi Hung, Lihua Xie", "link": "http://arxiv.org/abs/2003.13035", "summary": "Point clouds provide intrinsic geometric information and surface context for\nscene understanding. Existing methods for point cloud segmentation require a\nlarge amount of fully labeled data. Using advanced depth sensors, collection of\nlarge scale 3D dataset is no longer a cumbersome process. However, manually\nproducing point-level label on the large scale dataset is time and\nlabor-intensive. In this paper, we propose a weakly supervised approach to\npredict point-level results using weak labels on 3D point clouds. We introduce\nour multi-path region mining module to generate pseudo point-level label from a\nclassification network trained with weak labels. It mines the localization cues\nfor each class from various aspects of the network feature using different\nattention modules. Then, we use the point-level pseudo labels to train a point\ncloud segmentation network in a fully supervised manner. To the best of our\nknowledge, this is the first method that uses cloud-level weak labels on raw 3D\nspace to train a point cloud semantic segmentation network. In our setting, the\n3D weak labels only indicate the classes that appeared in our input sample. We\ndiscuss both scene- and subcloud-level weakly labels on raw 3D point cloud data\nand perform in-depth experiments on them. On ScanNet dataset, our result\ntrained with subcloud-level labels is compatible with some fully supervised\nmethods."}, {"title": "Reliable Weighted Optimal Transport for Unsupervised Domain Adaptation", "authors": "Renjun Xu, Pelen Liu, Liyan Wang, Chao Chen, Jindong Wang"}, {"title": "ImVoteNet: Boosting 3D Object Detection in Point Clouds With Image Votes", "authors": "Charles R. Qi, Xinlei Chen, Or Litany, Leonidas J. Guibas", "link": "https://arxiv.org/abs/2001.10692", "summary": "3D object detection has seen quick progress thanks to advances in deep\nlearning on point clouds. A few recent works have even shown state-of-the-art\nperformance with just point clouds input (e.g. VoteNet). However, point cloud\ndata have inherent limitations. They are sparse, lack color information and\noften suffer from sensor noise. Images, on the other hand, have high resolution\nand rich texture. Thus they can complement the 3D geometry provided by point\nclouds. Yet how to effectively use image information to assist point cloud\nbased detection is still an open question. In this work, we build on top of\nVoteNet and propose a 3D detection architecture called ImVoteNet specialized\nfor RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes\nin point clouds. Compared to prior work on multi-modal detection, we explicitly\nextract both geometric and semantic features from the 2D images. We leverage\ncamera parameters to lift these features to 3D. To improve the synergy of 2D-3D\nfeature fusion, we also propose a multi-tower training scheme. We validate our\nmodel on the challenging SUN RGB-D dataset, advancing state-of-the-art results\nby 5.7 mAP. We also provide rich ablation studies to analyze the contribution\nof each design choice."}, {"title": "Understanding Road Layout From Videos as a Whole", "authors": "Buyu Liu, Bingbing Zhuang, Samuel Schulter, Pan Ji, Manmohan Chandraker"}, {"title": "Bi-Directional Relationship Inferring Network for Referring Image Segmentation", "authors": "Zhiwei Hu, Guang Feng, Jiayu Sun, Lihe Zhang, Huchuan Lu"}, {"title": "Perspective Plane Program Induction From a Single Image", "authors": "Yikai Li, Jiayuan Mao, Xiuming Zhang, William T. Freeman, Joshua B. Tenenbaum, Jiajun Wu"}, {"title": "DeepFLASH: An Efficient Network for Learning-Based Medical Image Registration", "authors": "Jian Wang, Miaomiao Zhang", "link": "https://arxiv.org/abs/2004.02097", "summary": "This paper presents DeepFLASH, a novel network with efficient training and\ninference for learning-based medical image registration. In contrast to\nexisting approaches that learn spatial transformations from training data in\nthe high dimensional imaging space, we develop a new registration network\nentirely in a low dimensional bandlimited space. This dramatically reduces the\ncomputational cost and memory footprint of an expensive training and inference.\nTo achieve this goal, we first introduce complex-valued operations and\nrepresentations of neural architectures that provide key components for\nlearning-based registration models. We then construct an explicit loss function\nof transformation fields fully characterized in a bandlimited space with much\nfewer parameterizations. Experimental results show that our method is\nsignificantly faster than the state-of-the-art deep learning based image\nregistration methods, while producing equally accurate alignment. We\ndemonstrate our algorithm in two different applications of image registration:\n2D synthetic data and 3D real brain magnetic resonance (MR) images. Our code is\navailable at https://github.com/jw4hv/deepflash."}, {"title": "Semi-Supervised Learning for Few-Shot Image-to-Image Translation", "authors": "Yaxing Wang, Salman Khan, Abel Gonzalez-Garcia, Joost van de Weijer, Fahad Shahbaz Khan", "link": "http://arxiv.org/abs/2003.13853", "summary": "In the last few years, unpaired image-to-image translation has witnessed\nremarkable progress. Although the latest methods are able to generate realistic\nimages, they crucially rely on a large number of labeled images. Recently, some\nmethods have tackled the challenging setting of few-shot image-to-image\ntranslation, reducing the labeled data requirements for the target domain\nduring inference. In this work, we go one step further and reduce the amount of\nrequired labeled data also from the source domain during training. To do so, we\npropose applying semi-supervised learning via a noise-tolerant pseudo-labeling\nprocedure. We also apply a cycle consistency constraint to further exploit the\ninformation from unlabeled images, either from the same dataset or external.\nAdditionally, we propose several structural modifications to facilitate the\nimage translation task under these circumstances. Our semi-supervised method\nfor few-shot image translation, called SEMIT, achieves excellent results on\nfour different datasets using as little as 10% of the source labels, and\nmatches the performance of the main fully-supervised competitor using only 20%\nlabeled data. Our code and models are made public at:\nhttps://github.com/yaxingwang/SEMIT."}, {"title": "Semantic Correspondence as an Optimal Transport Problem", "authors": "Yanbin Liu, Linchao Zhu, Makoto Yamada, Yi Yang", "link": "", "summary": ""}, {"title": "How Much Time Do You Have? Modeling Multi-Duration Saliency", "authors": "Camilo Fosco, Anelise Newman, Pat Sukhum, Yun Bin Zhang, Nanxuan Zhao, Aude Oliva, Zoya Bylinskii"}, {"title": "Fine-Grained Generalized Zero-Shot Learning via Dense Attribute-Based Attention", "authors": "Dat Huynh, Ehsan Elhamifar"}, {"title": "Online Depth Learning Against Forgetting in Monocular Videos", "authors": "Zhenyu Zhang, St\u00e9phane Lathuili\u00e8re, Elisa Ricci, Nicu Sebe, Yan Yan, Jian Yang", "link": "", "summary": ""}, {"title": "Few-Shot Learning of Part-Specific Probability Space for 3D Shape Segmentation", "authors": "Lingjing Wang, Xiang Li, Yi Fang"}, {"title": "Pattern-Structure Diffusion for Multi-Task Learning", "authors": "Ling Zhou, Zhen Cui, Chunyan Xu, Zhenyu Zhang, Chaoqun Wang, Tong Zhang, Jian Yang"}, {"title": "Training Noise-Robust Deep Neural Networks via Meta-Learning", "authors": "Zhen Wang, Guosheng Hu, Qinghua Hu", "link": "", "summary": ""}, {"title": "Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation", "authors": "Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, Kai Xu", "link": "https://arxiv.org/abs/2003.06233", "summary": "Online semantic 3D segmentation in company with real-time RGB-D\nreconstruction poses special challenges such as how to perform 3D convolution\ndirectly over the progressively fused 3D geometric data, and how to smartly\nfuse information from frame to frame. We propose a novel fusion-aware 3D point\nconvolution which operates directly on the geometric surface being\nreconstructed and exploits effectively the inter-frame correlation for high\nquality 3D feature learning. This is enabled by a dedicated dynamic data\nstructure which organizes the online acquired point cloud with global-local\ntrees. Globally, we compile the online reconstructed 3D points into an\nincrementally growing coordinate interval tree, enabling fast point insertion\nand neighborhood query. Locally, we maintain the neighborhood information for\neach point using an octree whose construction benefits from the fast query of\nthe global tree.Both levels of trees update dynamically and help the 3D\nconvolution effectively exploits the temporal coherence for effective\ninformation fusion across RGB-D frames."}, {"title": "Universal Source-Free Domain Adaptation", "authors": "Jogendra Nath Kundu, Naveen Venkat, Rahul M V, R. Venkatesh Babu", "link": "http://arxiv.org/abs/2004.04393", "summary": "There is a strong incentive to develop versatile learning techniques that can\ntransfer the knowledge of class-separability from a labeled source domain to an\nunlabeled target domain in the presence of a domain-shift. Existing domain\nadaptation (DA) approaches are not equipped for practical DA scenarios as a\nresult of their reliance on the knowledge of source-target label-set\nrelationship (e.g. Closed-set, Open-set or Partial DA). Furthermore, almost all\nprior unsupervised DA works require coexistence of source and target samples\neven during deployment, making them unsuitable for real-time adaptation. Devoid\nof such impractical assumptions, we propose a novel two-stage learning process.\n1) In the Procurement stage, we aim to equip the model for future source-free\ndeployment, assuming no prior knowledge of the upcoming category-gap and\ndomain-shift. To achieve this, we enhance the model's ability to reject\nout-of-source distribution samples by leveraging the available source data, in\na novel generative classifier framework. 2) In the Deployment stage, the goal\nis to design a unified adaptation algorithm capable of operating across a wide\nrange of category-gaps, with no access to the previously seen source samples.\nTo this end, in contrast to the usage of complex adversarial training regimes,\nwe define a simple yet effective source-free adaptation objective by utilizing\na novel instance-level weighting mechanism, named as Source Similarity Metric\n(SSM). A thorough evaluation shows the practical usability of the proposed\nlearning framework with superior DA performance even over state-of-the-art\nsource-dependent approaches."}, {"title": "Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and Temporal-Consistency Video Prediction", "authors": "Beibei Jin, Yu Hu, Qiankun Tang, Jingyu Niu, Zhiping Shi, Yinhe Han, Xiaowei Li", "link": "https://arxiv.org/abs/2002.09905", "summary": "Video prediction is a pixel-wise dense prediction task to infer future frames\nbased on past frames. Missing appearance details and motion blur are still two\nmajor problems for current predictive models, which lead to image distortion\nand temporal inconsistency. In this paper, we point out the necessity of\nexploring multi-frequency analysis to deal with the two problems. Inspired by\nthe frequency band decomposition characteristic of Human Vision System (HVS),\nwe propose a video prediction network based on multi-level wavelet analysis to\ndeal with spatial and temporal information in a unified manner. Specifically,\nthe multi-level spatial discrete wavelet transform decomposes each video frame\ninto anisotropic sub-bands with multiple frequencies, helping to enrich\nstructural information and reserve fine details. On the other hand, multi-level\ntemporal discrete wavelet transform which operates on time axis decomposes the\nframe sequence into sub-band groups of different frequencies to accurately\ncapture multi-frequency motions under a fixed frame rate. Extensive experiments\non diverse datasets demonstrate that our model shows significant improvements\non fidelity and temporal consistency over state-of-the-art works."}, {"title": "Varicolored Image De-Hazing", "authors": "Akshay Dudhane, Kuldeep M. Biradar, Prashant W. Patil, Praful Hambarde, Subrahmanyam Murala"}, {"title": "SpSequenceNet: Semantic Segmentation Network on 4D Point Clouds", "authors": "Hanyu Shi, Guosheng Lin, Hao Wang, Tzu-Yi Hung, Zhenhua Wang"}, {"title": "Separating Particulate Matter From a Single Microscopic Image", "authors": "Tushar Sandhan, Jin Young Choi"}, {"title": "Adaptive Dilated Network With Self-Correction Supervision for Counting", "authors": "Shuai Bai, Zhiqun He, Yu Qiao, Hanzhe Hu, Wei Wu, Junjie Yan"}, {"title": "PointPainting: Sequential Fusion for 3D Object Detection", "authors": "Sourabh Vora, Alex H. Lang, Bassam Helou, Oscar Beijbom", "link": "https://arxiv.org/abs/1911.10150", "summary": "Camera and lidar are important sensor modalities for robotics in general and\nself-driving cars in particular. The sensors provide complementary information\noffering an opportunity for tight sensor-fusion. Surprisingly, lidar-only\nmethods outperform fusion methods on the main benchmark datasets, suggesting a\ngap in the literature. In this work, we propose PointPainting: a sequential\nfusion method to fill this gap. PointPainting works by projecting lidar points\ninto the output of an image-only semantic segmentation network and appending\nthe class scores to each point. The appended (painted) point cloud can then be\nfed to any lidar-only method. Experiments show large improvements on three\ndifferent state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on\nthe KITTI and nuScenes datasets. The painted version of PointRCNN represents a\nnew state of the art on the KITTI leaderboard for the bird's-eye view detection\ntask. In ablation, we study how the effects of Painting depends on the quality\nand format of the semantic segmentation output, and demonstrate how latency can\nbe minimized through pipelining."}, {"title": "Rethinking Zero-Shot Video Classification: End-to-End Training for Realistic Applications", "authors": "Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, Krzysztof Chalupka", "link": "https://arxiv.org/abs/2003.01455", "summary": "Trained on large datasets, deep learning (DL) can accurately classify videos\ninto hundreds of diverse classes. However, video data is expensive to annotate.\nZero-shot learning (ZSL) proposes one solution to this problem. ZSL trains a\nmodel once, and generalizes to new tasks whose classes are not present in the\ntraining dataset. We propose the first end-to-end algorithm for ZSL in video\nclassification. Our training procedure builds on insights from recent video\nclassification literature and uses a trainable 3D CNN to learn the visual\nfeatures. This is in contrast to previous video ZSL methods, which use\npretrained feature extractors. We also extend the current benchmarking\nparadigm: Previous techniques aim to make the test task unknown at training\ntime but fall short of this goal. We encourage domain shift across training and\ntest data and disallow tailoring a ZSL model to a specific test dataset. We\noutperform the state-of-the-art by a wide margin. Our code, evaluation\nprocedure and model weights are available at\ngithub.com/bbrattoli/ZeroShotVideoClassification."}, {"title": "Learning to Select Base Classes for Few-Shot Classification", "authors": "Linjun Zhou, Peng Cui, Xu Jia, Shiqiang Yang, Qi Tian", "link": "https://arxiv.org/abs/2004.00315", "summary": "Few-shot learning has attracted intensive research attention in recent years.\nMany methods have been proposed to generalize a model learned from provided\nbase classes to novel classes, but no previous work studies how to select base\nclasses, or even whether different base classes will result in different\ngeneralization performance of the learned model. In this paper, we utilize a\nsimple yet effective measure, the Similarity Ratio, as an indicator for the\ngeneralization performance of a few-shot model. We then formulate the base\nclass selection problem as a submodular optimization problem over Similarity\nRatio. We further provide theoretical analysis on the optimization lower bound\nof different optimization methods, which could be used to identify the most\nappropriate algorithm for different experimental settings. The extensive\nexperiments on ImageNet, Caltech256 and CUB-200-2011 demonstrate that our\nproposed method is effective in selecting a better base dataset."}, {"title": "CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus", "authors": "Florian Kluger, Eric Brachmann, Hanno Ackermann, Carsten Rother, Michael Ying Yang, Bodo Rosenhahn", "link": "https://arxiv.org/abs/2001.02643", "summary": "We present a robust estimator for fitting multiple parametric models of the\nsame form to noisy measurements. Applications include finding multiple\nvanishing points in man-made scenes, fitting planes to architectural imagery,\nor estimating multiple rigid motions within the same sequence. In contrast to\nprevious works, which resorted to hand-crafted search strategies for multiple\nmodel detection, we learn the search strategy from data. A neural network\nconditioned on previously detected models guides a RANSAC estimator to\ndifferent subsets of all measurements, thereby finding model instances one\nafter another. We train our method supervised as well as self-supervised. For\nsupervised training of the search strategy, we contribute a new dataset for\nvanishing point estimation. Leveraging this dataset, the proposed algorithm is\nsuperior with respect to other robust estimators as well as to designated\nvanishing point estimation algorithms. For self-supervised learning of the\nsearch, we evaluate the proposed algorithm on multi-homography estimation and\ndemonstrate an accuracy that is superior to state-of-the-art methods."}, {"title": "Fast Symmetric Diffeomorphic Image Registration with Convolutional Neural Networks", "authors": "Tony C.W. Mok, Albert C.S. Chung", "link": "http://arxiv.org/abs/2003.09514", "summary": "Diffeomorphic deformable image registration is crucial in many medical image\nstudies, as it offers unique, special properties including topology\npreservation and invertibility of the transformation. Recent deep\nlearning-based deformable image registration methods achieve fast image\nregistration by leveraging a convolutional neural network (CNN) to learn the\nspatial transformation from the synthetic ground truth or the similarity\nmetric. However, these approaches often ignore the topology preservation of the\ntransformation and the smoothness of the transformation which is enforced by a\nglobal smoothing energy function alone. Moreover, deep learning-based\napproaches often estimate the displacement field directly, which cannot\nguarantee the existence of the inverse transformation. In this paper, we\npresent a novel, efficient unsupervised symmetric image registration method\nwhich maximizes the similarity between images within the space of diffeomorphic\nmaps and estimates both forward and inverse transformations simultaneously. We\nevaluate our method on 3D image registration with a large scale brain image\ndataset. Our method achieves state-of-the-art registration accuracy and running\ntime while maintaining desirable diffeomorphic properties."}, {"title": "Distilled Semantics for Comprehensive Scene Understanding from Videos", "authors": "Fabio Tosi, Filippo Aleotti, Pierluigi Zama Ramirez, Matteo Poggi, Samuele Salti, Luigi Di Stefano, Stefano Mattoccia", "link": "https://arxiv.org/abs/2003.14030", "summary": "Whole understanding of the surroundings is paramount to autonomous systems.\nRecent works have shown that deep neural networks can learn geometry (depth)\nand motion (optical flow) from a monocular video without any explicit\nsupervision from ground truth annotations, particularly hard to source for\nthese two tasks. In this paper, we take an additional step toward holistic\nscene understanding with monocular cameras by learning depth and motion\nalongside with semantics, with supervision for the latter provided by a\npre-trained network distilling proxy ground truth images. We address the three\ntasks jointly by a) a novel training protocol based on knowledge distillation\nand self-supervision and b) a compact network architecture which enables\nefficient scene understanding on both power hungry GPUs and low-power embedded\nplatforms. We thoroughly assess the performance of our framework and show that\nit yields state-of-the-art results for monocular depth estimation, optical flow\nand motion segmentation."}, {"title": "Modeling Biological Immunity to Adversarial Examples", "authors": "Edward Kim, Jocelyn Rego, Yijing Watkins, Garrett T. Kenyon"}, {"title": "DOA-GAN: Dual-Order Attentive Generative Adversarial Network for Image Copy-Move Forgery Detection and Localization", "authors": "Ashraful Islam, Chengjiang Long, Arslan Basharat, Anthony Hoogs"}, {"title": "Correspondence-Free Material Reconstruction using Sparse Surface Constraints", "authors": "Sebastian Weiss, Robert Maier, Daniel Cremers, R\u00fcdiger Westermann, Nils Thuerey"}, {"title": "Augmenting Colonoscopy Using Extended and Directional CycleGAN for Lossy Image Translation", "authors": "Shawn Mathew, Saad Nadeem, Sruti Kumari, Arie Kaufman", "link": "https://arxiv.org/abs/2003.12473", "summary": "Colorectal cancer screening modalities, such as optical colonoscopy (OC) and\nvirtual colonoscopy (VC), are critical for diagnosing and ultimately removing\npolyps (precursors of colon cancer). The non-invasive VC is normally used to\ninspect a 3D reconstructed colon (from CT scans) for polyps and if found, the\nOC procedure is performed to physically traverse the colon via endoscope and\nremove these polyps. In this paper, we present a deep learning framework,\nExtended and Directional CycleGAN, for lossy unpaired image-to-image\ntranslation between OC and VC to augment OC video sequences with\nscale-consistent depth information from VC, and augment VC with\npatient-specific textures, color and specular highlights from OC (e.g, for\nrealistic polyp synthesis). Both OC and VC contain structural information, but\nit is obscured in OC by additional patient-specific texture and specular\nhighlights, hence making the translation from OC to VC lossy. The existing\nCycleGAN approaches do not handle lossy transformations. To address this\nshortcoming, we introduce an extended cycle consistency loss, which compares\nthe geometric structures from OC in the VC domain. This loss removes the need\nfor the CycleGAN to embed OC information in the VC domain. To handle a stronger\nremoval of the textures and lighting, a Directional Discriminator is introduced\nto differentiate the direction of translation (by creating paired information\nfor the discriminator), as opposed to the standard CycleGAN which is\ndirection-agnostic. Combining the extended cycle consistency loss and the\nDirectional Discriminator, we show state-of-the-art results on scale-consistent\ndepth inference for phantom, textured VC and for real polyp and normal colon\nvideo sequences. We also present results for realistic pendunculated and flat\npolyp synthesis from bumps introduced in 3D VC models."}, {"title": "Attention Scaling for Crowd Counting", "authors": "Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, Yanwei Pang", "link": "", "summary": ""}, {"title": "Shape Reconstruction by Learning Differentiable Surface Representations", "authors": "Jan Bedna\u0159\u00edk, Shaifali Parashar, Erhan G\u00fcndo\u011fdu, Mathieu Salzmann, Pascal Fua", "link": "https://arxiv.org/abs/1911.11227", "summary": "Generative models that produce point clouds have emerged as a powerful tool\nto represent 3D surfaces, and the best current ones rely on learning an\nensemble of parametric representations. Unfortunately, they offer no control\nover the deformations of the surface patches that form the ensemble and thus\nfail to prevent them from either overlapping or collapsing into single points\nor lines. As a consequence, computing shape properties such as surface normals\nand curvatures becomes difficult and unreliable.\n  In this paper, we show that we can exploit the inherent differentiability of\ndeep networks to leverage differential surface properties during training so as\nto prevent patch collapse and strongly reduce patch overlap. Furthermore, this\nlets us reliably compute quantities such as surface normals and curvatures. We\nwill demonstrate on several tasks that this yields more accurate surface\nreconstructions than the state-of-the-art methods in terms of normals\nestimation and amount of collapsed and overlapped patches."}, {"title": "A Spatiotemporal Volumetric Interpolation Network for 4D Dynamic Medical Image", "authors": "Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang, Jinman Kim", "link": "https://arxiv.org/abs/2002.12680", "summary": "Dynamic medical imaging is usually limited in application due to the large\nradiation doses and longer image scanning and reconstruction times. Existing\nmethods attempt to reduce the dynamic sequence by interpolating the volumes\nbetween the acquired image volumes. However, these methods are limited to\neither 2D images and/or are unable to support large variations in the motion\nbetween the image volume sequences. In this paper, we present a spatiotemporal\nvolumetric interpolation network (SVIN) designed for 4D dynamic medical images.\nSVIN introduces dual networks: first is the spatiotemporal motion network that\nleverages the 3D convolutional neural network (CNN) for unsupervised parametric\nvolumetric registration to derive spatiotemporal motion field from two-image\nvolumes; the second is the sequential volumetric interpolation network, which\nuses the derived motion field to interpolate image volumes, together with a new\nregression-based module to characterize the periodic motion cycles in\nfunctional organ structures. We also introduce an adaptive multi-scale\narchitecture to capture the volumetric large anatomy motions. Experimental\nresults demonstrated that our SVIN outperformed state-of-the-art temporal\nmedical interpolation methods and natural video interpolation methods that have\nbeen extended to support volumetric images. Our ablation study further\nexemplified that our motion network was able to better represent the large\nfunctional motion compared with the state-of-the-art unsupervised medical\nregistration methods."}, {"title": "Attention-Based Context Aware Reasoning for Situation Recognition", "authors": "Thilini Cooray, Ngai-Man Cheung, Wei Lu"}, {"title": "PatchVAE: Learning Local Latent Codes for Recognition", "authors": "Kamal Gupta, Saurabh Singh, Abhinav Shrivastava", "link": "https://arxiv.org/abs/2004.03623", "summary": "Unsupervised representation learning holds the promise of exploiting large\namounts of unlabeled data to learn general representations. A promising\ntechnique for unsupervised learning is the framework of Variational\nAuto-encoders (VAEs). However, unsupervised representations learned by VAEs are\nsignificantly outperformed by those learned by supervised learning for\nrecognition. Our hypothesis is that to learn useful representations for\nrecognition the model needs to be encouraged to learn about repeating and\nconsistent patterns in data. Drawing inspiration from the mid-level\nrepresentation discovery work, we propose PatchVAE, that reasons about images\nat patch level. Our key contribution is a bottleneck formulation that\nencourages mid-level style representations in the VAE framework. Our\nexperiments demonstrate that representations learned by our method perform much\nbetter on the recognition tasks compared to those learned by vanilla VAEs."}, {"title": "Self-Supervised Monocular Trained Depth Estimation Using Self-Attention and Discrete Disparity Volume", "authors": "Adrian Johnston, Gustavo Carneiro", "link": "https://arxiv.org/abs/2003.13951", "summary": "Monocular depth estimation has become one of the most studied applications in\ncomputer vision, where the most accurate approaches are based on fully\nsupervised learning models. However, the acquisition of accurate and large\nground truth data sets to model these fully supervised methods is a major\nchallenge for the further development of the area. Self-supervised methods\ntrained with monocular videos constitute one the most promising approaches to\nmitigate the challenge mentioned above due to the wide-spread availability of\ntraining data. Consequently, they have been intensively studied, where the main\nideas explored consist of different types of model architectures, loss\nfunctions, and occlusion masks to address non-rigid motion. In this paper, we\npropose two new ideas to improve self-supervised monocular trained depth\nestimation: 1) self-attention, and 2) discrete disparity prediction. Compared\nwith the usual localised convolution operation, self-attention can explore a\nmore general contextual information that allows the inference of similar\ndisparity values at non-contiguous regions of the image. Discrete disparity\nprediction has been shown by fully supervised methods to provide a more robust\nand sharper depth estimation than the more common continuous disparity\nprediction, besides enabling the estimation of depth uncertainty. We show that\nthe extension of the state-of-the-art self-supervised monocular trained depth\nestimator Monodepth2 with these two ideas allows us to design a model that\nproduces the best results in the field in KITTI 2015 and Make3D, closing the\ngap with respect self-supervised stereo training and fully supervised\napproaches."}, {"title": "STAViS: Spatio-Temporal AudioVisual Saliency Network", "authors": "Antigoni Tsiami, Petros Koutras, Petros Maragos", "link": "", "summary": ""}, {"title": "More Grounded Image Captioning by Distilling Image-Text Matching Model", "authors": "Yuanen Zhou, Meng Wang, Daqing Liu, Zhenzhen Hu, Hanwang Zhang", "link": "https://arxiv.org/abs/2004.00390", "summary": "Visual attention not only improves the performance of image captioners, but\nalso serves as a visual interpretation to qualitatively measure the caption\nrationality and model transparency. Specifically, we expect that a captioner\ncan fix its attentive gaze on the correct objects while generating the\ncorresponding words. This ability is also known as grounded image captioning.\nHowever, the grounding accuracy of existing captioners is far from\nsatisfactory. To improve the grounding accuracy while retaining the captioning\nquality, it is expensive to collect the word-region alignment as strong\nsupervision. To this end, we propose a Part-of-Speech (POS) enhanced image-text\nmatching model (SCAN \\cite{lee2018stacked}): POS-SCAN, as the effective\nknowledge distillation for more grounded image captioning. The benefits are\ntwo-fold: 1) given a sentence and an image, POS-SCAN can ground the objects\nmore accurately than SCAN; 2) POS-SCAN serves as a word-region alignment\nregularization for the captioner's visual attention module. By showing\nbenchmark experimental results, we demonstrate that conventional image\ncaptioners equipped with POS-SCAN can significantly improve the grounding\naccuracy without strong supervision. Last but not the least, we explore the\nindispensable Self-Critical Sequence Training (SCST) \\cite{Rennie_2017_CVPR} in\nthe context of grounded image captioning and show that the image-text matching\nscore can serve as a reward for more grounded captioning\n\\footnote{https://github.com/YuanEZhou/Grounded-Image-Captioning}."}, {"title": "DUNIT: Detection-Based Unsupervised Image-to-Image Translation", "authors": "Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, Mathieu Salzmann"}, {"title": "Learning to Observe: Approximating Human Perceptual Thresholds for Detection of Suprathreshold Image Transformations", "authors": "Alan Dolhasz, Carlo Harvey, Ian Williams", "link": "https://arxiv.org/abs/1912.06433", "summary": "Many tasks in computer vision are often calibrated and evaluated relative to\nhuman perception. In this paper, we propose to directly approximate the\nperceptual function performed by human observers completing a visual detection\ntask. Specifically, we present a novel methodology for learning to detect image\ntransformations visible to human observers through approximating perceptual\nthresholds. To do this, we carry out a subjective two-alternative forced-choice\nstudy to estimate perceptual thresholds of human observers detecting local\nexposure shifts in images. We then leverage transformation equivariant\nrepresentation learning to overcome issues of limited perceptual data. This\nrepresentation is then used to train a dense convolutional classifier capable\nof detecting local suprathreshold exposure shifts - a distortion common to\nimage composites. In this context, our model can approximate perceptual\nthresholds with an average error of 0.1148 exposure stops between empirical and\npredicted thresholds. It can also be trained to detect a range of different\nlocal transformations."}, {"title": "Show, Edit and Tell: A Framework for Editing Image Captions", "authors": "Fawaz Sammani, Luke Melas-Kyriazi", "link": "https://arxiv.org/abs/2003.03107", "summary": "Most image captioning frameworks generate captions directly from images,\nlearning a mapping from visual features to natural language. However, editing\nexisting captions can be easier than generating new ones from scratch.\nIntuitively, when editing captions, a model is not required to learn\ninformation that is already present in the caption (i.e. sentence structure),\nenabling it to focus on fixing details (e.g. replacing repetitive words). This\npaper proposes a novel approach to image captioning based on iterative adaptive\nrefinement of an existing caption. Specifically, our caption-editing model\nconsisting of two sub-modules: (1) EditNet, a language module with an adaptive\ncopy mechanism (Copy-LSTM) and a Selective Copy Memory Attention mechanism\n(SCMA), and (2) DCNet, an LSTM-based denoising auto-encoder. These components\nenable our model to directly copy from and modify existing captions.\nExperiments demonstrate that our new approach achieves state-of-art performance\non the MS COCO dataset both with and without sequence-level training."}, {"title": "Structure Boundary Preserving Segmentation for Medical Image With Ambiguous Boundary", "authors": "Hong Joo Lee, Jung Uk Kim, Sangmin Lee, Hak Gu Kim, Yong Man Ro"}, {"title": "Predicting Cognitive Declines Using Longitudinally Enriched Representations for Imaging Biomarkers", "authors": "Lyujian Lu, Hua Wang, Saad Elbeleidy, Feiping Nie"}, {"title": "Predicting Lymph Node Metastasis Using Histopathological Images Based on Multiple Instance Learning With Deep Graph Convolution", "authors": "Yu Zhao, Fan Yang, Yuqi Fang, Hailing Liu, Niyun Zhou, Jun Zhang, Jiarui Sun, Sen Yang, Bjoern Menze, Xinjuan Fan, Jianhua Yao"}, {"title": "Extremely Dense Point Correspondences Using a Learned Feature Descriptor", "authors": "Xingtong Liu, Yiping Zheng, Benjamin Killeen, Masaru Ishii, Gregory D. Hager, Russell H. Taylor, Mathias Unberath", "link": "http://arxiv.org/abs/2003.00619", "summary": "High-quality 3D reconstructions from endoscopy video play an important role\nin many clinical applications, including surgical navigation where they enable\ndirect video-CT registration. While many methods exist for general multi-view\n3D reconstruction, these methods often fail to deliver satisfactory performance\non endoscopic video. Part of the reason is that local descriptors that\nestablish pair-wise point correspondences, and thus drive reconstruction,\nstruggle when confronted with the texture-scarce surface of anatomy.\nLearning-based dense descriptors usually have larger receptive fields enabling\nthe encoding of global information, which can be used to disambiguate matches.\nIn this work, we present an effective self-supervised training scheme and novel\nloss design for dense descriptor learning. In direct comparison to recent local\nand dense descriptors on an in-house sinus endoscopy dataset, we demonstrate\nthat our proposed dense descriptor can generalize to unseen patients and\nscopes, thereby largely improving the performance of Structure from Motion\n(SfM) in terms of model density and completeness. We also evaluate our method\non a public dense optical flow dataset and a small-scale SfM public dataset to\nfurther demonstrate the effectiveness and generality of our method. The source\ncode is available at\nhttps://github.com/lppllppl920/DenseDescriptorLearning-Pytorch."}, {"title": "Local Deep Implicit Functions for 3D Shape", "authors": "Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas Funkhouser", "link": "", "summary": ""}, {"title": "PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation", "authors": "Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia", "link": "http://arxiv.org/abs/2004.01658", "summary": "Instance segmentation is an important task for scene understanding. Compared\nto the fully-developed 2D, 3D instance segmentation for point clouds have much\nroom to improve. In this paper, we present PointGroup, a new end-to-end\nbottom-up architecture, specifically focused on better grouping the points by\nexploring the void space between objects. We design a two-branch network to\nextract point features and predict semantic labels and offsets, for shifting\neach point towards its respective instance centroid. A clustering component is\nfollowed to utilize both the original and offset-shifted point coordinate sets,\ntaking advantage of their complementary strength. Further, we formulate the\nScoreNet to evaluate the candidate instances, followed by the Non-Maximum\nSuppression (NMS) to remove duplicates. We conduct extensive experiments on two\nchallenging datasets, ScanNet v2 and S3DIS, on which our method achieves the\nhighest performance, 63.6% and 64.0%, compared to 54.9% and 54.4% achieved by\nformer best solutions in terms of mAP with IoU threshold 0.5."}, {"title": "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo", "authors": "Jiayu Yang, Wei Mao, Jose M. Alvarez, Miaomiao Liu", "link": "https://arxiv.org/abs/1912.08329", "summary": "We propose a cost volume-based neural network for depth inference from\nmulti-view images. We demonstrate that building a cost volume pyramid in a\ncoarse-to-fine manner instead of constructing a cost volume at a fixed\nresolution leads to a compact, lightweight network and allows us inferring high\nresolution depth maps to achieve better reconstruction results. To this end, we\nfirst build a cost volume based on uniform sampling of fronto-parallel planes\nacross the entire depth range at the coarsest resolution of an image. Then,\ngiven current depth estimate, we construct new cost volumes iteratively on the\npixelwise depth residual to perform depth map refinement. While sharing similar\ninsight with Point-MVSNet as predicting and refining depth iteratively, we show\nthat working on cost volume pyramid can lead to a more compact, yet efficient\nnetwork structure compared with the Point-MVSNet on 3D points. We further\nprovide detailed analyses of the relation between (residual) depth sampling and\nimage resolution, which serves as a principle for building compact cost volume\npyramid. Experimental results on benchmark datasets show that our model can\nperform 6x faster and has similar performance as state-of-the-art methods. Code\nis available at https://github.com/JiayuYANG/CVP-MVSNet"}, {"title": "RoutedFusion: Learning Real-Time Depth Map Fusion", "authors": "Silvan Weder, Johannes Sch\u00f6nberger, Marc Pollefeys, Martin R. Oswald", "link": "https://arxiv.org/abs/2001.04388", "summary": "The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes."}, {"title": "VOLDOR: Visual Odometry From Log-Logistic Dense Optical Flow Residuals", "authors": "Zhixiang Min, Yiding Yang, Enrique Dunn", "link": "", "summary": ""}, {"title": "Learning to Optimize Non-Rigid Tracking", "authors": "Yang Li, Alja\u017e Bo\u017ei\u010d, Tianwei Zhang, Yanli Ji, Tatsuya Harada, Matthias Nie\u00dfner", "link": "https://arxiv.org/abs/2003.12230", "summary": "One of the widespread solutions for non-rigid tracking has a nested-loop\nstructure: with Gauss-Newton to minimize a tracking objective in the outer\nloop, and Preconditioned Conjugate Gradient (PCG) to solve a sparse linear\nsystem in the inner loop. In this paper, we employ learnable optimizations to\nimprove tracking robustness and speed up solver convergence. First, we upgrade\nthe tracking objective by integrating an alignment data term on deep features\nwhich are learned end-to-end through CNN. The new tracking objective can\ncapture the global deformation which helps Gauss-Newton to jump over local\nminimum, leading to robust tracking on large non-rigid motions. Second, we\nbridge the gap between the preconditioning technique and learning method by\nintroducing a ConditionNet which is trained to generate a preconditioner such\nthat PCG can converge within a small number of steps. Experimental results\nindicate that the proposed learning method converges faster than the original\nPCG by a large margin."}, {"title": "KFNet: Learning Temporal Camera Relocalization Using Kalman Filtering", "authors": "Lei Zhou, Zixin Luo, Tianwei Shen, Jiahui Zhang, Mingmin Zhen, Yao Yao, Tian Fang, Long Quan", "link": "https://arxiv.org/abs/2003.10629", "summary": "Temporal camera relocalization estimates the pose with respect to each video\nframe in sequence, as opposed to one-shot relocalization which focuses on a\nstill image. Even though the time dependency has been taken into account,\ncurrent temporal relocalization methods still generally underperform the\nstate-of-the-art one-shot approaches in terms of accuracy. In this work, we\nimprove the temporal relocalization method by using a network architecture that\nincorporates Kalman filtering (KFNet) for online camera relocalization. In\nparticular, KFNet extends the scene coordinate regression problem to the time\ndomain in order to recursively establish 2D and 3D correspondences for the pose\ndetermination. The network architecture design and the loss formulation are\nbased on Kalman filtering in the context of Bayesian learning. Extensive\nexperiments on multiple relocalization benchmarks demonstrate the high accuracy\nof KFNet at the top of both one-shot and temporal relocalization approaches.\nOur codes are released at https://github.com/zlthinker/KFNet."}, {"title": "Information-Driven Direct RGB-D Odometry", "authors": "Alejandro Font\u00e1n, Javier Civera, Rudolph Triebel"}, {"title": "SuperGlue: Learning Feature Matching With Graph Neural Networks", "authors": "Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Andrew Rabinovich", "link": "https://arxiv.org/abs/1911.11763", "summary": "This paper introduces SuperGlue, a neural network that matches two sets of\nlocal features by jointly finding correspondences and rejecting non-matchable\npoints. Assignments are estimated by solving a differentiable optimal transport\nproblem, whose costs are predicted by a graph neural network. We introduce a\nflexible context aggregation mechanism based on attention, enabling SuperGlue\nto reason about the underlying 3D scene and feature assignments jointly.\nCompared to traditional, hand-designed heuristics, our technique learns priors\nover geometric transformations and regularities of the 3D world through\nend-to-end training from image pairs. SuperGlue outperforms other learned\napproaches and achieves state-of-the-art results on the task of pose estimation\nin challenging real-world indoor and outdoor environments. The proposed method\nperforms matching in real-time on a modern GPU and can be readily integrated\ninto modern SfM or SLAM systems. The code and trained weights are publicly\navailable at https://github.com/magicleap/SuperGluePretrainedNetwork."}, {"title": "Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task", "authors": "Aritra Bhowmik, Stefan Gumhold, Carsten Rother, Eric Brachmann", "link": "https://arxiv.org/abs/1912.00623", "summary": "We address a core problem of computer vision: Detection and description of 2D\nfeature points for image matching. For a long time, hand-crafted designs, like\nthe seminal SIFT algorithm, were unsurpassed in accuracy and efficiency.\nRecently, learned feature detectors emerged that implement detection and\ndescription using neural networks. Training these networks usually resorts to\noptimizing low-level matching scores, often pre-defining sets of image patches\nwhich should or should not match, or which should or should not contain key\npoints. Unfortunately, increased accuracy for these low-level matching scores\ndoes not necessarily translate to better performance in high-level vision\ntasks. We propose a new training methodology which embeds the feature detector\nin a complete vision pipeline, and where the learnable parameters are trained\nin an end-to-end fashion. We overcome the discrete nature of key point\nselection and descriptor matching using principles from reinforcement learning.\nAs an example, we address the task of relative pose estimation between a pair\nof images. We demonstrate that the accuracy of a state-of-the-art\nlearning-based feature detector can be increased when trained for the task it\nis supposed to solve at test time. Our training methodology poses little\nrestrictions on the task to learn, and works for any architecture which\npredicts key point heat maps, and descriptors for key point locations."}, {"title": "ReDA:Reinforced Differentiable Attribute for 3D Face Reconstruction", "authors": "Wenbin Zhu, HsiangTao Wu, Zeyu Chen, Noranart Vesdapunt, Baoyuan Wang"}, {"title": "EventCap: Monocular 3D Capture of High-Speed Human Motions Using an Event Camera", "authors": "Lan Xu, Weipeng Xu, Vladislav Golyanik, Marc Habermann, Lu Fang, Christian Theobalt", "link": "https://arxiv.org/abs/1908.11505", "summary": "The high frame rate is a critical requirement for capturing fast human\nmotions. In this setting, existing markerless image-based methods are\nconstrained by the lighting requirement, the high data bandwidth and the\nconsequent high computation overhead. In this paper, we propose EventCap ---\nthe first approach for 3D capturing of high-speed human motions using a single\nevent camera. Our method combines model-based optimization and CNN-based human\npose detection to capture high-frequency motion details and to reduce the\ndrifting in the tracking. As a result, we can capture fast motions at\nmillisecond resolution with significantly higher data efficiency than using\nhigh frame rate videos. Experiments on our new event-based fast human motion\ndataset demonstrate the effectiveness and accuracy of our method, as well as\nits robustness to challenging lighting conditions."}, {"title": "Cross-Modal Deep Face Normals With Deactivable Skip Connections", "authors": "Victoria Fern\u00e1ndez Abrevaya, Adnane Boukhayma, Philip H.S. Torr, Edmond Boyer", "link": "http://arxiv.org/abs/2003.09691", "summary": "We present an approach for estimating surface normals from in-the-wild color\nimages of faces. While data-driven strategies have been proposed for single\nface images, limited available ground truth data makes this problem difficult.\nTo alleviate this issue, we propose a method that can leverage all available\nimage and normal data, whether paired or not, thanks to a novel cross-modal\nlearning architecture. In particular, we enable additional training with single\nmodality data, either color or normal, by using two encoder-decoder networks\nwith a shared latent space. The proposed architecture also enables face details\nto be transferred between the image and normal domains, given paired data,\nthrough skip connections between the image encoder and normal decoder. Core to\nour approach is a novel module that we call deactivable skip connections, which\nallows integrating both the auto-encoded and image-to-normal branches within\nthe same architecture that can be trained end-to-end. This allows learning of a\nrich latent space that can accurately capture the normal information. We\ncompare against state-of-the-art methods and show that our approach can achieve\nsignificant improvements, both quantitative and qualitative, with natural face\nimages."}, {"title": "Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild", "authors": "Dominik Kulon, Riza Alp G\u00fcler, Iasonas Kokkinos, Michael M. Bronstein, Stefanos Zafeiriou", "link": "https://arxiv.org/abs/2004.01946", "summary": "We introduce a simple and effective network architecture for monocular 3D\nhand pose estimation consisting of an image encoder followed by a mesh\nconvolutional decoder that is trained through a direct 3D hand mesh\nreconstruction loss. We train our network by gathering a large-scale dataset of\nhand action in YouTube videos and use it as a source of weak supervision. Our\nweakly-supervised mesh convolutions-based system largely outperforms\nstate-of-the-art methods, even halving the errors on the in the wild benchmark.\nThe dataset and additional resources are available at\nhttps://arielai.com/mesh_hands."}, {"title": "Face X-Ray for More General Face Forgery Detection", "authors": "Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, Baining Guo", "link": "https://arxiv.org/abs/1912.13458", "summary": "In this paper we propose a novel image representation called face X-ray for\ndetecting forgery in face images. The face X-ray of an input face image is a\ngreyscale image that reveals whether the input image can be decomposed into the\nblending of two images from different sources. It does so by showing the\nblending boundary for a forged image and the absence of blending for a real\nimage. We observe that most existing face manipulation methods share a common\nstep: blending the altered face into an existing background image. For this\nreason, face X-ray provides an effective way for detecting forgery generated by\nmost existing face manipulation algorithms. Face X-ray is general in the sense\nthat it only assumes the existence of a blending step and does not rely on any\nknowledge of the artifacts associated with a specific face manipulation\ntechnique. Indeed, the algorithm for computing face X-ray can be trained\nwithout fake images generated by any of the state-of-the-art face manipulation\nmethods. Extensive experiments show that face X-ray remains effective when\napplied to forgery generated by unseen face manipulation techniques, while most\nexisting face forgery detection or deepfake detection algorithms experience a\nsignificant performance drop."}, {"title": "A Morphable Face Albedo Model", "authors": "William A. P. Smith, Alassane Seck, Hannah Dee, Bernard Tiddeman, Joshua B. Tenenbaum, Bernhard Egger", "link": "https://arxiv.org/abs/2004.02711", "summary": "In this paper, we bring together two divergent strands of research:\nphotometric face capture and statistical 3D face appearance modelling. We\npropose a novel lightstage capture and processing pipeline for acquiring\near-to-ear, truly intrinsic diffuse and specular albedo maps that fully factor\nout the effects of illumination, camera and geometry. Using this pipeline, we\ncapture a dataset of 50 scans and combine them with the only existing publicly\navailable albedo dataset (3DRFE) of 23 scans. This allows us to build the first\nmorphable face albedo model. We believe this is the first statistical analysis\nof the variability of facial specular albedo maps. This model can be used as a\nplug in replacement for the texture model of the Basel Face Model and we make\nour new albedo model publicly available. We ensure careful spectral calibration\nsuch that our model is built in a linear sRGB space, suitable for inverse\nrendering of images taken by typical cameras. We demonstrate our model in a\nstate of the art analysis-by-synthesis 3DMM fitting pipeline, are the first to\nintegrate specular map estimation and outperform the Basel Face Model in albedo\nreconstruction."}, {"title": "Cascade EF-GAN: Progressive Facial Expression Editing With Local Focuses", "authors": "Rongliang Wu, Gongjie Zhang, Shijian Lu, Tao Chen", "link": "https://arxiv.org/abs/2003.05905", "summary": "Recent advances in Generative Adversarial Nets (GANs) have shown remarkable\nimprovements for facial expression editing. However, current methods are still\nprone to generate artifacts and blurs around expression-intensive regions, and\noften introduce undesired overlapping artifacts while handling large-gap\nexpression transformations such as transformation from furious to laughing. To\naddress these limitations, we propose Cascade Expression Focal GAN (Cascade\nEF-GAN), a novel network that performs progressive facial expression editing\nwith local expression focuses. The introduction of the local focus enables the\nCascade EF-GAN to better preserve identity-related features and details around\neyes, noses and mouths, which further helps reduce artifacts and blurs within\nthe generated facial images. In addition, an innovative cascade transformation\nstrategy is designed by dividing a large facial expression transformation into\nmultiple small ones in cascade, which helps suppress overlapping artifacts and\nproduce more realistic editing while dealing with large-gap expression\ntransformations. Extensive experiments over two publicly available facial\nexpression datasets show that our proposed Cascade EF-GAN achieves superior\nperformance for facial expression editing."}, {"title": "GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes", "authors": "Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Francesc Moreno-Noguer, Gr\u00e9gory Rogez"}, {"title": "Deep Spatial Gradient and Temporal Depth Learning for Face Anti-Spoofing", "authors": "Zezheng Wang, Zitong Yu, Chenxu Zhao, Xiangyu Zhu, Yunxiao Qin, Qiusheng Zhou, Feng Zhou, Zhen Lei", "link": "https://arxiv.org/abs/2003.08061", "summary": "Face anti-spoofing is critical to the security of face recognition systems.\nDepth supervised learning has been proven as one of the most effective methods\nfor face anti-spoofing. Despite the great success, most previous works still\nformulate the problem as a single-frame multi-task one by simply augmenting the\nloss with depth, while neglecting the detailed fine-grained information and the\ninterplay between facial depths and moving patterns. In contrast, we design a\nnew approach to detect presentation attacks from multiple frames based on two\ninsights: 1) detailed discriminative clues (e.g., spatial gradient magnitude)\nbetween living and spoofing face may be discarded through stacked vanilla\nconvolutions, and 2) the dynamics of 3D moving faces provide important clues in\ndetecting the spoofing faces. The proposed method is able to capture\ndiscriminative details via Residual Spatial Gradient Block (RSGB) and encode\nspatio-temporal information from Spatio-Temporal Propagation Module (STPM)\nefficiently. Moreover, a novel Contrastive Depth Loss is presented for more\naccurate depth supervision. To assess the efficacy of our method, we also\ncollect a Double-modal Anti-spoofing Dataset (DMAD) which provides actual depth\nfor each sample. The experiments demonstrate that the proposed approach\nachieves state-of-the-art results on five benchmark datasets including\nOULU-NPU, SiW, CASIA-MFSD, Replay-Attack, and the new DMAD. Codes will be\navailable at https://github.com/clks-wzz/FAS-SGTD."}, {"title": "DeepCap: Monocular Human Performance Capture Using Weak Supervision", "authors": "Marc Habermann, Weipeng Xu, Michael Zollh\u00f6fer, Gerard Pons-Moll, Christian Theobalt", "link": "https://arxiv.org/abs/2003.08325", "summary": "Human performance capture is a highly important computer vision problem with\nmany applications in movie production and virtual/augmented reality. Many\nprevious performance capture approaches either required expensive multi-view\nsetups or did not recover dense space-time coherent geometry with\nframe-to-frame correspondences. We propose a novel deep learning approach for\nmonocular dense human performance capture. Our method is trained in a weakly\nsupervised manner based on multi-view supervision completely removing the need\nfor training data with 3D ground truth annotations. The network architecture is\nbased on two separate networks that disentangle the task into a pose estimation\nand a non-rigid surface deformation step. Extensive qualitative and\nquantitative evaluations show that our approach outperforms the state of the\nart in terms of quality and robustness."}, {"title": "Attention Mechanism Exploits Temporal Contexts: Real-Time 3D Human Pose Reconstruction", "authors": "Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung, Vijayan Asari"}, {"title": "Advancing High Fidelity Identity Swapping for Forgery Detection", "authors": "Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen"}, {"title": "Controllable Person Image Synthesis With Attribute-Decomposed GAN", "authors": "Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, Zhouhui Lian", "link": "https://arxiv.org/abs/2003.12267", "summary": "This paper introduces the Attribute-Decomposed GAN, a novel generative model\nfor controllable person image synthesis, which can produce realistic person\nimages with desired human attributes (e.g., pose, head, upper clothes and\npants) provided in various source inputs. The core idea of the proposed model\nis to embed human attributes into the latent space as independent codes and\nthus achieve flexible and continuous control of attributes via mixing and\ninterpolation operations in explicit style representations. Specifically, a new\narchitecture consisting of two encoding pathways with style block connections\nis proposed to decompose the original hard mapping into multiple more\naccessible subtasks. In source pathway, we further extract component layouts\nwith an off-the-shelf human parser and feed them into a shared global texture\nencoder for decomposed latent codes. This strategy allows for the synthesis of\nmore realistic output images and automatic separation of un-annotated\nattributes. Experimental results demonstrate the proposed method's superiority\nover the state of the art in pose transfer and its effectiveness in the\nbrand-new task of component attribute transfer."}, {"title": "Attentive Normalization for Conditional Image Generation", "authors": "Yi Wang, Ying-Cong Chen, Xiangyu Zhang, Jian Sun, Jiaya Jia", "link": "https://arxiv.org/abs/2004.03828", "summary": "Traditional convolution-based generative adversarial networks synthesize\nimages based on hierarchical local operations, where long-range dependency\nrelation is implicitly modeled with a Markov chain. It is still not sufficient\nfor categories with complicated structures. In this paper, we characterize\nlong-range dependence with attentive normalization (AN), which is an extension\nto traditional instance normalization. Specifically, the input feature map is\nsoftly divided into several regions based on its internal semantic similarity,\nwhich are respectively normalized. It enhances consistency between distant\nregions with semantic correspondence. Compared with self-attention GAN, our\nattentive normalization does not need to measure the correlation of all\nlocations, and thus can be directly applied to large-size feature maps without\nmuch computational burden. Extensive experiments on class-conditional image\ngeneration and semantic inpainting verify the efficacy of our proposed module."}, {"title": "SEAN: Image Synthesis With Semantic Region-Adaptive Normalization", "authors": "Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka", "link": "https://arxiv.org/abs/1911.12861", "summary": "We propose semantic region-adaptive normalization (SEAN), a simple but\neffective building block for Generative Adversarial Networks conditioned on\nsegmentation masks that describe the semantic regions in the desired output\nimage. Using SEAN normalization, we can build a network architecture that can\ncontrol the style of each semantic region individually, e.g., we can specify\none style reference image per region. SEAN is better suited to encode,\ntransfer, and synthesize style than the best previous method in terms of\nreconstruction quality, variability, and visual quality. We evaluate SEAN on\nmultiple datasets and report better quantitative metrics (e.g. FID, PSNR) than\nthe current state of the art. SEAN also pushes the frontier of interactive\nimage editing. We can interactively edit images by changing segmentation masks\nor the style for any given region. We can also interpolate styles from two\nreference images per region."}, {"title": "Blurry Video Frame Interpolation", "authors": "Wang Shen, Wenbo Bao, Guangtao Zhai, Li Chen, Xiongkuo Min, Zhiyong Gao", "link": "https://arxiv.org/abs/2002.12259", "summary": "Existing works reduce motion blur and up-convert frame rate through two\nseparate ways, including frame deblurring and frame interpolation. However, few\nstudies have approached the joint video enhancement problem, namely\nsynthesizing high-frame-rate clear results from low-frame-rate blurry inputs.\nIn this paper, we propose a blurry video frame interpolation method to reduce\nmotion blur and up-convert frame rate simultaneously. Specifically, we develop\na pyramid module to cyclically synthesize clear intermediate frames. The\npyramid module features adjustable spatial receptive field and temporal scope,\nthus contributing to controllable computational complexity and restoration\nability. Besides, we propose an inter-pyramid recurrent module to connect\nsequential models to exploit the temporal relationship. The pyramid module\nintegrates a recurrent module, thus can iteratively synthesize temporally\nsmooth results without significantly increasing the model size. Extensive\nexperimental results demonstrate that our method performs favorably against\nstate-of-the-art methods."}, {"title": "Learning Physics-Guided Face Relighting Under Directional Light", "authors": "Thomas Nestmeyer, Jean-Fran\u00e7ois Lalonde, Iain Matthews, Andreas Lehrmann"}, {"title": "Disentangled Image Generation Through Structured Noise Injection", "authors": "Yazeed Alharbi, Peter Wonka", "link": "https://arxiv.org/abs/2004.12411", "summary": "We explore different design choices for injecting noise into generative\nadversarial networks (GANs) with the goal of disentangling the latent space.\nInstead of traditional approaches, we propose feeding multiple noise codes\nthrough separate fully-connected layers respectively. The aim is restricting\nthe influence of each noise code to specific parts of the generated image. We\nshow that disentanglement in the first layer of the generator network leads to\ndisentanglement in the generated image. Through a grid-based structure, we\nachieve several aspects of disentanglement without complicating the network\narchitecture and without requiring labels. We achieve spatial disentanglement,\nscale-space disentanglement, and disentanglement of the foreground object from\nthe background style allowing fine-grained control over the generated images.\nExamples include changing facial expressions in face images, changing beak\nlength in bird images, and changing car dimensions in car images. This\nempirically leads to better disentanglement scores than state-of-the-art\nmethods on the FFHQ dataset."}, {"title": "Cross-Domain Correspondence Learning for Exemplar-Based Image Translation", "authors": "Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, Fang Wen", "link": "https://arxiv.org/abs/2004.05571", "summary": "We present a general framework for exemplar-based image translation, which\nsynthesizes a photo-realistic image from the input in a distinct domain (e.g.,\nsemantic segmentation mask, or edge map, or pose keypoints), given an exemplar\nimage. The output has the style (e.g., color, texture) in consistency with the\nsemantically corresponding objects in the exemplar. We propose to jointly learn\nthe crossdomain correspondence and the image translation, where both tasks\nfacilitate each other and thus can be learned with weak supervision. The images\nfrom distinct domains are first aligned to an intermediate domain where dense\ncorrespondence is established. Then, the network synthesizes images based on\nthe appearance of semantically corresponding patches in the exemplar. We\ndemonstrate the effectiveness of our approach in several image translation\ntasks. Our method is superior to state-of-the-art methods in terms of image\nquality significantly, with the image style faithful to the exemplar with\nsemantic consistency. Moreover, we show the utility of our method for several\napplications"}, {"title": "Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning", "authors": "Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong", "link": "https://arxiv.org/abs/2004.11660", "summary": "We propose an approach for face image generation of virtual people with\ndisentangled, precisely-controllable latent representations for identity of\nnon-existing people, expression, pose, and illumination. We embed 3D priors\ninto adversarial learning and train the network to imitate the image formation\nof an analytic 3D face deformation and rendering process. To deal with the\ngeneration freedom induced by the domain gap between real and rendered faces,\nwe further introduce contrastive learning to promote disentanglement by\ncomparing pairs of generated images. Experiments show that through our\nimitative-contrastive learning, the factor variations are very well\ndisentangled and the properties of a generated face can be precisely\ncontrolled. We also analyze the learned latent space and present several\nmeaningful properties supporting factor disentanglement. Our method can also be\nused to embed real images into the disentangled latent space. We hope our\nmethod could provide new understandings of the relationship between physical\nproperties and deep image synthesis."}, {"title": "Single Image Reflection Removal With Physically-Based Training Images", "authors": "Soomin Kim, Yuchi Huo, Sung-Eui Yoon", "link": "", "summary": ""}, {"title": "SketchyCOCO: Image Generation From Freehand Scene Sketches", "authors": "Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, Changqing Zou", "link": "https://arxiv.org/abs/2003.02683", "summary": "We introduce the first method for automatic image generation from scene-level\nfreehand sketches. Our model allows for controllable image generation by\nspecifying the synthesis goal via freehand sketches. The key contribution is an\nattribute vector bridged Generative Adversarial Network called EdgeGAN, which\nsupports high visual-quality object-level image content generation without\nusing freehand sketches as training data. We have built a large-scale composite\ndataset called SketchyCOCO to support and evaluate the solution. We validate\nour approach on the tasks of both object-level and scene-level image generation\non SketchyCOCO. Through quantitative, qualitative results, human evaluation and\nablation studies, we demonstrate the method's capacity to generate realistic\ncomplex scene-level images from various freehand sketches."}, {"title": "Image Based Virtual Try-On Network From Unpaired Data", "authors": "Assaf Neuberger, Eran Borenstein, Bar Hilleli, Eduard Oks, Sharon Alpert", "link": "", "summary": ""}, {"title": "PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer", "authors": "Wentao Jiang, Si Liu, Chen Gao, Jie Cao, Ran He, Jiashi Feng, Shuicheng Yan", "link": "https://arxiv.org/abs/1909.06956", "summary": "In this paper, we address the makeup transfer task, which aims to transfer\nthe makeup from a reference image to a source image. Existing methods have\nachieved promising progress in constrained scenarios, but transferring between\nimages with large pose and expression differences is still challenging.\nBesides, they cannot realize customizable transfer that allows a controllable\nshade of makeup or specifies the part to transfer, which limits their\napplications. To address these issues, we propose Pose and expression robust\nSpatial-aware GAN (PSGAN). It first utilizes Makeup Distill Network to\ndisentangle the makeup of the reference image as two spatial-aware makeup\nmatrices. Then, Attentive Makeup Morphing module is introduced to specify how\nthe makeup of a pixel in the source image is morphed from the reference image.\nWith the makeup matrices and the source image, Makeup Apply Network is used to\nperform makeup transfer. Our PSGAN not only achieves state-of-the-art results\neven when large pose and expression differences exist but also is able to\nperform partial and shade-controllable makeup transfer. We also collected a\ndataset containing facial images with various poses and expressions for\nevaluations."}, {"title": "RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild", "authors": "Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, Stefanos Zafeiriou", "link": "", "summary": ""}, {"title": "Semantic Image Manipulation Using Scene Graphs", "authors": "Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D. Hager, Federico Tombari, Christian Rupprecht", "link": "https://arxiv.org/abs/2004.03677", "summary": "Image manipulation can be considered a special case of image generation where\nthe image to be produced is a modification of an existing image. Image\ngeneration and manipulation have been, for the most part, tasks that operate on\nraw pixels. However, the remarkable progress in learning rich image and object\nrepresentations has opened the way for tasks such as text-to-image or\nlayout-to-image generation that are mainly driven by semantics. In our work, we\naddress the novel problem of image manipulation from scene graphs, in which a\nuser can edit images by merely applying changes in the nodes or edges of a\nsemantic graph that is generated from the image. Our goal is to encode image\ninformation in a given constellation and from there on generate new\nconstellations, such as replacing objects or even changing relationships\nbetween objects, while respecting the semantics and style from the original\nimage. We introduce a spatio-semantic scene graph network that does not require\ndirect supervision for constellation changes or image edits. This makes it\npossible to train the system from existing real-world datasets with no\nadditional annotation effort."}, {"title": "A Stochastic Conditioning Scheme for Diverse Human Motion Prediction", "authors": "Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould"}, {"title": "Transferring Dense Pose to Proximal Animal Classes", "authors": "Artsiom Sanakoyeu, Vasil Khalidov, Maureen S. McCarthy, Andrea Vedaldi, Natalia Neverova", "link": "http://arxiv.org/abs/2003.00080", "summary": "Recent contributions have demonstrated that it is possible to recognize the\npose of humans densely and accurately given a large dataset of poses annotated\nin detail. In principle, the same approach could be extended to any animal\nclass, but the effort required for collecting new annotations for each case\nmakes this strategy impractical, despite important applications in natural\nconservation, science and business. We show that, at least for proximal animal\nclasses such as chimpanzees, it is possible to transfer the knowledge existing\nin dense pose recognition for humans, as well as in more general object\ndetectors and segmenters, to the problem of dense pose recognition in other\nclasses. We do this by (1) establishing a DensePose model for the new animal\nwhich is also geometrically aligned to humans (2) introducing a multi-head\nR-CNN architecture that facilitates transfer of multiple recognition tasks\nbetween classes, (3) finding which combination of known classes can be\ntransferred most effectively to the new animal and (4) using self-calibrated\nuncertainty heads to generate pseudo-labels graded by quality for training a\nmodel for this class. We also introduce two benchmark datasets labelled in the\nmanner of DensePose for the class chimpanzee and use them to evaluate our\napproach, showing excellent transfer learning performance."}, {"title": "Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild", "authors": "Umar Iqbal, Pavlo Molchanov, Jan Kautz", "link": "http://arxiv.org/abs/2003.07581", "summary": "One major challenge for monocular 3D human pose estimation in-the-wild is the\nacquisition of training data that contains unconstrained images annotated with\naccurate 3D poses. In this paper, we address this challenge by proposing a\nweakly-supervised approach that does not require 3D annotations and learns to\nestimate 3D poses from unlabeled multi-view data, which can be acquired easily\nin in-the-wild environments. We propose a novel end-to-end learning framework\nthat enables weakly-supervised training using multi-view consistency. Since\nmulti-view consistency is prone to degenerated solutions, we adopt a 2.5D pose\nrepresentation and propose a novel objective function that can only be\nminimized when the predictions of the trained model are consistent and\nplausible across all camera views. We evaluate our proposed approach on two\nlarge scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves\nstate-of-the-art performance among semi-/weakly-supervised methods."}, {"title": "VIBE: Video Inference for Human Body Pose and Shape Estimation", "authors": "Muhammed Kocabas, Nikos Athanasiou, Michael J. Black", "link": "https://arxiv.org/abs/1912.05656", "summary": "Human motion is fundamental to understanding behavior. Despite progress on\nsingle-image 3D pose and shape estimation, existing video-based\nstate-of-the-art methods fail to produce accurate and natural motion sequences\ndue to a lack of ground-truth 3D motion data for training. To address this\nproblem, we propose Video Inference for Body Pose and Shape Estimation (VIBE),\nwhich makes use of an existing large-scale motion capture dataset (AMASS)\ntogether with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty\nis an adversarial learning framework that leverages AMASS to discriminate\nbetween real human motions and those produced by our temporal pose and shape\nregression networks. We define a temporal network architecture and show that\nadversarial training, at the sequence level, produces kinematically plausible\nmotion sequences without in-the-wild ground-truth 3D labels. We perform\nextensive experimentation to analyze the importance of motion and demonstrate\nthe effectiveness of VIBE on challenging 3D pose estimation datasets, achieving\nstate-of-the-art performance. Code and pretrained models are available at\nhttps://github.com/mkocabas/VIBE."}, {"title": "G3AN: Disentangling Appearance and Motion for Video Generation", "authors": "Yaohui Wang, Piotr Bilinski, Francois Bremond, Antitza Dantcheva", "link": "https://arxiv.org/abs/1912.05523", "summary": "Creating realistic human videos entails the challenge of being able to\nsimultaneously generate both appearance, as well as motion. To tackle this\nchallenge, we introduce G$^{3}$AN, a novel spatio-temporal generative model,\nwhich seeks to capture the distribution of high dimensional video data and to\nmodel appearance and motion in disentangled manner. The latter is achieved by\ndecomposing appearance and motion in a three-stream Generator, where the main\nstream aims to model spatio-temporal consistency, whereas the two auxiliary\nstreams augment the main stream with multi-scale appearance and motion\nfeatures, respectively. An extensive quantitative and qualitative analysis\nshows that our model systematically and significantly outperforms\nstate-of-the-art methods on the facial expression datasets MUG and UvA-NEMO, as\nwell as the Weizmann and UCF101 datasets on human action. Additional analysis\non the learned latent representations confirms the successful decomposition of\nappearance and motion. Source code and pre-trained models are publicly\navailable."}, {"title": "Domain Adaptive Image-to-Image Translation", "authors": "Ying-Cong Chen, Xiaogang Xu, Jiaya Jia"}, {"title": "GAN Compression: Efficient Architectures for Interactive Conditional GANs", "authors": "Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, Song Han", "link": "https://arxiv.org/abs/2003.08936", "summary": "Conditional Generative Adversarial Networks (cGANs) have enabled controllable\nimage synthesis for many computer vision and graphics applications. However,\nrecent cGANs are 1-2 orders of magnitude more computationally-intensive than\nmodern recognition CNNs. For example, GauGAN consumes 281G MACs per image,\ncompared to 0.44G MACs for MobileNet-v3, making it difficult for interactive\ndeployment. In this work, we propose a general-purpose compression framework\nfor reducing the inference time and model size of the generator in cGANs.\nDirectly applying existing CNNs compression methods yields poor performance due\nto the difficulty of GAN training and the differences in generator\narchitectures. We address these challenges in two ways. First, to stabilize the\nGAN training, we transfer knowledge of multiple intermediate representations of\nthe original model to its compressed model, and unify unpaired and paired\nlearning. Second, instead of reusing existing CNN designs, our method\nautomatically finds efficient architectures via neural architecture search\n(NAS). To accelerate the search process, we decouple the model training and\narchitecture search via weight sharing. Experiments demonstrate the\neffectiveness of our method across different supervision settings (paired and\nunpaired), model architectures, and learning methods (e.g., pix2pix, GauGAN,\nCycleGAN). Without losing image quality, we reduce the computation of CycleGAN\nby more than 20X and GauGAN by 9X, paving the way for interactive image\nsynthesis. The code and demo are publicly available."}, {"title": "Searching Central Difference Convolutional Networks for Face Anti-Spoofing", "authors": "Zitong Yu, Chenxu Zhao, Zezheng Wang, Yunxiao Qin, Zhuo Su, Xiaobai Li, Feng Zhou, Guoying Zhao", "link": "https://arxiv.org/abs/2003.04092", "summary": "Face anti-spoofing (FAS) plays a vital role in face recognition systems. Most\nstate-of-the-art FAS methods 1) rely on stacked convolutions and\nexpert-designed network, which is weak in describing detailed fine-grained\ninformation and easily being ineffective when the environment varies (e.g.,\ndifferent illumination), and 2) prefer to use long sequence as input to extract\ndynamic features, making them difficult to deploy into scenarios which need\nquick response. Here we propose a novel frame level FAS method based on Central\nDifference Convolution (CDC), which is able to capture intrinsic detailed\npatterns via aggregating both intensity and gradient information. A network\nbuilt with CDC, called the Central Difference Convolutional Network (CDCN), is\nable to provide more robust modeling capacity than its counterpart built with\nvanilla convolution. Furthermore, over a specifically designed CDC search\nspace, Neural Architecture Search (NAS) is utilized to discover a more powerful\nnetwork structure (CDCN++), which can be assembled with Multiscale Attention\nFusion Module (MAFM) for further boosting performance. Comprehensive\nexperiments are performed on six benchmark datasets to show that 1) the\nproposed method not only achieves superior performance on intra-dataset testing\n(especially 0.2% ACER in Protocol-1 of OULU-NPU dataset), 2) it also\ngeneralizes well on cross-dataset testing (particularly 6.5% HTER from\nCASIA-MFSD to Replay-Attack datasets). The codes are available at\n\\href{https://github.com/ZitongYu/CDCN}{https://github.com/ZitongYu/CDCN}."}, {"title": "TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting", "authors": "Zhuoqian Yang, Wentao Zhu, Wayne Wu, Chen Qian, Qiang Zhou, Bolei Zhou, Chen Change Loy", "link": "https://arxiv.org/abs/2003.14401", "summary": "We present a lightweight video motion retargeting approach TransMoMo that is\ncapable of transferring motion of a person in a source video realistically to\nanother video of a target person. Without using any paired data for\nsupervision, the proposed method can be trained in an unsupervised manner by\nexploiting invariance properties of three orthogonal factors of variation\nincluding motion, structure, and view-angle. Specifically, with loss functions\ncarefully derived based on invariance, we train an auto-encoder to disentangle\nthe latent representations of such factors given the source and target video\nclips. This allows us to selectively transfer motion extracted from the source\nvideo seamlessly to the target video in spite of structural and view-angle\ndisparities between the source and the target. The relaxed assumption of paired\ndata allows our method to be trained on a vast amount of videos needless of\nmanual annotation of source-target pairing, leading to improved robustness\nagainst large structural variations and extreme motion in videos. We\ndemonstrate the effectiveness of our method over the state-of-the-art methods.\nCode, model and data are publicly available on our project page\n(https://yzhq97.github.io/transmomo)."}, {"title": "AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation", "authors": "Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban, Sangyoun Lee", "link": "https://arxiv.org/abs/1907.10244", "summary": "Video frame interpolation is one of the most challenging tasks in video\nprocessing research. Recently, many studies based on deep learning have been\nsuggested. Most of these methods focus on finding locations with useful\ninformation to estimate each output pixel using their own frame warping\noperations. However, many of them have Degrees of Freedom (DoF) limitations and\nfail to deal with the complex motions found in real world videos. To solve this\nproblem, we propose a new warping module named Adaptive Collaboration of Flows\n(AdaCoF). Our method estimates both kernel weights and offset vectors for each\ntarget pixel to synthesize the output frame. AdaCoF is one of the most\ngeneralized warping modules compared to other approaches, and covers most of\nthem as special cases of it. Therefore, it can deal with a significantly wide\ndomain of complex motions. To further improve our framework and synthesize more\nrealistic outputs, we introduce dual-frame adversarial loss which is applicable\nonly to video frame interpolation tasks. The experimental results show that our\nmethod outperforms the state-of-the-art methods for both fixed training set\nenvironments and the Middlebury benchmark."}, {"title": "FReeNet: Multi-Identity Face Reenactment", "authors": "Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu, Yong Liu, Yu Ding, Changjie Fan", "link": "https://arxiv.org/abs/1905.11805", "summary": "This paper presents a novel multi-identity face reenactment framework, named\nFReeNet, to transfer facial expressions from an arbitrary source face to a\ntarget face with a shared model. The proposed FReeNet consists of two parts:\nUnified Landmark Converter (ULC) and Geometry-aware Generator (GAG). The ULC\nadopts an encode-decoder architecture to efficiently convert expression in a\nlatent landmark space, which significantly narrows the gap of the face contour\nbetween source and target identities. The GAG leverages the converted landmark\nto reenact the photorealistic image with a reference image of the target\nperson. Moreover, a new triplet perceptual loss is proposed to force the GAG\nmodule to learn appearance and geometry information simultaneously, which also\nenriches facial details of the reenacted images. Further experiments\ndemonstrate the superiority of our approach for generating photorealistic and\nexpression-alike faces, as well as the flexibility for transferring facial\nexpressions between identities."}, {"title": "Novel View Synthesis of Dynamic Scenes With Globally Coherent Depths From a Monocular Camera", "authors": "Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, Jan Kautz", "link": "https://arxiv.org/abs/2004.01294", "summary": "This paper presents a new method to synthesize an image from arbitrary views\nand times given a collection of images of a dynamic scene. A key challenge for\nthe novel view synthesis arises from dynamic scene reconstruction where\nepipolar geometry does not apply to the local motion of dynamic contents. To\naddress this challenge, we propose to combine the depth from single view (DSV)\nand the depth from multi-view stereo (DMV), where DSV is complete, i.e., a\ndepth is assigned to every pixel, yet view-variant in its scale, while DMV is\nview-invariant yet incomplete. Our insight is that although its scale and\nquality are inconsistent with other views, the depth estimation from a single\nview can be used to reason about the globally coherent geometry of dynamic\ncontents. We cast this problem as learning to correct the scale of DSV, and to\nrefine each depth with locally consistent motions between views to form a\ncoherent depth estimation. We integrate these tasks into a depth fusion network\nin a self-supervised fashion. Given the fused depth maps, we synthesize a\nphotorealistic virtual view in a specific location and time with our deep\nblending network that completes the scene and renders the virtual view. We\nevaluate our method of depth estimation and view synthesis on diverse\nreal-world dynamic scenes and show the outstanding performance over existing\nmethods."}, {"title": "Monocular Real-Time Hand Shape and Motion Capture Using Multi-Modal Data", "authors": "Yuxiao Zhou, Marc Habermann, Weipeng Xu, Ikhsanul Habibie, Christian Theobalt, Feng Xu", "link": "https://arxiv.org/abs/2003.09572", "summary": "We present a novel method for monocular hand shape and pose estimation at\nunprecedented runtime performance of 100fps and at state-of-the-art accuracy.\nThis is enabled by a new learning based architecture designed such that it can\nmake use of all the sources of available hand training data: image data with\neither 2D or 3D annotations, as well as stand-alone 3D animations without\ncorresponding image data. It features a 3D hand joint detection module and an\ninverse kinematics module which regresses not only 3D joint positions but also\nmaps them to joint rotations in a single feed-forward pass. This output makes\nthe method more directly usable for applications in computer vision and\ngraphics compared to only regressing 3D joint positions. We demonstrate that\nour architectural design leads to a significant quantitative and qualitative\nimprovement over the state of the art on several challenging benchmarks. Our\nmodel is publicly available for future research."}, {"title": "The GAN That Warped: Semantic Attribute Editing With Unpaired Data", "authors": "Garoe Dorta, Sara Vicente, Neill D. F. Campbell, Ivor J. A. Simpson", "link": "https://arxiv.org/abs/1811.12784", "summary": "Deep neural networks have recently been used to edit images with great\nsuccess, in particular for faces. However, they are often limited to only being\nable to work at a restricted range of resolutions. Many methods are so flexible\nthat face edits can often result in an unwanted loss of identity. This work\nproposes to learn how to perform semantic image edits through the application\nof smooth warp fields. Previous approaches that attempted to use warping for\nsemantic edits required paired data, i.e. example images of the same subject\nwith different semantic attributes. In contrast, we employ recent advances in\nGenerative Adversarial Networks that allow our model to be trained with\nunpaired data. We demonstrate face editing at very high resolutions (4k images)\nwith a single forward pass of a deep network at a lower resolution. We also\nshow that our edits are substantially better at preserving the subject's\nidentity. The robustness of our approach is demonstrated by showing plausible\nimage editing results on the Cub200 birds dataset. To our knowledge this has\nnot been previously accomplished, due the challenging nature of the dataset."}, {"title": "4D Visualization of Dynamic Events From Unconstrained Multi-View Videos", "authors": "Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa Narasimhan"}, {"title": "Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds", "authors": "Yongming Rao, Jiwen Lu, Jie Zhou", "link": "https://arxiv.org/abs/2003.12971", "summary": "Local and global patterns of an object are closely related. Although each\npart of an object is incomplete, the underlying attributes about the object are\nshared among all parts, which makes reasoning the whole object from a single\npart possible. We hypothesize that a powerful representation of a 3D object\nshould model the attributes that are shared between parts and the whole object,\nand distinguishable from other objects. Based on this hypothesis, we propose to\nlearn point cloud representation by bidirectional reasoning between the local\nstructures at different abstraction hierarchies and the global shape without\nhuman supervision. Experimental results on various benchmark datasets\ndemonstrate the unsupervisedly learned representation is even better than\nsupervised representation in discriminative power, generalization ability, and\nrobustness. We show that unsupervisedly trained point cloud models can\noutperform their supervised counterparts on downstream classification tasks.\nMost notably, by simply increasing the channel width of an SSG PointNet++, our\nunsupervised model surpasses the state-of-the-art supervised methods on both\nsynthetic and real-world 3D object classification datasets. We expect our\nobservations to offer a new perspective on learning better representation from\ndata structures instead of human annotations for point cloud understanding."}, {"title": "HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation", "authors": "Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S. Huang, Lei Zhang", "link": "https://arxiv.org/abs/1908.10357", "summary": "Bottom-up human pose estimation methods have difficulties in predicting the\ncorrect pose for small persons due to challenges in scale variation. In this\npaper, we present HigherHRNet: a novel bottom-up human pose estimation method\nfor learning scale-aware representations using high-resolution feature\npyramids. Equipped with multi-resolution supervision for training and\nmulti-resolution aggregation for inference, the proposed approach is able to\nsolve the scale variation challenge in bottom-up multi-person pose estimation\nand localize keypoints more precisely, especially for small person. The feature\npyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled\nhigher-resolution outputs through a transposed convolution. HigherHRNet\noutperforms the previous best bottom-up method by 2.5% AP for medium person on\nCOCO test-dev, showing its effectiveness in handling scale variation.\nFurthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev\n(70.5% AP) without using refinement or other post-processing techniques,\nsurpassing all existing bottom-up methods. HigherHRNet even surpasses all\ntop-down methods on CrowdPose test (67.6% AP), suggesting its robustness in\ncrowded scene. The code and models are available at\nhttps://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation."}, {"title": "Detecting Attended Visual Targets in Video", "authors": "Eunji Chong, Yongxin Wang, Nataniel Ruiz, James M. Rehg", "link": "https://arxiv.org/abs/2003.02501", "summary": "We address the problem of detecting attention targets in video. Our goal is\nto identify where each person in each frame of a video is looking, and\ncorrectly handle the case where the gaze target is out-of-frame. Our novel\narchitecture models the dynamic interaction between the scene and head features\nand infers time-varying attention targets. We introduce a new annotated\ndataset, VideoAttentionTarget, containing complex and dynamic patterns of\nreal-world gaze behavior. Our experiments show that our model can effectively\ninfer dynamic attention in videos. In addition, we apply our predicted\nattention maps to two social gaze behavior recognition tasks, and show that the\nresulting classifiers significantly outperform existing methods. We achieve\nstate-of-the-art performance on three datasets: GazeFollow (static images),\nVideoAttentionTarget (videos), and VideoCoAtt (videos), and obtain the first\nresults for automatically classifying clinically-relevant gaze behavior without\nwearable cameras or eye trackers."}, {"title": "Closed-Loop Matters: Dual Regression Networks for Single Image Super-Resolution", "authors": "Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai Deng, Yanwu Xu, Mingkui Tan", "link": "https://arxiv.org/abs/2003.07018", "summary": "Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) by learning a nonlinear mapping function from\nlow-resolution (LR) images to high-resolution (HR) images. However, there are\ntwo underlying limitations to existing SR methods. First, learning the mapping\nfunction from LR to HR images is typically an ill-posed problem, because there\nexist infinite HR images that can be downsampled to the same LR image. As a\nresult, the space of the possible functions can be extremely large, which makes\nit hard to find a good solution. Second, the paired LR-HR data may be\nunavailable in real-world applications and the underlying degradation method is\noften unknown. For such a more general case, existing SR models often incur the\nadaptation problem and yield poor performance. To address the above issues, we\npropose a dual regression scheme by introducing an additional constraint on LR\ndata to reduce the space of the possible functions. Specifically, besides the\nmapping from LR to HR images, we learn an additional dual regression mapping\nestimates the down-sampling kernel and reconstruct LR images, which forms a\nclosed-loop to provide additional supervision. More critically, since the dual\nregression process does not depend on HR images, we can directly learn from LR\nimages. In this sense, we can easily adapt SR models to real-world data, e.g.,\nraw video frames from YouTube. Extensive experiments with paired training data\nand unpaired real-world data demonstrate our superiority over existing methods."}, {"title": "Neural Voxel Renderer: Learning an Accurate and Controllable Rendering Tool", "authors": "Konstantinos Rematas, Vittorio Ferrari", "link": "https://arxiv.org/abs/1912.04591", "summary": "We present a neural rendering framework that maps a voxelized scene into a\nhigh quality image. Highly-textured objects and scene element interactions are\nrealistically rendered by our method, despite having a rough representation as\nan input. Moreover, our approach allows controllable rendering: geometric and\nappearance modifications in the input are accurately propagated to the output.\nThe user can move, rotate and scale an object, change its appearance and\ntexture or modify the position of the light and all these edits are represented\nin the final rendering. We demonstrate the effectiveness of our approach by\nrendering scenes with varying appearance, from single color per object to\ncomplex, high-frequency textures. We show that our rerendering network can\ngenerate very detailed images that represent precisely the appearance of the\ninput scene. Our experiments illustrate that our approach achieves more\naccurate image synthesis results compared to alternatives and can also handle\nlow voxel grid resolutions. Finally, we show how our neural rendering framework\ncan capture and faithfully render objects from real images and from a diverse\nset of classes."}, {"title": "Neural Contours: Learning to Draw Lines From 3D Shapes", "authors": "Difan Liu, Mohamed Nabail, Aaron Hertzmann, Evangelos Kalogerakis", "link": "https://arxiv.org/abs/2003.10333", "summary": "This paper introduces a method for learning to generate line drawings from 3D\nmodels. Our architecture incorporates a differentiable module operating on\ngeometric features of the 3D model, and an image-based module operating on\nview-based shape representations. At test time, geometric and view-based\nreasoning are combined with the help of a neural module to create a line\ndrawing. The model is trained on a large number of crowdsourced comparisons of\nline drawings. Experiments demonstrate that our method achieves significant\nimprovements in line drawing over the state-of-the-art when evaluated on\nstandard benchmarks, resulting in drawings that are comparable to those\nproduced by experienced human artists."}, {"title": "Softmax Splatting for Video Frame Interpolation", "authors": "Simon Niklaus, Feng Liu", "link": "https://arxiv.org/abs/2003.05534", "summary": "Differentiable image sampling in the form of backward warping has seen broad\nadoption in tasks like depth estimation and optical flow prediction. In\ncontrast, how to perform forward warping has seen less attention, partly due to\nadditional challenges such as resolving the conflict of mapping multiple pixels\nto the same target location in a differentiable way. We propose softmax\nsplatting to address this paradigm shift and show its effectiveness on the\napplication of frame interpolation. Specifically, given two input frames, we\nforward-warp the frames and their feature pyramid representations based on an\noptical flow estimate using softmax splatting. In doing so, the softmax\nsplatting seamlessly handles cases where multiple source pixels map to the same\ntarget location. We then use a synthesis network to predict the interpolation\nresult from the warped representations. Our softmax splatting allows us to not\nonly interpolate frames at an arbitrary time but also to fine tune the feature\npyramid and the optical flow. We show that our synthesis approach, empowered by\nsoftmax splatting, achieves new state-of-the-art results for video frame\ninterpolation."}, {"title": "CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks", "authors": "Maxim Maximov, Ismail Elezi, Laura Leal-Taix\u00e9", "link": "https://arxiv.org/abs/2005.09544", "summary": "The unprecedented increase in the usage of computer vision technology in\nsociety goes hand in hand with an increased concern in data privacy. In many\nreal-world scenarios like people tracking or action recognition, it is\nimportant to be able to process the data while taking careful consideration in\nprotecting people's identity. We propose and develop CIAGAN, a model for image\nand video anonymization based on conditional generative adversarial networks.\nOur model is able to remove the identifying characteristics of faces and bodies\nwhile producing high-quality images and videos that can be used for any\ncomputer vision task, such as detection or tracking. Unlike previous methods,\nwe have full control over the de-identification (anonymization) procedure,\nensuring both anonymization as well as diversity. We compare our method to\nseveral baselines and achieve state-of-the-art results."}, {"title": "Probabilistic Structural Latent Representation for Unsupervised Embedding", "authors": "Mang Ye, Jianbing Shen"}, {"title": "Semantically Multi-Modal Image Synthesis", "authors": "Zhen Zhu, Zhiliang Xu, Ansheng You, Xiang Bai", "link": "https://arxiv.org/abs/2003.12697", "summary": "In this paper, we focus on semantically multi-modal image synthesis (SMIS)\ntask, namely, generating multi-modal images at the semantic level. Previous\nwork seeks to use multiple class-specific generators, constraining its usage in\ndatasets with a small number of classes. We instead propose a novel Group\nDecreasing Network (GroupDNet) that leverages group convolutions in the\ngenerator and progressively decreases the group numbers of the convolutions in\nthe decoder. Consequently, GroupDNet is armed with much more controllability on\ntranslating semantic labels to natural images and has plausible high-quality\nyields for datasets with many classes. Experiments on several challenging\ndatasets demonstrate the superiority of GroupDNet on performing the SMIS task.\nWe also show that GroupDNet is capable of performing a wide range of\ninteresting synthesis applications. Codes and models are available at:\nhttps://github.com/Seanseattle/SMIS."}, {"title": "Nested Scale-Editing for Conditional Image Synthesis", "authors": "Lingzhi Zhang, Jiancong Wang, Yinshuang Xu, Jie Min, Tarmily Wen, James C. Gee, Jianbo Shi", "link": "http://arxiv.org/abs/2006.02038", "summary": "We propose an image synthesis approach that provides stratified navigation in\nthe latent code space. With a tiny amount of partial or very low-resolution\nimage, our approach can consistently out-perform state-of-the-art counterparts\nin terms of generating the closest sampled image to the ground truth. We\nachieve this through scale-independent editing while expanding scale-specific\ndiversity. Scale-independence is achieved with a nested scale disentanglement\nloss. Scale-specific diversity is created by incorporating a progressive\ndiversification constraint. We introduce semantic persistency across the scales\nby sharing common latent codes. Together they provide better control of the\nimage synthesis process. We evaluate the effectiveness of our proposed approach\nthrough various tasks, including image outpainting, image superresolution, and\ncross-domain image translation."}, {"title": "UnrealText: Synthesizing Realistic Scene Text Images From the Unreal World", "authors": "Shangbang Long, Cong Yao", "link": "https://arxiv.org/abs/2003.10608", "summary": "Synthetic data has been a critical tool for training scene text detection and\nrecognition models. On the one hand, synthetic word images have proven to be a\nsuccessful substitute for real images in training scene text recognizers. On\nthe other hand, however, scene text detectors still heavily rely on a large\namount of manually annotated real-world images, which are expensive. In this\npaper, we introduce UnrealText, an efficient image synthesis method that\nrenders realistic images via a 3D graphics engine. 3D synthetic engine provides\nrealistic appearance by rendering scene and text as a whole, and allows for\nbetter text region proposals with access to precise scene information, e.g.\nnormal and even object meshes. The comprehensive experiments verify its\neffectiveness on both scene text detection and recognition. We also generate a\nmultilingual version for future research into multilingual scene text detection\nand recognition. Additionally, we re-annotate scene text recognition datasets\nin a case-sensitive way and include punctuation marks for more comprehensive\nevaluations. The code and the generated datasets are released at:\nhttps://github.com/Jyouhou/UnrealText/ ."}, {"title": "Fast Texture Synthesis via Pseudo Optimizer", "authors": "Wu Shi, Yu Qiao"}, {"title": "Towards Learning Structure via Consensus for Face Segmentation and Parsing", "authors": "Iacopo Masi, Joe Mathai, Wael AbdAlmageed", "link": "https://arxiv.org/abs/1911.00957", "summary": "Face segmentation is the task of densely labeling pixels on the face\naccording to their semantics. While current methods place an emphasis on\ndeveloping sophisticated architectures, use conditional random fields for\nsmoothness, or rather employ adversarial training, we follow an alternative\npath towards robust face segmentation and parsing. Occlusions, along with other\nparts of the face, have a proper structure that needs to be propagated in the\nmodel during training. Unlike state-of-the-art methods that treat face\nsegmentation as an independent pixel prediction problem, we argue instead that\nit should hold highly correlated outputs within the same object pixels. We\nthereby offer a novel learning mechanism to enforce structure in the prediction\nvia consensus, guided by a robust loss function that forces pixel objects to be\nconsistent with each other. Our face parser is trained by transferring\nknowledge from another model, yet it encourages spatial consistency while\nfitting the labels. Different than current practice, our method enjoys\npixel-wise predictions, yet paves the way for fewer artifacts, less sparse\nmasks, and spatially coherent outputs."}, {"title": "CookGAN: Causality Based Text-to-Image Synthesis", "authors": "Bin Zhu, Chong-Wah Ngo"}, {"title": "Weakly Supervised Discriminative Feature Learning With State Information for Person Identification", "authors": "Hong-Xing Yu, Wei-Shi Zheng", "link": "http://arxiv.org/abs/2002.11939", "summary": "Unsupervised learning of identity-discriminative visual feature is appealing\nin real-world tasks where manual labelling is costly. However, the images of an\nidentity can be visually discrepant when images are taken under different\nstates, e.g. different camera views and poses. This visual discrepancy leads to\ngreat difficulty in unsupervised discriminative learning. Fortunately, in\nreal-world tasks we could often know the states without human annotation, e.g.\nwe can easily have the camera view labels in person re-identification and\nfacial pose labels in face recognition. In this work we propose utilizing the\nstate information as weak supervision to address the visual discrepancy caused\nby different states. We formulate a simple pseudo label model and utilize the\nstate information in an attempt to refine the assigned pseudo labels by the\nweakly supervised decision boundary rectification and weakly supervised feature\ndrift regularization. We evaluate our model on unsupervised person\nre-identification and pose-invariant face recognition. Despite the simplicity\nof our method, it could outperform the state-of-the-art results on Duke-reID,\nMultiPIE and CFP datasets with a standard ResNet-50 backbone. We also find our\nmodel could perform comparably with the standard supervised fine-tuning results\non the three datasets. Code is available at\nhttps://github.com/KovenYu/state-information"}, {"title": "Future Video Synthesis With Object Motion Prediction", "authors": "Yue Wu, Rongrong Gao, Jaesik Park, Qifeng Chen", "link": "http://arxiv.org/abs/2004.00542", "summary": "We present an approach to predict future video frames given a sequence of\ncontinuous video frames in the past. Instead of synthesizing images directly,\nour approach is designed to understand the complex scene dynamics by decoupling\nthe background scene and moving objects. The appearance of the scene components\nin the future is predicted by non-rigid deformation of the background and\naffine transformation of moving objects. The anticipated appearances are\ncombined to create a reasonable video in the future. With this procedure, our\nmethod exhibits much less tearing or distortion artifact compared to other\napproaches. Experimental results on the Cityscapes and KITTI datasets show that\nour model outperforms the state-of-the-art in terms of visual quality and\naccuracy."}, {"title": "MaskGAN: Towards Diverse and Interactive Facial Image Manipulation", "authors": "Cheng-Han Lee, Ziwei Liu, Lingyun Wu, Ping Luo", "link": "https://arxiv.org/abs/1907.11922", "summary": "Facial image manipulation has achieved great progress in recent years.\nHowever, previous methods either operate on a predefined set of face attributes\nor leave users little freedom to interactively manipulate images. To overcome\nthese drawbacks, we propose a novel framework termed MaskGAN, enabling diverse\nand interactive face manipulation. Our key insight is that semantic masks serve\nas a suitable intermediate representation for flexible face manipulation with\nfidelity preservation. MaskGAN has two main components: 1) Dense Mapping\nNetwork (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically,\nDMN learns style mapping between a free-form user modified mask and a target\nimage, enabling diverse generation results. EBST models the user editing\nbehavior on the source mask, making the overall framework more robust to\nvarious manipulated inputs. Specifically, it introduces dual-editing\nconsistency as the auxiliary supervision signal. To facilitate extensive\nstudies, we construct a large-scale high-resolution face dataset with\nfine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively\nevaluated on two challenging tasks: attribute transfer and style copy,\ndemonstrating superior performance over other state-of-the-art methods. The\ncode, models, and dataset are available at\nhttps://github.com/switchablenorms/CelebAMask-HQ."}, {"title": "A Graduated Filter Method for Large Scale Robust Estimation", "authors": "Huu Le, Christopher Zach", "link": "https://arxiv.org/abs/2003.09080", "summary": "Due to the highly non-convex nature of large-scale robust parameter\nestimation, avoiding poor local minima is challenging in real-world\napplications where input data is contaminated by a large or unknown fraction of\noutliers. In this paper, we introduce a novel solver for robust estimation that\npossesses a strong ability to escape poor local minima. Our algorithm is built\nupon the class of traditional graduated optimization techniques, which are\nconsidered state-of-the-art local methods to solve problems having many poor\nminima. The novelty of our work lies in the introduction of an adaptive kernel\n(or residual) scaling scheme, which allows us to achieve faster convergence\nrates. Like other existing methods that aim to return good local minima for\nrobust estimation tasks, our method relaxes the original robust problem but\nadapts a filter framework from non-linear constrained optimization to\nautomatically choose the level of relaxation. Experimental results on real\nlarge-scale datasets such as bundle adjustment instances demonstrate that our\nproposed method achieves competitive results."}, {"title": "Deep Face Super-Resolution With Iterative Collaboration Between Attentive Recovery and Landmark Estimation", "authors": "Cheng Ma, Zhenyu Jiang, Yongming Rao, Jiwen Lu, Jie Zhou", "link": "https://arxiv.org/abs/2003.13063", "summary": "Recent works based on deep learning and facial priors have succeeded in\nsuper-resolving severely degraded facial images. However, the prior knowledge\nis not fully exploited in existing methods, since facial priors such as\nlandmark and component maps are always estimated by low-resolution or coarsely\nsuper-resolved images, which may be inaccurate and thus affect the recovery\nperformance. In this paper, we propose a deep face super-resolution (FSR)\nmethod with iterative collaboration between two recurrent networks which focus\non facial image recovery and landmark estimation respectively. In each\nrecurrent step, the recovery branch utilizes the prior knowledge of landmarks\nto yield higher-quality images which facilitate more accurate landmark\nestimation in turn. Therefore, the iterative information interaction between\ntwo processes boosts the performance of each other progressively. Moreover, a\nnew attentive fusion module is designed to strengthen the guidance of landmark\nmaps, where facial components are generated individually and aggregated\nattentively for better restoration. Quantitative and qualitative experimental\nresults show the proposed method significantly outperforms state-of-the-art FSR\nmethods in recovering high-quality face images."}, {"title": "Coherent Reconstruction of Multiple Humans From a Single Image", "authors": "Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei Zhou, Kostas Daniilidis"}, {"title": "PointASNL: Robust Point Clouds Processing Using Nonlocal Neural Networks With Adaptive Sampling", "authors": "Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, Shuguang Cui", "link": "https://arxiv.org/abs/2003.00492", "summary": "Raw point clouds data inevitably contains outliers or noise through\nacquisition from 3D sensors or reconstruction algorithms. In this paper, we\npresent a novel end-to-end network for robust point clouds processing, named\nPointASNL, which can deal with point clouds with noise effectively. The key\ncomponent in our approach is the adaptive sampling (AS) module. It first\nre-weights the neighbors around the initial sampled points from farthest point\nsampling (FPS), and then adaptively adjusts the sampled points beyond the\nentire point cloud. Our AS module can not only benefit the feature learning of\npoint clouds, but also ease the biased effect of outliers. To further capture\nthe neighbor and long-range dependencies of the sampled point, we proposed a\nlocal-nonlocal (L-NL) module inspired by the nonlocal operation. Such L-NL\nmodule enables the learning process insensitive to noise. Extensive experiments\nverify the robustness and superiority of our approach in point clouds\nprocessing tasks regardless of synthesis data, indoor data, and outdoor data\nwith or without noise. Specifically, PointASNL achieves state-of-the-art robust\nperformance for classification and segmentation tasks on all datasets, and\nsignificantly outperforms previous methods on real-world outdoor SemanticKITTI\ndataset with considerate noise. Our code is released through\nhttps://github.com/yanx27/PointASNL."}, {"title": "A Neural Rendering Framework for Free-Viewpoint Relighting", "authors": "Zhang Chen, Anpei Chen, Guli Zhang, Chengyuan Wang, Yu Ji, Kiriakos N. Kutulakos, Jingyi Yu", "link": "https://arxiv.org/abs/1911.11530", "summary": "We present a novel Relightable Neural Renderer (RNR) for simultaneous view\nsynthesis and relighting using multi-view image inputs. Existing neural\nrendering (NR) does not explicitly model the physical rendering process and\nhence has limited capabilities on relighting. RNR instead models image\nformation in terms of environment lighting, object intrinsic attributes, and\nthe light transport function (LTF), each corresponding to a learnable\ncomponent. In particular, the incorporation of a physically based rendering\nprocess not only enables relighting but also improves the quality of novel view\nsynthesis. Comprehensive experiments on synthetic and real data show that RNR\nprovides a practical and effective solution for conducting free-viewpoint\nrelighting."}, {"title": "A Multi-Task Mean Teacher for Semi-Supervised Shadow Detection", "authors": "Zhihao Chen, Lei Zhu, Liang Wan, Song Wang, Wei Feng, Pheng-Ann Heng"}, {"title": "GroupFace: Learning Latent Groups and Constructing Group-Based Representations for Face Recognition", "authors": "Yonghyun Kim, Wonpyo Park, Myung-Cheol Roh, Jongju Shin", "link": "https://arxiv.org/abs/2005.10497", "summary": "In the field of face recognition, a model learns to distinguish millions of\nface images with fewer dimensional embedding features, and such vast\ninformation may not be properly encoded in the conventional model with a single\nbranch. We propose a novel face-recognition-specialized architecture called\nGroupFace that utilizes multiple group-aware representations, simultaneously,\nto improve the quality of the embedding feature. The proposed method provides\nself-distributed labels that balance the number of samples belonging to each\ngroup without additional human annotations, and learns the group-aware\nrepresentations that can narrow down the search space of the target identity.\nWe prove the effectiveness of the proposed method by showing extensive ablation\nstudies and visualizations. All the components of the proposed method can be\ntrained in an end-to-end manner with a marginal increase of computational\ncomplexity. Finally, the proposed method achieves the state-of-the-art results\nwith significant improvements in 1:1 face verification and 1:N face\nidentification tasks on the following public datasets: LFW, YTF, CALFW, CPLFW,\nCFP, AgeDB-30, MegaFace, IJB-B and IJB-C."}, {"title": "Channel Attention Based Iterative Residual Learning for Depth Map Super-Resolution", "authors": "Xibin Song, Yuchao Dai, Dingfu Zhou, Liu Liu, Wei Li, Hongdong Li, Ruigang Yang", "link": "https://arxiv.org/abs/2006.01469", "summary": "Despite the remarkable progresses made in deep-learning based depth map\nsuper-resolution (DSR), how to tackle real-world degradation in low-resolution\n(LR) depth maps remains a major challenge. Existing DSR model is generally\ntrained and tested on synthetic dataset, which is very different from what\nwould get from a real depth sensor. In this paper, we argue that DSR models\ntrained under this setting are restrictive and not effective in dealing with\nreal-world DSR tasks. We make two contributions in tackling real-world\ndegradation of different depth sensors. First, we propose to classify the\ngeneration of LR depth maps into two types: non-linear downsampling with noise\nand interval downsampling, for which DSR models are learned correspondingly.\nSecond, we propose a new framework for real-world DSR, which consists of four\nmodules : 1) An iterative residual learning module with deep supervision to\nlearn effective high-frequency components of depth maps in a coarse-to-fine\nmanner; 2) A channel attention strategy to enhance channels with abundant\nhigh-frequency components; 3) A multi-stage fusion module to effectively\nre-exploit the results in the coarse-to-fine process; and 4) A depth refinement\nmodule to improve the depth map by TGV regularization and input loss. Extensive\nexperiments on benchmarking datasets demonstrate the superiority of our method\nover current state-of-the-art DSR methods."}, {"title": "Time Flies: Animating a Still Image With Time-Lapse Video As Reference", "authors": "Chia-Chi Cheng, Hung-Yu Chen, Wei-Chen Chiu"}, {"title": "SER-FIQ: Unsupervised Estimation of Face Image Quality Based on Stochastic Embedding Robustness", "authors": "Philipp Terh\u00f6rst, Jan Niklas Kolf, Naser Damer, Florian Kirchbuchner, Arjan Kuijper", "link": "https://arxiv.org/abs/2003.09373", "summary": "Face image quality is an important factor to enable high performance face\nrecognition systems. Face quality assessment aims at estimating the suitability\nof a face image for recognition. Previous work proposed supervised solutions\nthat require artificially or human labelled quality values. However, both\nlabelling mechanisms are error-prone as they do not rely on a clear definition\nof quality and may not know the best characteristics for the utilized face\nrecognition system. Avoiding the use of inaccurate quality labels, we proposed\na novel concept to measure face quality based on an arbitrary face recognition\nmodel. By determining the embedding variations generated from random\nsubnetworks of a face model, the robustness of a sample representation and\nthus, its quality is estimated. The experiments are conducted in a\ncross-database evaluation setting on three publicly available databases. We\ncompare our proposed solution on two face embeddings against six\nstate-of-the-art approaches from academia and industry. The results show that\nour unsupervised solution outperforms all other approaches in the majority of\nthe investigated scenarios. In contrast to previous works, the proposed\nsolution shows a stable performance over all scenarios. Utilizing the deployed\nface recognition model for our face quality assessment methodology avoids the\ntraining phase completely and further outperforms all baseline approaches by a\nlarge margin. Our solution can be easily integrated into current face\nrecognition systems and can be modified to other tasks beyond face recognition."}, {"title": "Grid-GCN for Fast and Scalable Point Cloud Learning", "authors": "Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, Ulrich Neumann", "link": "https://arxiv.org/abs/1912.02984", "summary": "Due to the sparsity and irregularity of the point cloud data, methods that\ndirectly consume points have become popular. Among all point-based models,\ngraph convolutional networks (GCN) lead to notable performance by fully\npreserving the data granularity and exploiting point interrelation. However,\npoint-based networks spend a significant amount of time on data structuring\n(e.g., Farthest Point Sampling (FPS) and neighbor points querying), which limit\nthe speed and scalability. In this paper, we present a method, named Grid-GCN,\nfor fast and scalable point cloud learning. Grid-GCN uses a novel data\nstructuring strategy, Coverage-Aware Grid Query (CAGQ). By leveraging the\nefficiency of grid space, CAGQ improves spatial coverage while reducing the\ntheoretical time complexity. Compared with popular sampling methods such as\nFarthest Point Sampling (FPS) and Ball Query, CAGQ achieves up to 50X speed-up.\nWith a Grid Context Aggregation (GCA) module, Grid-GCN achieves\nstate-of-the-art performance on major point cloud classification and\nsegmentation benchmarks with significantly faster runtime than previous\nstudies. Remarkably, Grid-GCN achieves the inference speed of 50fps on ScanNet\nusing 81920 points per scene as input."}, {"title": "Domain Balancing: Face Recognition on Long-Tailed Domains", "authors": "Dong Cao, Xiangyu Zhu, Xingyu Huang, Jianzhu Guo, Zhen Lei", "link": "https://arxiv.org/abs/2003.13791", "summary": "Long-tailed problem has been an important topic in face recognition task.\nHowever, existing methods only concentrate on the long-tailed distribution of\nclasses. Differently, we devote to the long-tailed domain distribution problem,\nwhich refers to the fact that a small number of domains frequently appear while\nother domains far less existing. The key challenge of the problem is that\ndomain labels are too complicated (related to race, age, pose, illumination,\netc.) and inaccessible in real applications. In this paper, we propose a novel\nDomain Balancing (DB) mechanism to handle this problem. Specifically, we first\npropose a Domain Frequency Indicator (DFI) to judge whether a sample is from\nhead domains or tail domains. Secondly, we formulate a light-weighted Residual\nBalancing Mapping (RBM) block to balance the domain distribution by adjusting\nthe network according to DFI. Finally, we propose a Domain Balancing Margin\n(DBM) in the loss function to further optimize the feature space of the tail\ndomains to improve generalization. Extensive analysis and experiments on\nseveral face recognition benchmarks demonstrate that the proposed method\neffectively enhances the generalization capacities and achieves superior\nperformance."}, {"title": "AdversarialNAS: Adversarial Neural Architecture Search for GANs", "authors": "Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, Shuicheng Yan", "link": "https://arxiv.org/abs/1912.02037", "summary": "Neural Architecture Search (NAS) that aims to automate the procedure of\narchitecture design has achieved promising results in many computer vision\nfields. In this paper, we propose an AdversarialNAS method specially tailored\nfor Generative Adversarial Networks (GANs) to search for a superior generative\nmodel on the task of unconditional image generation. The AdversarialNAS is the\nfirst method that can search the architectures of generator and discriminator\nsimultaneously in a differentiable manner. During searching, the designed\nadversarial search algorithm does not need to comput any extra metric to\nevaluate the performance of the searched architecture, and the search paradigm\nconsiders the relevance between the two network architectures and improves\ntheir mutual balance. Therefore, AdversarialNAS is very efficient and only\ntakes 1 GPU day to search for a superior generative model in the proposed large\nsearch space ($10^{38}$). Experiments demonstrate the effectiveness and\nsuperiority of our method. The discovered generative model sets a new\nstate-of-the-art FID score of $10.87$ and highly competitive Inception Score of\n$8.74$ on CIFAR-10. Its transferability is also proven by setting new\nstate-of-the-art FID score of $26.98$ and Inception score of $9.63$ on STL-10.\nCode is at: \\url{https://github.com/chengaopro/AdversarialNAS}."}, {"title": "Image Super-Resolution With Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining", "authors": "Yiqun Mei, Yuchen Fan, Yuqian Zhou, Lichao Huang, Thomas S. Huang, Honghui Shi", "link": "http://arxiv.org/abs/2006.01424", "summary": "Deep convolution-based single image super-resolution (SISR) networks embrace\nthe benefits of learning from large-scale external image resources for local\nrecovery, yet most existing works have ignored the long-range feature-wise\nsimilarities in natural images. Some recent works have successfully leveraged\nthis intrinsic feature correlation by exploring non-local attention modules.\nHowever, none of the current deep models have studied another inherent property\nof images: cross-scale feature correlation. In this paper, we propose the first\nCross-Scale Non-Local (CS-NL) attention module with integration into a\nrecurrent neural network. By combining the new CS-NL prior with local and\nin-scale non-local priors in a powerful recurrent fusion cell, we can find more\ncross-scale feature correlations within a single low-resolution (LR) image. The\nperformance of SISR is significantly improved by exhaustively integrating all\npossible priors. Extensive experiments demonstrate the effectiveness of the\nproposed CS-NL module by setting new state-of-the-arts on multiple SISR\nbenchmarks."}, {"title": "The Devil Is in the Details: Delving Into Unbiased Data Processing for Human Pose Estimation", "authors": "Junjie Huang, Zheng Zhu, Feng Guo, Guan Huang", "link": "https://arxiv.org/abs/1911.07524", "summary": "Recently, the leading performance of human pose estimation is dominated by\ntop-down methods. Being a fundamental component in training and inference, data\nprocessing has not been systematically considered in pose estimation community,\nto the best of our knowledge. In this paper, we focus on this problem and find\nthat the devil of top-down pose estimator is in the biased data processing.\nSpecifically, by investigating the standard data processing in state-of-the-art\napproaches mainly including data transformation and encoding-decoding, we find\nthat the results obtained by common flipping strategy are unaligned with the\noriginal ones in inference. Moreover, there is statistical error in standard\nencoding-decoding during both training and inference. Two problems couple\ntogether and significantly degrade the pose estimation performance. Based on\nquantitative analyses, we then formulate a principled way to tackle this\ndilemma. Data is processed based on unit length instead of pixel, and an\noffset-based strategy is adopted to perform encoding-decoding. The Unbiased\nData Processing (UDP) for human pose estimation can be achieved by combining\nthe two together. UDP not only boosts the performance of existing methods by a\nlarge margin but also plays a important role in result reproducing and future\nexploration. As a model-agnostic approach, UDP promotes\nSimpleBaseline-ResNet-50-256x192 by 1.5 AP (70.2 to 71.7) and HRNet-W32-256x192\nby 1.7 AP (73.5 to 75.2) on COCO test-dev set. The HRNet-W48-384x288 equipped\nwith UDP achieves 76.5 AP and sets a new state-of-the-art for human pose\nestimation. The code will be released."}, {"title": "Data Uncertainty Learning in Face Recognition", "authors": "Jie Chang, Zhonghao Lan, Changmao Cheng, Yichen Wei", "link": "https://arxiv.org/abs/2003.11339", "summary": "Modeling data uncertainty is important for noisy images, but seldom explored\nfor face recognition. The pioneer work, PFE, considers uncertainty by modeling\neach face image embedding as a Gaussian distribution. It is quite effective.\nHowever, it uses fixed feature (mean of the Gaussian) from an existing model.\nIt only estimates the variance and relies on an ad-hoc and costly metric. Thus,\nit is not easy to use. It is unclear how uncertainty affects feature learning.\n  This work applies data uncertainty learning to face recognition, such that\nthe feature (mean) and uncertainty (variance) are learnt simultaneously, for\nthe first time. Two learning methods are proposed. They are easy to use and\noutperform existing deterministic methods as well as PFE on challenging\nunconstrained scenarios. We also provide insightful analysis on how\nincorporating uncertainty estimation helps reducing the adverse effects of\nnoisy samples and affects the feature learning."}, {"title": "Regularizing Discriminative Capability of CGANs for Semi-Supervised Generative Learning", "authors": "Yi Liu, Guangchang Deng, Xiangping Zeng, Si Wu, Zhiwen Yu, Hau-San Wong"}, {"title": "FM2u-Net: Face Morphological Multi-Branch Network for Makeup-Invariant Face Verification", "authors": "Wenxuan Wang, Yanwei Fu, Xuelin Qian, Yu-Gang Jiang, Qi Tian, Xiangyang Xue"}, {"title": "UCTGAN: Diverse Image Inpainting Based on Unsupervised Cross-Space Translation", "authors": "Lei Zhao, Qihang Mo, Sihuan Lin, Zhizhong Wang, Zhiwen Zuo, Haibo Chen, Wei Xing, Dongming Lu"}, {"title": "Decoupled Representation Learning for Skeleton-Based Gesture Recognition", "authors": "Jianbo Liu, Yongcheng Liu, Ying Wang, V\u00e9ronique Prinet, Shiming Xiang, Chunhong Pan"}, {"title": "An Efficient PointLSTM for Point Clouds Based Gesture Recognition", "authors": "Yuecong Min, Yanxiao Zhang, Xiujuan Chai, Xilin Chen"}, {"title": "Editing in Style: Uncovering the Local Semantics of GANs", "authors": "Edo Collins, Raja Bala, Bob Price, Sabine S\u00fcsstrunk", "link": "http://arxiv.org/abs/2004.14367", "summary": "While the quality of GAN image synthesis has improved tremendously in recent\nyears, our ability to control and condition the output is still limited.\nFocusing on StyleGAN, we introduce a simple and effective method for making\nlocal, semantically-aware edits to a target output image. This is accomplished\nby borrowing elements from a source image, also a GAN output, via a novel\nmanipulation of style vectors. Our method requires neither supervision from an\nexternal model, nor involves complex spatial morphing operations. Instead, it\nrelies on the emergent disentanglement of semantic objects that is learned by\nStyleGAN during its training. Semantic editing is demonstrated on GANs\nproducing human faces, indoor scenes, cats, and cars. We measure the locality\nand photorealism of the edits produced by our method, and find that it\naccomplishes both."}, {"title": "On the Detection of Digital Face Manipulation", "authors": "Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, Anil K. Jain", "link": "https://arxiv.org/abs/1910.01717", "summary": "Detecting manipulated facial images and videos is an increasingly important\ntopic in digital media forensics. As advanced face synthesis and manipulation\nmethods are made available, new types of fake face representations are being\ncreated which have raised significant concerns for their use in social media.\nHence, it is crucial to detect manipulated face images and localize manipulated\nregions. Instead of simply using multi-task learning to simultaneously detect\nmanipulated images and predict the manipulated mask (regions), we propose to\nutilize an attention mechanism to process and improve the feature maps for the\nclassification task. The learned attention maps highlight the informative\nregions to further improve the binary classification (genuine face v. fake\nface), and also visualize the manipulated regions. To enable our study of\nmanipulated face detection and localization, we collect a large-scale database\nthat contains numerous types of facial forgeries. With this dataset, we perform\na thorough analysis of data-driven fake face detection. We show that the use of\nan attention mechanism improves facial forgery detection and manipulated region\nlocalization."}, {"title": "Learning Texture Transformer Network for Image Super-Resolution", "authors": "Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, Baining Guo"}, {"title": "Reference-Based Sketch Image Colorization Using Augmented-Self Reference and Dense Semantic Correspondence", "authors": "Junsoo Lee, Eungyeup Kim, Yunsung Lee, Dongjun Kim, Jaehyuk Chang, Jaegul Choo", "link": "https://arxiv.org/abs/2005.05207", "summary": "This paper tackles the automatic colorization task of a sketch image given an\nalready-colored reference image. Colorizing a sketch image is in high demand in\ncomics, animation, and other content creation applications, but it suffers from\ninformation scarcity of a sketch image. To address this, a reference image can\nrender the colorization process in a reliable and user-driven manner. However,\nit is difficult to prepare for a training data set that has a sufficient amount\nof semantically meaningful pairs of images as well as the ground truth for a\ncolored image reflecting a given reference (e.g., coloring a sketch of an\noriginally blue car given a reference green car). To tackle this challenge, we\npropose to utilize the identical image with geometric distortion as a virtual\nreference, which makes it possible to secure the ground truth for a colored\noutput image. Furthermore, it naturally provides the ground truth for dense\nsemantic correspondence, which we utilize in our internal attention mechanism\nfor color transfer from reference to sketch input. We demonstrate the\neffectiveness of our approach in various types of sketch image colorization via\nquantitative as well as qualitative evaluation against existing methods."}, {"title": "Deblurring Using Analysis-Synthesis Networks Pair", "authors": "Adam Kaufman, Raanan Fattal", "link": "https://arxiv.org/abs/2004.02956", "summary": "Blind image deblurring remains a challenging problem for modern artificial\nneural networks. Unlike other image restoration problems, deblurring networks\nfail behind the performance of existing deblurring algorithms in case of\nuniform and 3D blur models. This follows from the diverse and profound effect\nthat the unknown blur-kernel has on the deblurring operator.\n  We propose a new architecture which breaks the deblurring network into an\nanalysis network which estimates the blur, and a synthesis network that uses\nthis kernel to deblur the image. Unlike existing deblurring networks, this\ndesign allows us to explicitly incorporate the blur-kernel in the network's\ntraining. In addition, we introduce new cross-correlation layers that allow\nbetter blur estimations, as well as unique components that allow the estimate\nblur to control the action of the synthesis deblurring action.\n  Evaluating the new approach over established benchmark datasets shows its\nability to achieve state-of-the-art deblurring accuracy on various tests, as\nwell as offer a major speedup in runtime."}, {"title": "Exploring Unlabeled Faces for Novel Attribute Discovery", "authors": "Hyojin Bahng, Sunghyo Chung, Seungjoo Yoo, Jaegul Choo", "link": "https://arxiv.org/abs/1912.03085", "summary": "Despite remarkable success in unpaired image-to-image translation, existing\nsystems still require a large amount of labeled images. This is a bottleneck\nfor their real-world applications; in practice, a model trained on labeled\nCelebA dataset does not work well for test images from a different distribution\n-- greatly limiting their application to unlabeled images of a much larger\nquantity. In this paper, we attempt to alleviate this necessity for labeled\ndata in the facial image translation domain. We aim to explore the degree to\nwhich you can discover novel attributes from unlabeled faces and perform\nhigh-quality translation. To this end, we use prior knowledge about the visual\nworld as guidance to discover novel attributes and transfer them via a novel\nnormalization method. Experiments show that our method trained on unlabeled\ndata produces high-quality translations, preserves identity, and be\nperceptually realistic as good as, or better than, state-of-the-art methods\ntrained on labeled data."}, {"title": "Neural Pose Transfer by Spatially Adaptive Instance Normalization", "authors": "Jiashun Wang, Chao Wen, Yanwei Fu, Haitao Lin, Tianyun Zou, Xiangyang Xue, Yinda Zhang", "link": "https://arxiv.org/abs/2003.07254", "summary": "Pose transfer has been studied for decades, in which the pose of a source\nmesh is applied to a target mesh. Particularly in this paper, we are interested\nin transferring the pose of source human mesh to deform the target human mesh,\nwhile the source and target meshes may have different identity information.\nTraditional studies assume that the paired source and target meshes are existed\nwith the point-wise correspondences of user annotated landmarks/mesh points,\nwhich requires heavy labelling efforts. On the other hand, the generalization\nability of deep models is limited, when the source and target meshes have\ndifferent identities. To break this limitation, we proposes the first neural\npose transfer model that solves the pose transfer via the latest technique for\nimage style transfer, leveraging the newly proposed component -- spatially\nadaptive instance normalization. Our model does not require any correspondences\nbetween the source and target meshes. Extensive experiments show that the\nproposed model can effectively transfer deformation from source to target\nmeshes, and has good generalization ability to deal with unseen identities or\nposes of meshes. Code is available at\nhttps://github.com/jiashunwang/Neural-Pose-Transfer ."}, {"title": "Fine-Grained Image-to-Image Transformation Towards Visual Recognition", "authors": "Wei Xiong, Yutong He, Yixuan Zhang, Wenhan Luo, Lin Ma, Jiebo Luo", "link": "http://arxiv.org/abs/2001.03856", "summary": "Existing image-to-image transformation approaches primarily focus on\nsynthesizing visually pleasing data. Generating images with correct identity\nlabels is challenging yet much less explored. It is even more challenging to\ndeal with image transformation tasks with large deformation in poses,\nviewpoints or scales while preserving the identity, such as face rotation and\nobject viewpoint morphing. In this paper, we aim at transforming an image with\na fine-grained category to synthesize new images that preserve the identity of\nthe input image, which can thereby benefit the subsequent fine-grained image\nrecognition and few-shot learning tasks. The generated images, transformed with\nlarge geometric deformation, do not necessarily need to be of high visual\nquality, but are required to maintain as much identity information as possible.\nTo this end, we adopt a model based on generative adversarial networks to\ndisentangle the identity related and unrelated factors of an image. In order to\npreserve the fine-grained contextual details of the input image during the\ndeformable transformation, a constrained nonalignment connection method is\nproposed to construct learnable highways between intermediate convolution\nblocks in the generator. Moreover, an adaptive identity modulation mechanism is\nproposed to effectively transfer the identity information into the output\nimage. Extensive experiments on the CompCars and Multi-PIE datasets demonstrate\nthat our model preserves the identity of the generated images much better than\nthe state-of-the-art image-to-image transformation models, and as a result\nsignificantly boosts the visual recognition performance in fine-grained\nfew-shot learning."}, {"title": "Deep Facial Non-Rigid Multi-View Stereo", "authors": "Ziqian Bai, Zhaopeng Cui, Jamal Ahmed Rahim, Xiaoming Liu, Ping Tan"}, {"title": "Attention-Driven Cropping for Very High Resolution Facial Landmark Detection", "authors": "Prashanth Chandran, Derek Bradley, Markus Gross, Thabo Beeler"}, {"title": "Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis", "authors": "Yiyi Liao, Katja Schwarz, Lars Mescheder, Andreas Geiger", "link": "https://arxiv.org/abs/1912.05237", "summary": "In recent years, Generative Adversarial Networks have achieved impressive\nresults in photorealistic image synthesis. This progress nurtures hopes that\none day the classical rendering pipeline can be replaced by efficient models\nthat are learned directly from images. However, current image synthesis models\noperate in the 2D domain where disentangling 3D properties such as camera\nviewpoint or object pose is challenging. Furthermore, they lack an\ninterpretable and controllable representation. Our key hypothesis is that the\nimage generation process should be modeled in 3D space as the physical world\nsurrounding us is intrinsically three-dimensional. We define the new task of 3D\ncontrollable image synthesis and propose an approach for solving it by\nreasoning both in 3D space and in the 2D image domain. We demonstrate that our\nmodel is able to disentangle latent 3D factors of simple multi-object scenes in\nan unsupervised fashion from raw images. Compared to pure 2D baselines, it\nallows for synthesizing scenes that are consistent wrt. changes in viewpoint or\nobject pose. We further evaluate various 3D representations in terms of their\nusefulness for this challenging task."}, {"title": "End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection", "authors": "Rui Qian, Divyansh Garg, Yan Wang, Yurong You, Serge Belongie, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger, Wei-Lun Chao", "link": "", "summary": ""}, {"title": "Towards High-Fidelity 3D Face Reconstruction From In-the-Wild Images Using Graph Convolutional Networks", "authors": "Jiangke Lin, Yi Yuan, Tianjia Shao, Kun Zhou", "link": "https://arxiv.org/abs/2003.05653", "summary": "3D Morphable Model (3DMM) based methods have achieved great success in\nrecovering 3D face shapes from single-view images. However, the facial textures\nrecovered by such methods lack the fidelity as exhibited in the input images.\nRecent work demonstrates high-quality facial texture recovering with generative\nnetworks trained from a large-scale database of high-resolution UV maps of face\ntextures, which is hard to prepare and not publicly available. In this paper,\nwe introduce a method to reconstruct 3D facial shapes with high-fidelity\ntextures from single-view images in-the-wild, without the need to capture a\nlarge-scale face texture database. The main idea is to refine the initial\ntexture generated by a 3DMM based method with facial details from the input\nimage. To this end, we propose to use graph convolutional networks to\nreconstruct the detailed colors for the mesh vertices instead of reconstructing\nthe UV map. Experiments show that our method can generate high-quality results\nand outperforms state-of-the-art methods in both qualitative and quantitative\ncomparisons."}, {"title": "CurricularFace: Adaptive Curriculum Learning Loss for Deep Face Recognition", "authors": "Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, Feiyue Huang", "link": "https://arxiv.org/abs/2004.00288", "summary": "As an emerging topic in face recognition, designing margin-based loss\nfunctions can increase the feature margin between different classes for\nenhanced discriminability. More recently, the idea of mining-based strategies\nis adopted to emphasize the misclassified samples, achieving promising results.\nHowever, during the entire training process, the prior methods either do not\nexplicitly emphasize the sample based on its importance that renders the hard\nsamples not fully exploited; or explicitly emphasize the effects of\nsemi-hard/hard samples even at the early training stage that may lead to\nconvergence issue. In this work, we propose a novel Adaptive Curriculum\nLearning loss (CurricularFace) that embeds the idea of curriculum learning into\nthe loss function to achieve a novel training strategy for deep face\nrecognition, which mainly addresses easy samples in the early training stage\nand hard ones in the later stage. Specifically, our CurricularFace adaptively\nadjusts the relative importance of easy and hard samples during different\ntraining stages. In each stage, different samples are assigned with different\nimportance according to their corresponding difficultness. Extensive\nexperimental results on popular benchmarks demonstrate the superiority of our\nCurricularFace over the state-of-the-art competitors."}, {"title": "Rotate-and-Render: Unsupervised Photorealistic Face Rotation From Single-View Images", "authors": "Hang Zhou, Jihao Liu, Ziwei Liu, Yu Liu, Xiaogang Wang", "link": "https://arxiv.org/abs/2003.08124", "summary": "Though face rotation has achieved rapid progress in recent years, the lack of\nhigh-quality paired training data remains a great hurdle for existing methods.\nThe current generative models heavily rely on datasets with multi-view images\nof the same person. Thus, their generated results are restricted by the scale\nand domain of the data source. To overcome these challenges, we propose a novel\nunsupervised framework that can synthesize photo-realistic rotated faces using\nonly single-view image collections in the wild. Our key insight is that\nrotating faces in the 3D space back and forth, and re-rendering them to the 2D\nplane can serve as a strong self-supervision. We leverage the recent advances\nin 3D face modeling and high-resolution GAN to constitute our building blocks.\nSince the 3D rotation-and-render on faces can be applied to arbitrary angles\nwithout losing details, our approach is extremely suitable for in-the-wild\nscenarios (i.e. no paired data are available), where existing methods fall\nshort. Extensive experiments demonstrate that our approach has superior\nsynthesis quality as well as identity preservation over the state-of-the-art\nmethods, across a wide range of poses and domains. Furthermore, we validate\nthat our rotate-and-render framework naturally can act as an effective data\naugmentation engine for boosting modern face recognition systems even on strong\nbaseline models."}, {"title": "One-Shot Domain Adaptation for Face Generation", "authors": "Chao Yang, Ser-Nam Lim", "link": "https://arxiv.org/abs/2003.12869", "summary": "In this paper, we propose a framework capable of generating face images that\nfall into the same distribution as that of a given one-shot example. We\nleverage a pre-trained StyleGAN model that already learned the generic face\ndistribution. Given the one-shot target, we develop an iterative optimization\nscheme that rapidly adapts the weights of the model to shift the output's\nhigh-level distribution to the target's. To generate images of the same\ndistribution, we introduce a style-mixing technique that transfers the\nlow-level statistics from the target to faces randomly generated with the\nmodel. With that, we are able to generate an unlimited number of faces that\ninherit from the distribution of both generic human faces and the one-shot\nexample. The newly generated faces can serve as augmented training data for\nother downstream tasks. Such setting is appealing as it requires labeling very\nfew, or even one example, in the target domain, which is often the case of\nreal-world face manipulations that result from a variety of unknown and unique\ndistributions, each with extremely low prevalence. We show the effectiveness of\nour one-shot approach for detecting face manipulations and compare it with\nother few-shot domain adaptation methods qualitatively and quantitatively."}, {"title": "BidNet: Binocular Image Dehazing Without Explicit Disparity Estimation", "authors": "Yanwei Pang, Jing Nie, Jin Xie, Jungong Han, Xuelong Li"}, {"title": "Deep Shutter Unrolling Network", "authors": "Peidong Liu, Zhaopeng Cui, Viktor Larsson, Marc Pollefeys"}, {"title": "Joint Texture and Geometry Optimization for RGB-D Reconstruction", "authors": "Yanping Fu, Qingan Yan, Jie Liao, Chunxia Xiao"}, {"title": "Deep 3D Capture: Geometry and Reflectance From Sparse Multi-View Images", "authors": "Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, Ravi Ramamoorthi", "link": "https://arxiv.org/abs/2003.12642", "summary": "We introduce a novel learning-based method to reconstruct the high-quality\ngeometry and complex, spatially-varying BRDF of an arbitrary object from a\nsparse set of only six images captured by wide-baseline cameras under\ncollocated point lighting. We first estimate per-view depth maps using a deep\nmulti-view stereo network; these depth maps are used to coarsely align the\ndifferent views. We propose a novel multi-view reflectance estimation network\narchitecture that is trained to pool features from these coarsely aligned\nimages and predict per-view spatially-varying diffuse albedo, surface normals,\nspecular roughness and specular albedo. We do this by jointly optimizing the\nlatent space of our multi-view reflectance network to minimize the photometric\nerror between images rendered with our predictions and the input images. While\nprevious state-of-the-art methods fail on such sparse acquisition setups, we\ndemonstrate, via extensive experiments on synthetic and real data, that our\nmethod produces high-quality reconstructions that can be used to render\nphotorealistic images."}, {"title": "Auto-Tuning Structured Light by Optical Stochastic Gradient Descent", "authors": "Wenzheng Chen, Parsa Mirdehghan, Sanja Fidler, Kiriakos N. Kutulakos"}, {"title": "MARMVS: Matching Ambiguity Reduced Multiple View Stereo for Efficient Large Scale Scene Reconstruction", "authors": "Zhenyu Xu, Yiguang Liu, Xuelei Shi, Ying Wang, Yunan Zheng"}, {"title": "Uncertainty Based Camera Model Selection", "authors": "Michal Polic, Stanislav Steidl, Cenek Albl, Zuzana Kukelova, Tomas Pajdla"}, {"title": "Local Implicit Grid Representations for 3D Scenes", "authors": "Chiyu \"Max\" Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nie\u00dfner, Thomas Funkhouser", "link": "https://arxiv.org/abs/2003.08981", "summary": "Shape priors learned from data are commonly used to reconstruct 3D objects\nfrom partial or noisy data. Yet no such shape priors are available for indoor\nscenes, since typical 3D autoencoders cannot handle their scale, complexity, or\ndiversity. In this paper, we introduce Local Implicit Grid Representations, a\nnew 3D shape representation designed for scalability and generality. The\nmotivating idea is that most 3D surfaces share geometric details at some scale\n-- i.e., at a scale smaller than an entire object and larger than a small\npatch. We train an autoencoder to learn an embedding of local crops of 3D\nshapes at that size. Then, we use the decoder as a component in a shape\noptimization that solves for a set of latent codes on a regular grid of\noverlapping crops such that an interpolation of the decoded local shapes\nmatches a partial or noisy observation. We demonstrate the value of this\nproposed approach for 3D surface reconstruction from sparse point observations,\nshowing significantly better results than alternative approaches."}, {"title": "TetraTSDF: 3D Human Reconstruction From a Single Image With a Tetrahedral Outer Shell", "authors": "Hayato Onizuka, Zehra Hayirci, Diego Thomas, Akihiro Sugimoto, Hideaki Uchiyama, Rin-ichiro Taniguchi", "link": "https://arxiv.org/abs/2004.10534", "summary": "Recovering the 3D shape of a person from its 2D appearance is ill-posed due\nto ambiguities. Nevertheless, with the help of convolutional neural networks\n(CNN) and prior knowledge on the 3D human body, it is possible to overcome such\nambiguities to recover detailed 3D shapes of human bodies from single images.\nCurrent solutions, however, fail to reconstruct all the details of a person\nwearing loose clothes. This is because of either (a) huge memory requirement\nthat cannot be maintained even on modern GPUs or (b) the compact 3D\nrepresentation that cannot encode all the details. In this paper, we propose\nthe tetrahedral outer shell volumetric truncated signed distance function\n(TetraTSDF) model for the human body, and its corresponding part connection\nnetwork (PCN) for 3D human body shape regression. Our proposed model is\ncompact, dense, accurate, and yet well suited for CNN-based regression task.\nOur proposed PCN allows us to learn the distribution of the TSDF in the\ntetrahedral volume from a single image in an end-to-end manner. Results show\nthat our proposed method allows to reconstruct detailed shapes of humans\nwearing loose clothes from single RGB images."}, {"title": "Averaging Essential and Fundamental Matrices in Collinear Camera Settings", "authors": "Amnon Geifman, Yoni Kasten, Meirav Galun, Ronen Basri", "link": "https://arxiv.org/abs/1912.00254", "summary": "Global methods to Structure from Motion have gained popularity in recent\nyears. A significant drawback of global methods is their sensitivity to\ncollinear camera settings. In this paper, we introduce an analysis and\nalgorithms for averaging bifocal tensors (essential or fundamental matrices)\nwhen either subsets or all of the camera centers are collinear.\n  We provide a complete spectral characterization of bifocal tensors in\ncollinear scenarios and further propose two averaging algorithms. The first\nalgorithm uses rank constrained minimization to recover camera matrices in\nfully collinear settings. The second algorithm enriches the set of possibly\nmixed collinear and non-collinear cameras with additional, \"virtual cameras,\"\nwhich are placed in general position, enabling the application of existing\naveraging methods to the enriched set of bifocal tensors. Our algorithms are\nshown to achieve state of the art results on various benchmarks that include\nautonomous car datasets and unordered image collections in both calibrated and\nunclibrated settings."}, {"title": "On the Distribution of Minima in Intrinsic-Metric Rotation Averaging", "authors": "Kyle Wilson, David Bindel", "link": "https://arxiv.org/abs/2003.08310", "summary": "Rotation Averaging is a non-convex optimization problem that determines\norientations of a collection of cameras from their images of a 3D scene. The\nproblem has been studied using a variety of distances and robustifiers. The\nintrinsic (or geodesic) distance on SO(3) is geometrically meaningful; but\nwhile some extrinsic distance-based solvers admit (conditional) guarantees of\ncorrectness, no comparable results have been found under the intrinsic metric.\n  In this paper, we study the spatial distribution of local minima. First, we\ndo a novel empirical study to demonstrate sharp transitions in qualitative\nbehavior: as problems become noisier, they transition from a single\n(easy-to-find) dominant minimum to a cost surface filled with minima. In the\nsecond part of this paper we derive a theoretical bound for when this\ntransition occurs. This is an extension of the results of [24], which used\nlocal convexity as a proxy to study the difficulty of problem. By recognizing\nthe underlying quotient manifold geometry of the problem we achieve an n-fold\nimprovement over prior work. Incidentally, our analysis also extends the prior\n$l_2$ work to general $l_p$ costs. Our results suggest using algebraic\nconnectivity as an indicator of problem difficulty."}, {"title": "Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled Representation", "authors": "Edoardo Remelli, Shangchen Han, Sina Honari, Pascal Fua, Robert Wang", "link": "https://arxiv.org/abs/2004.02186", "summary": "We present a lightweight solution to recover 3D pose from multi-view images\ncaptured with spatially calibrated cameras. Building upon recent advances in\ninterpretable representation learning, we exploit 3D geometry to fuse input\nimages into a unified latent representation of pose, which is disentangled from\ncamera view-points. This allows us to reason effectively about 3D pose across\ndifferent views without using compute-intensive volumetric grids. Our\narchitecture then conditions the learned representation on camera projection\noperators to produce accurate per-view 2d detections, that can be simply lifted\nto 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do\nit efficiently, we propose a novel implementation of DLT that is orders of\nmagnitude faster on GPU architectures than standard SVD-based triangulation\nmethods. We evaluate our approach on two large-scale human pose datasets (H36M\nand Total Capture): our method outperforms or performs comparably to the\nstate-of-the-art volumetric methods, while, unlike them, yielding real-time\nperformance."}, {"title": "A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-View Stereo Reconstruction From an Open Aerial Dataset", "authors": "Jin Liu, Shunping Ji", "link": "https://arxiv.org/abs/2003.00637", "summary": "A great deal of research has demonstrated recently that multi-view stereo\n(MVS) matching can be solved with deep learning methods. However, these efforts\nwere focused on close-range objects and only a very few of the deep\nlearning-based methods were specifically designed for large-scale 3D urban\nreconstruction due to the lack of multi-view aerial image benchmarks. In this\npaper, we present a synthetic aerial dataset, called the WHU dataset, we\ncreated for MVS tasks, which, to our knowledge, is the first large-scale\nmulti-view aerial dataset. It was generated from a highly accurate 3D digital\nsurface model produced from thousands of real aerial images with precise camera\nparameters. We also introduce in this paper a novel network, called RED-Net,\nfor wide-range depth inference, which we developed from a recurrent\nencoder-decoder structure to regularize cost maps across depths and a 2D fully\nconvolutional network as framework. RED-Net's low memory requirements and high\nperformance make it suitable for large-scale and highly accurate 3D Earth\nsurface reconstruction. Our experiments confirmed that not only did our method\nexceed the current state-of-the-art MVS methods by more than 50% mean absolute\nerror (MAE) with less memory and computational cost, but its efficiency as\nwell. It outperformed one of the best commercial software programs based on\nconventional methods, improving their efficiency 16 times over. Moreover, we\nproved that our RED-Net model pre-trained on the synthetic WHU dataset can be\nefficiently transferred to very different multi-view aerial image datasets\nwithout any fine-tuning. Dataset are available at http://gpcv.whu.edu.cn/data."}, {"title": "Factorized Higher-Order CNNs With an Application to Spatio-Temporal Emotion Estimation", "authors": "Jean Kossaifi, Antoine Toisoul, Adrian Bulat, Yannis Panagakis, Timothy M. Hospedales, Maja Pantic", "link": "https://arxiv.org/abs/1906.06196", "summary": "Training deep neural networks with spatio-temporal (i.e., 3D) or\nmultidimensional convolutions of higher-order is computationally challenging\ndue to millions of unknown parameters across dozens of layers. To alleviate\nthis, one approach is to apply low-rank tensor decompositions to convolution\nkernels in order to compress the network and reduce its number of parameters.\nAlternatively, new convolutional blocks, such as MobileNet, can be directly\ndesigned for efficiency. In this paper, we unify these two approaches by\nproposing a tensor factorization framework for efficient multidimensional\n(separable) convolutions of higher-order. Interestingly, the proposed framework\nenables a novel higher-order transduction, allowing to train a network on a\ngiven domain (e.g., 2D images or N-dimensional data in general) and using\ntransduction to generalize to higher-order data such as videos (or\n(N+K)-dimensional data in general), capturing for instance temporal dynamics\nwhile preserving the learnt spatial information.\n  We apply the proposed methodology, coined CP-Higher-Order Convolution\n(HO-CPConv), to spatio-temporal facial emotion analysis. Most existing facial\naffect models focus on static imagery and discard all temporal information.\nThis is due to the above-mentioned burden of training 3D convolutional nets and\nthe lack of large bodies of video data annotated by experts. We address both\nissues with our proposed framework. Initial training is first done on static\nimagery before using transduction to generalize to the temporal domain. We\ndemonstrate superior performance on three challenging large scale affect\nestimation datasets, AffectNet, SEWA, and AFEW-VA."}, {"title": "Effectively Unbiased FID and Inception Score and Where to Find Them", "authors": "Min Jin Chong, David Forsyth", "link": "https://arxiv.org/abs/1911.07023", "summary": "This paper shows that two commonly used evaluation metrics for generative\nmodels, the Fr\\'echet Inception Distance (FID) and the Inception Score (IS),\nare biased -- the expected value of the score computed for a finite sample set\nis not the true value of the score. Worse, the paper shows that the bias term\ndepends on the particular model being evaluated, so model A may get a better\nscore than model B simply because model A's bias term is smaller. This effect\ncannot be fixed by evaluating at a fixed number of samples. This means all\ncomparisons using FID or IS as currently computed are unreliable.\n  We then show how to extrapolate the score to obtain an effectively bias-free\nestimate of scores computed with an infinite number of samples, which we term\n$\\overline{\\textrm{FID}}_\\infty$ and $\\overline{\\textrm{IS}}_\\infty$. In turn,\nthis effectively bias-free estimate requires good estimates of scores with a\nfinite number of samples. We show that using Quasi-Monte Carlo integration\nnotably improves estimates of FID and IS for finite sample sets. Our\nextrapolated scores are simple, drop-in replacements for the finite sample\nscores. Additionally, we show that using low discrepancy sequence in GAN\ntraining offers small improvements in the resulting generator."}, {"title": "Robust Homography Estimation via Dual Principal Component Pursuit", "authors": "Tianjiao Ding, Yunchen Yang, Zhihui Zhu, Daniel P. Robinson, Ren\u00e9 Vidal, Laurent Kneip, Manolis C. Tsakiris"}, {"title": "Non-Adversarial Video Synthesis With Learned Priors", "authors": "Abhishek Aich, Akash Gupta, Rameswar Panda, Rakib Hyder, M. Salman Asif, Amit K. Roy-Chowdhury"}, {"title": "Uncertainty-Aware Mesh Decoder for High Fidelity 3D Face Reconstruction", "authors": "Gun-Hee Lee, Seong-Whan Lee"}, {"title": "3FabRec: Fast Few-Shot Face Alignment by Reconstruction", "authors": "Bj\u00f6rn Browatzki, Christian Wallraven", "link": "https://arxiv.org/abs/1911.10448", "summary": "Current supervised methods for facial landmark detection require a large\namount of training data and may suffer from overfitting to specific datasets\ndue to the massive number of parameters. We introduce a semi-supervised method\nin which the crucial idea is to first generate implicit face knowledge from the\nlarge amounts of unlabeled images of faces available today. In a first,\ncompletely unsupervised stage, we train an adversarial autoencoder to\nreconstruct faces via a low-dimensional face embedding. In a second, supervised\nstage, we interleave the decoder with transfer layers to retask the generation\nof color images to the prediction of landmark heatmaps. Our framework (3FabRec)\nachieves state-of-the-art performance on several common benchmarks and, most\nimportantly, is able to maintain impressive accuracy on extremely small\ntraining sets down to as few as 10 images. As the interleaved layers only add a\nlow amount of parameters to the decoder, inference runs at several hundred FPS\non a GPU."}, {"title": "Weakly-Supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects", "authors": "Seungryul Baek, Kwang In Kim, Tae-Kyun Kim"}, {"title": "Vec2Face: Unveil Human Faces From Their Blackbox Features in Face Recognition", "authors": "Chi Nhan Duong, Thanh-Dat Truong, Khoa Luu, Kha Gia Quach, Hung Bui, Kaushik Roy", "link": "https://arxiv.org/abs/2003.06958", "summary": "Unveiling face images of a subject given his/her high-level representations\nextracted from a blackbox Face Recognition engine is extremely challenging. It\nis because the limitations of accessible information from that engine including\nits structure and uninterpretable extracted features. This paper presents a\nnovel generative structure with Bijective Metric Learning, namely Bijective\nGenerative Adversarial Networks in a Distillation framework (DiBiGAN), for\nsynthesizing faces of an identity given that person's features. In order to\neffectively address this problem, this work firstly introduces a bijective\nmetric so that the distance measurement and metric learning process can be\ndirectly adopted in image domain for an image reconstruction task. Secondly, a\ndistillation process is introduced to maximize the information exploited from\nthe blackbox face recognition engine. Then a Feature-Conditional Generator\nStructure with Exponential Weighting Strategy is presented for a more robust\ngenerator that can synthesize realistic faces with ID preservation. Results on\nseveral benchmarking datasets including CelebA, LFW, AgeDB, CFP-FP against\nmatching engines have demonstrated the effectiveness of DiBiGAN on both image\nrealism and ID preservation properties."}, {"title": "StyleRig: Rigging StyleGAN for 3D Control Over Portrait Images", "authors": "Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick P\u00e9rez, Michael Zollh\u00f6fer, Christian Theobalt", "link": "https://arxiv.org/abs/2004.00121", "summary": "StyleGAN generates photorealistic portrait images of faces with eyes, teeth,\nhair and context (neck, shoulders, background), but lacks a rig-like control\nover semantic face parameters that are interpretable in 3D, such as face pose,\nexpressions, and scene illumination. Three-dimensional morphable face models\n(3DMMs) on the other hand offer control over the semantic parameters, but lack\nphotorealism when rendered and only model the face interior, not other parts of\na portrait image (hair, mouth interior, background). We present the first\nmethod to provide a face rig-like control over a pretrained and fixed StyleGAN\nvia a 3DMM. A new rigging network, RigNet is trained between the 3DMM's\nsemantic parameters and StyleGAN's input. The network is trained in a\nself-supervised manner, without the need for manual annotations. At test time,\nour method generates portrait images with the photorealism of StyleGAN and\nprovides explicit control over the 3D semantic parameters of the face."}, {"title": "Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis", "authors": "Jogendra Nath Kundu, Siddharth Seth, Varun Jampani, Mugalodi Rakesh, R. Venkatesh Babu, Anirban Chakraborty", "link": "https://arxiv.org/abs/2004.04400", "summary": "Camera captured human pose is an outcome of several sources of variation.\nPerformance of supervised 3D pose estimation approaches comes at the cost of\ndispensing with variations, such as shape and appearance, that may be useful\nfor solving other related tasks. As a result, the learned model not only\ninculcates task-bias but also dataset-bias because of its strong reliance on\nthe annotated samples, which also holds true for weakly-supervised models.\nAcknowledging this, we propose a self-supervised learning framework to\ndisentangle such variations from unlabeled video frames. We leverage the prior\nknowledge on human skeleton and poses in the form of a single part-based 2D\npuppet model, human pose articulation constraints, and a set of unpaired 3D\nposes. Our differentiable formalization, bridging the representation gap\nbetween the 3D pose and spatial part maps, not only facilitates discovery of\ninterpretable pose disentanglement but also allows us to operate on videos with\ndiverse camera movements. Qualitative results on unseen in-the-wild datasets\nestablish our superior generalization across multiple tasks beyond the primary\ntasks of 3D pose estimation and part segmentation. Furthermore, we demonstrate\nstate-of-the-art weakly-supervised 3D pose estimation performance on both\nHuman3.6M and MPI-INF-3DHP datasets."}, {"title": "Learning Meta Face Recognition in Unseen Domains", "authors": "Jianzhu Guo, Xiangyu Zhu, Chenxu Zhao, Dong Cao, Zhen Lei, Stan Z. Li", "link": "http://arxiv.org/abs/2003.07733", "summary": "Face recognition systems are usually faced with unseen domains in real-world\napplications and show unsatisfactory performance due to their poor\ngeneralization. For example, a well-trained model on webface data cannot deal\nwith the ID vs. Spot task in surveillance scenario. In this paper, we aim to\nlearn a generalized model that can directly handle new unseen domains without\nany model updating. To this end, we propose a novel face recognition method via\nmeta-learning named Meta Face Recognition (MFR). MFR synthesizes the\nsource/target domain shift with a meta-optimization objective, which requires\nthe model to learn effective representations not only on synthesized source\ndomains but also on synthesized target domains. Specifically, we build\ndomain-shift batches through a domain-level sampling strategy and get\nback-propagated gradients/meta-gradients on synthesized source/target domains\nby optimizing multi-domain distributions. The gradients and meta-gradients are\nfurther combined to update the model to improve generalization. Besides, we\npropose two benchmarks for generalized face recognition evaluation. Experiments\non our benchmarks validate the generalization of our method compared to several\nbaselines and other state-of-the-arts. The proposed benchmarks will be\navailable at https://github.com/cleardusk/MFR."}, {"title": "Cascaded Deep Monocular 3D Human Pose Estimation With Evolutionary Training Data", "authors": "Shichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung Tang, Kwang-Ting Cheng", "link": "", "summary": ""}, {"title": "GHUM & GHUML: Generative 3D Human Shape and Articulated Pose Models", "authors": "Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T. Freeman, Rahul Sukthankar, Cristian Sminchisescu"}, {"title": "Generating 3D People in Scenes Without People", "authors": "Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J. Black, Siyu Tang", "link": "https://arxiv.org/abs/1912.02923", "summary": "We present a fully automatic system that takes a 3D scene and generates\nplausible 3D human bodies that are posed naturally in that 3D scene. Given a 3D\nscene without people, humans can easily imagine how people could interact with\nthe scene and the objects in it. However, this is a challenging task for a\ncomputer as solving it requires that (1) the generated human bodies to be\nsemantically plausible within the 3D environment (e.g. people sitting on the\nsofa or cooking near the stove), and (2) the generated human-scene interaction\nto be physically feasible such that the human body and scene do not\ninterpenetrate while, at the same time, body-scene contact supports physical\ninteractions. To that end, we make use of the surface-based 3D human model\nSMPL-X. We first train a conditional variational autoencoder to predict\nsemantically plausible 3D human poses conditioned on latent scene\nrepresentations, then we further refine the generated 3D bodies using scene\nconstraints to enforce feasible physical interaction. We show that our approach\nis able to synthesize realistic and expressive 3D human bodies that naturally\ninteract with 3D environment. We perform extensive experiments demonstrating\nthat our generative framework compares favorably with existing methods, both\nqualitatively and quantitatively. We believe that our scene-conditioned 3D\nhuman generation pipeline will be useful for numerous applications; e.g. to\ngenerate training data for human pose estimation, in video games and in VR/AR.\nOur project page for data and code can be seen at:\n\\url{https://vlg.inf.ethz.ch/projects/PSI/}."}, {"title": "Transferring Cross-Domain Knowledge for Video Sign Language Recognition", "authors": "Dongxu Li, Xin Yu, Chenchen Xu, Lars Petersson, Hongdong Li", "link": "https://arxiv.org/abs/2003.03703", "summary": "Word-level sign language recognition (WSLR) is a fundamental task in sign\nlanguage interpretation. It requires models to recognize isolated sign words\nfrom videos. However, annotating WSLR data needs expert knowledge, thus\nlimiting WSLR dataset acquisition. On the contrary, there are abundant\nsubtitled sign news videos on the internet. Since these videos have no\nword-level annotation and exhibit a large domain gap from isolated signs, they\ncannot be directly used for training WSLR models. We observe that despite the\nexistence of a large domain gap, isolated and news signs share the same visual\nconcepts, such as hand gestures and body movements. Motivated by this\nobservation, we propose a novel method that learns domain-invariant visual\nconcepts and fertilizes WSLR models by transferring knowledge of subtitled news\nsign to them. To this end, we extract news signs using a base WSLR model, and\nthen design a classifier jointly trained on news and isolated signs to coarsely\nalign these two domain features. In order to learn domain-invariant features\nwithin each class and suppress domain-specific features, our method further\nresorts to an external memory to store the class centroids of the aligned news\nsigns. We then design a temporal attention based on the learnt descriptor to\nimprove recognition performance. Experimental results on standard WSLR datasets\nshow that our method outperforms previous state-of-the-art methods\nsignificantly. We also demonstrate the effectiveness of our method on\nautomatically localizing signs from sign news, achieving 28.1 for AP@0.5."}, {"title": "Bodies at Rest: 3D Human Pose and Shape Estimation From a Pressure Image Using Synthetic Data", "authors": "Henry M. Clever, Zackory Erickson, Ariel Kapusta, Greg Turk, Karen Liu, Charles C. Kemp", "link": "https://arxiv.org/abs/2004.01166", "summary": "People spend a substantial part of their lives at rest in bed. 3D human pose\nand shape estimation for this activity would have numerous beneficial\napplications, yet line-of-sight perception is complicated by occlusion from\nbedding. Pressure sensing mats are a promising alternative, but training data\nis challenging to collect at scale. We describe a physics-based method that\nsimulates human bodies at rest in a bed with a pressure sensing mat, and\npresent PressurePose, a synthetic dataset with 206K pressure images with 3D\nhuman poses and shapes. We also present PressureNet, a deep learning model that\nestimates human pose and shape given a pressure image and gender. PressureNet\nincorporates a pressure map reconstruction (PMR) network that models pressure\nimage generation to promote consistency between estimated 3D body models and\npressure image input. In our evaluations, PressureNet performed well with real\ndata from participants in diverse poses, even though it had only been trained\nwith synthetic data. When we ablated the PMR network, performance dropped\nsubstantially."}, {"title": "Bayesian Adversarial Human Motion Synthesis", "authors": "Rui Zhao, Hui Su, Qiang Ji"}, {"title": "LSM: Learning Subspace Minimization for Low-Level Vision", "authors": "Chengzhou Tang, Lu Yuan, Ping Tan", "link": "https://arxiv.org/abs/2004.09197", "summary": "We study the energy minimization problem in low-level vision tasks from a\nnovel perspective. We replace the heuristic regularization term with a\nlearnable subspace constraint, and preserve the data term to exploit domain\nknowledge derived from the first principle of a task. This learning subspace\nminimization (LSM) framework unifies the network structures and the parameters\nfor many low-level vision tasks, which allows us to train a single network for\nmultiple tasks simultaneously with completely shared parameters, and even\ngeneralizes the trained network to an unseen task as long as its data term can\nbe formulated. We demonstrate our LSM framework on four low-level tasks\nincluding interactive image segmentation, video segmentation, stereo matching,\nand optical flow, and validate the network on various datasets. The experiments\nshow that the proposed LSM generates state-of-the-art results with smaller\nmodel size, faster training convergence, and real-time inference."}, {"title": "Learning a Neural Solver for Multiple Object Tracking", "authors": "Guillem Bras\u00f3, Laura Leal-Taix\u00e9", "link": "https://arxiv.org/abs/1912.07515", "summary": "Graphs offer a natural way to formulate Multiple Object Tracking (MOT) within\nthe tracking-by-detection paradigm. However, they also introduce a major\nchallenge for learning methods, as defining a model that can operate on such\n\\textit{structured domain} is not trivial. As a consequence, most\nlearning-based work has been devoted to learning better features for MOT, and\nthen using these with well-established optimization frameworks. In this work,\nwe exploit the classical network flow formulation of MOT to define a fully\ndifferentiable framework based on Message Passing Networks (MPNs). By operating\ndirectly on the graph domain, our method can reason globally over an entire set\nof detections and predict final solutions. Hence, we show that learning in MOT\ndoes not need to be restricted to feature extraction, but it can also be\napplied to the data association step. We show a significant improvement in both\nMOTA and IDF1 on three publicly available benchmarks. Our code is available at\nhttps://bit.ly/motsolv ."}, {"title": "GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences", "authors": "Prune Truong, Martin Danelljan, Radu Timofte", "link": "https://arxiv.org/abs/1912.05524", "summary": "Establishing dense correspondences between a pair of images is an important\nand general problem, covering geometric matching, optical flow and semantic\ncorrespondences. While these applications share fundamental challenges, such as\nlarge displacements, pixel-accuracy, and appearance changes, they are currently\naddressed with specialized network architectures, designed for only one\nparticular task. This severely limits the generalization capabilities of such\nnetworks to new scenarios, where e.g. robustness to larger displacements or\nhigher accuracy is required.\n  In this work, we propose a universal network architecture that is directly\napplicable to all the aforementioned dense correspondence problems. We achieve\nboth high accuracy and robustness to large displacements by investigating the\ncombined use of global and local correlation layers. We further propose an\nadaptive resolution strategy, allowing our network to operate on virtually any\ninput image resolution. The proposed GLU-Net achieves state-of-the-art\nperformance for geometric and semantic matching as well as optical flow, when\nusing the same network and weights. Code and trained models are available at\nhttps://github.com/PruneTruong/GLU-Net."}, {"title": "SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking", "authors": "Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, Shengyong Chen", "link": "https://arxiv.org/abs/1911.07241", "summary": "By decomposing the visual tracking task into two subproblems as\nclassification for pixel category and regression for object bounding box at\nthis pixel, we propose a novel fully convolutional Siamese network to solve\nvisual tracking end-to-end in a per-pixel manner. The proposed framework\nSiamCAR consists of two simple subnetworks: one Siamese subnetwork for feature\nextraction and one classification-regression subnetwork for bounding box\nprediction. Our framework takes ResNet-50 as backbone. Different from\nstate-of-the-art trackers like Siamese-RPN, SiamRPN++ and SPM, which are based\non region proposal, the proposed framework is both proposal and anchor free.\nConsequently, we are able to avoid the tricky hyper-parameter tuning of anchors\nand reduce human intervention. The proposed framework is simple, neat and\neffective. Extensive experiments and comparisons with state-of-the-art trackers\nare conducted on many challenging benchmarks like GOT-10K, LaSOT, UAV123 and\nOTB-50. Without bells and whistles, our SiamCAR achieves the leading\nperformance with a considerable real-time speed."}, {"title": "MaskFlownet: Asymmetric Feature Matching With Learnable Occlusion Mask", "authors": "Shengyu Zhao, Yilun Sheng, Yue Dong, Eric I-Chao Chang, Yan Xu", "link": "http://arxiv.org/abs/2003.10955", "summary": "Feature warping is a core technique in optical flow estimation; however, the\nambiguity caused by occluded areas during warping is a major problem that\nremains unsolved. In this paper, we propose an asymmetric occlusion-aware\nfeature matching module, which can learn a rough occlusion mask that filters\nuseless (occluded) areas immediately after feature warping without any explicit\nsupervision. The proposed module can be easily integrated into end-to-end\nnetwork architectures and enjoys performance gains while introducing negligible\ncomputational cost. The learned occlusion mask can be further fed into a\nsubsequent network cascade with dual feature pyramids with which we achieve\nstate-of-the-art performance. At the time of submission, our method, called\nMaskFlownet, surpasses all published optical flow methods on the MPI Sintel,\nKITTI 2012 and 2015 benchmarks. Code is available at\nhttps://github.com/microsoft/MaskFlownet."}, {"title": "Tracking by Instance Detection: A Meta-Learning Approach", "authors": "Guangting Wang, Chong Luo, Xiaoyan Sun, Zhiwei Xiong, Wenjun Zeng", "link": "https://arxiv.org/abs/2004.00830", "summary": "We consider the tracking problem as a special type of object detection\nproblem, which we call instance detection. With proper initialization, a\ndetector can be quickly converted into a tracker by learning the new instance\nfrom a single image. We find that model-agnostic meta-learning (MAML) offers a\nstrategy to initialize the detector that satisfies our needs. We propose a\nprincipled three-step approach to build a high-performance tracker. First, pick\nany modern object detector trained with gradient descent. Second, conduct\noffline training (or initialization) with MAML. Third, perform domain\nadaptation using the initial frame. We follow this procedure to build two\ntrackers, named Retina-MAML and FCOS-MAML, based on two modern detectors\nRetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are\ncompetitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves\nthe highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the\nleader board with an AUC of 0.757 and the normalized precision of 0.822. Both\ntrackers run in real-time at 40 FPS."}, {"title": "High-Performance Long-Term Tracking With Meta-Updater", "authors": "Kenan Dai, Yunhua Zhang, Dong Wang, Jianhua Li, Huchuan Lu, Xiaoyun Yang", "link": "http://arxiv.org/abs/2004.00305", "summary": "Long-term visual tracking has drawn increasing attention because it is much\ncloser to practical applications than short-term tracking. Most top-ranked\nlong-term trackers adopt the offline-trained Siamese architectures, thus, they\ncannot benefit from great progress of short-term trackers with online update.\nHowever, it is quite risky to straightforwardly introduce online-update-based\ntrackers to solve the long-term problem, due to long-term uncertain and noisy\nobservations. In this work, we propose a novel offline-trained Meta-Updater to\naddress an important but unsolved problem: Is the tracker ready for updating in\nthe current frame? The proposed meta-updater can effectively integrate\ngeometric, discriminative, and appearance cues in a sequential manner, and then\nmine the sequential information with a designed cascaded LSTM module. Our\nmeta-updater learns a binary output to guide the tracker's update and can be\neasily embedded into different trackers. This work also introduces a long-term\ntracking framework consisting of an online local tracker, an online verifier, a\nSiamRPN-based re-detector, and our meta-updater. Numerous experimental results\non the VOT2018LT, VOT2019LT, OxUvALT, TLP, and LaSOT benchmarks show that our\ntracker performs remarkably better than other competing algorithms. Our project\nis available on the website: https://github.com/Daikenan/LTMU."}, {"title": "TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training Model", "authors": "Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, Cewu Lu"}, {"title": "Collaborative Motion Prediction via Neural Motion Message Passing", "authors": "Yue Hu, Siheng Chen, Ya Zhang, Xiao Gu", "link": "https://arxiv.org/abs/2003.06594", "summary": "Motion prediction is essential and challenging for autonomous vehicles and\nsocial robots. One challenge of motion prediction is to model the interaction\namong traffic actors, which could cooperate with each other to avoid collisions\nor form groups. To address this challenge, we propose neural motion message\npassing (NMMP) to explicitly model the interaction and learn representations\nfor directed interactions between actors. Based on the proposed NMMP, we design\nthe motion prediction systems for two settings: the pedestrian setting and the\njoint pedestrian and vehicle setting. Both systems share a common pattern: we\nuse an individual branch to model the behavior of a single actor and an\ninteractive branch to model the interaction between actors, while with\ndifferent wrappers to handle the varied input formats and characteristics. The\nexperimental results show that both systems outperform the previous\nstate-of-the-art methods on several existing benchmarks. Besides, we provide\ninterpretability for interaction learning."}, {"title": "P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds", "authors": "Haozhe Qi, Chen Feng, Zhiguo Cao, Feng Zhao, Yang Xiao", "link": "http://arxiv.org/abs/2005.13888", "summary": "Towards 3D object tracking in point clouds, a novel point-to-box network\ntermed P2B is proposed in an end-to-end learning manner. Our main idea is to\nfirst localize potential target centers in 3D search area embedded with target\ninformation. Then point-driven 3D target proposal and verification are executed\njointly. In this way, the time-consuming 3D exhaustive search can be avoided.\nSpecifically, we first sample seeds from the point clouds in template and\nsearch area respectively. Then, we execute permutation-invariant feature\naugmentation to embed target clues from template into search area seeds and\nrepresent them with target-specific features. Consequently, the augmented\nsearch area seeds regress the potential target centers via Hough voting. The\ncenters are further strengthened with seed-wise targetness scores. Finally,\neach center clusters its neighbors to leverage the ensemble power for joint 3D\ntarget proposal and verification. We apply PointNet++ as our backbone and\nexperiments on KITTI tracking dataset demonstrate P2B's superiority (~10%'s\nimprovement over state-of-the-art). Note that P2B can run with 40FPS on a\nsingle NVIDIA 1080Ti GPU. Our code and model are available at\nhttps://github.com/HaozheQi/P2B."}, {"title": "Self-Supervised Deep Visual Odometry With Online Adaptation", "authors": "Shunkai Li, Xin Wang, Yingdian Cao, Fei Xue, Zike Yan, Hongbin Zha", "link": "https://arxiv.org/abs/2005.06136", "summary": "Self-supervised VO methods have shown great success in jointly estimating\ncamera pose and depth from videos. However, like most data-driven methods,\nexisting VO networks suffer from a notable decrease in performance when\nconfronted with scenes different from the training data, which makes them\nunsuitable for practical applications. In this paper, we propose an online\nmeta-learning algorithm to enable VO networks to continuously adapt to new\nenvironments in a self-supervised manner. The proposed method utilizes\nconvolutional long short-term memory (convLSTM) to aggregate rich\nspatial-temporal information in the past. The network is able to memorize and\nlearn from its past experience for better estimation and fast adaptation to the\ncurrent frame. When running VO in the open world, in order to deal with the\nchanging environment, we propose an online feature alignment method by aligning\nfeature distributions at different time. Our VO network is able to seamlessly\nadapt to different environments. Extensive experiments on unseen outdoor\nscenes, virtual to real world and outdoor to indoor environments demonstrate\nthat our method consistently outperforms state-of-the-art self-supervised VO\nbaselines considerably."}, {"title": "Globally Optimal Contrast Maximisation for Event-Based Motion Estimation", "authors": "Daqi Liu, \u00c1lvaro Parra, Tat-Jun Chin", "link": "https://arxiv.org/abs/2002.10686", "summary": "Contrast maximisation estimates the motion captured in an event stream by\nmaximising the sharpness of the motion compensated event image. To carry out\ncontrast maximisation, many previous works employ iterative optimisation\nalgorithms, such as conjugate gradient, which require good initialisation to\navoid converging to bad local minima. To alleviate this weakness, we propose a\nnew globally optimal event-based motion estimation algorithm. Based on\nbranch-and-bound (BnB), our method solves rotational (3DoF) motion estimation\non event streams, which supports practical applications such as video\nstabilisation and attitude estimation. Underpinning our method are novel\nbounding functions for contrast maximisation, whose theoretical validity is\nrigorously established. We show concrete examples from public datasets where\nglobally optimal solutions are vital to the success of contrast maximisation.\nDespite its exact nature, our algorithm is currently able to process a 50,000\nevent input in 300 seconds (a locally optimal solver takes 30 seconds on the\nsame input), and has the potential to be further speeded-up using GPUs."}, {"title": "D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features", "authors": "Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, Chiew-Lan Tai", "link": "https://arxiv.org/abs/2003.03164", "summary": "A successful point cloud registration often lies on robust establishment of\nsparse matches through discriminative 3D local features. Despite the fast\nevolution of learning-based 3D feature descriptors, little attention has been\ndrawn to the learning of 3D feature detectors, even less for a joint learning\nof the two tasks. In this paper, we leverage a 3D fully convolutional network\nfor 3D point clouds, and propose a novel and practical learning mechanism that\ndensely predicts both a detection score and a description feature for each 3D\npoint. In particular, we propose a keypoint selection strategy that overcomes\nthe inherent density variations of 3D point clouds, and further propose a\nself-supervised detector loss guided by the on-the-fly feature matching results\nduring training. Finally, our method achieves state-of-the-art results in both\nindoor and outdoor scenarios, evaluated on 3DMatch and KITTI datasets, and\nshows its strong generalization ability on the ETH dataset. Towards practical\nuse, we show that by adopting a reliable feature detector, sampling a smaller\nnumber of features is sufficient to achieve accurate and fast point cloud\nalignment.[code release](https://github.com/XuyangBai/D3Feat)"}, {"title": "Towards Backward-Compatible Representation Learning", "authors": "Yantao Shen, Yuanjun Xiong, Wei Xia, Stefano Soatto", "link": "https://arxiv.org/abs/2003.11942", "summary": "We propose a way to learn visual features that are compatible with previously\ncomputed ones even when they have different dimensions and are learned via\ndifferent neural network architectures and loss functions. Compatible means\nthat, if such features are used to compare images, then \"new\" features can be\ncompared directly to \"old\" features, so they can be used interchangeably. This\nenables visual search systems to bypass computing new features for all\npreviously seen images when updating the embedding models, a process known as\nbackfilling. Backward compatibility is critical to quickly deploy new embedding\nmodels that leverage ever-growing large-scale training datasets and\nimprovements in deep learning architectures and training methods. We propose a\nframework to train embedding models, called backward-compatible training (BCT),\nas a first step towards backward compatible representation learning. In\nexperiments on learning embeddings for face recognition, models trained with\nBCT successfully achieve backward compatibility without sacrificing accuracy,\nthus enabling backfill-free model updates of visual embeddings."}, {"title": "PointAugment: An Auto-Augmentation Framework for Point Cloud Classification", "authors": "Ruihui Li, Xianzhi Li, Pheng-Ann Heng, Chi-Wing Fu", "link": "https://arxiv.org/abs/2002.10876", "summary": "We present PointAugment, a new auto-augmentation framework that automatically\noptimizes and augments point cloud samples to enrich the data diversity when we\ntrain a classification network. Different from existing auto-augmentation\nmethods for 2D images, PointAugment is sample-aware and takes an adversarial\nlearning strategy to jointly optimize an augmentor network and a classifier\nnetwork, such that the augmentor can learn to produce augmented samples that\nbest fit the classifier. Moreover, we formulate a learnable point augmentation\nfunction with a shape-wise transformation and a point-wise displacement, and\ncarefully design loss functions to adopt the augmented samples based on the\nlearning progress of the classifier. Extensive experiments also confirm\nPointAugment's effectiveness and robustness to improve the performance of\nvarious networks on shape classification and retrieval."}, {"title": "Cross-Batch Memory for Embedding Learning", "authors": "Xun Wang, Haozhi Zhang, Weilin Huang, Matthew R. Scott", "link": "https://arxiv.org/abs/1912.06798", "summary": "Mining informative negative instances are of central importance to deep\nmetric learning (DML), however this task is intrinsically limited by mini-batch\ntraining, where only a mini-batch of instances is accessible at each iteration.\nIn this paper, we identify a \"slow drift\" phenomena by observing that the\nembedding features drift exceptionally slow even as the model parameters are\nupdating throughout the training process. This suggests that the features of\ninstances computed at preceding iterations can be used to considerably\napproximate their features extracted by the current model. We propose a\ncross-batch memory (XBM) mechanism that memorizes the embeddings of past\niterations, allowing the model to collect sufficient hard negative pairs across\nmultiple mini-batches - even over the whole dataset. Our XBM can be directly\nintegrated into a general pair-based DML framework, where the XBM augmented DML\ncan boost performance considerably. In particular, without bells and whistles,\na simple contrastive loss with our XBM can have large R@1 improvements of\n12%-22.5% on three large-scale image retrieval datasets, surpassing the most\nsophisticated state-of-the-art methods, by a large margin. Our XBM is\nconceptually simple, easy to implement - using several lines of codes, and is\nmemory efficient - with a negligible 0.2 GB extra GPU memory. Code is available\nat: https://github.com/MalongTech/research-xbm."}, {"title": "Circle Loss: A Unified Perspective of Pair Similarity Optimization", "authors": "Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, Yichen Wei", "link": "https://arxiv.org/abs/2002.10857", "summary": "This paper provides a pair similarity optimization viewpoint on deep feature\nlearning, aiming to maximize the within-class similarity $s_p$ and minimize the\nbetween-class similarity $s_n$. We find a majority of loss functions, including\nthe triplet loss and the softmax plus cross-entropy loss, embed $s_n$ and $s_p$\ninto similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization\nmanner is inflexible, because the penalty strength on every single similarity\nscore is restricted to be equal. Our intuition is that if a similarity score\ndeviates far from the optimum, it should be emphasized. To this end, we simply\nre-weight each similarity to highlight the less-optimized similarity scores. It\nresults in a Circle loss, which is named due to its circular decision boundary.\nThe Circle loss has a unified formula for two elemental deep feature learning\napproaches, i.e. learning with class-level labels and pair-wise labels.\nAnalytically, we show that the Circle loss offers a more flexible optimization\napproach towards a more definite convergence target, compared with the loss\nfunctions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the\nsuperiority of the Circle loss on a variety of deep feature learning tasks. On\nface recognition, person re-identification, as well as several fine-grained\nimage retrieval datasets, the achieved performance is on par with the state of\nthe art."}, {"title": "Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics", "authors": "Simon Jenni, Hailin Jin, Paolo Favaro", "link": "https://arxiv.org/abs/2004.02331", "summary": "We introduce a novel principle for self-supervised feature learning based on\nthe discrimination of specific transformations of an image. We argue that the\ngeneralization capability of learned features depends on what image\nneighborhood size is sufficient to discriminate different image\ntransformations: The larger the required neighborhood size and the more global\nthe image statistics that the feature can describe. An accurate description of\nglobal image statistics allows to better represent the shape and configuration\nof objects and their context, which ultimately generalizes better to new tasks\nsuch as object classification and detection. This suggests a criterion to\nchoose and design image transformations. Based on this criterion, we introduce\na novel image transformation that we call limited context inpainting (LCI).\nThis transformation inpaints an image patch conditioned only on a small\nrectangular pixel boundary (the limited context). Because of the limited\nboundary information, the inpainter can learn to match local pixel statistics,\nbut is unlikely to match the global statistics of the image. We claim that the\nsame principle can be used to justify the performance of transformations such\nas image rotations and warping. Indeed, we demonstrate experimentally that\nlearning to discriminate transformations such as LCI, image warping and\nrotations, yields features with state of the art generalization capabilities on\nseveral datasets such as Pascal VOC, STL-10, CelebA, and ImageNet. Remarkably,\nour trained features achieve a performance on Places on par with features\ntrained through supervised learning with ImageNet labels."}, {"title": "Hyperbolic Image Embeddings", "authors": "Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, Victor Lempitsky", "link": "https://arxiv.org/abs/1904.02239", "summary": "Computer vision tasks such as image classification, image retrieval and\nfew-shot learning are currently dominated by Euclidean and spherical\nembeddings, so that the final decisions about class belongings or the degree of\nsimilarity are made using linear hyperplanes, Euclidean distances, or spherical\ngeodesic distances (cosine similarity). In this work, we demonstrate that in\nmany practical scenarios hyperbolic embeddings provide a better alternative."}, {"title": "Controllable Orthogonalization in Training DNNs", "authors": "Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, Ling Shao", "link": "https://arxiv.org/abs/2004.00917", "summary": "Orthogonality is widely used for training deep neural networks (DNNs) due to\nits ability to maintain all singular values of the Jacobian close to 1 and\nreduce redundancy in representation. This paper proposes a computationally\nefficient and numerically stable orthogonalization method using Newton's\niteration (ONI), to learn a layer-wise orthogonal weight matrix in DNNs. ONI\nworks by iteratively stretching the singular values of a weight matrix towards\n1. This property enables it to control the orthogonality of a weight matrix by\nits number of iterations. We show that our method improves the performance of\nimage classification networks by effectively controlling the orthogonality to\nprovide an optimal tradeoff between optimization benefits and representational\ncapacity reduction. We also show that ONI stabilizes the training of generative\nadversarial networks (GANs) by maintaining the Lipschitz continuity of a\nnetwork, similar to spectral normalization (SN), and further outperforms SN by\nproviding controllable orthogonality."}, {"title": "An Investigation Into the Stochasticity of Batch Whitening", "authors": "Lei Huang, Lei Zhao, Yi Zhou, Fan Zhu, Li Liu, Ling Shao", "link": "http://arxiv.org/abs/2003.12327", "summary": "Batch Normalization (BN) is extensively employed in various network\narchitectures by performing standardization within mini-batches.\n  A full understanding of the process has been a central target in the deep\nlearning communities.\n  Unlike existing works, which usually only analyze the standardization\noperation, this paper investigates the more general Batch Whitening (BW). Our\nwork originates from the observation that while various whitening\ntransformations equivalently improve the conditioning, they show significantly\ndifferent behaviors in discriminative scenarios and training Generative\nAdversarial Networks (GANs).\n  We attribute this phenomenon to the stochasticity that BW introduces.\n  We quantitatively investigate the stochasticity of different whitening\ntransformations and show that it correlates well with the optimization\nbehaviors during training.\n  We also investigate how stochasticity relates to the estimation of population\nstatistics during inference.\n  Based on our analysis, we provide a framework for designing and comparing BW\nalgorithms in different scenarios.\n  Our proposed BW algorithm improves the residual networks by a significant\nmargin on ImageNet classification.\n  Besides, we show that the stochasticity of BW can improve the GAN's\nperformance with, however, the sacrifice of the training stability."}, {"title": "High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification", "authors": "Guan'an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang, Yang Yang, Shuliang Wang, Gang Yu, Erjin Zhou, Jian Sun", "link": "http://arxiv.org/abs/2003.08177", "summary": "Occluded person re-identification (ReID) aims to match occluded person images\nto holistic ones across dis-joint cameras. In this paper, we propose a novel\nframework by learning high-order relation and topology information for\ndiscriminative features and robust alignment. At first, we use a CNN backbone\nand a key-points estimation model to extract semantic local features. Even so,\noccluded images still suffer from occlusion and outliers. Then, we view the\nlocal features of an image as nodes of a graph and propose an adaptive\ndirection graph convolutional (ADGC)layer to pass relation information between\nnodes. The proposed ADGC layer can automatically suppress the message-passing\nof meaningless features by dynamically learning di-rection and degree of\nlinkage. When aligning two groups of local features from two images, we view it\nas a graph matching problem and propose a cross-graph embedded-alignment (CGEA)\nlayer to jointly learn and embed topology information to local features, and\nstraightly predict similarity score. The proposed CGEA layer not only take full\nuse of alignment learned by graph matching but also re-place sensitive\none-to-one matching with a robust soft one. Finally, extensive experiments on\noccluded, partial, and holistic ReID tasks show the effectiveness of our\nproposed method. Specifically, our framework significantly outperforms\nstate-of-the-art by6.5%mAP scores on Occluded-Duke dataset."}, {"title": "Same Features, Different Day: Weakly Supervised Feature Learning for Seasonal Invariance", "authors": "Jaime Spencer, Richard Bowden, Simon Hadfield", "link": "https://arxiv.org/abs/2003.13431", "summary": "\"Like night and day\" is a commonly used expression to imply that two things\nare completely different. Unfortunately, this tends to be the case for current\nvisual feature representations of the same scene across varying seasons or\ntimes of day. The aim of this paper is to provide a dense feature\nrepresentation that can be used to perform localization, sparse matching or\nimage retrieval, regardless of the current seasonal or temporal appearance.\n  Recently, there have been several proposed methodologies for deep learning\ndense feature representations. These methods make use of ground truth\npixel-wise correspondences between pairs of images and focus on the spatial\nproperties of the features. As such, they don't address temporal or seasonal\nvariation. Furthermore, obtaining the required pixel-wise correspondence data\nto train in cross-seasonal environments is highly complex in most scenarios.\n  We propose Deja-Vu, a weakly supervised approach to learning season invariant\nfeatures that does not require pixel-wise ground truth data. The proposed\nsystem only requires coarse labels indicating if two images correspond to the\nsame location or not. From these labels, the network is trained to produce\n\"similar\" dense feature maps for corresponding locations despite environmental\nchanges. Code will be made available at:\nhttps://github.com/jspenmar/DejaVu_Features"}, {"title": "Learning to Dress 3D People in Generative Clothing", "authors": "Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, Michael J. Black", "link": "https://arxiv.org/abs/1907.13615", "summary": "Three-dimensional human body models are widely used in the analysis of human\npose and motion. Existing models, however, are learned from minimally-clothed\n3D scans and thus do not generalize to the complexity of dressed people in\ncommon images and videos. Additionally, current models lack the expressive\npower needed to represent the complex non-linear geometry of pose-dependent\nclothing shapes. To address this, we learn a generative 3D mesh model of\nclothed people from 3D scans with varying pose and clothing. Specifically, we\ntrain a conditional Mesh-VAE-GAN to learn the clothing deformation from the\nSMPL body model, making clothing an additional term in SMPL. Our model is\nconditioned on both pose and clothing type, giving the ability to draw samples\nof clothing to dress different body shapes in a variety of styles and poses. To\npreserve wrinkle detail, our Mesh-VAE-GAN extends patchwise discriminators to\n3D meshes. Our model, named CAPE, represents global shape and fine local\nstructure, effectively extending the SMPL body model to clothing. To our\nknowledge, this is the first generative model that directly dresses 3D human\nbody meshes and generalizes to different poses. The model, code and data are\navailable for research purposes at https://cape.is.tue.mpg.de."}, {"title": "MAST: A Memory-Augmented Self-Supervised Tracker", "authors": "Zihang Lai, Erika Lu, Weidi Xie", "link": "", "summary": ""}, {"title": "Learning by Analogy: Reliable Supervision From Transformations for Unsupervised Optical Flow Estimation", "authors": "Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, Feiyue Huang", "link": "https://arxiv.org/abs/2003.13045", "summary": "Unsupervised learning of optical flow, which leverages the supervision from\nview synthesis, has emerged as a promising alternative to supervised methods.\nHowever, the objective of unsupervised learning is likely to be unreliable in\nchallenging scenes. In this work, we present a framework to use more reliable\nsupervision from transformations. It simply twists the general unsupervised\nlearning pipeline by running another forward pass with transformed data from\naugmentation, along with using transformed predictions of original data as the\nself-supervision signal. Besides, we further introduce a lightweight network\nwith multiple frames by a highly-shared flow decoder. Our method consistently\ngets a leap of performance on several benchmarks with the best accuracy among\ndeep unsupervised methods. Also, our method achieves competitive results to\nrecent fully supervised methods while with much fewer parameters."}, {"title": "GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking With 2D-3D Multi-Feature Learning", "authors": "Xinshuo Weng, Yongxin Wang, Yunze Man, Kris M. Kitani"}, {"title": "ClusterFit: Improving Generalization of Visual Representations", "authors": "Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti Ghadiyaram, Dhruv Mahajan", "link": "https://arxiv.org/abs/1912.03330", "summary": "Pre-training convolutional neural networks with weakly-supervised and\nself-supervised strategies is becoming increasingly popular for several\ncomputer vision tasks. However, due to the lack of strong discriminative\nsignals, these learned representations may overfit to the pre-training\nobjective (e.g., hashtag prediction) and not generalize well to downstream\ntasks. In this work, we present a simple strategy - ClusterFit (CF) to improve\nthe robustness of the visual representations learned during pre-training. Given\na dataset, we (a) cluster its features extracted from a pre-trained network\nusing k-means and (b) re-train a new network from scratch on this dataset using\ncluster assignments as pseudo-labels. We empirically show that clustering helps\nreduce the pre-training task-specific information from the extracted features\nthereby minimizing overfitting to the same. Our approach is extensible to\ndifferent pre-training frameworks -- weak- and self-supervised, modalities --\nimages and videos, and pre-training tasks -- object and action classification.\nThrough extensive transfer learning experiments on 11 different target datasets\nof varied vocabularies and granularities, we show that ClusterFit significantly\nimproves the representation quality compared to the state-of-the-art\nlarge-scale (millions / billions) weakly-supervised image and video models and\nself-supervised image models."}, {"title": "Learning Dynamic Relationships for 3D Human Motion Prediction", "authors": "Qiongjie Cui, Huaijiang Sun, Fei Yang"}, {"title": "Knowledge As Priors: Cross-Modal Knowledge Generalization for Datasets Without Superior Knowledge", "authors": "Long Zhao, Xi Peng, Yuxiao Chen, Mubbasir Kapadia, Dimitris N. Metaxas", "link": "http://arxiv.org/abs/2004.00176", "summary": "Cross-modal knowledge distillation deals with transferring knowledge from a\nmodel trained with superior modalities (Teacher) to another model trained with\nweak modalities (Student). Existing approaches require paired training examples\nexist in both modalities. However, accessing the data from superior modalities\nmay not always be feasible. For example, in the case of 3D hand pose\nestimation, depth maps, point clouds, or stereo images usually capture better\nhand structures than RGB images, but most of them are expensive to be\ncollected. In this paper, we propose a novel scheme to train the Student in a\nTarget dataset where the Teacher is unavailable. Our key idea is to generalize\nthe distilled cross-modal knowledge learned from a Source dataset, which\ncontains paired examples from both modalities, to the Target dataset by\nmodeling knowledge as priors on parameters of the Student. We name our method\n\"Cross-Modal Knowledge Generalization\" and demonstrate that our scheme results\nin competitive performance for 3D hand pose estimation on standard benchmark\ndatasets."}, {"title": "S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation", "authors": "Yizhe Zhu, Martin Renqiang Min, Asim Kadav, Hans Peter Graf", "link": "http://arxiv.org/abs/2005.11437", "summary": "We propose a sequential variational autoencoder to learn disentangled\nrepresentations of sequential data (e.g., videos and audios) under\nself-supervision. Specifically, we exploit the benefits of some readily\naccessible supervisory signals from input data itself or some off-the-shelf\nfunctional models and accordingly design auxiliary tasks for our model to\nutilize these signals. With the supervision of the signals, our model can\neasily disentangle the representation of an input sequence into static factors\nand dynamic factors (i.e., time-invariant and time-varying parts).\nComprehensive experiments across videos and audios verify the effectiveness of\nour model on representation disentanglement and generation of sequential data,\nand demonstrate that, our model with self-supervision performs comparable to,\nif not better than, the fully-supervised model with ground truth labels, and\noutperforms state-of-the-art unsupervised models by a large margin."}, {"title": "Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning", "authors": "Yuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, Qixiang Ye", "link": "", "summary": ""}, {"title": "Learning to Manipulate Individual Objects in an Image", "authors": "Yanchao Yang, Yutong Chen, Stefano Soatto", "link": "http://arxiv.org/abs/2004.05495", "summary": "We describe a method to train a generative model with latent factors that are\n(approximately) independent and localized. This means that perturbing the\nlatent variables affects only local regions of the synthesized image,\ncorresponding to objects. Unlike other unsupervised generative models, ours\nenables object-centric manipulation, without requiring object-level\nannotations, or any form of annotation for that matter. The key to our method\nis the combination of spatial disentanglement, enforced by a Contextual\nInformation Separation loss, and perceptual cycle-consistency, enforced by a\nloss that penalizes changes in the image partition in response to perturbations\nof the latent factors. We test our method's ability to allow independent\ncontrol of spatial and semantic factors of variability on existing datasets and\nalso introduce two new ones that highlight the limitations of current methods."}, {"title": "PADS: Policy-Adapted Sampling for Visual Similarity Learning", "authors": "Karsten Roth, Timo Milbich, Bj\u00f6rn Ommer", "link": "https://arxiv.org/abs/2003.11113", "summary": "Learning visual similarity requires to learn relations, typically between\ntriplets of images. Albeit triplet approaches being powerful, their\ncomputational complexity mostly limits training to only a subset of all\npossible training triplets. Thus, sampling strategies that decide when to use\nwhich training sample during learning are crucial. Currently, the prominent\nparadigm are fixed or curriculum sampling strategies that are predefined before\ntraining starts. However, the problem truly calls for a sampling process that\nadjusts based on the actual state of the similarity representation during\ntraining. We, therefore, employ reinforcement learning and have a teacher\nnetwork adjust the sampling distribution based on the current state of the\nlearner network, which represents visual similarity. Experiments on benchmark\ndatasets using standard triplet-based losses show that our adaptive sampling\nstrategy significantly outperforms fixed sampling strategies. Moreover,\nalthough our adaptive sampling is only applied on top of basic triplet-learning\nframeworks, we reach competitive results to state-of-the-art approaches that\nemploy diverse additional learning signals or strong ensemble architectures.\nCode can be found under https://github.com/Confusezius/CVPR2020_PADS."}, {"title": "Siam R-CNN: Visual Tracking by Re-Detection", "authors": "Paul Voigtlaender, Jonathon Luiten, Philip H.S. Torr, Bastian Leibe", "link": "", "summary": ""}, {"title": "ASLFeat: Learning Local Features of Accurate Shape and Localization", "authors": "Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, Long Quan", "link": "https://arxiv.org/abs/2003.10071", "summary": "This work focuses on mitigating two limitations in the joint learning of\nlocal feature detectors and descriptors. First, the ability to estimate the\nlocal shape (scale, orientation, etc.) of feature points is often neglected\nduring dense feature extraction, while the shape-awareness is crucial to\nacquire stronger geometric invariance. Second, the localization accuracy of\ndetected keypoints is not sufficient to reliably recover camera geometry, which\nhas become the bottleneck in tasks such as 3D reconstruction. In this paper, we\npresent ASLFeat, with three light-weight yet effective modifications to\nmitigate above issues. First, we resort to deformable convolutional networks to\ndensely estimate and apply local transformation. Second, we take advantage of\nthe inherent feature hierarchy to restore spatial resolution and low-level\ndetails for accurate keypoint localization. Finally, we use a peakiness\nmeasurement to relate feature responses and derive more indicative detection\nscores. The effect of each modification is thoroughly studied, and the\nevaluation is extensively conducted across a variety of practical scenarios.\nState-of-the-art results are reported that demonstrate the superiority of our\nmethods."}, {"title": "Filter Grafting for Deep Neural Networks", "authors": "Fanxu Meng, Hao Cheng, Ke Li, Zhixin Xu, Rongrong Ji, Xing Sun, Guangming Lu", "link": "http://arxiv.org/abs/2001.05868", "summary": "This paper proposes a new learning paradigm called filter grafting, which\naims to improve the representation capability of Deep Neural Networks (DNNs).\nThe motivation is that DNNs have unimportant (invalid) filters (e.g., l1 norm\nclose to 0). These filters limit the potential of DNNs since they are\nidentified as having little effect on the network. While filter pruning removes\nthese invalid filters for efficiency consideration, filter grafting\nre-activates them from an accuracy boosting perspective. The activation is\nprocessed by grafting external information (weights) into invalid filters. To\nbetter perform the grafting process, we develop an entropy-based criterion to\nmeasure the information of filters and an adaptive weighting strategy for\nbalancing the grafted information among networks. After the grafting operation,\nthe network has very few invalid filters compared with its untouched state,\nenpowering the model with more representation capacity. We also perform\nextensive experiments on the classification and recognition tasks to show the\nsuperiority of our method. For example, the grafted MobileNetV2 outperforms the\nnon-grafted MobileNetV2 by about 7 percent on CIFAR-100 dataset. Code is\navailable at https://github.com/fxmeng/filter-grafting.git."}, {"title": "HOPE-Net: A Graph-Based Model for Hand-Object Pose Estimation", "authors": "Bardia Doosti, Shujon Naha, Majid Mirbagheri, David J. Crandall", "link": "", "summary": ""}, {"title": "DeepFaceFlow: In-the-Wild Dense 3D Facial Motion Estimation", "authors": "Mohammad Rami Koujan, Anastasios Roussos, Stefanos Zafeiriou", "link": "https://arxiv.org/abs/2005.07298", "summary": "Dense 3D facial motion capture from only monocular in-the-wild pairs of RGB\nimages is a highly challenging problem with numerous applications, ranging from\nfacial expression recognition to facial reenactment. In this work, we propose\nDeepFaceFlow, a robust, fast, and highly-accurate framework for the dense\nestimation of 3D non-rigid facial flow between pairs of monocular images. Our\nDeepFaceFlow framework was trained and tested on two very large-scale facial\nvideo datasets, one of them of our own collection and annotation, with the aid\nof occlusion-aware and 3D-based loss function. We conduct comprehensive\nexperiments probing different aspects of our approach and demonstrating its\nimproved performance against state-of-the-art flow and 3D reconstruction\nmethods. Furthermore, we incorporate our framework in a full-head\nstate-of-the-art facial video synthesis method and demonstrate the ability of\nour method in better representing and capturing the facial dynamics, resulting\nin a highly-realistic facial video synthesis. Given registered pairs of images,\nour framework generates 3D flow maps at ~60 fps."}, {"title": "Learning for Video Compression With Hierarchical Quality and Recurrent Enhancement", "authors": "Ren Yang, Fabian Mentzer, Luc Van Gool, Radu Timofte", "link": "https://arxiv.org/abs/2003.01966", "summary": "In this paper, we propose a Hierarchical Learned Video Compression (HLVC)\nmethod with three hierarchical quality layers and a recurrent enhancement\nnetwork. The frames in the first layer are compressed by an image compression\nmethod with the highest quality. Using these frames as references, we propose\nthe Bi-Directional Deep Compression (BDDC) network to compress the second layer\nwith relatively high quality. Then, the third layer frames are compressed with\nthe lowest quality, by the proposed Single Motion Deep Compression (SMDC)\nnetwork, which adopts a single motion map to estimate the motions of multiple\nframes, thus saving bits for motion information. In our deep decoder, we\ndevelop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes\nboth compressed frames and the bit stream as inputs. In the recurrent cell of\nWRQE, the memory and update signal are weighted by quality features to\nreasonably leverage multi-frame information for enhancement. In our HLVC\napproach, the hierarchical quality benefits the coding efficiency, since the\nhigh quality information facilitates the compression and enhancement of low\nquality frames at encoder and decoder sides, respectively. Finally, the\nexperiments validate that our HLVC approach advances the state-of-the-art of\ndeep video compression methods, and outperforms the \"Low-Delay P (LDP) very\nfast\" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at\nhttps://github.com/RenYang-home/HLVC."}, {"title": "Learning Better Lossless Compression Using Lossy Compression", "authors": "Fabian Mentzer, Luc Van Gool, Michael Tschannen", "link": "https://arxiv.org/abs/2003.10184", "summary": "We leverage the powerful lossy image compression algorithm BPG to build a\nlossless image compression system. Specifically, the original image is first\ndecomposed into the lossy reconstruction obtained after compressing it with BPG\nand the corresponding residual. We then model the distribution of the residual\nwith a convolutional neural network-based probabilistic model that is\nconditioned on the BPG reconstruction, and combine it with entropy coding to\nlosslessly encode the residual. Finally, the image is stored using the\nconcatenation of the bitstreams produced by BPG and the learned residual coder.\nThe resulting compression system achieves state-of-the-art performance in\nlearned lossless full-resolution image compression, outperforming previous\nlearned approaches as well as PNG, WebP, and JPEG2000."}, {"title": "Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching", "authors": "Pengpeng Liu, Irwin King, Michael R. Lyu, Jia Xu", "link": "https://arxiv.org/abs/2004.02138", "summary": "In this paper, we propose a unified method to jointly learn optical flow and\nstereo matching. Our first intuition is stereo matching can be modeled as a\nspecial case of optical flow, and we can leverage 3D geometry behind\nstereoscopic videos to guide the learning of these two forms of\ncorrespondences. We then enroll this knowledge into the state-of-the-art\nself-supervised learning framework, and train one single network to estimate\nboth flow and stereo. Second, we unveil the bottlenecks in prior\nself-supervised learning approaches, and propose to create a new set of\nchallenging proxy tasks to boost performance. These two insights yield a single\nmodel that achieves the highest accuracy among all existing unsupervised flow\nand stereo methods on KITTI 2012 and 2015 benchmarks. More remarkably, our\nself-supervised method even outperforms several state-of-the-art fully\nsupervised methods, including PWC-Net and FlowNet2 on KITTI 2012."}, {"title": "Multi-Scale Fusion Subspace Clustering Using Similarity Constraint", "authors": "Zhiyuan Dang, Cheng Deng, Xu Yang, Heng Huang"}, {"title": "Siamese Box Adaptive Network for Visual Tracking", "authors": "Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, Rongrong Ji", "link": "https://arxiv.org/abs/2003.06761", "summary": "Most of the existing trackers usually rely on either a multi-scale searching\nscheme or pre-defined anchor boxes to accurately estimate the scale and aspect\nratio of a target. Unfortunately, they typically call for tedious and heuristic\nconfigurations. To address this issue, we propose a simple yet effective visual\ntracking framework (named Siamese Box Adaptive Network, SiamBAN) by exploiting\nthe expressive power of the fully convolutional network (FCN). SiamBAN views\nthe visual tracking problem as a parallel classification and regression\nproblem, and thus directly classifies objects and regresses their bounding\nboxes in a unified FCN. The no-prior box design avoids hyper-parameters\nassociated with the candidate boxes, making SiamBAN more flexible and general.\nExtensive experiments on visual tracking benchmarks including VOT2018, VOT2019,\nOTB100, NFS, UAV123, and LaSOT demonstrate that SiamBAN achieves\nstate-of-the-art performance and runs at 40 FPS, confirming its effectiveness\nand efficiency. The code will be available at https://github.com/hqucv/siamban."}, {"title": "Cross-Domain Face Presentation Attack Detection via Multi-Domain Disentangled Representation Learning", "authors": "Guoqing Wang, Hu Han, Shiguang Shan, Xilin Chen", "link": "https://arxiv.org/abs/2004.01959", "summary": "Face presentation attack detection (PAD) has been an urgent problem to be\nsolved in the face recognition systems. Conventional approaches usually assume\nthe testing and training are within the same domain; as a result, they may not\ngeneralize well into unseen scenarios because the representations learned for\nPAD may overfit to the subjects in the training set. In light of this, we\npropose an efficient disentangled representation learning for cross-domain face\nPAD. Our approach consists of disentangled representation learning (DR-Net) and\nmulti-domain learning (MD-Net). DR-Net learns a pair of encoders via generative\nmodels that can disentangle PAD informative features from subject\ndiscriminative features. The disentangled features from different domains are\nfed to MD-Net which learns domain-independent features for the final\ncross-domain face PAD task. Extensive experiments on several public datasets\nvalidate the effectiveness of the proposed approach for cross-domain PAD."}, {"title": "Online Deep Clustering for Unsupervised Representation Learning", "authors": "Xiaohang Zhan, Jiahao Xie, Ziwei Liu, Yew-Soon Ong, Chen Change Loy"}, {"title": "Density-Aware Feature Embedding for Face Clustering", "authors": "Senhui Guo, Jing Xu, Dapeng Chen, Chao Zhang, Xiaogang Wang, Rui Zhao", "link": "", "summary": ""}, {"title": "Self-Supervised Learning of Pretext-Invariant Representations", "authors": "Ishan Misra, Laurens van der Maaten", "link": "https://arxiv.org/abs/1912.01991", "summary": "The goal of self-supervised learning from images is to construct image\nrepresentations that are semantically meaningful via pretext tasks that do not\nrequire semantic annotations for a large training set of images. Many pretext\ntasks lead to representations that are covariant with image transformations. We\nargue that, instead, semantic representations ought to be invariant under such\ntransformations. Specifically, we develop Pretext-Invariant Representation\nLearning (PIRL, pronounced as \"pearl\") that learns invariant representations\nbased on pretext tasks. We use PIRL with a commonly used pretext task that\ninvolves solving jigsaw puzzles. We find that PIRL substantially improves the\nsemantic quality of the learned image representations. Our approach sets a new\nstate-of-the-art in self-supervised learning from images on several popular\nbenchmarks for self-supervised learning. Despite being unsupervised, PIRL\noutperforms supervised pre-training in learning image representations for\nobject detection. Altogether, our results demonstrate the potential of\nself-supervised learning of image representations with good invariance\nproperties."}, {"title": "ROAM: Recurrently Optimizing Tracking Model", "authors": "Tianyu Yang, Pengfei Xu, Runbo Hu, Hua Chai, Antoni B. Chan", "link": "https://arxiv.org/abs/1907.12006", "summary": "In this paper, we design a tracking model consisting of response generation\nand bounding box regression, where the first component produces a heat map to\nindicate the presence of the object at different positions and the second part\nregresses the relative bounding box shifts to anchors mounted on sliding-window\nlocations. Thanks to the resizable convolutional filters used in both\ncomponents to adapt to the shape changes of objects, our tracking model does\nnot need to enumerate different sized anchors, thus saving model parameters. To\neffectively adapt the model to appearance variations, we propose to offline\ntrain a recurrent neural optimizer to update tracking model in a meta-learning\nsetting, which can converge the model in a few gradient steps. This improves\nthe convergence speed of updating the tracking model while achieving better\nperformance. We extensively evaluate our trackers, ROAM and ROAM++, on the OTB,\nVOT, LaSOT, GOT-10K and TrackingNet benchmark and our methods perform favorably\nagainst state-of-the-art algorithms."}, {"title": "Deformable Siamese Attention Networks for Visual Object Tracking", "authors": "Yuechen Yu, Yilei Xiong, Weilin Huang, Matthew R. Scott", "link": "https://arxiv.org/abs/2004.06711", "summary": "Siamese-based trackers have achieved excellent performance on visual object\ntracking. However, the target template is not updated online, and the features\nof the target template and search image are computed independently in a Siamese\narchitecture. In this paper, we propose Deformable Siamese Attention Networks,\nreferred to as SiamAttn, by introducing a new Siamese attention mechanism that\ncomputes deformable self-attention and cross-attention. The self attention\nlearns strong context information via spatial attention, and selectively\nemphasizes interdependent channel-wise features with channel attention. The\ncross-attention is capable of aggregating rich contextual inter-dependencies\nbetween the target template and the search image, providing an implicit manner\nto adaptively update the target template. In addition, we design a region\nrefinement module that computes depth-wise cross correlations between the\nattentional features for more accurate tracking. We conduct experiments on six\nbenchmarks, where our method achieves new state of-the-art results,\noutperforming the strong baseline, SiamRPN++ [24], by 0.464->0.537 and\n0.415->0.470 EAO on VOT 2016 and 2018."}, {"title": "15 Keypoints Is All You Need", "authors": "Michael Snower, Asim Kadav, Farley Lai, Hans Peter Graf", "link": "https://arxiv.org/abs/1912.02323", "summary": "Pose tracking is an important problem that requires identifying unique human\npose-instances and matching them temporally across different frames of a video.\nHowever, existing pose tracking methods are unable to accurately model temporal\nrelationships and require significant computation, often computing the tracks\noffline. We present an efficient Multi-person Pose Tracking method, KeyTrack,\nthat only relies on keypoint information without using any RGB or optical flow\ninformation to track human keypoints in real-time. Keypoints are tracked using\nour Pose Entailment method, in which, first, a pair of pose estimates is\nsampled from different frames in a video and tokenized. Then, a\nTransformer-based network makes a binary classification as to whether one pose\ntemporally follows another. Furthermore, we improve our top-down pose\nestimation method with a novel, parameter-free, keypoint refinement technique\nthat improves the keypoint estimates used during the Pose Entailment step. We\nachieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18\nbenchmarks while using only a fraction of the computation required by most\nother methods for computing the tracking information."}, {"title": "Optical Flow in the Dark", "authors": "Yinqiang Zheng, Mingfang Zhang, Feng Lu"}, {"title": "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation From Transformers by Self-Supervised Learning of Sketch Gestalt", "authors": "Hangyu Lin, Yanwei Fu, Xiangyang Xue, Yu-Gang Jiang", "link": "https://arxiv.org/abs/2005.09159", "summary": "Previous researches of sketches often considered sketches in pixel format and\nleveraged CNN based models in the sketch understanding. Fundamentally, a sketch\nis stored as a sequence of data points, a vector format representation, rather\nthan the photo-realistic image of pixels. SketchRNN studied a generative neural\nrepresentation for sketches of vector format by Long Short Term Memory networks\n(LSTM). Unfortunately, the representation learned by SketchRNN is primarily for\nthe generation tasks, rather than the other tasks of recognition and retrieval\nof sketches. To this end and inspired by the recent BERT model, we present a\nmodel of learning Sketch Bidirectional Encoder Representation from Transformer\n(Sketch-BERT). We generalize BERT to sketch domain, with the novel proposed\ncomponents and pre-training algorithms, including the newly designed sketch\nembedding networks, and the self-supervised learning of sketch gestalt.\nParticularly, towards the pre-training task, we present a novel Sketch Gestalt\nModel (SGM) to help train the Sketch-BERT. Experimentally, we show that the\nlearned representation of Sketch-BERT can help and improve the performance of\nthe downstream tasks of sketch recognition, sketch retrieval, and sketch\ngestalt."}, {"title": "A Unified Object Motion and Affinity Model for Online Multi-Object Tracking", "authors": "Junbo Yin, Wenguan Wang, Qinghao Meng, Ruigang Yang, Jianbing Shen", "link": "http://arxiv.org/abs/2003.11291", "summary": "Current popular online multi-object tracking (MOT) solutions apply single\nobject trackers (SOTs) to capture object motions, while often requiring an\nextra affinity network to associate objects, especially for the occluded ones.\nThis brings extra computational overhead due to repetitive feature extraction\nfor SOT and affinity computation. Meanwhile, the model size of the\nsophisticated affinity network is usually non-trivial. In this paper, we\npropose a novel MOT framework that unifies object motion and affinity model\ninto a single network, named UMA, in order to learn a compact feature that is\ndiscriminative for both object motion and affinity measure. In particular, UMA\nintegrates single object tracking and metric learning into a unified triplet\nnetwork by means of multi-task learning. Such design brings advantages of\nimproved computation efficiency, low memory requirement and simplified training\nprocedure. In addition, we equip our model with a task-specific attention\nmodule, which is used to boost task-aware feature learning. The proposed UMA\ncan be easily trained end-to-end, and is elegant - requiring only one training\nstage. Experimental results show that it achieves promising performance on\nseveral MOT Challenge benchmarks."}, {"title": "Sub-Frame Appearance and 6D Pose Estimation of Fast Moving Objects", "authors": "Denys Rozumnyi, Jan Kotera, Filip \u0160roubek, Ji\u0159\u00ed Matas", "link": "https://arxiv.org/abs/1911.10927", "summary": "We propose a novel method that tracks fast moving objects, mainly non-uniform\nspherical, in full 6 degrees of freedom, estimating simultaneously their 3D\nmotion trajectory, 3D pose and object appearance changes with a time step that\nis a fraction of the video frame exposure time. The sub-frame object\nlocalization and appearance estimation allows realistic temporal\nsuper-resolution and precise shape estimation. The method, called TbD-3D\n(Tracking by Deblatting in 3D) relies on a novel reconstruction algorithm which\nsolves a piece-wise deblurring and matting problem. The 3D rotation is\nestimated by minimizing the reprojection error. As a second contribution, we\npresent a new challenging dataset with fast moving objects that change their\nappearance and distance to the camera. High speed camera recordings with zero\nlag between frame exposures were used to generate videos with different frame\nrates annotated with ground-truth trajectory and pose."}, {"title": "How to Train Your Deep Multi-Object Tracker", "authors": "Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura Leal-Taix\u00e9, Xavier Alameda-Pineda", "link": "https://arxiv.org/abs/1906.06618", "summary": "The recent trend in vision-based multi-object tracking (MOT) is heading\ntowards leveraging the representational power of deep learning to jointly learn\nto detect and track objects. However, existing methods train only certain\nsub-modules using loss functions that often do not correlate with established\ntracking evaluation measures such as Multi-Object Tracking Accuracy (MOTA) and\nPrecision (MOTP). As these measures are not differentiable, the choice of\nappropriate loss functions for end-to-end training of multi-object tracking\nmethods is still an open research problem. In this paper, we bridge this gap by\nproposing a differentiable proxy of MOTA and MOTP, which we combine in a loss\nfunction suitable for end-to-end training of deep multi-object trackers. As a\nkey ingredient, we propose a Deep Hungarian Net (DHN) module that approximates\nthe Hungarian matching algorithm. DHN allows estimating the correspondence\nbetween object tracks and ground truth objects to compute differentiable\nproxies of MOTA and MOTP, which are in turn used to optimize deep trackers\ndirectly. We experimentally demonstrate that the proposed differentiable\nframework improves the performance of existing multi-object trackers, and we\nestablish a new state of the art on the MOTChallenge benchmark. Our code is\npublicly available from https://github.com/yihongXU/deepMOT."}, {"title": "TPNet: Trajectory Proposal Network for Motion Prediction", "authors": "Liangji Fang, Qinhong Jiang, Jianping Shi, Bolei Zhou", "link": "https://arxiv.org/abs/2004.12255", "summary": "Making accurate motion prediction of the surrounding traffic agents such as\npedestrians, vehicles, and cyclists is crucial for autonomous driving. Recent\ndata-driven motion prediction methods have attempted to learn to directly\nregress the exact future position or its distribution from massive amount of\ntrajectory data. However, it remains difficult for these methods to provide\nmultimodal predictions as well as integrate physical constraints such as\ntraffic rules and movable areas. In this work we propose a novel two-stage\nmotion prediction framework, Trajectory Proposal Network (TPNet). TPNet first\ngenerates a candidate set of future trajectories as hypothesis proposals, then\nmakes the final predictions by classifying and refining the proposals which\nmeets the physical constraints. By steering the proposal generation process,\nsafe and multimodal predictions are realized. Thus this framework effectively\nmitigates the complexity of motion prediction problem while ensuring the\nmultimodal output. Experiments on four large-scale trajectory prediction\ndatasets, i.e. the ETH, UCY, Apollo and Argoverse datasets, show that TPNet\nachieves the state-of-the-art results both quantitatively and qualitatively."}, {"title": "Large Scale Video Representation Learning via Relational Graph Clustering", "authors": "Hyodong Lee, Joonseok Lee, Joe Yue-Hei Ng, Paul Natsev"}, {"title": "Towards Universal Representation Learning for Deep Face Recognition", "authors": "Yichun Shi, Xiang Yu, Kihyuk Sohn, Manmohan Chandraker, Anil K. Jain", "link": "https://arxiv.org/abs/2002.11841", "summary": "Recognizing wild faces is extremely hard as they appear with all kinds of\nvariations. Traditional methods either train with specifically annotated\nvariation data from target domains, or by introducing unlabeled target\nvariation data to adapt from the training data. Instead, we propose a universal\nrepresentation learning framework that can deal with larger variation unseen in\nthe given training data without leveraging target domain knowledge. We firstly\nsynthesize training data alongside some semantically meaningful variations,\nsuch as low resolution, occlusion and head pose. However, directly feeding the\naugmented data for training will not converge well as the newly introduced\nsamples are mostly hard examples. We propose to split the feature embedding\ninto multiple sub-embeddings, and associate different confidence values for\neach sub-embedding to smooth the training procedure. The sub-embeddings are\nfurther decorrelated by regularizing variation classification loss and\nvariation adversarial loss on different partitions of them. Experiments show\nthat our method achieves top performance on general face recognition datasets\nsuch as LFW and MegaFace, while significantly better on extreme benchmarks such\nas TinyFace and IJB-S."}, {"title": "Robust Partial Matching for Person Search in the Wild", "authors": "Yingji Zhong, Xiaoyu Wang, Shiliang Zhang", "link": "https://arxiv.org/abs/2004.09329", "summary": "Various factors like occlusions, backgrounds, etc., would lead to misaligned\ndetected bounding boxes , e.g., ones covering only portions of human body. This\nissue is common but overlooked by previous person search works. To alleviate\nthis issue, this paper proposes an Align-to-Part Network (APNet) for person\ndetection and re-Identification (reID). APNet refines detected bounding boxes\nto cover the estimated holistic body regions, from which discriminative part\nfeatures can be extracted and aligned. Aligned part features naturally\nformulate reID as a partial feature matching procedure, where valid part\nfeatures are selected for similarity computation, while part features on\noccluded or noisy regions are discarded. This design enhances the robustness of\nperson search to real-world challenges with marginal computation overhead. This\npaper also contributes a Large-Scale dataset for Person Search in the wild\n(LSPS), which is by far the largest and the most challenging dataset for person\nsearch. Experiments show that APNet brings considerable performance improvement\non LSPS. Meanwhile, it achieves competitive performance on existing person\nsearch benchmarks like CUHK-SYSU and PRW."}, {"title": "Correlation-Guided Attention for Corner Detection Based Visual Tracking", "authors": "Fei Du, Peng Liu, Wei Zhao, Xianglong Tang"}, {"title": "Learning Multi-Object Tracking and Segmentation From Automatic Annotations", "authors": "Lorenzo Porzi, Markus Hofinger, Idoia Ruiz, Joan Serrat, Samuel Rota Bul\u00f2, Peter Kontschieder", "link": "https://arxiv.org/abs/1912.02096", "summary": "In this work we contribute a novel pipeline to automatically generate\ntraining data, and to improve over state-of-the-art multi-object tracking and\nsegmentation (MOTS) methods. Our proposed track mining algorithm turns raw\nstreet-level videos into high-fidelity MOTS training data, is scalable and\novercomes the need of expensive and time-consuming manual annotation\napproaches. We leverage state-of-the-art instance segmentation results in\ncombination with optical flow predictions, also trained on automatically\nharvested training data. Our second major contribution is MOTSNet - a deep\nlearning, tracking-by-detection architecture for MOTS - deploying a novel\nmask-pooling layer for improved object association over time. Training MOTSNet\nwith our automatically extracted data leads to significantly improved sMOTSA\nscores on the novel KITTI MOTS dataset (+1.9%/+7.5% on cars/pedestrians), and\nMOTSNet improves by +4.1% over previously best methods on the MOTSChallenge\ndataset. Our most impressive finding is that we can improve over previous\nbest-performing works, even in complete absence of manually annotated MOTS\ntraining data."}, {"title": "PandaNet: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation", "authors": "Abdallah Benzine, Florian Chabot, Bertrand Luvison, Quoc Cuong Pham, Catherine Achard", "link": "", "summary": ""}, {"title": "Rotation Consistent Margin Loss for Efficient Low-Bit Face Recognition", "authors": "Yudong Wu, Yichao Wu, Ruihao Gong, Yuanhao Lv, Ken Chen, Ding Liang, Xiaolin Hu, Xianglong Liu, Junjie Yan"}, {"title": "Joint Spatial-Temporal Optimization for Stereo 3D Object Tracking", "authors": "Peiliang Li, Jieqi Shi, Shaojie Shen", "link": "https://arxiv.org/abs/2004.09305", "summary": "Directly learning multiple 3D objects motion from sequential images is\ndifficult, while the geometric bundle adjustment lacks the ability to localize\nthe invisible object centroid. To benefit from both the powerful object\nunderstanding skill from deep neural network meanwhile tackle precise geometry\nmodeling for consistent trajectory estimation, we propose a joint\nspatial-temporal optimization-based stereo 3D object tracking method. From the\nnetwork, we detect corresponding 2D bounding boxes on adjacent images and\nregress an initial 3D bounding box. Dense object cues (local depth and local\ncoordinates) that associating to the object centroid are then predicted using a\nregion-based network. Considering both the instant localization accuracy and\nmotion consistency, our optimization models the relations between the object\ncentroid and observed cues into a joint spatial-temporal error function. All\nhistoric cues will be summarized to contribute to the current estimation by a\nper-frame marginalization strategy without repeated computation. Quantitative\nevaluation on the KITTI tracking dataset shows our approach outperforms\nprevious image-based 3D tracking methods by significant margins. We also report\nextensive results on multiple categories and larger datasets (KITTI raw and\nArgoverse Tracking) for future benchmarking."}, {"title": "Unity Style Transfer for Person Re-Identification", "authors": "Chong Liu, Xiaojun Chang, Yi-Dong Shen", "link": "https://arxiv.org/abs/2003.02068", "summary": "Style variation has been a major challenge for person re-identification,\nwhich aims to match the same pedestrians across different cameras. Existing\nworks attempted to address this problem with camera-invariant descriptor\nsubspace learning. However, there will be more image artifacts when the\ndifference between the images taken by different cameras is larger. To solve\nthis problem, we propose a UnityStyle adaption method, which can smooth the\nstyle disparities within the same camera and across different cameras.\nSpecifically, we firstly create UnityGAN to learn the style changes between\ncameras, producing shape-stable style-unity images for each camera, which is\ncalled UnityStyle images. Meanwhile, we use UnityStyle images to eliminate\nstyle differences between different images, which makes a better match between\nquery and gallery. Then, we apply the proposed method to Re-ID models,\nexpecting to obtain more style-robust depth features for querying. We conduct\nextensive experiments on widely used benchmark datasets to evaluate the\nperformance of the proposed framework, the results of which confirm the\nsuperiority of the proposed model."}, {"title": "Suppressing Uncertainties for Large-Scale Facial Expression Recognition", "authors": "Kai Wang, Xiaojiang Peng, Jianfei Yang, Shijian Lu, Yu Qiao", "link": "https://arxiv.org/abs/2002.10392", "summary": "Annotating a qualitative large-scale facial expression dataset is extremely\ndifficult due to the uncertainties caused by ambiguous facial expressions,\nlow-quality facial images, and the subjectiveness of annotators. These\nuncertainties lead to a key challenge of large-scale Facial Expression\nRecognition (FER) in deep learning era. To address this problem, this paper\nproposes a simple yet efficient Self-Cure Network (SCN) which suppresses the\nuncertainties efficiently and prevents deep networks from over-fitting\nuncertain facial images. Specifically, SCN suppresses the uncertainty from two\ndifferent aspects: 1) a self-attention mechanism over mini-batch to weight each\ntraining sample with a ranking regularization, and 2) a careful relabeling\nmechanism to modify the labels of these samples in the lowest-ranked group.\nExperiments on synthetic FER datasets and our collected WebEmotion dataset\nvalidate the effectiveness of our method. Results on public benchmarks\ndemonstrate that our SCN outperforms current state-of-the-art methods with\n\\textbf{88.14}\\% on RAF-DB, \\textbf{60.23}\\% on AffectNet, and \\textbf{89.35}\\%\non FERPlus. The code will be available at\n\\href{https://github.com/kaiwang960112/Self-Cure-Network}{https://github.com/kaiwang960112/Self-Cure-Network}."}, {"title": "Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation", "authors": "Rahul Mitra, Nitesh B. Gundavarapu, Abhishek Sharma, Arjun Jain", "link": "https://arxiv.org/abs/1908.05293", "summary": "The best performing methods for 3D human pose estimation from monocular\nimages require large amounts of in-the-wild 2D and controlled 3D pose annotated\ndatasets which are costly and require sophisticated systems to acquire. To\nreduce this annotation dependency, we propose Multiview-Consistent Semi\nSupervised Learning (MCSS) framework that utilizes similarity in pose\ninformation from unannotated, uncalibrated but synchronized multi-view videos\nof human motions as additional weak supervision signal to guide 3D human pose\nregression. Our framework applies hard-negative mining based on temporal\nrelations in multi-view videos to arrive at a multi-view consistent pose\nembedding. When jointly trained with limited 3D pose annotations, our approach\nimproves the baseline by 25% and state-of-the-art by 8.7%, whilst using\nsubstantially smaller networks. Lastly, but importantly, we demonstrate the\nadvantages of the learned embedding and establish view-invariant pose retrieval\nbenchmarks on two popular, publicly available multi-view human pose datasets,\nHuman 3.6M and MPI-INF-3DHP, to facilitate future research."}, {"title": "Regularizing Neural Networks via Minimizing Hyperspherical Energy", "authors": "Rongmei Lin, Weiyang Liu, Zhen Liu, Chen Feng, Zhiding Yu, James M. Rehg, Li Xiong, Le Song", "link": "https://arxiv.org/abs/1906.04892", "summary": "Inspired by the Thomson problem in physics where the distribution of multiple\npropelling electrons on a unit sphere can be modeled via minimizing some\npotential energy, hyperspherical energy minimization has demonstrated its\npotential in regularizing neural networks and improving their generalization\npower. In this paper, we first study the important role that hyperspherical\nenergy plays in neural network training by analyzing its training dynamics.\nThen we show that naively minimizing hyperspherical energy suffers from some\ndifficulties due to highly non-linear and non-convex optimization as the space\ndimensionality becomes higher, therefore limiting the potential to further\nimprove the generalization. To address these problems, we propose the\ncompressive minimum hyperspherical energy (CoMHE) as a more effective\nregularization for neural networks. Specifically, CoMHE utilizes projection\nmappings to reduce the dimensionality of neurons and minimizes their\nhyperspherical energy. According to different designs for the projection\nmapping, we propose several distinct yet well-performing variants and provide\nsome theoretical guarantees to justify their effectiveness. Our experiments\nshow that CoMHE consistently outperforms existing regularization methods, and\ncan be easily applied to different neural networks."}, {"title": "Learning Representations by Predicting Bags of Visual Words", "authors": "Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, Matthieu Cord", "link": "https://arxiv.org/abs/2002.12247", "summary": "Self-supervised representation learning targets to learn convnet-based image\nrepresentations from unlabeled data. Inspired by the success of NLP methods in\nthis area, in this work we propose a self-supervised approach based on\nspatially dense image descriptions that encode discrete visual concepts, here\ncalled visual words. To build such discrete representations, we quantize the\nfeature maps of a first pre-trained self-supervised convnet, over a k-means\nbased vocabulary. Then, as a self-supervised task, we train another convnet to\npredict the histogram of visual words of an image (i.e., its Bag-of-Words\nrepresentation) given as input a perturbed version of that image. The proposed\ntask forces the convnet to learn perturbation-invariant and context-aware image\nfeatures, useful for downstream image understanding tasks. We extensively\nevaluate our method and demonstrate very strong empirical results, e.g., our\npre-trained self-supervised representations transfer better on detection task\nand similarly on classification over classes \"unseen\" during pre-training, when\ncompared to the supervised case.\n  This also shows that the process of image discretization into visual words\ncan provide the basis for very powerful self-supervised approaches in the image\ndomain, thus allowing further connections to be made to related methods from\nthe NLP domain that have been extremely successful so far."}, {"title": "AnimalWeb: A Large-Scale Hierarchical Dataset of Annotated Animal Faces", "authors": "Muhammad Haris Khan, John McDonagh, Salman Khan, Muhammad Shahabuddin, Aditya Arora, Fahad Shahbaz Khan, Ling Shao, Georgios Tzimiropoulos", "link": "https://arxiv.org/abs/1909.04951", "summary": "Being heavily reliant on animals, it is our ethical obligation to improve\ntheir well-being by understanding their needs. Several studies show that animal\nneeds are often expressed through their faces. Though remarkable progress has\nbeen made towards the automatic understanding of human faces, this has\nregrettably not been the case with animal faces. There exists significant room\nand appropriate need to develop automatic systems capable of interpreting\nanimal faces. Among many transformative impacts, such a technology will foster\nbetter and cheaper animal healthcare, and further advance animal psychology\nunderstanding.\n  We believe the underlying research progress is mainly obstructed by the lack\nof an adequately annotated dataset of animal faces, covering a wide spectrum of\nanimal species. To this end, we introduce a large-scale, hierarchical annotated\ndataset of animal faces, featuring 21.9K faces from 334 diverse species and 21\nanimal orders across biological taxonomy. These faces are captured\n`in-the-wild' conditions and are consistently annotated with 9 landmarks on key\nfacial features. The proposed dataset is structured and scalable by design; its\ndevelopment underwent four systematic stages involving rigorous, manual\nannotation effort of over 6K man-hours. We benchmark it for face alignment\nusing the existing art under novel problem settings. Results showcase its\nchallenging nature, unique attributes and present definite prospects for novel,\nadaptive, and generalized face-oriented CV algorithms. We further benchmark the\ndataset for face detection and fine-grained recognition tasks, to demonstrate\nmulti-task applications and room for improvement. Experiments indicate that\nthis dataset will push the algorithmic advancements across many related CV\ntasks and encourage the development of novel systems for animal facial\nbehaviour monitoring. We will make the dataset publicly available."}, {"title": "A Transductive Approach for Video Object Segmentation", "authors": "Yizhuo Zhang, Zhirong Wu, Houwen Peng, Stephen Lin", "link": "http://arxiv.org/abs/2004.07193", "summary": "Semi-supervised video object segmentation aims to separate a target object\nfrom a video sequence, given the mask in the first frame. Most of current\nprevailing methods utilize information from additional modules trained in other\ndomains like optical flow and instance segmentation, and as a result they do\nnot compete with other methods on common ground. To address this issue, we\npropose a simple yet strong transductive method, in which additional modules,\ndatasets, and dedicated architectural designs are not needed. Our method takes\na label propagation approach where pixel labels are passed forward based on\nfeature similarity in an embedding space. Different from other propagation\nmethods, ours diffuses temporal information in a holistic manner which take\naccounts of long-term object appearance. In addition, our method requires few\nadditional computational overhead, and runs at a fast $\\sim$37 fps speed. Our\nsingle model with a vanilla ResNet50 backbone achieves an overall score of 72.3\non the DAVIS 2017 validation set and 63.1 on the test set. This simple yet high\nperforming and efficient method can serve as a solid baseline that facilitates\nfuture research. Code and models are available at\n\\url{https://github.com/microsoft/transductive-vos.pytorch}."}, {"title": "Dynamic Face Video Segmentation via Reinforcement Learning", "authors": "Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, Maja Pantic", "link": "https://arxiv.org/abs/1907.01296", "summary": "For real-time semantic video segmentation, most recent works utilised a\ndynamic framework with a key scheduler to make online key/non-key decisions.\nSome works used a fixed key scheduling policy, while others proposed adaptive\nkey scheduling methods based on heuristic strategies, both of which may lead to\nsuboptimal global performance. To overcome this limitation, we model the online\nkey decision process in dynamic video segmentation as a deep reinforcement\nlearning problem and learn an efficient and effective scheduling policy from\nexpert information about decision history and from the process of maximising\nglobal return. Moreover, we study the application of dynamic video segmentation\non face videos, a field that has not been investigated before. By evaluating on\nthe 300VW dataset, we show that the performance of our reinforcement key\nscheduler outperforms that of various baselines in terms of both effective key\nselections and running speed. Further results on the Cityscapes dataset\ndemonstrate that our proposed method can also generalise to other scenarios. To\nthe best of our knowledge, this is the first work to use reinforcement learning\nfor online key-frame decision in dynamic video segmentation, and also the first\nwork on its application on face videos."}, {"title": "Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion", "authors": "Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll", "link": "https://arxiv.org/abs/2003.01456", "summary": "While many works focus on 3D reconstruction from images, in this paper, we\nfocus on 3D shape reconstruction and completion from a variety of 3D inputs,\nwhich are deficient in some respect: low and high resolution voxels, sparse and\ndense point clouds, complete or incomplete. Processing of such 3D inputs is an\nincreasingly important problem as they are the output of 3D scanners, which are\nbecoming more accessible, and are the intermediate output of 3D computer vision\nalgorithms. Recently, learned implicit functions have shown great promise as\nthey produce continuous reconstructions. However, we identified two limitations\nin reconstruction from 3D inputs: 1) details present in the input data are not\nretained, and 2) poor reconstruction of articulated humans. To solve this, we\npropose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,\ncan handle multiple topologies, and complete shapes for missing or sparse input\ndata retaining the nice properties of recent learned implicit functions, but\ncritically they can also retain detail when it is present in the input data,\nand can reconstruct articulated humans. Our work differs from prior work in two\ncrucial aspects. First, instead of using a single vector to encode a 3D shape,\nwe extract a learnable 3-dimensional multi-scale tensor of deep features, which\nis aligned with the original Euclidean space embedding the shape. Second,\ninstead of classifying x-y-z point coordinates directly, we classify deep\nfeatures extracted from the tensor at a continuous query point. We show that\nthis forces our model to make decisions based on global and local shape\nstructure, as opposed to point coordinates, which are arbitrary under Euclidean\ntransformations. Experiments demonstrate that IF-Nets clearly outperform prior\nwork in 3D object reconstruction in ShapeNet, and obtain significantly more\naccurate 3D human reconstructions."}, {"title": "Semantic Drift Compensation for Class-Incremental Learning", "authors": "Lu Yu, Bart\u0142omiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, Joost van de Weijer", "link": "https://arxiv.org/abs/2004.00440", "summary": "Class-incremental learning of deep networks sequentially increases the number\nof classes to be classified. During training, the network has only access to\ndata of one task at a time, where each task contains several classes. In this\nsetting, networks suffer from catastrophic forgetting which refers to the\ndrastic drop in performance on previous tasks. The vast majority of methods\nhave studied this scenario for classification networks, where for each new task\nthe classification layer of the network must be augmented with additional\nweights to make room for the newly added classes. Embedding networks have the\nadvantage that new classes can be naturally included into the network without\nadding new weights. Therefore, we study incremental learning for embedding\nnetworks. In addition, we propose a new method to estimate the drift, called\nsemantic drift, of features and compensate for it without the need of any\nexemplars. We approximate the drift of previous tasks based on the drift that\nis experienced by current task data. We perform experiments on fine-grained\ndatasets, CIFAR100 and ImageNet-Subset. We demonstrate that embedding networks\nsuffer significantly less from catastrophic forgetting. We outperform existing\nmethods which do not require exemplars and obtain competitive results compared\nto methods which store exemplars. Furthermore, we show that our proposed SDC\nwhen combined with existing methods to prevent forgetting consistently improves\nresults."}, {"title": "Context-Aware Human Motion Prediction", "authors": "Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Francesc Moreno-Noguer", "link": "https://arxiv.org/abs/1904.03419", "summary": "The problem of predicting human motion given a sequence of past observations\nis at the core of many applications in robotics and computer vision. Current\nstate-of-the-art formulate this problem as a sequence-to-sequence task, in\nwhich a historical of 3D skeletons feeds a Recurrent Neural Network (RNN) that\npredicts future movements, typically in the order of 1 to 2 seconds. However,\none aspect that has been obviated so far, is the fact that human motion is\ninherently driven by interactions with objects and/or other humans in the\nenvironment. In this paper, we explore this scenario using a novel\ncontext-aware motion prediction architecture. We use a semantic-graph model\nwhere the nodes parameterize the human and objects in the scene and the edges\ntheir mutual interactions. These interactions are iteratively learned through a\ngraph attention layer, fed with the past observations, which now include both\nobject and human body motions. Once this semantic graph is learned, we inject\nit to a standard RNN to predict future movements of the human/s and object/s.\nWe consider two variants of our architecture, either freezing the contextual\ninteractions in the future of updating them. A thorough evaluation in the\n\"Whole-Body Human Motion Database\" shows that in both cases, our context-aware\nnetworks clearly outperform baselines in which the context information is not\nconsidered."}, {"title": "DeepDeform: Learning Non-Rigid RGB-D Reconstruction With Semi-Supervised Data", "authors": "Alja\u017e Bo\u017ei\u010d, Michael Zollh\u00f6fer, Christian Theobalt, Matthias Nie\u00dfner", "link": "https://arxiv.org/abs/1912.04302", "summary": "Applying data-driven approaches to non-rigid 3D reconstruction has been\ndifficult, which we believe can be attributed to the lack of a large-scale\ntraining corpus. Unfortunately, this method fails for important cases such as\nhighly non-rigid deformations. We first address this problem of lack of data by\nintroducing a novel semi-supervised strategy to obtain dense inter-frame\ncorrespondences from a sparse set of annotations. This way, we obtain a large\ndataset of 400 scenes, over 390,000 RGB-D frames, and 5,533 densely aligned\nframe pairs; in addition, we provide a test set along with several metrics for\nevaluation. Based on this corpus, we introduce a data-driven non-rigid feature\nmatching approach, which we integrate into an optimization-based reconstruction\npipeline. Here, we propose a new neural network that operates on RGB-D frames,\nwhile maintaining robustness under large non-rigid deformations and producing\naccurate predictions. Our approach significantly outperforms existing non-rigid\nreconstruction methods that do not use learned data terms, as well as\nlearning-based approaches that only use self-supervision."}, {"title": "Optical Non-Line-of-Sight Physics-Based 3D Human Pose Estimation", "authors": "Mariko Isogawa, Ye Yuan, Matthew O'Toole, Kris M. Kitani", "link": "http://arxiv.org/abs/2003.14414", "summary": "We describe a method for 3D human pose estimation from transient images\n(i.e., a 3D spatio-temporal histogram of photons) acquired by an optical\nnon-line-of-sight (NLOS) imaging system. Our method can perceive 3D human pose\nby `looking around corners' through the use of light indirectly reflected by\nthe environment. We bring together a diverse set of technologies from NLOS\nimaging, human pose estimation and deep reinforcement learning to construct an\nend-to-end data processing pipeline that converts a raw stream of photon\nmeasurements into a full 3D human pose sequence estimate. Our contributions are\nthe design of data representation process which includes (1) a learnable\ninverse point spread function (PSF) to convert raw transient images into a deep\nfeature vector; (2) a neural humanoid control policy conditioned on the\ntransient image feature and learned from interactions with a physics simulator;\nand (3) a data synthesis and augmentation strategy based on depth data that can\nbe transferred to a real-world NLOS imaging system. Our preliminary experiments\nsuggest that our method is able to generalize to real-world NLOS measurement to\nestimate physically-valid 3D human poses."}, {"title": "Learning to Transfer Texture From Clothing Images to 3D Humans", "authors": "Aymen Mir, Thiemo Alldieck, Gerard Pons-Moll", "link": "https://arxiv.org/abs/2003.02050", "summary": "In this paper, we present a simple yet effective method to automatically\ntransfer textures of clothing images (front and back) to 3D garments worn on\ntop SMPL, in real time. We first automatically compute training pairs of images\nwith aligned 3D garments using a custom non-rigid 3D to 2D registration method,\nwhich is accurate but slow. Using these pairs, we learn a mapping from pixels\nto the 3D garment surface. Our idea is to learn dense correspondences from\ngarment image silhouettes to a 2D-UV map of a 3D garment surface using shape\ninformation alone, completely ignoring texture, which allows us to generalize\nto the wide range of web images. Several experiments demonstrate that our model\nis more accurate than widely used baselines such as thin-plate-spline warping\nand image-to-image translation networks while being orders of magnitude faster.\nOur model opens the door for applications such as virtual try-on, and allows\nfor generation of 3D humans with varied textures which is necessary for\nlearning."}, {"title": "UniPose: Unified Human Pose Estimation in Single Images and Videos", "authors": "Bruno Artacho, Andreas Savakis", "link": "https://arxiv.org/abs/2001.08095", "summary": "We propose UniPose, a unified framework for human pose estimation, based on\nour \"Waterfall\" Atrous Spatial Pooling architecture, that achieves\nstate-of-art-results on several pose estimation metrics. Current pose\nestimation methods utilizing standard CNN architectures heavily rely on\nstatistical postprocessing or predefined anchor poses for joint localization.\nUniPose incorporates contextual segmentation and joint localization to estimate\nthe human pose in a single stage, with high accuracy, without relying on\nstatistical postprocessing methods. The Waterfall module in UniPose leverages\nthe efficiency of progressive filtering in the cascade architecture, while\nmaintaining multi-scale fields-of-view comparable to spatial pyramid\nconfigurations. Additionally, our method is extended to UniPose-LSTM for\nmulti-frame processing and achieves state-of-the-art results for temporal pose\nestimation in Video. Our results on multiple datasets demonstrate that UniPose,\nwith a ResNet backbone and Waterfall module, is a robust and efficient\narchitecture for pose estimation obtaining state-of-the-art results in single\nperson pose detection for both single images and videos."}, {"title": "Minimal Solutions to Relative Pose Estimation From Two Views Sharing a Common Direction With Unknown Focal Length", "authors": "Yaqing Ding, Jian Yang, Jean Ponce, Hui Kong"}, {"title": "3D Human Mesh Regression With Dense Correspondence", "authors": "Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, Xiaogang Wang"}, {"title": "Cross-Modal Pattern-Propagation for RGB-T Tracking", "authors": "Chaoqun Wang, Chunyan Xu, Zhen Cui, Ling Zhou, Tong Zhang, Xiaoya Zhang, Jian Yang"}, {"title": "Distilling Knowledge From Graph Convolutional Networks", "authors": "Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, Xinchao Wang", "link": "https://arxiv.org/abs/2003.10477", "summary": "Existing knowledge distillation methods focus on convolutional neural\nnetworks~(CNNs), where the input samples like images lie in a grid domain, and\nhave largely overlooked graph convolutional networks~(GCN) that handle non-grid\ndata. In this paper, we propose to our best knowledge the first dedicated\napproach to {distilling} knowledge from a pre-trained GCN model. To enable the\nknowledge transfer from the teacher GCN to the student, we propose a local\nstructure preserving module that explicitly accounts for the topological\nsemantics of the teacher. In this module, the local structure information from\nboth the teacher and the student are extracted as distributions, and hence\nminimizing the distance between these distributions enables topology-aware\nknowledge transfer from the teacher, yielding a compact yet high-performance\nstudent model. Moreover, the proposed approach is readily extendable to dynamic\ngraph models, where the input graphs for the teacher and the student may\ndiffer. We evaluate the proposed method on two different datasets using GCN\nmodels of different architectures, and demonstrate that our method achieves the\nstate-of-the-art knowledge distillation performance for GCN models."}, {"title": "Learning Identity-Invariant Motion Representations for Cross-ID Face Reenactment", "authors": "Po-Hsiang Huang, Fu-En Yang, Yu-Chiang Frank Wang"}, {"title": "Distribution-Aware Coordinate Representation for Human Pose Estimation", "authors": "Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, Ce Zhu", "link": "https://arxiv.org/abs/1910.06278", "summary": "While being the de facto standard coordinate representation in human pose\nestimation, heatmap is never systematically investigated in the literature, to\nour best knowledge. This work fills this gap by studying the coordinate\nrepresentation with a particular focus on the heatmap. Interestingly, we found\nthat the process of decoding the predicted heatmaps into the final joint\ncoordinates in the original image space is surprisingly significant for human\npose estimation performance, which nevertheless was not recognised before. In\nlight of the discovered importance, we further probe the design limitations of\nthe standard coordinate decoding method widely used by existing methods, and\npropose a more principled distribution-aware decoding method. Meanwhile, we\nimprove the standard coordinate encoding process (i.e. transforming\nground-truth coordinates to heatmaps) by generating accurate heatmap\ndistributions for unbiased model training. Taking the two together, we\nformulate a novel Distribution-Aware coordinate Representation of Keypoint\n(DARK) method. Serving as a model-agnostic plug-in, DARK significantly improves\nthe performance of a variety of state-of-the-art human pose estimation models.\nExtensive experiments show that DARK yields the best results on two common\nbenchmarks, MPII and COCO, consistently validating the usefulness and\neffectiveness of our novel coordinate representation idea."}, {"title": "Parsing-Based View-Aware Embedding Network for Vehicle Re-Identification", "authors": "Dechao Meng, Liang Li, Xuejing Liu, Yadong Li, Shijie Yang, Zheng-Jun Zha, Xingyu Gao, Shuhui Wang, Qingming Huang", "link": "https://arxiv.org/abs/2004.05021", "summary": "Vehicle Re-Identification is to find images of the same vehicle from various\nviews in the cross-camera scenario. The main challenges of this task are the\nlarge intra-instance distance caused by different views and the subtle\ninter-instance discrepancy caused by similar vehicles. In this paper, we\npropose a parsing-based view-aware embedding network (PVEN) to achieve the\nview-aware feature alignment and enhancement for vehicle ReID. First, we\nintroduce a parsing network to parse a vehicle into four different views, and\nthen align the features by mask average pooling. Such alignment provides a\nfine-grained representation of the vehicle. Second, in order to enhance the\nview-aware features, we design a common-visible attention to focus on the\ncommon visible views, which not only shortens the distance among\nintra-instances, but also enlarges the discrepancy of inter-instances. The PVEN\nhelps capture the stable discriminative information of vehicle under different\nviews. The experiments conducted on three datasets show that our model\noutperforms state-of-the-art methods by a large margin."}, {"title": "HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose Estimation From a Single Depth Map", "authors": "Jameel Malik, Ibrahim Abdelaziz, Ahmed Elhayek, Soshi Shimada, Sk Aziz Ali, Vladislav Golyanik, Christian Theobalt, Didier Stricker", "link": "https://arxiv.org/abs/2004.01588", "summary": "3D hand shape and pose estimation from a single depth map is a new and\nchallenging computer vision problem with many applications. The\nstate-of-the-art methods directly regress 3D hand meshes from 2D depth images\nvia 2D convolutional neural networks, which leads to artefacts in the\nestimations due to perspective distortions in the images. In contrast, we\npropose a novel architecture with 3D convolutions trained in a\nweakly-supervised manner. The input to our method is a 3D voxelized depth map,\nand we rely on two hand shape representations. The first one is the 3D\nvoxelized grid of the shape which is accurate but does not preserve the mesh\ntopology and the number of mesh vertices. The second representation is the 3D\nhand surface which is less accurate but does not suffer from the limitations of\nthe first representation. We combine the advantages of these two\nrepresentations by registering the hand surface to the voxelized hand shape. In\nthe extensive experiments, the proposed approach improves over the state of the\nart by 47.8% on the SynHand5M dataset. Moreover, our augmentation policy for\nvoxelized depth maps further enhances the accuracy of 3D hand pose estimation\non real data. Our method produces visually more reasonable and realistic hand\nshapes on NYU and BigHand2.2M datasets compared to the existing approaches."}, {"title": "Determinant Regularization for Gradient-Efficient Graph Matching", "authors": "Tianshu Yu, Junchi Yan, Baoxin Li"}, {"title": "D3S \u2013 A Discriminative Single Shot Segmentation Tracker", "authors": "Alan Luke\u017ei\u010d, Ji\u0159\u00ed Matas, Matej Kristan", "link": "https://arxiv.org/abs/1911.08862", "summary": "Template-based discriminative trackers are currently the dominant tracking\nparadigm due to their robustness, but are restricted to bounding box tracking\nand a limited range of transformation models, which reduces their localization\naccuracy. We propose a discriminative single-shot segmentation tracker - D3S,\nwhich narrows the gap between visual object tracking and video object\nsegmentation. A single-shot network applies two target models with\ncomplementary geometric properties, one invariant to a broad range of\ntransformations, including non-rigid deformations, the other assuming a rigid\nobject to simultaneously achieve high robustness and online target\nsegmentation. Without per-dataset finetuning and trained only for segmentation\nas the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and\nGOT-10k benchmarks and performs close to the state-of-the-art trackers on the\nTrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video\nobject segmentation benchmark and performs on par with top video object\nsegmentation algorithms, while running an order of magnitude faster, close to\nreal-time."}, {"title": "MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction", "authors": "Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo", "link": "", "summary": ""}, {"title": "End-to-End Model-Free Reinforcement Learning for Urban Driving Using Implicit Affordances", "authors": "Marin Toromanoff, Emilie Wirbel, Fabien Moutarde", "link": "https://arxiv.org/abs/1911.10868", "summary": "Reinforcement Learning (RL) aims at learning an optimal behavior policy from\nits own experiments and not rule-based control methods. However, there is no RL\nalgorithm yet capable of handling a task as difficult as urban driving. We\npresent a novel technique, coined implicit affordances, to effectively leverage\nRL for urban driving thus including lane keeping, pedestrians and vehicles\navoidance, and traffic light detection. To our knowledge we are the first to\npresent a successful RL agent handling such a complex task especially regarding\nthe traffic light detection. Furthermore, we have demonstrated the\neffectiveness of our method by winning the Camera Only track of the CARLA\nchallenge."}, {"title": "GraphTER: Unsupervised Learning of Graph Transformation Equivariant Representations via Auto-Encoding Node-Wise Transformations", "authors": "Xiang Gao, Wei Hu, Guo-Jun Qi", "link": "https://arxiv.org/abs/1911.08142", "summary": "Recent advances in Graph Convolutional Neural Networks (GCNNs) have shown\ntheir efficiency for non-Euclidean data on graphs, which often require a large\namount of labeled data with high cost. It it thus critical to learn graph\nfeature representations in an unsupervised manner in practice. To this end, we\npropose a novel unsupervised learning of Graph Transformation Equivariant\nRepresentations (GraphTER), aiming to capture intrinsic patterns of graph\nstructure under both global and local transformations. Specifically, we allow\nto sample different groups of nodes from a graph and then transform them\nnode-wise isotropically or anisotropically. Then, we self-train a\nrepresentation encoder to capture the graph structures by reconstructing these\nnode-wise transformations from the feature representations of the original and\ntransformed graphs. In experiments, we apply the learned GraphTER to graphs of\n3D point cloud data, and results on point cloud segmentation/classification\nshow that GraphTER significantly outperforms state-of-the-art unsupervised\napproaches and pushes greatly closer towards the upper bound set by the fully\nsupervised counterparts. The code is available at:\nhttps://github.com/gyshgx868/graph-ter."}, {"title": "Can Facial Pose and Expression Be Separated With Weak Perspective Camera?", "authors": "Evangelos Sariyanidi, Casey J. Zampella, Robert T. Schultz, Birkan Tunc"}, {"title": "Probabilistic Regression for Visual Tracking", "authors": "Martin Danelljan, Luc Van Gool, Radu Timofte", "link": "https://arxiv.org/abs/2003.12565", "summary": "Visual tracking is fundamentally the problem of regressing the state of the\ntarget in each video frame. While significant progress has been achieved,\ntrackers are still prone to failures and inaccuracies. It is therefore crucial\nto represent the uncertainty in the target estimation. Although current\nprominent paradigms rely on estimating a state-dependent confidence score, this\nvalue lacks a clear probabilistic interpretation, complicating its use.\n  In this work, we therefore propose a probabilistic regression formulation and\napply it to tracking. Our network predicts the conditional probability density\nof the target state given an input image. Crucially, our formulation is capable\nof modeling label noise stemming from inaccurate annotations and ambiguities in\nthe task. The regression network is trained by minimizing the Kullback-Leibler\ndivergence. When applied for tracking, our formulation not only allows a\nprobabilistic representation of the output, but also substantially improves the\nperformance. Our tracker sets a new state-of-the-art on six datasets, achieving\n59.8% AUC on LaSOT and 75.8% Success on TrackingNet. The code and models are\navailable at https://github.com/visionml/pytracking."}, {"title": "3DRegNet: A Deep Neural Network for 3D Point Registration", "authors": "G. Dias Pais, Srikumar Ramalingam, Venu Madhav Govindu, Jacinto C. Nascimento, Rama Chellappa, Pedro Miraldo", "link": "https://arxiv.org/abs/1904.01701", "summary": "We present 3DRegNet, a novel deep learning architecture for the registration\nof 3D scans. Given a set of 3D point correspondences, we build a deep neural\nnetwork to address the following two challenges: (i) classification of the\npoint correspondences into inliers/outliers, and (ii) regression of the motion\nparameters that align the scans into a common reference frame. With regard to\nregression, we present two alternative approaches: (i) a Deep Neural Network\n(DNN) registration and (ii) a Procrustes approach using SVD to estimate the\ntransformation. Our correspondence-based approach achieves a higher speedup\ncompared to competing baselines. We further propose the use of a refinement\nnetwork, which consists of a smaller 3DRegNet as a refinement to improve the\naccuracy of the registration. Extensive experiments on two challenging datasets\ndemonstrate that we outperform other methods and achieve state-of-the-art\nresults. The code is available."}, {"title": "Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation", "authors": "Matteo Fabbri, Fabio Lanzi, Simone Calderara, Stefano Alletto, Rita Cucchiara", "link": "https://arxiv.org/abs/2004.00329", "summary": "In this paper we present a novel approach for bottom-up multi-person 3D human\npose estimation from monocular RGB images. We propose to use high resolution\nvolumetric heatmaps to model joint locations, devising a simple and effective\ncompression method to drastically reduce the size of this representation. At\nthe core of the proposed method lies our Volumetric Heatmap Autoencoder, a\nfully-convolutional network tasked with the compression of ground-truth\nheatmaps into a dense intermediate representation. A second model, the Code\nPredictor, is then trained to predict these codes, which can be decompressed at\ntest time to re-obtain the original representation. Our experimental evaluation\nshows that our method performs favorably when compared to state of the art on\nboth multi-person and single-person 3D human pose estimation datasets and,\nthanks to our novel compression strategy, can process full-HD images at the\nconstant runtime of 8 fps regardless of the number of subjects in the scene.\nCode and models available at https://github.com/fabbrimatteo/LoCO ."}, {"title": "Three-Dimensional Reconstruction of Human Interactions", "authors": "Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, Cristian Sminchisescu"}, {"title": "Distribution-Induced Bidirectional Generative Adversarial Network for Graph Representation Learning", "authors": "Shuai Zheng, Zhenfeng Zhu, Xingxing Zhang, Zhizhe Liu, Jian Cheng, Yao Zhao", "link": "https://arxiv.org/abs/1912.01899", "summary": "Graph representation learning aims to encode all nodes of a graph into\nlow-dimensional vectors that will serve as input of many compute vision tasks.\nHowever, most existing algorithms ignore the existence of inherent data\ndistribution and even noises. This may significantly increase the phenomenon of\nover-fitting and deteriorate the testing accuracy. In this paper, we propose a\nDistribution-induced Bidirectional Generative Adversarial Network (named DBGAN)\nfor graph representation learning. Instead of the widely used normal\ndistribution assumption, the prior distribution of latent representation in our\nDBGAN is estimated in a structure-aware way, which implicitly bridges the graph\nand feature spaces by prototype learning. Thus discriminative and robust\nrepresentations are generated for all nodes. Furthermore, to improve their\ngeneralization ability while preserving representation ability, the\nsample-level and distribution-level consistency is well balanced via a\nbidirectional adversarial learning framework. An extensive group of experiments\nare then carefully designed and presented, demonstrating that our DBGAN obtains\nremarkably more favorable trade-off between representation and robustness, and\nmeanwhile is dimension-efficient, over currently available alternatives in\nvarious tasks."}, {"title": "Minimal Solvers for 3D Scan Alignment With Pairs of Intersecting Lines", "authors": "Andr\u00e9 Mateus, Srikumar Ramalingam, Pedro Miraldo"}, {"title": "Wavelet Integrated CNNs for Noise-Robust Image Classification", "authors": "Qiufu Li, Linlin Shen, Sheng Guo, Zhihui Lai", "link": "https://arxiv.org/abs/2005.03337", "summary": "Convolutional Neural Networks (CNNs) are generally prone to noise\ninterruptions, i.e., small image noise can cause drastic changes in the output.\nTo suppress the noise effect to the final predication, we enhance CNNs by\nreplacing max-pooling, strided-convolution, and average-pooling with Discrete\nWavelet Transform (DWT). We present general DWT and Inverse DWT (IDWT) layers\napplicable to various wavelets like Haar, Daubechies, and Cohen, etc., and\ndesign wavelet integrated CNNs (WaveCNets) using these layers for image\nclassification. In WaveCNets, feature maps are decomposed into the\nlow-frequency and high-frequency components during the down-sampling. The\nlow-frequency component stores main information including the basic object\nstructures, which is transmitted into the subsequent layers to extract robust\nhigh-level features. The high-frequency components, containing most of the data\nnoise, are dropped during inference to improve the noise-robustness of the\nWaveCNets. Our experimental results on ImageNet and ImageNet-C (the noisy\nversion of ImageNet) show that WaveCNets, the wavelet integrated versions of\nVGG, ResNets, and DenseNet, achieve higher accuracy and better noise-robustness\nthan their vanilla versions."}, {"title": "Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning", "authors": "Byungsoo Ko, Geonmo Gu", "link": "https://arxiv.org/abs/2003.02546", "summary": "Learning the distance metric between pairs of samples has been studied for\nimage retrieval and clustering. With the remarkable success of pair-based\nmetric learning losses, recent works have proposed the use of generated\nsynthetic points on metric learning losses for augmentation and generalization.\nHowever, these methods require additional generative networks along with the\nmain network, which can lead to a larger model size, slower training speed, and\nharder optimization. Meanwhile, post-processing techniques, such as query\nexpansion and database augmentation, have proposed the combination of feature\npoints to obtain additional semantic information. In this paper, inspired by\nquery expansion and database augmentation, we propose an augmentation method in\nan embedding space for pair-based metric learning losses, called embedding\nexpansion. The proposed method generates synthetic points containing augmented\ninformation by a combination of feature points and performs hard negative pair\nmining to learn with the most informative feature representations. Because of\nits simplicity and flexibility, it can be used for existing metric learning\nlosses without affecting model size, training speed, or optimization\ndifficulty. Finally, the combination of embedding expansion and representative\nmetric learning losses outperforms the state-of-the-art losses and previous\nsample generation methods in both image retrieval and clustering tasks. The\nimplementation is publicly available."}, {"title": "PropagationNet: Propagate Points to Curve to Learn Structure Information", "authors": "Xiehe Huang, Weihong Deng, Haifeng Shen, Xiubao Zhang, Jieping Ye"}, {"title": "Sequential 3D Human Pose and Shape Estimation From Point Clouds", "authors": "Kangkan Wang, Jin Xie, Guofeng Zhang, Lei Liu, Jian Yang"}, {"title": "Improving the Robustness of Capsule Networks to Image Affine Transformations", "authors": "Jindong Gu, Volker Tresp", "link": "https://arxiv.org/abs/1911.07968", "summary": "Convolutional neural networks (CNNs) achieve translational invariance by\nusing pooling operations. However, the operations do not preserve the spatial\nrelationships in the learned representations. Hence, CNNs cannot extrapolate to\nvarious geometric transformations of inputs. Recently, Capsule Networks\n(CapsNets) have been proposed to tackle this problem. In CapsNets, each entity\nis represented by a vector and routed to high-level entity representations by a\ndynamic routing algorithm. CapsNets have been shown to be more robust than CNNs\nto affine transformations of inputs. However, there is still a huge gap between\ntheir performance on transformed inputs compared to untransformed versions. In\nthis work, we first revisit the routing procedure by (un)rolling its forward\nand backward passes. Our investigation reveals that the routing procedure\ncontributes neither to the generalization ability nor to the affine robustness\nof the CapsNets. Furthermore, we explore the limitations of capsule\ntransformations and propose affine CapsNets (Aff-CapsNets), which are more\nrobust to affine transformations. On our benchmark task, where models are\ntrained on the MNIST dataset and tested on the AffNIST dataset, our\nAff-CapsNets improve the benchmark performance by a large margin (from 79% to\n93.21%), without using any routing mechanism."}, {"title": "Noise Modeling, Synthesis and Classification for Generic Object Anti-Spoofing", "authors": "Joel Stehouwer, Amin Jourabloo, Yaojie Liu, Xiaoming Liu", "link": "https://arxiv.org/abs/2003.13043", "summary": "Using printed photograph and replaying videos of biometric modalities, such\nas iris, fingerprint and face, are common attacks to fool the recognition\nsystems for granting access as the genuine user. With the growing online\nperson-to-person shopping (e.g., Ebay and Craigslist), such attacks also\nthreaten those services, where the online photo illustration might not be\ncaptured from real items but from paper or digital screen. Thus, the study of\nanti-spoofing should be extended from modality-specific solutions to\ngeneric-object-based ones. In this work, we define and tackle the problem of\nGeneric Object Anti-Spoofing (GOAS) for the first time. One significant cue to\ndetect these attacks is the noise patterns introduced by the capture sensors\nand spoof mediums. Different sensor/medium combinations can result in diverse\nnoise patterns. We propose a GAN-based architecture to synthesize and identify\nthe noise patterns from seen and unseen medium/sensor combinations. We show\nthat the procedure of synthesis and identification are mutually beneficial. We\nfurther demonstrate the learned GOAS models can directly contribute to\nmodality-specific anti-spoofing without domain transfer. The code and GOSet\ndataset are available at cvlab.cse.msu.edu/project-goas.html."}, {"title": "Quaternion Product Units for Deep Learning on 3D Rotation Groups", "authors": "Xuan Zhang, Shaofei Qin, Yi Xu, Hongteng Xu", "link": "https://arxiv.org/abs/1912.07791", "summary": "We propose a novel quaternion product unit (QPU) to represent data on 3D\nrotation groups. The QPU leverages quaternion algebra and the law of 3D\nrotation group, representing 3D rotation data as quaternions and merging them\nvia a weighted chain of Hamilton products. We prove that the representations\nderived by the proposed QPU can be disentangled into \"rotation-invariant\"\nfeatures and \"rotation-equivariant\" features, respectively, which supports the\nrationality and the efficiency of the QPU in theory. We design quaternion\nneural networks based on our QPUs and make our models compatible with existing\ndeep learning models. Experiments on both synthetic and real-world data show\nthat the proposed QPU is beneficial for the learning tasks requiring rotation\nrobustness."}, {"title": "Unsupervised Representation Learning for Gaze Estimation", "authors": "Yu Yu, Jean-Marc Odobez", "link": "https://arxiv.org/abs/1911.06939", "summary": "Although automatic gaze estimation is very important to a large variety of\napplication areas, it is difficult to train accurate and robust gaze models, in\ngreat part due to the difficulty in collecting large and diverse data\n(annotating 3D gaze is expensive and existing datasets use different setups).\nTo address this issue, our main contribution in this paper is to propose an\neffective approach to learn a low dimensional gaze representation without gaze\nannotations, which to the best of our best knowledge, is the first work to do\nso. The main idea is to rely on a gaze redirection network and use the gaze\nrepresentation difference of the input and target images (of the redirection\nnetwork) as the redirection variable. A redirection loss in image domain allows\nthe joint training of both the redirection network and the gaze representation\nnetwork. In addition, we propose a warping field regularization which not only\nprovides an explicit physical meaning to the gaze representations but also\navoids redirection distortions. Promising results on few-shot gaze estimation\n(competitive results can be achieved with as few as <= 100 calibration\nsamples), cross-dataset gaze estimation, gaze network pretraining, and another\ntask (head pose estimation) demonstrate the validity of our framework."}, {"title": "P\u2013nets: Deep Polynomial Neural Networks", "authors": "Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, Stefanos Zafeiriou", "link": "http://arxiv.org/abs/2003.03828", "summary": "Deep Convolutional Neural Networks (DCNNs) is currently the method of choice\nboth for generative, as well as for discriminative learning in computer vision\nand machine learning. The success of DCNNs can be attributed to the careful\nselection of their building blocks (e.g., residual blocks, rectifiers,\nsophisticated normalization schemes, to mention but a few). In this paper, we\npropose $\\Pi$-Nets, a new class of DCNNs. $\\Pi$-Nets are polynomial neural\nnetworks, i.e., the output is a high-order polynomial of the input. $\\Pi$-Nets\ncan be implemented using special kind of skip connections and their parameters\ncan be represented via high-order tensors. We empirically demonstrate that\n$\\Pi$-Nets have better representation power than standard DCNNs and they even\nproduce good results without the use of non-linear activation functions in a\nlarge battery of tasks and signals, i.e., images, graphs, and audio. When used\nin conjunction with activation functions, $\\Pi$-Nets produce state-of-the-art\nresults in challenging tasks, such as image generation. Lastly, our framework\nelucidates why recent generative models, such as StyleGAN, improve upon their\npredecessors, e.g., ProGAN."}, {"title": "Hierarchically Robust Representation Learning", "authors": "Qi Qian, Juhua Hu, Hao Li", "link": "https://arxiv.org/abs/1911.04047", "summary": "With the tremendous success of deep learning in visual tasks, the\nrepresentations extracted from intermediate layers of learned models, that is,\ndeep features, attract much attention of researchers. Previous empirical\nanalysis shows that those features can contain appropriate semantic\ninformation. Therefore, with a model trained on a large-scale benchmark data\nset (e.g., ImageNet), the extracted features can work well on other tasks. In\nthis work, we investigate this phenomenon and demonstrate that deep features\ncan be suboptimal due to the fact that they are learned by minimizing the\nempirical risk. When the data distribution of the target task is different from\nthat of the benchmark data set, the performance of deep features can degrade.\nHence, we propose a hierarchically robust optimization method to learn more\ngeneric features. Considering the example-level and concept-level robustness\nsimultaneously, we formulate the problem as a distributionally robust\noptimization problem with Wasserstein ambiguity set constraints, and an\nefficient algorithm with the conventional training pipeline is proposed.\nExperiments on benchmark data sets demonstrate the effectiveness of the robust\ndeep representations."}, {"title": "How Useful Is Self-Supervised Pretraining for Visual Tasks?", "authors": "Alejandro Newell, Jia Deng", "link": "http://arxiv.org/abs/2003.14323", "summary": "Recent advances have spurred incredible progress in self-supervised\npretraining for vision. We investigate what factors may play a role in the\nutility of these pretraining methods for practitioners. To do this, we evaluate\nvarious self-supervised algorithms across a comprehensive array of synthetic\ndatasets and downstream tasks. We prepare a suite of synthetic data that\nenables an endless supply of annotated images as well as full control over\ndataset difficulty. Our experiments offer insights into how the utility of\nself-supervision changes as the number of available labels grows as well as how\nthe utility changes as a function of the downstream task and the properties of\nthe training data. We also find that linear evaluation does not correlate with\nfinetuning performance. Code and data is available at\n\\href{https://www.github.com/princeton-vl/selfstudy}{github.com/princeton-vl/selfstudy}."}, {"title": "Copy and Paste GAN: Face Hallucination From Shaded Thumbnails", "authors": "Yang Zhang, Ivor W. Tsang, Yawei Luo, Chang-Hui Hu, Xiaobo Lu, Xin Yu", "link": "https://arxiv.org/abs/2002.10650", "summary": "Existing face hallucination methods based on convolutional neural networks\n(CNN) have achieved impressive performance on low-resolution (LR) faces in a\nnormal illumination condition. However, their performance degrades dramatically\nwhen LR faces are captured in low or non-uniform illumination conditions. This\npaper proposes a Copy and Paste Generative Adversarial Network (CPGAN) to\nrecover authentic high-resolution (HR) face images while compensating for low\nand non-uniform illumination. To this end, we develop two key components in our\nCPGAN: internal and external Copy and Paste nets (CPnets). Specifically, our\ninternal CPnet exploits facial information residing in the input image to\nenhance facial details; while our external CPnet leverages an external HR face\nfor illumination compensation. A new illumination compensation loss is thus\ndeveloped to capture illumination from the external guided face image\neffectively. Furthermore, our method offsets illumination and upsamples facial\ndetails alternately in a coarse-to-fine fashion, thus alleviating the\ncorrespondence ambiguity between LR inputs and external HR inputs. Extensive\nexperiments demonstrate that our method manifests authentic HR face images in a\nuniform illumination condition and outperforms state-of-the-art methods\nqualitatively and quantitatively."}, {"title": "TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style", "authors": "Chaitanya Patel, Zhouyingcheng Liao, Gerard Pons-Moll", "link": "https://arxiv.org/abs/2003.04583", "summary": "In this paper, we present TailorNet, a neural model which predicts clothing\ndeformation in 3D as a function of three factors: pose, shape and style\n(garment geometry), while retaining wrinkle detail. This goes beyond prior\nmodels, which are either specific to one style and shape, or generalize to\ndifferent shapes producing smooth results, despite being style specific. Our\nhypothesis is that (even non-linear) combinations of examples smooth out high\nfrequency components such as fine-wrinkles, which makes learning the three\nfactors jointly hard. At the heart of our technique is a decomposition of\ndeformation into a high frequency and a low frequency component. While the\nlow-frequency component is predicted from pose, shape and style parameters with\nan MLP, the high-frequency component is predicted with a mixture of shape-style\nspecific pose models. The weights of the mixture are computed with a narrow\nbandwidth kernel to guarantee that only predictions with similar high-frequency\npatterns are combined. The style variation is obtained by computing, in a\ncanonical pose, a subspace of deformation, which satisfies physical constraints\nsuch as inter-penetration, and draping on the body. TailorNet delivers 3D\ngarments which retain the wrinkles from the physics based simulations (PBS) it\nis learned from, while running more than 1000 times faster. In contrast to PBS,\nTailorNet is easy to use and fully differentiable, which is crucial for\ncomputer vision algorithms. Several experiments demonstrate TailorNet produces\nmore realistic results than prior work, and even generates temporally coherent\ndeformations on sequences of the AMASS dataset, despite being trained on static\nposes from a different dataset. To stimulate further research in this\ndirection, we will make a dataset consisting of 55800 frames, as well as our\nmodel publicly available at https://virtualhumans.mpi-inf.mpg.de/tailornet."}, {"title": "Object-Occluded Human Shape and Pose Estimation From a Single Color Image", "authors": "Tianshu Zhang, Buzhen Huang, Yangang Wang"}, {"title": "Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking", "authors": "Jin Gao, Weiming Hu, Yan Lu"}, {"title": "Self-Supervised Monocular Scene Flow Estimation", "authors": "Junhwa Hur, Stefan Roth", "link": "https://arxiv.org/abs/2004.04143", "summary": "Scene flow estimation has been receiving increasing attention for 3D\nenvironment perception. Monocular scene flow estimation -- obtaining 3D\nstructure and 3D motion from two temporally consecutive images -- is a highly\nill-posed problem, and practical solutions are lacking to date. We propose a\nnovel monocular scene flow method that yields competitive accuracy and\nreal-time performance. By taking an inverse problem view, we design a single\nconvolutional neural network (CNN) that successfully estimates depth and 3D\nmotion simultaneously from a classical optical flow cost volume. We adopt\nself-supervised learning with 3D loss functions and occlusion reasoning to\nleverage unlabeled data. We validate our design choices, including the proxy\nloss and augmentation setup. Our model achieves state-of-the-art accuracy among\nunsupervised/self-supervised learning approaches to monocular scene flow, and\nyields competitive results for the optical flow and monocular depth estimation\nsub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields\npromising results in real-time."}, {"title": "Learning Fast and Robust Target Models for Video Object Segmentation", "authors": "Andreas Robinson, Felix J\u00e4remo Lawin, Martin Danelljan, Fahad Shahbaz Khan, Michael Felsberg", "link": "https://arxiv.org/abs/2003.00908", "summary": "Video object segmentation (VOS) is a highly challenging problem since the\ninitial mask, defining the target object, is only given at test-time. The main\ndifficulty is to effectively handle appearance changes and similar background\nobjects, while maintaining accurate segmentation. Most previous approaches\nfine-tune segmentation networks on the first frame, resulting in impractical\nframe-rates and risk of overfitting. More recent methods integrate generative\ntarget appearance models, but either achieve limited robustness or require\nlarge amounts of training data.\n  We propose a novel VOS architecture consisting of two network components. The\ntarget appearance model consists of a light-weight module, which is learned\nduring the inference stage using fast optimization techniques to predict a\ncoarse but robust target segmentation. The segmentation model is exclusively\ntrained offline, designed to process the coarse scores into high quality\nsegmentation masks. Our method is fast, easily trainable and remains highly\neffective in cases of limited training data. We perform extensive experiments\non the challenging YouTube-VOS and DAVIS datasets. Our network achieves\nfavorable performance, while operating at higher frame-rates compared to\nstate-of-the-art. Code and trained models are available at\nhttps://github.com/andr345/frtm-vos."}, {"title": "Reciprocal Learning Networks for Human Trajectory Prediction", "authors": "Hao Sun, Zhiqun Zhao, Zhihai He", "link": "https://arxiv.org/abs/2004.04340", "summary": "We observe that the human trajectory is not only forward predictable, but\nalso backward predictable. Both forward and backward trajectories follow the\nsame social norms and obey the same physical constraints with the only\ndifference in their time directions. Based on this unique property, we develop\na new approach, called reciprocal learning, for human trajectory prediction.\nTwo networks, forward and backward prediction networks, are tightly coupled,\nsatisfying the reciprocal constraint, which allows them to be jointly learned.\nBased on this constraint, we borrow the concept of adversarial attacks of deep\nneural networks, which iteratively modifies the input of the network to match\nthe given or forced network output, and develop a new method for network\nprediction, called reciprocal attack for matched prediction. It further\nimproves the prediction accuracy. Our experimental results on benchmark\ndatasets demonstrate that our new method outperforms the state-of-the-art\nmethods for human trajectory prediction."}, {"title": "Nonparametric Object and Parts Modeling With Lie Group Dynamics", "authors": "David S. Hayden, Jason Pacheco, John W. Fisher III"}, {"title": "Learning to Shadow Hand-Drawn Sketches", "authors": "Qingyuan Zheng, Zhuoru Li, Adam Bargteil", "link": "https://arxiv.org/abs/2002.11812", "summary": "We present a fully automatic method to generate detailed and accurate\nartistic shadows from pairs of line drawing sketches and lighting directions.\nWe also contribute a new dataset of one thousand examples of pairs of line\ndrawings and shadows that are tagged with lighting directions. Remarkably, the\ngenerated shadows quickly communicate the underlying 3D structure of the\nsketched scene. Consequently, the shadows generated by our approach can be used\ndirectly or as an excellent starting point for artists. We demonstrate that the\ndeep learning network we propose takes a hand-drawn sketch, builds a 3D model\nin latent space, and renders the resulting shadows. The generated shadows\nrespect the hand-drawn lines and underlying 3D space and contain sophisticated\nand accurate details, such as self-shadowing effects. Moreover, the generated\nshadows contain artistic effects, such as rim lighting or halos appearing from\nback lighting, that would be achievable with traditional 3D rendering methods."}, {"title": "Intuitive, Interactive Beard and Hair Synthesis With Generative Models", "authors": "Kyle Olszewski, Duygu Ceylan, Jun Xing, Jose Echevarria, Zhili Chen, Weikai Chen, Hao Li", "link": "https://arxiv.org/abs/2004.06848", "summary": "We present an interactive approach to synthesizing realistic variations in\nfacial hair in images, ranging from subtle edits to existing hair to the\naddition of complex and challenging hair in images of clean-shaven subjects. To\ncircumvent the tedious and computationally expensive tasks of modeling,\nrendering and compositing the 3D geometry of the target hairstyle using the\ntraditional graphics pipeline, we employ a neural network pipeline that\nsynthesizes realistic and detailed images of facial hair directly in the target\nimage in under one second. The synthesis is controlled by simple and sparse\nguide strokes from the user defining the general structural and color\nproperties of the target hairstyle. We qualitatively and quantitatively\nevaluate our chosen method compared to several alternative approaches. We show\ncompelling interactive editing results with a prototype user interface that\nallows novice users to progressively refine the generated image to match their\ndesired hairstyle, and demonstrate that our approach also allows for flexible\nand high-fidelity scalp hair synthesis."}, {"title": "Semantic Pyramid for Image Generation", "authors": "Assaf Shocher, Yossi Gandelsman, Inbar Mosseri, Michal Yarom, Michal Irani, William T. Freeman, Tali Dekel", "link": "https://arxiv.org/abs/2003.06221", "summary": "We present a novel GAN-based model that utilizes the space of deep features\nlearned by a pre-trained classification model. Inspired by classical image\npyramid representations, we construct our model as a Semantic Generation\nPyramid -- a hierarchical framework which leverages the continuum of semantic\ninformation encapsulated in such deep features; this ranges from low level\ninformation contained in fine features to high level, semantic information\ncontained in deeper features. More specifically, given a set of features\nextracted from a reference image, our model generates diverse image samples,\neach with matching features at each semantic level of the classification model.\nWe demonstrate that our model results in a versatile and flexible framework\nthat can be used in various classic and novel image generation tasks. These\ninclude: generating images with a controllable extent of semantic similarity to\na reference image, and different manipulation tasks such as\nsemantically-controlled inpainting and compositing; all achieved with the same\nmodel, with no further training."}, {"title": "SynSin: End-to-End View Synthesis From a Single Image", "authors": "Olivia Wiles, Georgia Gkioxari, Richard Szeliski, Justin Johnson", "link": "https://arxiv.org/abs/1912.08804", "summary": "Single image view synthesis allows for the generation of new views of a scene\ngiven a single input image. This is challenging, as it requires comprehensively\nunderstanding the 3D scene from a single image. As a result, current methods\ntypically use multiple images, train on ground-truth depth, or are limited to\nsynthetic data. We propose a novel end-to-end model for this task; it is\ntrained on real images without any ground-truth 3D information. To this end, we\nintroduce a novel differentiable point cloud renderer that is used to transform\na latent 3D point cloud of features into the target view. The projected\nfeatures are decoded by our refinement network to inpaint missing regions and\ngenerate a realistic output image. The 3D component inside of our generative\nmodel allows for interpretable manipulation of the latent feature space at test\ntime, e.g. we can animate trajectories from a single image. Unlike prior work,\nwe can generate high resolution images and generalise to other input\nresolutions. We outperform baselines and prior work on the Matterport, Replica,\nand RealEstate10K datasets."}, {"title": "A Characteristic Function Approach to Deep Implicit Generative Modeling", "authors": "Abdul Fatir Ansari, Jonathan Scarlett, Harold Soh", "link": "https://arxiv.org/abs/1909.07425", "summary": "In this paper, we formulate the problem of learning an Implicit Generative\nModel (IGM) as minimizing the expected distance between characteristic\nfunctions. Specifically, we match the characteristic functions of the real and\ngenerated data distributions under a suitably-chosen weighting distribution.\nThis distance measure, which we term as the characteristic function distance\n(CFD), can be (approximately) computed with linear time-complexity in the\nnumber of samples, compared to the quadratic-time Maximum Mean Discrepancy\n(MMD). By replacing the discrepancy measure in the critic of a GAN with the\nCFD, we obtain a model that is simple to implement and stable to train; the\nproposed metric enjoys desirable theoretical properties including continuity\nand differentiability with respect to generator parameters, and continuity in\nthe weak topology. We further propose a variation of the CFD in which the\nweighting distribution parameters are also optimized during training; this\nobviates the need for manual tuning and leads to an improvement in test power\nrelative to CFD. Experiments show that our proposed method outperforms WGAN and\nMMD-GAN variants on a variety of unsupervised image generation benchmark\ndatasets."}, {"title": "High-Resolution Daytime Translation Without Domain Labels", "authors": "Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras Khakhulin, Aleksei Silvestrov, Sergey Nikolenko, Victor Lempitsky, Gleb Sterkin", "link": "https://arxiv.org/abs/2003.08791", "summary": "Modeling daytime changes in high resolution photographs, e.g., re-rendering\nthe same scene under different illuminations typical for day, night, or dawn,\nis a challenging image manipulation task. We present the high-resolution\ndaytime translation (HiDT) model for this task. HiDT combines a generative\nimage-to-image model and a new upsampling scheme that allows to apply image\ntranslation at high resolution. The model demonstrates competitive results in\nterms of both commonly used GAN metrics and human evaluation. Importantly, this\ngood performance comes as a result of training on a dataset of still landscape\nimages with no daytime labels available. Our results are available at\nhttps://saic-mdal.github.io/HiDT/."}, {"title": "Leveraging 2D Data to Learn Textured 3D Mesh Generation", "authors": "Paul Henderson, Vagia Tsiminaki, Christoph H. Lampert", "link": "https://arxiv.org/abs/2004.04180", "summary": "Numerous methods have been proposed for probabilistic generative modelling of\n3D objects. However, none of these is able to produce textured objects, which\nrenders them of limited use for practical tasks. In this work, we present the\nfirst generative model of textured 3D meshes. Training such a model would\ntraditionally require a large dataset of textured meshes, but unfortunately,\nexisting datasets of meshes lack detailed textures. We instead propose a new\ntraining methodology that allows learning from collections of 2D images without\nany 3D information. To do so, we train our model to explain a distribution of\nimages by modelling each image as a 3D foreground object placed in front of a\n2D background. Thus, it learns to generate meshes that when rendered, produce\nimages similar to those in its training set.\n  A well-known problem when generating meshes with deep networks is the\nemergence of self-intersections, which are problematic for many use-cases. As a\nsecond contribution we therefore introduce a new generation process for 3D\nmeshes that guarantees no self-intersections arise, based on the physical\nintuition that faces should push one another out of the way as they move.\n  We conduct extensive experiments on our approach, reporting quantitative and\nqualitative results on both synthetic data and natural images. These show our\nmethod successfully learns to generate plausible and diverse textured 3D\nsamples for five challenging object classes."}, {"title": "Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting", "authors": "Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, Zhan Xu", "link": "https://arxiv.org/abs/2005.09704", "summary": "Recently data-driven image inpainting methods have made inspiring progress,\nimpacting fundamental image editing tasks such as object removal and damaged\nimage repairing. These methods are more effective than classic approaches,\nhowever, due to memory limitations they can only handle low-resolution inputs,\ntypically smaller than 1K. Meanwhile, the resolution of photos captured with\nmobile devices increases up to 8K. Naive up-sampling of the low-resolution\ninpainted result can merely yield a large yet blurry result. Whereas, adding a\nhigh-frequency residual image onto the large blurry image can generate a sharp\nresult, rich in details and textures. Motivated by this, we propose a\nContextual Residual Aggregation (CRA) mechanism that can produce high-frequency\nresiduals for missing contents by weighted aggregating residuals from\ncontextual patches, thus only requiring a low-resolution prediction from the\nnetwork. Since convolutional layers of the neural network only need to operate\non low-resolution inputs and outputs, the cost of memory and computing power is\nthus well suppressed. Moreover, the need for high-resolution training datasets\nis alleviated. In our experiments, we train the proposed model on small images\nwith resolutions 512x512 and perform inference on high-resolution images,\nachieving compelling inpainting quality. Our model can inpaint images as large\nas 8K with considerable hole sizes, which is intractable with previous\nlearning-based approaches. We further elaborate on the light-weight design of\nthe network architecture, achieving real-time performance on 2K images on a GTX\n1080 Ti GPU. Codes are available at: Atlas200dk/sample-imageinpainting-HiFill."}, {"title": "Flow Contrastive Estimation of Energy-Based Models", "authors": "Ruiqi Gao, Erik Nijkamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai, Ying Nian Wu", "link": "https://arxiv.org/abs/1912.00589", "summary": "This paper studies a training method to jointly estimate an energy-based\nmodel and a flow-based model, in which the two models are iteratively updated\nbased on a shared adversarial value function. This joint training method has\nthe following traits. (1) The update of the energy-based model is based on\nnoise contrastive estimation, with the flow model serving as a strong noise\ndistribution. (2) The update of the flow model approximately minimizes the\nJensen-Shannon divergence between the flow model and the data distribution. (3)\nUnlike generative adversarial networks (GAN) which estimates an implicit\nprobability distribution defined by a generator model, our method estimates two\nexplicit probabilistic distributions on the data. Using the proposed method we\ndemonstrate a significant improvement on the synthesis quality of the flow\nmodel, and show the effectiveness of unsupervised feature learning by the\nlearned energy-based model. Furthermore, the proposed training method can be\neasily adapted to semi-supervised learning. We achieve competitive results to\nthe state-of-the-art semi-supervised learning methods."}, {"title": "Hardware-in-the-Loop End-to-End Optimization of Camera Image Processing Pipelines", "authors": "Ali Mosleh, Avinash Sharma, Emmanuel Onzon, Fahim Mannan, Nicolas Robidoux, Felix Heide"}, {"title": "Search to Distill: Pearls Are Everywhere but Not the Eyes", "authors": "Yu Liu, Xuhui Jia, Mingxing Tan, Raviteja Vemulapalli, Yukun Zhu, Bradley Green, Xiaogang Wang", "link": "https://arxiv.org/abs/1911.09074", "summary": "Standard Knowledge Distillation (KD) approaches distill the knowledge of a\ncumbersome teacher model into the parameters of a student model with a\npre-defined architecture. However, the knowledge of a neural network, which is\nrepresented by the network's output distribution conditioned on its input,\ndepends not only on its parameters but also on its architecture. Hence, a more\ngeneralized approach for KD is to distill the teacher's knowledge into both the\nparameters and architecture of the student. To achieve this, we present a new\nArchitecture-aware Knowledge Distillation (AKD) approach that finds student\nmodels (pearls for the teacher) that are best for distilling the given teacher\nmodel. In particular, we leverage Neural Architecture Search (NAS), equipped\nwith our KD-guided reward, to search for the best student architectures for a\ngiven teacher. Experimental results show our proposed AKD consistently\noutperforms the conventional NAS plus KD approach, and achieves\nstate-of-the-art results on the ImageNet classification task under various\nlatency settings. Furthermore, the best AKD student architecture for the\nImageNet classification task also transfers well to other tasks such as million\nlevel face recognition and ensemble learning."}, {"title": "Total Deep Variation for Linear Inverse Problems", "authors": "Erich Kobler, Alexander Effland, Karl Kunisch, Thomas Pock", "link": "https://arxiv.org/abs/2001.05005", "summary": "Diverse inverse problems in imaging can be cast as variational problems\ncomposed of a task-specific data fidelity term and a regularization term. In\nthis paper, we propose a novel learnable general-purpose regularizer exploiting\nrecent architectural design patterns from deep learning. We cast the learning\nproblem as a discrete sampled optimal control problem, for which we derive the\nadjoint state equations and an optimality condition. By exploiting the\nvariational structure of our approach, we perform a sensitivity analysis with\nrespect to the learned parameters obtained from different training datasets.\nMoreover, we carry out a nonlinear eigenfunction analysis, which reveals\ninteresting properties of the learned regularizer. We show state-of-the-art\nperformance for classical image restoration and medical image reconstruction\nproblems."}, {"title": "Relative Interior Rule in Block-Coordinate Descent", "authors": "Tom\u00e1\u0161 Werner, Daniel Pr\u016f\u0161a, Tom\u00e1\u0161 Dlask", "link": "", "summary": ""}, {"title": "Learning Combinatorial Solver for Graph Matching", "authors": "Tao Wang, He Liu, Yidong Li, Yi Jin, Xiaohui Hou, Haibin Ling"}, {"title": "SampleNet: Differentiable Point Cloud Sampling", "authors": "Itai Lang, Asaf Manor, Shai Avidan", "link": "https://arxiv.org/abs/1912.03663", "summary": "There is a growing number of tasks that work directly on point clouds. As the\nsize of the point cloud grows, so do the computational demands of these tasks.\nA possible solution is to sample the point cloud first. Classic sampling\napproaches, such as farthest point sampling (FPS), do not consider the\ndownstream task. A recent work showed that learning a task-specific sampling\ncan improve results significantly. However, the proposed technique did not deal\nwith the non-differentiability of the sampling operation and offered a\nworkaround instead. We introduce a novel differentiable relaxation for point\ncloud sampling that approximates sampled points as a mixture of points in the\nprimary input cloud. Our approximation scheme leads to consistently good\nresults on classification and geometry reconstruction applications. We also\nshow that the proposed sampling method can be used as a front to a point cloud\nregistration network. This is a challenging task since sampling must be\nconsistent across two different point clouds for a shared downstream task. In\nall cases, our approach outperforms existing non-learned and learned sampling\nalternatives. Our code is publicly available at\nhttps://github.com/itailang/SampleNet."}, {"title": "Can We Learn Heuristics for Graphical Model Inference Using Reinforcement Learning?", "authors": "Safa Messaoud, Maghav Kumar, Alexander G. Schwing", "link": "https://arxiv.org/abs/2005.01508", "summary": "Combinatorial optimization is frequently used in computer vision. For\ninstance, in applications like semantic segmentation, human pose estimation and\naction recognition, programs are formulated for solving inference in\nConditional Random Fields (CRFs) to produce a structured output that is\nconsistent with visual features of the image. However, solving inference in\nCRFs is in general intractable, and approximation methods are computationally\ndemanding and limited to unary, pairwise and hand-crafted forms of higher order\npotentials. In this paper, we show that we can learn program heuristics, i.e.,\npolicies, for solving inference in higher order CRFs for the task of semantic\nsegmentation, using reinforcement learning. Our method solves inference tasks\nefficiently without imposing any constraints on the form of the potentials. We\nshow compelling results on the Pascal VOC and MOTS datasets."}, {"title": "Quasi-Newton Solver for Robust Non-Rigid Registration", "authors": "Yuxin Yao, Bailin Deng, Weiwei Xu, Juyong Zhang", "link": "https://arxiv.org/abs/2004.04322", "summary": "Imperfect data (noise, outliers and partial overlap) and high degrees of\nfreedom make non-rigid registration a classical challenging problem in computer\nvision. Existing methods typically adopt the $\\ell_{p}$ type robust estimator\nto regularize the fitting and smoothness, and the proximal operator is used to\nsolve the resulting non-smooth problem. However, the slow convergence of these\nalgorithms limits its wide applications. In this paper, we propose a\nformulation for robust non-rigid registration based on a globally smooth robust\nestimator for data fitting and regularization, which can handle outliers and\npartial overlaps. We apply the majorization-minimization algorithm to the\nproblem, which reduces each iteration to solving a simple least-squares problem\nwith L-BFGS. Extensive experiments demonstrate the effectiveness of our method\nfor non-rigid alignment between two shapes with outliers and partial overlap,\nwith quantitative evaluation showing that it outperforms state-of-the-art\nmethods in terms of registration accuracy and computational speed. The source\ncode is available at https://github.com/Juyong/Fast_RNRR."}, {"title": "Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition From a Domain Adaptation Perspective", "authors": "Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, Boqing Gong", "link": "https://arxiv.org/abs/2003.10780", "summary": "Object frequency in the real world often follows a power law, leading to a\nmismatch between datasets with long-tailed class distributions seen by a\nmachine learning model and our expectation of the model to perform well on all\nclasses. We analyze this mismatch from a domain adaptation point of view. First\nof all, we connect existing class-balanced methods for long-tailed\nclassification to target shift, a well-studied scenario in domain adaptation.\nThe connection reveals that these methods implicitly assume that the training\ndata and test data share the same class-conditioned distribution, which does\nnot hold in general and especially for the tail classes. While a head class\ncould contain abundant and diverse training examples that well represent the\nexpected data at inference time, the tail classes are often short of\nrepresentative training data. To this end, we propose to augment the classic\nclass-balanced learning by explicitly estimating the differences between the\nclass-conditioned distributions with a meta-learning approach. We validate our\napproach with six benchmark datasets and three loss functions."}, {"title": "Optimizing Rank-Based Metrics With Blackbox Differentiation", "authors": "Michal Rol\u00ednek, V\u00edt Musil, Anselm Paulus, Marin Vlastelica, Claudio Michaelis, Georg Martius", "link": "https://arxiv.org/abs/1912.03500", "summary": "Rank-based metrics are some of the most widely used criteria for performance\nevaluation of computer vision models. Despite years of effort, direct\noptimization for these metrics remains a challenge due to their\nnon-differentiable and non-decomposable nature. We present an efficient,\ntheoretically sound, and general method for differentiating rank-based metrics\nwith mini-batch gradient descent. In addition, we address optimization\ninstability and sparsity of the supervision signal that both arise from using\nrank-based metrics as optimization targets. Resulting losses based on recall\nand Average Precision are applied to image retrieval and object detection\ntasks. We obtain performance that is competitive with state-of-the-art on\nstandard image retrieval datasets and consistently improve performance of near\nstate-of-the-art object detectors. The code is available at\nhttps://github.com/martius-lab/blackbox-backprop"}, {"title": "DualSDF: Semantic Shape Manipulation Using a Two-Level Representation", "authors": "Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie", "link": "https://arxiv.org/abs/2004.02869", "summary": "We are seeing a Cambrian explosion of 3D shape representations for use in\nmachine learning. Some representations seek high expressive power in capturing\nhigh-resolution detail. Other approaches seek to represent shapes as\ncompositions of simple parts, which are intuitive for people to understand and\neasy to edit and manipulate. However, it is difficult to achieve both fidelity\nand interpretability in the same representation. We propose DualSDF, a\nrepresentation expressing shapes at two levels of granularity, one capturing\nfine details and the other representing an abstracted proxy shape using simple\nand semantically consistent shape primitives. To achieve a tight coupling\nbetween the two representations, we use a variational objective over a shared\nlatent space. Our two-level model gives rise to a new shape manipulation\ntechnique in which a user can interactively manipulate the coarse proxy shape\nand see the changes instantly mirrored in the high-resolution shape. Moreover,\nour model actively augments and guides the manipulation towards producing\nsemantically meaningful shapes, making complex manipulations possible with\nminimal user input."}, {"title": "Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives", "authors": "Duo Li, Qifeng Chen", "link": "http://arxiv.org/abs/2003.10739", "summary": "While the depth of modern Convolutional Neural Networks (CNNs) surpasses that\nof the pioneering networks with a significant margin, the traditional way of\nappending supervision only over the final classifier and progressively\npropagating gradient flow upstream remains the training mainstay. Seminal\nDeeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of\noptimization arising from gradient flow through a long chain. However, it is\nstill vulnerable to issues including interference to the hierarchical\nrepresentation generation process and inconsistent optimization objectives, as\nillustrated theoretically and empirically in this paper. Complementary to\nprevious training strategies, we propose Dynamic Hierarchical Mimicking, a\ngeneric feature learning mechanism, to advance CNN training with enhanced\ngeneralization ability. Partially inspired by DSN, we fork delicately designed\nside branches from the intermediate layers of a given neural network. Each\nbranch can emerge from certain locations of the main branch dynamically, which\nnot only retains representation rooted in the backbone network but also\ngenerates more diverse representations along its own pathway. We go one step\nfurther to promote multi-level interactions among different branches through an\noptimization formula with probabilistic prediction matching losses, thus\nguaranteeing a more robust optimization process and better representation\nability. Experiments on both category and instance recognition tasks\ndemonstrate the substantial improvements of our proposed method over its\ncorresponding counterparts using diverse state-of-the-art CNN architectures.\nCode and models are publicly available at https://github.com/d-li14/DHM"}, {"title": "Deep Homography Estimation for Dynamic Scenes", "authors": "Hoang Le, Feng Liu, Shu Zhang, Aseem Agarwala", "link": "https://arxiv.org/abs/2004.02132", "summary": "Homography estimation is an important step in many computer vision problems.\nRecently, deep neural network methods have shown to be favorable for this\nproblem when compared to traditional methods. However, these new methods do not\nconsider dynamic content in input images. They train neural networks with only\nimage pairs that can be perfectly aligned using homographies. This paper\ninvestigates and discusses how to design and train a deep neural network that\nhandles dynamic scenes. We first collect a large video dataset with dynamic\ncontent. We then develop a multi-scale neural network and show that when\nproperly trained using our new dataset, this neural network can already handle\ndynamic scenes to some extent. To estimate a homography of a dynamic scene in a\nmore principled way, we need to identify the dynamic content. Since dynamic\ncontent detection and homography estimation are two tightly coupled tasks, we\nfollow the multi-task learning principles and augment our multi-scale network\nsuch that it jointly estimates the dynamics masks and homographies. Our\nexperiments show that our method can robustly estimate homography for\nchallenging scenarios with dynamic scenes, blur artifacts, or lack of textures."}, {"title": "PF-Net: Point Fractal Network for 3D Point Cloud Completion", "authors": "Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, Xinyi Le", "link": "https://arxiv.org/abs/2003.00410", "summary": "In this paper, we propose a Point Fractal Network (PF-Net), a novel\nlearning-based approach for precise and high-fidelity point cloud completion.\nUnlike existing point cloud completion networks, which generate the overall\nshape of the point cloud from the incomplete point cloud and always change\nexisting points and encounter noise and geometrical loss, PF-Net preserves the\nspatial arrangements of the incomplete point cloud and can figure out the\ndetailed geometrical structure of the missing region(s) in the prediction. To\nsucceed at this task, PF-Net estimates the missing point cloud hierarchically\nby utilizing a feature-points-based multi-scale generating network. Further, we\nadd up multi-stage completion loss and adversarial loss to generate more\nrealistic missing region(s). The adversarial loss can better tackle multiple\nmodes in the prediction. Our experiments demonstrate the effectiveness of our\nmethod for several challenging point cloud completion tasks."}, {"title": "On the Regularization Properties of Structured Dropout", "authors": "Ambar Pal, Connor Lane, Ren\u00e9 Vidal, Benjamin D. Haeffele", "link": "https://arxiv.org/abs/1910.14186", "summary": "Dropout and its extensions (eg. DropBlock and DropConnect) are popular\nheuristics for training neural networks, which have been shown to improve\ngeneralization performance in practice. However, a theoretical understanding of\ntheir optimization and regularization properties remains elusive. Recent work\nshows that in the case of single hidden-layer linear networks, Dropout is a\nstochastic gradient descent method for minimizing a regularized loss, and that\nthe regularizer induces solutions that are low-rank and balanced. In this work\nwe show that for single hidden-layer linear networks, DropBlock induces\nspectral k-support norm regularization, and promotes solutions that are\nlow-rank and have factors with equal norm. We also show that the global\nminimizer for DropBlock can be computed in closed form, and that DropConnect is\nequivalent to Dropout. We then show that some of these results can be extended\nto a general class of Dropout-strategies, and, with some assumptions, to deep\nnon-linear networks when Dropout is applied to the last layer. We verify our\ntheoretical claims and assumptions experimentally with commonly used network\narchitectures."}, {"title": "Learning Oracle Attention for High-Fidelity Face Completion", "authors": "Tong Zhou, Changxing Ding, Shaowen Lin, Xinchao Wang, Dacheng Tao", "link": "https://arxiv.org/abs/2003.13903", "summary": "High-fidelity face completion is a challenging task due to the rich and\nsubtle facial textures involved. What makes it more complicated is the\ncorrelations between different facial components, for example, the symmetry in\ntexture and structure between both eyes. While recent works adopted the\nattention mechanism to learn the contextual relations among elements of the\nface, they have largely overlooked the disastrous impacts of inaccurate\nattention scores; in addition, they fail to pay sufficient attention to key\nfacial components, the completion results of which largely determine the\nauthenticity of a face image. Accordingly, in this paper, we design a\ncomprehensive framework for face completion based on the U-Net structure.\nSpecifically, we propose a dual spatial attention module to efficiently learn\nthe correlations between facial textures at multiple scales; moreover, we\nprovide an oracle supervision signal to the attention module to ensure that the\nobtained attention scores are reasonable. Furthermore, we take the location of\nthe facial components as prior knowledge and impose a multi-discriminator on\nthese regions, with which the fidelity of facial components is significantly\npromoted. Extensive experiments on two high-resolution face datasets including\nCelebA-HQ and Flickr-Faces-HQ demonstrate that the proposed approach\noutperforms state-of-the-art methods by large margins."}, {"title": "Deep Image Spatial Transformation for Person Image Generation", "authors": "Yurui Ren, Xiaoming Yu, Junming Chen, Thomas H. Li, Ge Li", "link": "https://arxiv.org/abs/2003.00696", "summary": "Pose-guided person image generation is to transform a source person image to\na target pose. This task requires spatial manipulations of source data.\nHowever, Convolutional Neural Networks are limited by the lack of ability to\nspatially transform the inputs. In this paper, we propose a differentiable\nglobal-flow local-attention framework to reassemble the inputs at the feature\nlevel. Specifically, our model first calculates the global correlations between\nsources and targets to predict flow fields. Then, the flowed local patch pairs\nare extracted from the feature maps to calculate the local attention\ncoefficients. Finally, we warp the source features using a content-aware\nsampling method with the obtained local attention coefficients. The results of\nboth subjective and objective experiments demonstrate the superiority of our\nmodel. Besides, additional results in video animation and view synthesis show\nthat our model is applicable to other tasks requiring spatial transformation.\nOur source code is available at\nhttps://github.com/RenYurui/Global-Flow-Local-Attention."}, {"title": "Learning to Optimize on SPD Manifolds", "authors": "Zhi Gao, Yuwei Wu, Yunde Jia, Mehrtash Harandi"}, {"title": "Deep 3D Portrait From a Single Image", "authors": "Sicheng Xu, Jiaolong Yang, Dong Chen, Fang Wen, Yu Deng, Yunde Jia, Xin Tong", "link": "http://arxiv.org/abs/2004.11598", "summary": "In this paper, we present a learning-based approach for recovering the 3D\ngeometry of human head from a single portrait image. Our method is learned in\nan unsupervised manner without any ground-truth 3D data.\n  We represent the head geometry with a parametric 3D face model together with\na depth map for other head regions including hair and ear. A two-step geometry\nlearning scheme is proposed to learn 3D head reconstruction from in-the-wild\nface images, where we first learn face shape on single images using\nself-reconstruction and then learn hair and ear geometry using pairs of images\nin a stereo-matching fashion. The second step is based on the output of the\nfirst to not only improve the accuracy but also ensure the consistency of\noverall head geometry.\n  We evaluate the accuracy of our method both in 3D and with pose manipulation\ntasks on 2D images. We alter pose based on the recovered geometry and apply a\nrefinement network trained with adversarial learning to ameliorate the\nreprojected images and translate them to the real image domain. Extensive\nevaluations and comparison with previous methods show that our new method can\nproduce high-fidelity 3D head geometry and head pose manipulation results."}, {"title": "RDCFace: Radial Distortion Correction for Face Recognition", "authors": "He Zhao, Xianghua Ying, Yongjie Shi, Xin Tong, Jingsi Wen, Hongbin Zha", "link": "", "summary": ""}, {"title": "Global-Local GCN: Large-Scale Label Noise Cleansing for Face Recognition", "authors": "Yaobin Zhang, Weihong Deng, Mei Wang, Jiani Hu, Xian Li, Dongyue Zhao, Dongchao Wen"}, {"title": "MISC: Multi-Condition Injection and Spatially-Adaptive Compositing for Conditional Person Image Synthesis", "authors": "Shuchen Weng, Wenbo Li, Dawei Li, Hongxia Jin, Boxin Shi"}, {"title": "SAINT: Spatially Aware Interpolation NeTwork for Medical Slice Synthesis", "authors": "Cheng Peng, Wei-An Lin, Haofu Liao, Rama Chellappa, S. Kevin Zhou", "link": "https://arxiv.org/abs/2001.00704", "summary": "Deep learning-based single image super-resolution (SISR) methods face various\nchallenges when applied to 3D medical volumetric data (i.e., CT and MR images)\ndue to the high memory cost and anisotropic resolution, which adversely affect\ntheir performance. Furthermore, mainstream SISR methods are designed to work\nover specific upsampling factors, which makes them ineffective in clinical\npractice. In this paper, we introduce a Spatially Aware Interpolation NeTwork\n(SAINT) for medical slice synthesis to alleviate the memory constraint that\nvolumetric data poses. Compared to other super-resolution methods, SAINT\nutilizes voxel spacing information to provide desirable levels of details, and\nallows for the upsampling factor to be determined on the fly. Our evaluations\nbased on 853 CT scans from four datasets that contain liver, colon, hepatic\nvessels, and kidneys show that SAINT consistently outperforms other SISR\nmethods in terms of medical slice synthesis quality, while using only a single\nmodel to deal with different upsampling factors."}, {"title": "Recurrent Feature Reasoning for Image Inpainting", "authors": "Jingyuan Li, Ning Wang, Lefei Zhang, Bo Du, Dacheng Tao"}, {"title": "Structure-Preserving Super Resolution With Gradient Guidance", "authors": "Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen Lu, Jie Zhou", "link": "https://arxiv.org/abs/2003.13081", "summary": "Structures matter in single image super resolution (SISR). Recent studies\nbenefiting from generative adversarial network (GAN) have promoted the\ndevelopment of SISR by recovering photo-realistic images. However, there are\nalways undesired structural distortions in the recovered images. In this paper,\nwe propose a structure-preserving super resolution method to alleviate the\nabove issue while maintaining the merits of GAN-based methods to generate\nperceptual-pleasant details. Specifically, we exploit gradient maps of images\nto guide the recovery in two aspects. On the one hand, we restore\nhigh-resolution gradient maps by a gradient branch to provide additional\nstructure priors for the SR process. On the other hand, we propose a gradient\nloss which imposes a second-order restriction on the super-resolved images.\nAlong with the previous image-space loss functions, the gradient-space\nobjectives help generative networks concentrate more on geometric structures.\nMoreover, our method is model-agnostic, which can be potentially used for\noff-the-shelf SR networks. Experimental results show that we achieve the best\nPI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with\nstate-of-the-art perceptual-driven SR methods. Visual results demonstrate our\nsuperiority in restoring structures while generating natural SR images."}, {"title": "Epipolar Transformers", "authors": "Yihui He, Rui Yan, Katerina Fragkiadaki, Shoou-I Yu", "link": "https://arxiv.org/abs/2005.04551", "summary": "A common approach to localize 3D human joints in a synchronized and\ncalibrated multi-view setup consists of two-steps: (1) apply a 2D detector\nseparately on each view to localize joints in 2D, and (2) perform robust\ntriangulation on 2D detections from each view to acquire the 3D joint\nlocations. However, in step 1, the 2D detector is limited to solving\nchallenging cases which could potentially be better resolved in 3D, such as\nocclusions and oblique viewing angles, purely in 2D without leveraging any 3D\ninformation. Therefore, we propose the differentiable \"epipolar transformer\",\nwhich enables the 2D detector to leverage 3D-aware features to improve 2D pose\nestimation. The intuition is: given a 2D location p in the current view, we\nwould like to first find its corresponding point p' in a neighboring view, and\nthen combine the features at p' with the features at p, thus leading to a\n3D-aware feature at p. Inspired by stereo matching, the epipolar transformer\nleverages epipolar constraints and feature matching to approximate the features\nat p'. Experiments on InterHand and Human3.6M show that our approach has\nconsistent improvements over the baselines. Specifically, in the condition\nwhere no external data is used, our Human3.6M model trained with ResNet-50\nbackbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and\nachieves MPJPE 26.9 mm."}, {"title": "Diversified Arbitrary Style Transfer via Deep Feature Perturbation", "authors": "Zhizhong Wang, Lei Zhao, Haibo Chen, Lihong Qiu, Qihang Mo, Sihuan Lin, Wei Xing, Dongming Lu", "link": "https://arxiv.org/abs/1909.08223", "summary": "Image style transfer is an underdetermined problem, where a large number of\nsolutions can satisfy the same constraint (the content and style). Although\nthere have been some efforts to improve the diversity of style transfer by\nintroducing an alternative diversity loss, they have restricted generalization,\nlimited diversity and poor scalability. In this paper, we tackle these\nlimitations and propose a simple yet effective method for diversified arbitrary\nstyle transfer. The key idea of our method is an operation called deep feature\nperturbation (DFP), which uses an orthogonal random noise matrix to perturb the\ndeep image feature maps while keeping the original style information unchanged.\nOur DFP operation can be easily integrated into many existing WCT (whitening\nand coloring transform)-based methods, and empower them to generate diverse\nresults for arbitrary styles. Experimental results demonstrate that this\nlearning-free and universal method can greatly increase the diversity while\nmaintaining the quality of stylization."}, {"title": "MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks", "authors": "Animesh Karnewar, Oliver Wang", "link": "", "summary": ""}, {"title": "Overcoming Multi-Model Forgetting in One-Shot NAS With Diversity Maximization", "authors": "Miao Zhang, Huiqi Li, Shirui Pan, Xiaojun Chang, Steven Su"}, {"title": "Select to Better Learn: Fast and Accurate Deep Learning Using Data Selection From Nonlinear Manifolds", "authors": "Mohsen Joneidi, Saeed Vahidian, Ashkan Esmaeili, Weijia Wang, Nazanin Rahnavard, Bill Lin, Mubarak Shah"}, {"title": "Neural Point Cloud Rendering via Multi-Plane Projection", "authors": "Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, Bing Zeng", "link": "https://arxiv.org/abs/1912.04645", "summary": "We present a new deep point cloud rendering pipeline through multi-plane\nprojections. The input to the network is the raw point cloud of a scene and the\noutput are image or image sequences from a novel view or along a novel camera\ntrajectory. Unlike previous approaches that directly project features from 3D\npoints onto 2D image domain, we propose to project these features into a\nlayered volume of camera frustum. In this way, the visibility of 3D points can\nbe automatically learnt by the network, such that ghosting effects due to false\nvisibility check as well as occlusions caused by noise interferences are both\navoided successfully. Next, the 3D feature volume is fed into a 3D CNN to\nproduce multiple layers of images w.r.t. the space division in the depth\ndirections. The layered images are then blended based on learned weights to\nproduce the final rendering results. Experiments show that our network produces\nmore stable renderings compared to previous methods, especially near the object\nboundaries. Moreover, our pipeline is robust to noisy and relatively sparse\npoint cloud for a variety of challenging scenes."}, {"title": "Wish You Were Here: Context-Aware Human Generation", "authors": "Oran Gafni, Lior Wolf", "link": "https://arxiv.org/abs/2005.10663", "summary": "We present a novel method for inserting objects, specifically humans, into\nexisting images, such that they blend in a photorealistic manner, while\nrespecting the semantic context of the scene. Our method involves three\nsubnetworks: the first generates the semantic map of the new person, given the\npose of the other persons in the scene and an optional bounding box\nspecification. The second network renders the pixels of the novel person and\nits blending mask, based on specifications in the form of multiple appearance\ncomponents. A third network refines the generated face in order to match those\nof the target person. Our experiments present convincing high-resolution\noutputs in this novel and challenging application domain. In addition, the\nthree networks are evaluated individually, demonstrating for example, state of\nthe art results in pose transfer benchmarks."}, {"title": "Towards Photo-Realistic Virtual Try-On by Adaptively Generating\u2194Preserving Image Content", "authors": "Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, Ping Luo", "link": "", "summary": ""}, {"title": "Breaking the Cycle \u2013 Colleagues Are All You Need", "authors": "Ori Nizan, Ayellet Tal", "link": "https://arxiv.org/abs/1911.10538", "summary": "This paper proposes a novel approach to performing image-to-image translation\nbetween unpaired domains. Rather than relying on a cycle constraint, our method\ntakes advantage of collaboration between various GANs. This results in a\nmulti-modal method, in which multiple optional and diverse images are produced\nfor a given image. Our model addresses some of the shortcomings of classical\nGANs: (1) It is able to remove large objects, such as glasses. (2) Since it\ndoes not need to support the cycle constraint, no irrelevant traces of the\ninput are left on the generated image. (3) It manages to translate between\ndomains that require large shape modifications. Our results are shown to\noutperform those generated by state-of-the-art methods for several challenging\napplications on commonly-used datasets, both qualitatively and quantitatively."}, {"title": "Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation", "authors": "Hao Tang, Dan Xu, Yan Yan, Philip H.S. Torr, Nicu Sebe", "link": "https://arxiv.org/abs/1912.12215", "summary": "In this paper, we address the task of semantic-guided scene generation. One\nopen challenge in scene generation is the difficulty of the generation of small\nobjects and detailed local texture, which has been widely observed in global\nimage-level generation methods. To tackle this issue, in this work we consider\nlearning the scene generation in a local context, and correspondingly design a\nlocal class-specific generative network with semantic maps as a guidance, which\nseparately constructs and learns sub-generators concentrating on the generation\nof different classes, and is able to provide more scene details. To learn more\ndiscriminative class-specific feature representations for the local generation,\na novel classification module is also proposed. To combine the advantage of\nboth the global image-level and the local class-specific generation, a joint\ngeneration network is designed with an attention fusion module and a\ndual-discriminator structure embedded. Extensive experiments on two scene image\ngeneration tasks show superior generation performance of the proposed model.\nThe state-of-the-art results are established by large margins on both tasks and\non challenging public benchmarks. The source code and trained models are\navailable at https://github.com/Ha0Tang/LGGAN."}, {"title": "ManiGAN: Text-Guided Image Manipulation", "authors": "Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H.S. Torr", "link": "", "summary": ""}, {"title": "Watch Your Up-Convolution: CNN Based Generative Deep Neural Networks Are Failing to Reproduce Spectral Distributions", "authors": "Ricard Durall, Margret Keuper, Janis Keuper", "link": "http://arxiv.org/abs/2003.01826", "summary": "Generative convolutional deep neural networks, e.g. popular GAN\narchitectures, are relying on convolution based up-sampling methods to produce\nnon-scalar outputs like images or video sequences. In this paper, we show that\ncommon up-sampling methods, i.e. known as up-convolution or transposed\nconvolution, are causing the inability of such models to reproduce spectral\ndistributions of natural training data correctly. This effect is independent of\nthe underlying architecture and we show that it can be used to easily detect\ngenerated data like deepfakes with up to 100% accuracy on public benchmarks.\n  To overcome this drawback of current generative models, we propose to add a\nnovel spectral regularization term to the training optimization objective. We\nshow that this approach not only allows to train spectral consistent GANs that\nare avoiding high frequency errors. Also, we show that a correct approximation\nof the frequency spectrum has positive effects on the training stability and\noutput quality of generative networks."}, {"title": "Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems", "authors": "Patrick Kn\u00f6belreiter, Christian Sormann, Alexander Shekhovtsov, Friedrich Fraundorfer, Thomas Pock", "link": "https://arxiv.org/abs/2003.06258", "summary": "It has been proposed by many researchers that combining deep neural networks\nwith graphical models can create more efficient and better regularized\ncomposite models. The main difficulties in implementing this in practice are\nassociated with a discrepancy in suitable learning objectives as well as with\nthe necessity of approximations for the inference. In this work we take one of\nthe simplest inference methods, a truncated max-product Belief Propagation, and\nadd what is necessary to make it a proper component of a deep learning model:\nWe connect it to learning formulations with losses on marginals and compute the\nbackprop operation. This BP-Layer can be used as the final or an intermediate\nblock in convolutional neural networks (CNNs), allowing us to design a\nhierarchical model composing BP inference and CNNs at different scale levels.\nThe model is applicable to a range of dense prediction problems, is\nwell-trainable and provides parameter-efficient and robust solutions in stereo,\noptical flow and semantic segmentation."}, {"title": "Barycenters of Natural Images \u00ad Constrained Wasserstein Barycenters for Image Morphing", "authors": "Dror Simon, Aviad Aberdam", "link": "https://arxiv.org/abs/1912.11545", "summary": "Image interpolation, or image morphing, refers to a visual transition between\ntwo (or more) input images. For such a transition to look visually appealing,\nits desirable properties are (i) to be smooth; (ii) to apply the minimal\nrequired change in the image; and (iii) to seem \"real\", avoiding unnatural\nartifacts in each image in the transition. To obtain a smooth and\nstraightforward transition, one may adopt the well-known Wasserstein Barycenter\nProblem (WBP). While this approach guarantees minimal changes under the\nWasserstein metric, the resulting images might seem unnatural. In this work, we\npropose a novel approach for image morphing that possesses all three desired\nproperties. To this end, we define a constrained variant of the WBP that\nenforces the intermediate images to satisfy an image prior. We describe an\nalgorithm that solves this problem and demonstrate it using the sparse prior\nand generative adversarial networks."}, {"title": "Guided Variational Autoencoder for Disentanglement Learning", "authors": "Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang Yang, Max Welling, Zhuowen Tu", "link": "https://arxiv.org/abs/2004.01255", "summary": "We propose an algorithm, guided variational autoencoder (Guided-VAE), that is\nable to learn a controllable generative model by performing latent\nrepresentation disentanglement learning. The learning objective is achieved by\nproviding signals to the latent encoding/embedding in VAE without changing its\nmain backbone architecture, hence retaining the desirable properties of the\nVAE. We design an unsupervised strategy and a supervised strategy in Guided-VAE\nand observe enhanced modeling and controlling capability over the vanilla VAE.\nIn the unsupervised strategy, we guide the VAE learning by introducing a\nlightweight decoder that learns latent geometric transformation and principal\ncomponents; in the supervised strategy, we use an adversarial excitation and\ninhibition mechanism to encourage the disentanglement of the latent variables.\nGuided-VAE enjoys its transparency and simplicity for the general\nrepresentation learning task, as well as disentanglement learning. On a number\nof experiments for representation learning, improved synthesis/sampling, better\ndisentanglement for classification, and reduced classification errors in\nmeta-learning have been observed."}, {"title": "Cross-Spectral Face Hallucination via Disentangling Independent Factors", "authors": "Boyan Duan, Chaoyou Fu, Yi Li, Xingguang Song, Ran He", "link": "https://arxiv.org/abs/1909.04365", "summary": "The cross-sensor gap is one of the challenges that have aroused much research\ninterests in Heterogeneous Face Recognition (HFR). Although recent methods have\nattempted to fill the gap with deep generative networks, most of them suffer\nfrom the inevitable misalignment between different face modalities. Instead of\nimaging sensors, the misalignment primarily results from facial geometric\nvariations that are independent of the spectrum. Rather than building a\nmonolithic but complex structure, this paper proposes a Pose Aligned\nCross-spectral Hallucination (PACH) approach to disentangle the independent\nfactors and deal with them in individual stages. In the first stage, an\nUnsupervised Face Alignment (UFA) module is designed to align the facial shapes\nof the near-infrared (NIR) images with those of the visible (VIS) images in a\ngenerative way, where UV maps are effectively utilized as the shape guidance.\nThus the task of the second stage becomes spectrum translation with aligned\npaired data. We develop a Texture Prior Synthesis (TPS) module to achieve\ncomplexion control and consequently generate more realistic VIS images than\nexisting methods. Experiments on three challenging NIR-VIS datasets verify the\neffectiveness of our approach in producing visually appealing images and\nachieving state-of-the-art performance in HFR."}, {"title": "Learned Image Compression With Discretized Gaussian Mixture Likelihoods and Attention Modules", "authors": "Zhengxue Cheng, Heming Sun, Masaru Takeuchi, Jiro Katto", "link": "https://arxiv.org/abs/2001.01568", "summary": "Image compression is a fundamental research field and many well-known\ncompression standards have been developed for many decades. Recently, learned\ncompression methods exhibit a fast development trend with promising results.\nHowever, there is still a performance gap between learned compression\nalgorithms and reigning compression standards, especially in terms of widely\nused PSNR metric. In this paper, we explore the remaining redundancy of recent\nlearned compression algorithms. We have found accurate entropy models for rate\nestimation largely affect the optimization of network parameters and thus\naffect the rate-distortion performance. Therefore, in this paper, we propose to\nuse discretized Gaussian Mixture Likelihoods to parameterize the distributions\nof latent codes, which can achieve a more accurate and flexible entropy model.\nBesides, we take advantage of recent attention modules and incorporate them\ninto network architecture to enhance the performance. Experimental results\ndemonstrate our proposed method achieves a state-of-the-art performance\ncompared to existing learned compression methods on both Kodak and\nhigh-resolution datasets. To our knowledge our approach is the first work to\nachieve comparable performance with latest compression standard Versatile Video\nCoding (VVC) regarding PSNR. More importantly, our approach generates more\nvisually pleasant results when optimized by MS-SSIM. This project page is at\nthis https URL\nhttps://github.com/ZhengxueCheng/Learned-Image-Compression-with-GMM-and-Attention"}, {"title": "C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds", "authors": "Albert Pumarola, Stefan Popov, Francesc Moreno-Noguer, Vittorio Ferrari", "link": "https://arxiv.org/abs/1912.07009", "summary": "Flow-based generative models have highly desirable properties like exact\nlog-likelihood evaluation and exact latent-variable inference, however they are\nstill in their infancy and have not received as much attention as alternative\ngenerative models. In this paper, we introduce C-Flow, a novel conditioning\nscheme that brings normalizing flows to an entirely new scenario with great\npossibilities for multi-modal data modeling. C-Flow is based on a parallel\nsequence of invertible mappings in which a source flow guides the target flow\nat every step, enabling fine-grained control over the generation process. We\nalso devise a new strategy to model unordered 3D point clouds that, in\ncombination with the conditioning scheme, makes it possible to address 3D\nreconstruction from a single image and its inverse problem of rendering an\nimage given a point cloud. We demonstrate our conditioning method to be very\nadaptable, being also applicable to image manipulation, style transfer and\nmulti-modal image-to-image mapping in a diversity of domains, including RGB\nimages, segmentation maps, and edge masks."}, {"title": "Cogradient Descent for Bilinear Optimization", "authors": "Li'an Zhuo, Baochang Zhang, Linlin Yang, Hanlin Chen, Qixiang Ye, David Doermann, Rongrong Ji, Guodong Guo"}, {"title": "Instance-Aware Image Colorization", "authors": "Jheng-Wei Su, Hung-Kuo Chu, Jia-Bin Huang"}, {"title": "Joint Training of Variational Auto-Encoder and Latent Energy-Based Model", "authors": "Tian Han, Erik Nijkamp, Linqi Zhou, Bo Pang, Song-Chun Zhu, Ying Nian Wu"}, {"title": "Adaptive Loss-Aware Quantization for Multi-Bit Networks", "authors": "Zhongnan Qu, Zimu Zhou, Yun Cheng, Lothar Thiele", "link": "https://arxiv.org/abs/1912.08883", "summary": "We investigate the compression of deep neural networks by quantizing their\nweights and activations into multiple binary bases, known as multi-bit networks\n(MBNs), which accelerate the inference and reduce the storage for the\ndeployment on low-resource mobile and embedded platforms. We propose Adaptive\nLoss-aware Quantization (ALQ), a new MBN quantization pipeline that is able to\nachieve an average bitwidth below one-bit without notable loss in inference\naccuracy. Unlike previous MBN quantization solutions that train a quantizer by\nminimizing the error to reconstruct full precision weights, ALQ directly\nminimizes the quantization-induced error on the loss function involving neither\ngradient approximation nor full precision maintenance. ALQ also exploits\nstrategies including adaptive bitwidth, smooth bitwidth reduction, and\niterative trained quantization to allow a smaller network size without loss in\naccuracy. Experiment results on popular image datasets show that ALQ\noutperforms state-of-the-art compressed networks in terms of both storage and\naccuracy."}, {"title": "ScopeFlow: Dynamic Scene Scoping for Optical Flow", "authors": "Aviram Bar-Haim, Lior Wolf", "link": "http://arxiv.org/abs/2002.10770", "summary": "We propose to modify the common training protocols of optical flow, leading\nto sizable accuracy improvements without adding to the computational complexity\nof the training process. The improvement is based on observing the bias in\nsampling challenging data that exists in the current training protocol, and\nimproving the sampling process. In addition, we find that both regularization\nand augmentation should decrease during the training protocol.\n  Using an existing low parameters architecture, the method is ranked first on\nthe MPI Sintel benchmark among all other methods, improving the best two frames\nmethod accuracy by more than 10%. The method also surpasses all similar\narchitecture variants by more than 12% and 19.7% on the KITTI benchmarks,\nachieving the lowest Average End-Point Error on KITTI2012 among two-frame\nmethods, without using extra datasets."}, {"title": "Video Super-Resolution With Temporal Group Attention", "authors": "Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, Qi Tian"}, {"title": "Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression", "authors": "Yawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, Radu Timofte", "link": "https://arxiv.org/abs/2003.08935", "summary": "In this paper, we analyze two popular network compression techniques, i.e.\nfilter pruning and low-rank decomposition, in a unified sense. By simply\nchanging the way the sparsity regularization is enforced, filter pruning and\nlow-rank decomposition can be derived accordingly. This provides another\nflexible choice for network compression because the techniques complement each\nother. For example, in popular network architectures with shortcut connections\n(e.g. ResNet), filter pruning cannot deal with the last convolutional layer in\na ResBlock while the low-rank decomposition methods can. In addition, we\npropose to compress the whole network jointly instead of in a layer-wise\nmanner. Our approach proves its potential as it compares favorably to the\nstate-of-the-art on several benchmarks."}, {"title": "3D Photography Using Context-Aware Layered Depth Inpainting", "authors": "Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang", "link": "https://arxiv.org/abs/2004.04727", "summary": "We propose a method for converting a single RGB-D input image into a 3D photo\n- a multi-layer representation for novel view synthesis that contains\nhallucinated color and depth structures in regions occluded in the original\nview. We use a Layered Depth Image with explicit pixel connectivity as\nunderlying representation, and present a learning-based inpainting model that\nsynthesizes new local color-and-depth content into the occluded region in a\nspatial context-aware manner. The resulting 3D photos can be efficiently\nrendered with motion parallax using standard graphics engines. We validate the\neffectiveness of our method on a wide range of challenging everyday scenes and\nshow fewer artifacts compared with the state of the arts."}, {"title": "MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation", "authors": "Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee", "link": "https://arxiv.org/abs/1911.11758", "summary": "We present MixNMatch, a conditional generative model that learns to\ndisentangle and encode background, object pose, shape, and texture from real\nimages with minimal supervision, for mix-and-match image generation. We build\nupon FineGAN, an unconditional generative model, to learn the desired\ndisentanglement and image generator, and leverage adversarial joint image-code\ndistribution matching to learn the latent factor encoders. MixNMatch requires\nbounding boxes during training to model background, but requires no other\nsupervision. Through extensive experiments, we demonstrate MixNMatch's ability\nto accurately disentangle, encode, and combine multiple factors for\nmix-and-match image generation, including sketch2color, cartoon2img, and\nimg2gif applications. Our code/models/demo can be found at\nhttps://github.com/Yuheng-Li/MixNMatch"}, {"title": "Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer", "authors": "Yerlan Idelbayev, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"}, {"title": "Global Texture Enhancement for Fake Face Detection in the Wild", "authors": "Zhengzhe Liu, Xiaojuan Qi, Philip H.S. Torr", "link": "http://arxiv.org/abs/2002.00133", "summary": "Generative Adversarial Networks (GANs) can generate realistic fake face\nimages that can easily fool human beings.On the contrary, a common\nConvolutional Neural Network(CNN) discriminator can achieve more than 99.9%\naccuracyin discerning fake/real images. In this paper, we conduct an empirical\nstudy on fake/real faces, and have two important observations: firstly, the\ntexture of fake faces is substantially different from real ones; secondly,\nglobal texture statistics are more robust to image editing and transferable to\nfake faces from different GANs and datasets. Motivated by the above\nobservations, we propose a new architecture coined as Gram-Net, which leverages\nglobal image texture representations for robust fake image detection.\nExperimental results on several datasets demonstrate that our Gram-Net\noutperforms existing approaches. Especially, our Gram-Netis more robust to\nimage editings, e.g. down-sampling, JPEG compression, blur, and noise. More\nimportantly, our Gram-Net generalizes significantly better in detecting fake\nfaces from GAN models not seen in the training phase and can perform decently\nin detecting fake natural images."}, {"title": "Panoptic-Based Image Synthesis", "authors": "Aysegul Dundar, Karan Sapra, Guilin Liu, Andrew Tao, Bryan Catanzaro", "link": "http://arxiv.org/abs/2004.10289", "summary": "Conditional image synthesis for generating photorealistic images serves\nvarious applications for content editing to content generation. Previous\nconditional image synthesis algorithms mostly rely on semantic maps, and often\nfail in complex environments where multiple instances occlude each other. We\npropose a panoptic aware image synthesis network to generate high fidelity and\nphotorealistic images conditioned on panoptic maps which unify semantic and\ninstance information. To achieve this, we efficiently use panoptic maps in\nconvolution and upsampling layers. We show that with the proposed changes to\nthe generator, we can improve on the previous state-of-the-art methods by\ngenerating images in complex instance interaction environments in higher\nfidelity and tiny objects in more details. Furthermore, our proposed method\nalso outperforms the previous state-of-the-art methods in metrics of mean IoU\n(Intersection over Union), and detAP (Detection Average Precision)."}, {"title": "Lighthouse: Predicting Lighting Volumes for Spatially-Coherent Illumination", "authors": "Pratul P. Srinivasan, Ben Mildenhall, Matthew Tancik, Jonathan T. Barron, Richard Tucker, Noah Snavely", "link": "https://arxiv.org/abs/2003.08367", "summary": "We present a deep learning solution for estimating the incident illumination\nat any 3D location within a scene from an input narrow-baseline stereo image\npair. Previous approaches for predicting global illumination from images either\npredict just a single illumination for the entire scene, or separately estimate\nthe illumination at each 3D location without enforcing that the predictions are\nconsistent with the same 3D scene. Instead, we propose a deep learning model\nthat estimates a 3D volumetric RGBA model of a scene, including content outside\nthe observed field of view, and then uses standard volume rendering to estimate\nthe incident illumination at any 3D location within that volume. Our model is\ntrained without any ground truth 3D data and only requires a held-out\nperspective view near the input stereo pair and a spherical panorama taken\nwithin each scene as supervision, as opposed to prior methods for\nspatially-varying lighting estimation, which require ground truth scene\ngeometry for training. We demonstrate that our method can predict consistent\nspatially-varying lighting that is convincing enough to plausibly relight and\ninsert highly specular virtual objects into real images."}, {"title": "Learning to Cartoonize Using White-Box Cartoon Representations", "authors": "Xinrui Wang, Jinze Yu"}, {"title": "End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization", "authors": "Bo Chen, \u00c1lvaro Parra, Jiewei Cao, Nan Li, Tat-Jun Chin", "link": "https://arxiv.org/abs/1909.06043", "summary": "Deep networks excel in learning patterns from large amounts of data. On the\nother hand, many geometric vision tasks are specified as optimization problems.\nTo seamlessly combine deep learning and geometric vision, it is vital to\nperform learning and geometric optimization end-to-end. Towards this aim, we\npresent BPnP, a novel network module that backpropagates gradients through a\nPerspective-n-Points (PnP) solver to guide parameter updates of a neural\nnetwork. Based on implicit differentiation, we show that the gradients of a\n\"self-contained\" PnP solver can be derived accurately and efficiently, as if\nthe optimizer block were a differentiable function. We validate BPnP by\nincorporating it in a deep model that can learn camera intrinsics, camera\nextrinsics (poses) and 3D structure from training datasets. Further, we develop\nan end-to-end trainable pipeline for object pose estimation, which achieves\ngreater accuracy by combining feature-based heatmap losses with 2D-3D\nreprojection errors. Since our approach can be extended to other optimization\nproblems, our work helps to pave the way to perform learnable geometric vision\nin a principled manner. Our PyTorch implementation of BPnP is available on\nhttp://github.com/BoChenYS/BPnP."}, {"title": "Analyzing and Improving the Image Quality of StyleGAN", "authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila", "link": "https://arxiv.org/abs/1912.04958", "summary": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results\nin data-driven unconditional generative image modeling. We expose and analyze\nseveral of its characteristic artifacts, and propose changes in both model\narchitecture and training methods to address them. In particular, we redesign\nthe generator normalization, revisit progressive growing, and regularize the\ngenerator to encourage good conditioning in the mapping from latent codes to\nimages. In addition to improving image quality, this path length regularizer\nyields the additional benefit that the generator becomes significantly easier\nto invert. This makes it possible to reliably attribute a generated image to a\nparticular network. We furthermore visualize how well the generator utilizes\nits output resolution, and identify a capacity problem, motivating us to train\nlarger models for additional quality improvements. Overall, our improved model\nredefines the state of the art in unconditional image modeling, both in terms\nof existing distribution quality metrics as well as perceived image quality."}, {"title": "Fashion Editing With Adversarial Parsing Learning", "authors": "Haoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Xiaohui Shen, Zhenyu Xie, Bowen Wu, Jian Yin", "link": "https://arxiv.org/abs/1906.00884", "summary": "Interactive fashion image manipulation, which enables users to edit images\nwith sketches and color strokes, is an interesting research problem with great\napplication value. Existing works often treat it as a general inpainting task\nand do not fully leverage the semantic structural information in fashion\nimages. Moreover, they directly utilize conventional convolution and\nnormalization layers to restore the incomplete image, which tends to wash away\nthe sketch and color information. In this paper, we propose a novel Fashion\nEditing Generative Adversarial Network (FE-GAN), which is capable of\nmanipulating fashion images by free-form sketches and sparse color strokes.\nFE-GAN consists of two modules: 1) a free-form parsing network that learns to\ncontrol the human parsing generation by manipulating sketch and color; 2) a\nparsing-aware inpainting network that renders detailed textures with semantic\nguidance from the human parsing map. A new attention normalization layer is\nfurther applied at multiple scales in the decoder of the inpainting network to\nenhance the quality of the synthesized image. Extensive experiments on\nhigh-resolution fashion image datasets demonstrate that the proposed method\nsignificantly outperforms the state-of-the-art methods on image manipulation."}, {"title": "Augment Your Batch: Improving Generalization Through Instance Repetition", "authors": "Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, Daniel Soudry", "link": "", "summary": ""}, {"title": "ARShadowGAN: Shadow Generative Adversarial Network for Augmented Reality in Single Light Scenes", "authors": "Daquan Liu, Chengjiang Long, Hongpan Zhang, Hanning Yu, Xinzhi Dong, Chunxia Xiao"}, {"title": "An End-to-End Edge Aggregation Network for Moving Object Segmentation", "authors": "Prashant W. Patil, Kuldeep M. Biradar, Akshay Dudhane, Subrahmanyam Murala"}, {"title": "Learning Video Stabilization Using Optical Flow", "authors": "Jiyang Yu, Ravi Ramamoorthi"}, {"title": "Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation", "authors": "Runfa Chen, Wenbing Huang, Binghui Huang, Fuchun Sun, Bin Fang", "link": "https://arxiv.org/abs/2003.00273", "summary": "Unsupervised image-to-image translation is a central task in computer vision.\nCurrent translation frameworks will abandon the discriminator once the training\nprocess is completed. This paper contends a novel role of the discriminator by\nreusing it for encoding the images of the target domain. The proposed\narchitecture, termed as NICE-GAN, exhibits two advantageous patterns over\nprevious approaches: First, it is more compact since no independent encoding\ncomponent is required; Second, this plug-in encoder is directly trained by the\nadversary loss, making it more informative and trained more effectively if a\nmulti-scale discriminator is applied. The main issue in NICE-GAN is the\ncoupling of translation with discrimination along the encoder, which could\nincur training inconsistency when we play the min-max game via GAN. To tackle\nthis issue, we develop a decoupled training strategy by which the encoder is\nonly trained when maximizing the adversary loss while keeping frozen otherwise.\nExtensive experiments on four popular benchmarks demonstrate the superior\nperformance of NICE-GAN over state-of-the-art methods in terms of FID, KID, and\nalso human preference. Comprehensive ablation studies are also carried out to\nisolate the validity of each proposed component. Our codes are available at\nhttps://github.com/alpc91/NICE-GAN-pytorch."}, {"title": "Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory", "authors": "Arash Rahnama, Andre T. Nguyen, Edward Raff", "link": "https://arxiv.org/abs/1911.04636", "summary": "Deep neural networks (DNNs) are vulnerable to subtle adversarial\nperturbations applied to the input. These adversarial perturbations, though\nimperceptible, can easily mislead the DNN. In this work, we take a control\ntheoretic approach to the problem of robustness in DNNs. We treat each\nindividual layer of the DNN as a nonlinear dynamical system and use Lyapunov\ntheory to prove stability and robustness locally. We then proceed to prove\nstability and robustness globally for the entire DNN. We develop empirically\ntight bounds on the response of the output layer, or any hidden layer, to\nadversarial perturbations added to the input, or the input of hidden layers.\nRecent works have proposed spectral norm regularization as a solution for\nimproving robustness against l2 adversarial attacks. Our results give new\ninsights into how spectral norm regularization can mitigate the adversarial\neffects. Finally, we evaluate the power of our approach on a variety of data\nsets and network architectures and against some of the well-known adversarial\nattacks."}, {"title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "authors": "Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha", "link": "https://arxiv.org/abs/1912.01865", "summary": "A good image-to-image translation model should learn a mapping between\ndifferent visual domains while satisfying the following properties: 1)\ndiversity of generated images and 2) scalability over multiple domains.\nExisting methods address either of the issues, having limited diversity or\nmultiple models for all domains. We propose StarGAN v2, a single framework that\ntackles both and shows significantly improved results over the baselines.\nExperiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our\nsuperiority in terms of visual quality, diversity, and scalability. To better\nassess image-to-image translation models, we release AFHQ, high-quality animal\nfaces with large inter- and intra-domain differences. The code, pretrained\nmodels, and dataset can be found at https://github.com/clovaai/stargan-v2."}, {"title": "Warping Residual Based Image Stitching for Large Parallax", "authors": "Kyu-Yul Lee, Jae-Young Sim"}, {"title": "A U-Net Based Discriminator for Generative Adversarial Networks", "authors": "Edgar Sch\u00f6nfeld, Bernt Schiele, Anna Khoreva", "link": "https://arxiv.org/abs/2002.12655", "summary": "Among the major remaining challenges for generative adversarial networks\n(GANs) is the capacity to synthesize globally and locally coherent images with\nobject shapes and textures indistinguishable from real images. To target this\nissue we propose an alternative U-Net based discriminator architecture,\nborrowing the insights from the segmentation literature. The proposed U-Net\nbased architecture allows to provide detailed per-pixel feedback to the\ngenerator while maintaining the global coherence of synthesized images, by\nproviding the global image feedback as well. Empowered by the per-pixel\nresponse of the discriminator, we further propose a per-pixel consistency\nregularization technique based on the CutMix data augmentation, encouraging the\nU-Net discriminator to focus more on semantic and structural changes between\nreal and fake images. This improves the U-Net discriminator training, further\nenhancing the quality of generated samples. The novel discriminator improves\nover the state of the art in terms of the standard distribution and image\nquality metrics, enabling the generator to synthesize images with varying\nstructure, appearance and levels of detail, maintaining global and local\nrealism. Compared to the BigGAN baseline, we achieve an average improvement of\n2.7 FID points across FFHQ, CelebA, and the newly introduced COCO-Animals\ndataset."}, {"title": "Unpaired Portrait Drawing Generation via Asymmetric Cycle Mapping", "authors": "Ran Yi, Yong-Jin Liu, Yu-Kun Lai, Paul L. Rosin"}, {"title": "When to Use Convolutional Neural Networks for Inverse Problems", "authors": "Nathaniel Chodosh, Simon Lucey", "link": "https://arxiv.org/abs/2003.13820", "summary": "Reconstruction tasks in computer vision aim fundamentally to recover an\nundetermined signal from a set of noisy measurements. Examples include\nsuper-resolution, image denoising, and non-rigid structure from motion, all of\nwhich have seen recent advancements through deep learning. However, earlier\nwork made extensive use of sparse signal reconstruction frameworks (e.g\nconvolutional sparse coding). While this work was ultimately surpassed by deep\nlearning, it rested on a much more developed theoretical framework. Recent work\nby Papyan et. al provides a bridge between the two approaches by showing how a\nconvolutional neural network (CNN) can be viewed as an approximate solution to\na convolutional sparse coding (CSC) problem. In this work we argue that for\nsome types of inverse problems the CNN approximation breaks down leading to\npoor performance. We argue that for these types of problems the CSC approach\nshould be used instead and validate this argument with empirical evidence.\nSpecifically we identify JPEG artifact reduction and non-rigid trajectory\nreconstruction as challenging inverse problems for CNNs and demonstrate state\nof the art performance on them using a CSC method. Furthermore, we offer some\npractical improvements to this model and its application, and also show how\ninsights from the CSC model can be used to make CNNs effective in tasks where\ntheir naive application fails."}, {"title": "LUVLi Face Alignment: Estimating Landmarks\u2019 Location, Uncertainty, and Visibility Likelihood", "authors": "Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang, Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, Chen Feng", "link": "https://arxiv.org/abs/2004.02980", "summary": "Modern face alignment methods have become quite accurate at predicting the\nlocations of facial landmarks, but they do not typically estimate the\nuncertainty of their predicted locations nor predict whether landmarks are\nvisible. In this paper, we present a novel framework for jointly predicting\nlandmark locations, associated uncertainties of these predicted locations, and\nlandmark visibilities. We model these as mixed random variables and estimate\nthem using a deep network trained with our proposed Location, Uncertainty, and\nVisibility Likelihood (LUVLi) loss. In addition, we release an entirely new\nlabeling of a large face alignment dataset with over 19,000 face images in a\nfull range of head poses. Each face is manually labeled with the ground-truth\nlocations of 68 landmarks, with the additional information of whether each\nlandmark is unoccluded, self-occluded (due to extreme head poses), or\nexternally occluded. Not only does our joint estimation yield accurate\nestimates of the uncertainty of predicted landmark locations, but it also\nyields state-of-the-art estimates for the landmark locations themselves on\nmultiple standard face alignment datasets. Our method's estimates of the\nuncertainty of predicted landmark locations could be used to automatically\nidentify input images on which face alignment fails, which can be critical for\ndownstream tasks."}, {"title": "Affinity Graph Supervision for Visual Recognition", "authors": "Chu Wang, Babak Samari, Vladimir G. Kim, Siddhartha Chaudhuri, Kaleem Siddiqi", "link": "https://arxiv.org/abs/2003.09049", "summary": "Affinity graphs are widely used in deep architectures, including graph\nconvolutional neural networks and attention networks. Thus far, the literature\nhas focused on abstracting features from such graphs, while the learning of the\naffinities themselves has been overlooked. Here we propose a principled method\nto directly supervise the learning of weights in affinity graphs, to exploit\nmeaningful connections between entities in the data source. Applied to a visual\nattention network, our affinity supervision improves relationship recovery\nbetween objects, even without the use of manually annotated relationship\nlabels. We further show that affinity learning between objects boosts scene\ncategorization performance and that the supervision of affinity can also be\napplied to graphs built from mini-batches, for neural network training. In an\nimage classification task we demonstrate consistent improvement over the\nbaseline, with diverse network architectures and datasets."}, {"title": "Unsupervised Magnification of Posture Deviations Across Subjects", "authors": "Michael Dorkenwald, Uta B\u00fcchler, Bj\u00f6rn Ommer"}, {"title": "Accurate Estimation of Body Height From a Single Depth Image via a Four-Stage Developing Network", "authors": "Fukun Yin, Shizhe Zhou"}, {"title": "Fast Soft Color Segmentation", "authors": "Naofumi Akimoto, Huachun Zhu, Yanghua Jin, Yoshimitsu Aoki", "link": "https://arxiv.org/abs/2004.08096", "summary": "We address the problem of soft color segmentation, defined as decomposing a\ngiven image into several RGBA layers, each containing only homogeneous color\nregions. The resulting layers from decomposition pave the way for applications\nthat benefit from layer-based editing, such as recoloring and compositing of\nimages and videos. The current state-of-the-art approach for this problem is\nhindered by slow processing time due to its iterative nature, and consequently\ndoes not scale to certain real-world scenarios. To address this issue, we\npropose a neural network based method for this task that decomposes a given\nimage into multiple layers in a single forward pass. Furthermore, our method\nseparately decomposes the color layers and the alpha channel layers. By\nleveraging a novel training objective, our method achieves proper assignment of\ncolors amongst layers. As a consequence, our method achieve promising quality\nwithout existing issue of inference speed for iterative approaches. Our\nthorough experimental analysis shows that our method produces qualitative and\nquantitative results comparable to previous methods while achieving a 300,000x\nspeed improvement. Finally, we utilize our proposed method on several\napplications, and demonstrate its speed advantage, especially in video editing."}, {"title": "Global Optimality for Point Set Registration Using Semidefinite Programming", "authors": "Jos\u00e9 Pedro Iglesias, Carl Olsson, Fredrik Kahl"}, {"title": "Image2StyleGAN++: How to Edit the Embedded Images?", "authors": "Rameen Abdal, Yipeng Qin, Peter Wonka", "link": "https://arxiv.org/abs/1911.11544", "summary": "We propose Image2StyleGAN++, a flexible image editing framework with many\napplications. Our framework extends the recent Image2StyleGAN in three ways.\nFirst, we introduce noise optimization as a complement to the $W^+$ latent\nspace embedding. Our noise optimization can restore high frequency features in\nimages and thus significantly improves the quality of reconstructed images,\ne.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global\n$W^+$ latent space embedding to enable local embeddings. Third, we combine\nembedding with activation tensor manipulation to perform high quality local\nedits along with global semantic edits on images. Such edits motivate various\nhigh quality image editing applications, e.g. image reconstruction, image\ninpainting, image crossover, local style transfer, image editing using\nscribbles, and attribute level feature transfer. Examples of the edited images\nare shown across the paper for visual inspection."}, {"title": "SQE: a Self Quality Evaluation Metric for Parameters Optimization in Multi-Object Tracking", "authors": "Yanru Huang, Feiyu Zhu, Zheni Zeng, Xi Qiu, Yuan Shen, Jianan Wu", "link": "https://arxiv.org/abs/2004.07472", "summary": "We present a novel self quality evaluation metric SQE for parameters\noptimization in the challenging yet critical multi-object tracking task.\nCurrent evaluation metrics all require annotated ground truth, thus will fail\nin the test environment and realistic circumstances prohibiting further\noptimization after training. By contrast, our metric reflects the internal\ncharacteristics of trajectory hypotheses and measures tracking performance\nwithout ground truth. We demonstrate that trajectories with different qualities\nexhibit different single or multiple peaks over feature distance distribution,\ninspiring us to design a simple yet effective method to assess the quality of\ntrajectories using a two-class Gaussian mixture model. Experiments mainly on\nMOT16 Challenge data sets verify the effectiveness of our method in both\ncorrelating with existing metrics and enabling parameters self-optimization to\nachieve better performance. We believe that our conclusions and method are\ninspiring for future multi-object tracking in practice."}, {"title": "EventSR: From Asynchronous Events to Image Reconstruction, Restoration, and Super-Resolution via End-to-End Adversarial Learning", "authors": "Lin Wang, Tae-Kyun Kim, Kuk-Jin Yoon", "link": "https://arxiv.org/abs/2003.07640", "summary": "Event cameras sense intensity changes and have many advantages over\nconventional cameras. To take advantage of event cameras, some methods have\nbeen proposed to reconstruct intensity images from event streams. However, the\noutputs are still in low resolution (LR), noisy, and unrealistic. The\nlow-quality outputs stem broader applications of event cameras, where high\nspatial resolution (HR) is needed as well as high temporal resolution, dynamic\nrange, and no motion blur. We consider the problem of reconstructing and\nsuper-resolving intensity images from LR events, when no ground truth (GT) HR\nimages and down-sampling kernels are available. To tackle the challenges, we\npropose a novel end-to-end pipeline that reconstructs LR images from event\nstreams, enhances the image qualities and upsamples the enhanced images, called\nEventSR. For the absence of real GT images, our method is primarily\nunsupervised, deploying adversarial learning. To train EventSR, we create an\nopen dataset including both real-world and simulated scenes. The use of both\ndatasets boosts up the network performance, and the network architectures and\nvarious loss functions in each phase help improve the image qualities. The\nwhole pipeline is trained in three phases. While each phase is mainly for one\nof the three tasks, the networks in earlier phases are fine-tuned by respective\nloss functions in an end-to-end manner. Experimental results show that EventSR\nreconstructs high-quality SR images from events for both simulated and\nreal-world data."}, {"title": "Hierarchical Pyramid Diverse Attention Networks for Face Recognition", "authors": "Qiangchang Wang, Tianyi Wu, He Zheng, Guodong Guo"}, {"title": "RGBD-Dog: Predicting Canine Pose from RGBD Sensors", "authors": "Sin\u00e9ad Kearney, Wenbin Li, Martin Parsons, Kwang In Kim, Darren Cosker", "link": "", "summary": ""}, {"title": "Multi-Scale Progressive Fusion Network for Single Image Deraining", "authors": "Kui Jiang, Zhongyuan Wang, Peng Yi, Chen Chen, Baojin Huang, Yimin Luo, Jiayi Ma, Junjun Jiang", "link": "http://arxiv.org/abs/2003.10985", "summary": "Rain streaks in the air appear in various blurring degrees and resolutions\ndue to different distances from their positions to the camera. Similar rain\npatterns are visible in a rain image as well as its multi-scale (or\nmulti-resolution) versions, which makes it possible to exploit such\ncomplementary information for rain streak representation. In this work, we\nexplore the multi-scale collaborative representation for rain streaks from the\nperspective of input image scales and hierarchical deep features in a unified\nframework, termed multi-scale progressive fusion network (MSPFN) for single\nimage rain streak removal. For similar rain streaks at different positions, we\nemploy recurrent calculation to capture the global texture, thus allowing to\nexplore the complementary and redundant information at the spatial dimension to\ncharacterize target rain streaks. Besides, we construct multi-scale pyramid\nstructure, and further introduce the attention mechanism to guide the fine\nfusion of this correlated information from different scales. This multi-scale\nprogressive fusion strategy not only promotes the cooperative representation,\nbut also boosts the end-to-end training. Our proposed method is extensively\nevaluated on several benchmark datasets and achieves state-of-the-art results.\nMoreover, we conduct experiments on joint deraining, detection, and\nsegmentation tasks, and inspire a new research direction of vision task-driven\nimage deraining. The source code is available at\n\\url{https://github.com/kuihua/MSPFN}."}, {"title": "Learning a Neural 3D Texture Space From 2D Exemplars", "authors": "Philipp Henzler, Niloy J. Mitra, Tobias Ritschel", "link": "https://arxiv.org/abs/1912.04158", "summary": "We propose a generative model of 2D and 3D natural textures with diversity,\nvisual fidelity and at high computational efficiency. This is enabled by a\nfamily of methods that extend ideas from classic stochastic procedural\ntexturing (Perlin noise) to learned, deep, non-linearities. The key idea is a\nhard-coded, tunable and differentiable step that feeds multiple transformed\nrandom 2D or 3D fields into an MLP that can be sampled over infinite domains.\nOur model encodes all exemplars from a diverse set of textures without a need\nto be re-trained for each exemplar. Applications include texture interpolation,\nand learning 3D textures from 2D exemplars."}, {"title": "BachGAN: High-Resolution Image Synthesis From Salient Object Layout", "authors": "Yandong Li, Yu Cheng, Zhe Gan, Licheng Yu, Liqiang Wang, Jingjing Liu", "link": "https://arxiv.org/abs/2003.11690", "summary": "We propose a new task towards more practical application for image generation\n- high-quality image synthesis from salient object layout. This new setting\nallows users to provide the layout of salient objects only (i.e., foreground\nbounding boxes and categories), and lets the model complete the drawing with an\ninvented background and a matching foreground. Two main challenges spring from\nthis new task: (i) how to generate fine-grained details and realistic textures\nwithout segmentation map input; and (ii) how to create a background and weave\nit seamlessly into standalone objects. To tackle this, we propose Background\nHallucination Generative Adversarial Network (BachGAN), which first selects a\nset of segmentation maps from a large candidate pool via a background retrieval\nmodule, then encodes these candidate layouts via a background fusion module to\nhallucinate a suitable background for the given objects. By generating the\nhallucinated background representation dynamically, our model can synthesize\nhigh-resolution images with both photo-realistic foreground and integral\nbackground. Experiments on Cityscapes and ADE20K datasets demonstrate the\nadvantage of BachGAN over existing methods, measured on both visual fidelity of\ngenerated images and visual alignment between output images and input layouts."}, {"title": "Rethinking Data Augmentation for Image Super-resolution: A Comprehensive Analysis and a New Strategy", "authors": "Jaejun Yoo, Namhyuk Ahn, Kyung-Ah Sohn", "link": "https://arxiv.org/abs/2004.00448", "summary": "Data augmentation is an effective way to improve the performance of deep\nnetworks. Unfortunately, current methods are mostly developed for high-level\nvision tasks (e.g., classification) and few are studied for low-level vision\ntasks (e.g., image restoration). In this paper, we provide a comprehensive\nanalysis of the existing augmentation methods applied to the super-resolution\ntask. We find that the methods discarding or manipulating the pixels or\nfeatures too much hamper the image restoration, where the spatial relationship\nis very important. Based on our analyses, we propose CutBlur that cuts a\nlow-resolution patch and pastes it to the corresponding high-resolution image\nregion and vice versa. The key intuition of CutBlur is to enable a model to\nlearn not only \"how\" but also \"where\" to super-resolve an image. By doing so,\nthe model can understand \"how much\", instead of blindly learning to apply\nsuper-resolution to every given pixel. Our method consistently and\nsignificantly improves the performance across various scenarios, especially\nwhen the model size is big and the data is collected under real-world\nenvironments. We also show that our method improves other low-level vision\ntasks, such as denoising and compression artifact removal."}, {"title": "On Positive-Unlabeled Classification in GAN", "authors": "Tianyu Guo, Chang Xu, Jiajun Huang, Yunhe Wang, Boxin Shi, Chao Xu, Dacheng Tao", "link": "http://arxiv.org/abs/2002.01136", "summary": "This paper defines a positive and unlabeled classification problem for\nstandard GANs, which then leads to a novel technique to stabilize the training\nof the discriminator in GANs. Traditionally, real data are taken as positive\nwhile generated data are negative. This positive-negative classification\ncriterion was kept fixed all through the learning process of the discriminator\nwithout considering the gradually improved quality of generated data, even if\nthey could be more realistic than real data at times. In contrast, it is more\nreasonable to treat the generated data as unlabeled, which could be positive or\nnegative according to their quality. The discriminator is thus a classifier for\nthis positive and unlabeled classification problem, and we derive a new\nPositive-Unlabeled GAN (PUGAN). We theoretically discuss the global optimality\nthe proposed model will achieve and the equivalent optimization goal.\nEmpirically, we find that PUGAN can achieve comparable or even better\nperformance than those sophisticated discriminator stabilization methods."}, {"title": "DoveNet: Deep Image Harmonization via Domain Verification", "authors": "Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, Liqing Zhang", "link": "https://arxiv.org/abs/1911.13239", "summary": "Image composition is an important operation in image processing, but the\ninconsistency between foreground and background significantly degrades the\nquality of composite image. Image harmonization, aiming to make the foreground\ncompatible with the background, is a promising yet challenging task. However,\nthe lack of high-quality publicly available dataset for image harmonization\ngreatly hinders the development of image harmonization techniques. In this\nwork, we contribute an image harmonization dataset iHarmony4 by generating\nsynthesized composite images based on COCO (resp., Adobe5k, Flickr, day2night)\ndataset, leading to our HCOCO (resp., HAdobe5k, HFlickr, Hday2night)\nsub-dataset. Moreover, we propose a new deep image harmonization method DoveNet\nusing a novel domain verification discriminator, with the insight that the\nforeground needs to be translated to the same domain as background. Extensive\nexperiments on our constructed dataset demonstrate the effectiveness of our\nproposed method. Our dataset and code are available at\nhttps://github.com/bcmi/Image_Harmonization_Datasets."}, {"title": "Noise Robust Generative Adversarial Networks", "authors": "Takuhiro Kaneko, Tatsuya Harada", "link": "https://arxiv.org/abs/1911.11776", "summary": "Generative adversarial networks (GANs) are neural networks that learn data\ndistributions through adversarial training. In intensive studies, recent GANs\nhave shown promising results for reproducing training images. However, in spite\nof noise, they reproduce images with fidelity. As an alternative, we propose a\nnovel family of GANs called noise robust GANs (NR-GANs), which can learn a\nclean image generator even when training images are noisy. In particular,\nNR-GANs can solve this problem without having complete noise information (e.g.,\nthe noise distribution type, noise amount, or signal-noise relationship). To\nachieve this, we introduce a noise generator and train it along with a clean\nimage generator. However, without any constraints, there is no incentive to\ngenerate an image and noise separately. Therefore, we propose distribution and\ntransformation constraints that encourage the noise generator to capture only\nthe noise-specific components. In particular, considering such constraints\nunder different assumptions, we devise two variants of NR-GANs for\nsignal-independent noise and three variants of NR-GANs for signal-dependent\nnoise. On three benchmark datasets, we demonstrate the effectiveness of NR-GANs\nin noise robust image generation. Furthermore, we show the applicability of\nNR-GANs in image denoising. Our code is available at\nhttps://github.com/takuhirok/NR-GAN/."}, {"title": "Normalizing Flows With Multi-Scale Autoregressive Priors", "authors": "Apratim Bhattacharyya, Shweta Mahajan, Mario Fritz, Bernt Schiele, Stefan Roth", "link": "https://arxiv.org/abs/2004.03891", "summary": "Flow-based generative models are an important class of exact inference models\nthat admit efficient inference and sampling for image synthesis. Owing to the\nefficiency constraints on the design of the flow layers, e.g. split coupling\nflow layers in which approximately half the pixels do not undergo further\ntransformations, they have limited expressiveness for modeling long-range data\ndependencies compared to autoregressive models that rely on conditional\npixel-wise generation. In this work, we improve the representational power of\nflow-based models by introducing channel-wise dependencies in their latent\nspace through multi-scale autoregressive priors (mAR). Our mAR prior for models\nwith split coupling flow layers (mAR-SCF) can better capture dependencies in\ncomplex multimodal data. The resulting model achieves state-of-the-art density\nestimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that\nmAR-SCF allows for improved image generation quality, with gains in FID and\nInception scores compared to state-of-the-art flow-based models."}, {"title": "Robust Reference-Based Super-Resolution With Similarity-Aware Deformable Convolution", "authors": "Gyumin Shim, Jinsun Park, In So Kweon"}, {"title": "Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings", "authors": "Amy Zhao, Guha Balakrishnan, Kathleen M. Lewis, Fr\u00e9do Durand, John V. Guttag, Adrian V. Dalca", "link": "https://arxiv.org/abs/2001.01026", "summary": "We introduce a new video synthesis task: synthesizing time lapse videos\ndepicting how a given painting might have been created. Artists paint using\nunique combinations of brushes, strokes, and colors. There are often many\npossible ways to create a given painting. Our goal is to learn to capture this\nrich range of possibilities.\n  Creating distributions of long-term videos is a challenge for learning-based\nvideo synthesis methods. We present a probabilistic model that, given a single\nimage of a completed painting, recurrently synthesizes steps of the painting\nprocess. We implement this model as a convolutional neural network, and\nintroduce a novel training scheme to enable learning from a limited dataset of\npainting time lapses. We demonstrate that this model can be used to sample many\ntime steps, enabling long-term stochastic video synthesis. We evaluate our\nmethod on digital and watercolor paintings collected from video websites, and\nshow that human raters find our synthetic videos to be similar to time lapse\nvideos produced by real artists. Our code is available at\nhttps://xamyzhao.github.io/timecraft."}, {"title": "GeoDA: A Geometric Framework for Black-Box Adversarial Attacks", "authors": "Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, Huaiyu Dai", "link": "https://arxiv.org/abs/2003.06468", "summary": "Adversarial examples are known as carefully perturbed images fooling image\nclassifiers. We propose a geometric framework to generate adversarial examples\nin one of the most challenging black-box settings where the adversary can only\ngenerate a small number of queries, each of them returning the top-$1$ label of\nthe classifier. Our framework is based on the observation that the decision\nboundary of deep networks usually has a small mean curvature in the vicinity of\ndata samples. We propose an effective iterative algorithm to generate\nquery-efficient black-box perturbations with small $\\ell_p$ norms for $p \\ge\n1$, which is confirmed via experimental evaluations on state-of-the-art natural\nimage classifiers. Moreover, for $p=2$, we theoretically show that our\nalgorithm actually converges to the minimal $\\ell_2$-perturbation when the\ncurvature of the decision boundary is bounded. We also obtain the optimal\ndistribution of the queries over the iterations of the algorithm. Finally,\nexperimental results confirm that our principled black-box attack algorithm\nperforms better than state-of-the-art algorithms as it generates smaller\nperturbations with a reduced number of queries."}, {"title": "GAMIN: Generative Adversarial Multiple Imputation Network for Highly Missing Data", "authors": "Seongwook Yoon, Sanghoon Sull"}, {"title": "An Internal Covariate Shift Bounding Algorithm for Deep Neural Networks by Unitizing Layers\u2019 Outputs", "authors": "You Huang, Yuanlong Yu", "link": "https://arxiv.org/abs/2001.02814", "summary": "Batch Normalization (BN) techniques have been proposed to reduce the\nso-called Internal Covariate Shift (ICS) by attempting to keep the\ndistributions of layer outputs unchanged. Experiments have shown their\neffectiveness on training deep neural networks. However, since only the first\ntwo moments are controlled in these BN techniques, it seems that a weak\nconstraint is imposed on layer distributions and furthermore whether such\nconstraint can reduce ICS is unknown. Thus this paper proposes a measure for\nICS by using the Earth Mover (EM) distance and then derives the upper and lower\nbounds for the measure to provide a theoretical analysis of BN. The upper bound\nhas shown that BN techniques can control ICS only for the outputs with low\ndimensions and small noise whereas their control is NOT effective in other\ncases. This paper also proves that such control is just a bounding of ICS\nrather than a reduction of ICS. Meanwhile, the analysis shows that the\nhigh-order moments and noise, which BN cannot control, have great impact on the\nlower bound. Based on such analysis, this paper furthermore proposes an\nalgorithm that unitizes the outputs with an adjustable parameter to further\nbound ICS in order to cope with the problems of BN. The upper bound for the\nproposed unitization is noise-free and only dominated by the parameter. Thus,\nthe parameter can be trained to tune the bound and further to control ICS.\nBesides, the unitization is embedded into the framework of BN to reduce the\ninformation loss. The experiments show that this proposed algorithm outperforms\nexisting BN techniques on CIFAR-10, CIFAR-100 and ImageNet datasets."}, {"title": "A Unified Optimization Framework for Low-Rank Inducing Penalties", "authors": "Marcus Valtonen \u00d6rnhag, Carl Olsson", "link": "http://arxiv.org/abs/2001.08415", "summary": "In this paper we study the convex envelopes of a new class of functions.\nUsing this approach, we are able to unify two important classes of regularizers\nfrom unbiased non-convex formulations and weighted nuclear norm penalties. This\nopens up for possibilities of combining the best of both worlds, and to\nleverage each methods contribution to cases where simply enforcing one of the\nregularizers are insufficient.\n  We show that the proposed regularizers can be incorporated in standard\nsplitting schemes such as Alternating Direction Methods of Multipliers (ADMM),\nand other subgradient methods. Furthermore, we provide an efficient way of\ncomputing the proximal operator.\n  Lastly, we show on real non-rigid structure-from-motion (NRSfM) datasets, the\nissues that arise from using weighted nuclear norm penalties, and how this can\nbe remedied using our proposed method."}, {"title": "Single-Side Domain Generalization for Face Anti-Spoofing", "authors": "Yunpei Jia, Jie Zhang, Shiguang Shan, Xilin Chen", "link": "http://arxiv.org/abs/2004.14043", "summary": "Existing domain generalization methods for face anti-spoofing endeavor to\nextract common differentiation features to improve the generalization. However,\ndue to large distribution discrepancies among fake faces of different domains,\nit is difficult to seek a compact and generalized feature space for the fake\nfaces. In this work, we propose an end-to-end single-side domain generalization\nframework (SSDG) to improve the generalization ability of face anti-spoofing.\nThe main idea is to learn a generalized feature space, where the feature\ndistribution of the real faces is compact while that of the fake ones is\ndispersed among domains but compact within each domain. Specifically, a feature\ngenerator is trained to make only the real faces from different domains\nundistinguishable, but not for the fake ones, thus forming a single-side\nadversarial learning. Moreover, an asymmetric triplet loss is designed to\nconstrain the fake faces of different domains separated while the real ones\naggregated. The above two points are integrated into a unified framework in an\nend-to-end training manner, resulting in a more generalized class boundary,\nespecially good for samples from novel domains. Feature and weight\nnormalization is incorporated to further improve the generalization ability.\nExtensive experiments show that our proposed approach is effective and\noutperforms the state-of-the-art methods on four public databases."}, {"title": "The Knowledge Within: Methods for Data-Free Model Compression", "authors": "Matan Haroush, Itay Hubara, Elad Hoffer, Daniel Soudry", "link": "https://arxiv.org/abs/1912.01274", "summary": "Recently, an extensive amount of research has been focused on compressing and\naccelerating Deep Neural Networks (DNN). So far, high compression rate\nalgorithms require part of the training dataset for a low precision\ncalibration, or a fine-tuning process. However, this requirement is\nunacceptable when the data is unavailable or contains sensitive information, as\nin medical and biometric use-cases. We present three methods for generating\nsynthetic samples from trained models. Then, we demonstrate how these samples\ncan be used to calibrate and fine-tune quantized models without using any real\ndata in the process. Our best performing method has a negligible accuracy\ndegradation compared to the original training set. This method, which leverages\nintrinsic batch normalization layers' statistics of the trained model, can be\nused to evaluate data similarity. Our approach opens a path towards genuine\ndata-free model compression, alleviating the need for training data during\nmodel deployment."}, {"title": "Scale-Space Flow for End-to-End Optimized Video Compression", "authors": "Eirikur Agustsson, David Minnen, Nick Johnston, Johannes Ball\u00e9, Sung Jin Hwang, George Toderici"}, {"title": "Dynamic Neural Relational Inference", "authors": "Colin Graber, Alexander G. Schwing", "link": "", "summary": ""}, {"title": "Real-Time Panoptic Segmentation From Dense Detections", "authors": "Rui Hou, Jie Li, Arjun Bhargava, Allan Raventos, Vitor Guizilini, Chao Fang, Jerome Lynch, Adrien Gaidon", "link": "https://arxiv.org/abs/1912.01202", "summary": "Panoptic segmentation is a complex full scene parsing task requiring\nsimultaneous instance and semantic segmentation at high resolution. Current\nstate-of-the-art approaches cannot run in real-time, and simplifying these\narchitectures to improve efficiency severely degrades their accuracy. In this\npaper, we propose a new single-shot panoptic segmentation network that\nleverages dense detections and a global self-attention mechanism to operate in\nreal-time with performance approaching the state of the art. We introduce a\nnovel parameter-free mask construction method that substantially reduces\ncomputational complexity by efficiently reusing information from the object\ndetection and semantic segmentation sub-tasks. The resulting network has a\nsimple data flow that does not require feature map re-sampling or clustering\npost-processing, enabling significant hardware acceleration. Our experiments on\nthe Cityscapes and COCO benchmarks show that our network works at 30 FPS on\n1024x2048 resolution, trading a 3% relative performance degradation from the\ncurrent state of the art for up to 440% faster inference."}, {"title": "Deep Snake for Real-Time Instance Segmentation", "authors": "Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, Xiaowei Zhou", "link": "https://arxiv.org/abs/2001.01629", "summary": "This paper introduces a novel contour-based approach named deep snake for\nreal-time instance segmentation. Unlike some recent methods that directly\nregress the coordinates of the object boundary points from an image, deep snake\nuses a neural network to iteratively deform an initial contour to match the\nobject boundary, which implements the classic idea of snake algorithms with a\nlearning-based approach. For structured feature learning on the contour, we\npropose to use circular convolution in deep snake, which better exploits the\ncycle-graph structure of a contour compared against generic graph convolution.\nBased on deep snake, we develop a two-stage pipeline for instance segmentation:\ninitial contour proposal and contour deformation, which can handle errors in\nobject localization. Experiments show that the proposed approach achieves\ncompetitive performances on the Cityscapes, KINS, SBD and COCO datasets while\nbeing efficient for real-time applications with a speed of 32.3 fps for\n512$\\times$512 images on a 1080Ti GPU. The code is available at\nhttps://github.com/zju3dv/snake/."}, {"title": "AdaCoSeg: Adaptive Shape Co-Segmentation With Group Consistency Loss", "authors": "Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas J. Guibas, Hao Zhang", "link": "https://arxiv.org/abs/1903.10297", "summary": "We introduce AdaCoSeg, a deep neural network architecture for adaptive\nco-segmentation of a set of 3D shapes represented as point clouds. Differently\nfrom the familiar single-instance segmentation problem, co-segmentation is\nintrinsically contextual: how a shape is segmented can vary depending on the\nset it is in. Hence, our network features an adaptive learning module to\nproduce a consistent shape segmentation which adapts to a set. Specifically,\ngiven an input set of unsegmented shapes, we first employ an offline\npre-trained part prior network to propose per-shape parts. Then, the\nco-segmentation network iteratively and} jointly optimizes the part labelings\nacross the set subjected to a novel group consistency loss defined by matrix\nranks. While the part prior network can be trained with noisy and\ninconsistently segmented shapes, the final output of AdaCoSeg is a consistent\npart labeling for the input set, with each shape segmented into up to (a\nuser-specified) K parts. Overall, our method is weakly supervised, producing\nsegmentations tailored to the test set, without consistent ground-truth\nsegmentations. We show qualitative and quantitative results from AdaCoSeg and\nevaluate it via ablation studies and comparisons to state-of-the-art\nco-segmentation methods."}, {"title": "Learning Dynamic Routing for Semantic Segmentation", "authors": "Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, Jian Sun", "link": "https://arxiv.org/abs/2003.10401", "summary": "Recently, numerous handcrafted and searched networks have been applied for\nsemantic segmentation. However, previous works intend to handle inputs with\nvarious scales in pre-defined static architectures, such as FCN, U-Net, and\nDeepLab series. This paper studies a conceptually new method to alleviate the\nscale variance in semantic representation, named dynamic routing. The proposed\nframework generates data-dependent routes, adapting to the scale distribution\nof each image. To this end, a differentiable gating function, called soft\nconditional gate, is proposed to select scale transform paths on the fly. In\naddition, the computational cost can be further reduced in an end-to-end manner\nby giving budget constraints to the gating function. We further relax the\nnetwork level routing space to support multi-path propagations and\nskip-connections in each forward, bringing substantial network capacity. To\ndemonstrate the superiority of the dynamic property, we compare with several\nstatic architectures, which can be modeled as special cases in the routing\nspace. Extensive experiments are conducted on Cityscapes and PASCAL VOC 2012 to\nillustrate the effectiveness of the dynamic framework. Code is available at\nhttps://github.com/yanwei-li/DynamicRouting."}, {"title": "Boosting Semantic Human Matting With Coarse Annotations", "authors": "Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang, Xian-Sheng Hua", "link": "https://arxiv.org/abs/2004.04955", "summary": "Semantic human matting aims to estimate the per-pixel opacity of the\nforeground human regions. It is quite challenging and usually requires user\ninteractive trimaps and plenty of high quality annotated data. Annotating such\nkind of data is labor intensive and requires great skills beyond normal users,\nespecially considering the very detailed hair part of humans. In contrast,\ncoarse annotated human dataset is much easier to acquire and collect from the\npublic dataset. In this paper, we propose to use coarse annotated data coupled\nwith fine annotated data to boost end-to-end semantic human matting without\ntrimaps as extra input. Specifically, we train a mask prediction network to\nestimate the coarse semantic mask using the hybrid data, and then propose a\nquality unification network to unify the quality of the previous coarse mask\noutputs. A matting refinement network takes in the unified mask and the input\nimage to predict the final alpha matte. The collected coarse annotated dataset\nenriches our dataset significantly, allows generating high quality alpha matte\nfor real images. Experimental results show that the proposed method performs\ncomparably against state-of-the-art methods. Moreover, the proposed method can\nbe used for refining coarse annotated public dataset, as well as semantic\nsegmentation methods, which reduces the cost of annotating high quality human\ndata to a great extent."}, {"title": "BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation", "authors": "Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, Youliang Yan", "link": "https://arxiv.org/abs/2001.00309", "summary": "Instance segmentation is one of the fundamental vision tasks. Recently, fully\nconvolutional instance segmentation methods have drawn much attention as they\nare often simpler and more efficient than two-stage approaches like Mask R-CNN.\nTo date, almost all such approaches fall behind the two-stage Mask R-CNN method\nin mask precision when models have similar computation complexity, leaving\ngreat room for improvement.\n  In this work, we achieve improved mask prediction by effectively combining\ninstance-level information with semantic information with lower-level\nfine-granularity. Our main contribution is a blender module which draws\ninspiration from both top-down and bottom-up instance segmentation approaches.\nThe proposed BlendMask can effectively predict dense per-pixel\nposition-sensitive instance features with very few channels, and learn\nattention maps for each instance with merely one convolution layer, thus being\nfast in inference. BlendMask can be easily incorporated with the\nstate-of-the-art one-stage detection frameworks and outperforms Mask R-CNN\nunder the same training schedule while being 20% faster. A light-weight version\nof BlendMask achieves $ 34.2% $ mAP at 25 FPS evaluated on a single 1080Ti GPU\ncard. Because of its simplicity and efficacy, we hope that our BlendMask could\nserve as a simple yet strong baseline for a wide range of instance-wise\nprediction tasks.\n  Code is available at https://git.io/AdelaiDet"}, {"title": "UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders", "authors": "Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, Nick Barnes", "link": "http://arxiv.org/abs/2004.05763", "summary": "In this paper, we propose the first framework (UCNet) to employ uncertainty\nfor RGB-D saliency detection by learning from the data labeling process.\nExisting RGB-D saliency detection methods treat the saliency detection task as\na point estimation problem, and produce a single saliency map following a\ndeterministic learning pipeline. Inspired by the saliency data labeling\nprocess, we propose probabilistic RGB-D saliency detection network via\nconditional variational autoencoders to model human annotation uncertainty and\ngenerate multiple saliency maps for each input image by sampling in the latent\nspace. With the proposed saliency consensus process, we are able to generate an\naccurate saliency map based on these multiple predictions. Quantitative and\nqualitative evaluations on six challenging benchmark datasets against 18\ncompeting algorithms demonstrate the effectiveness of our approach in learning\nthe distribution of saliency maps, leading to a new state-of-the-art in RGB-D\nsaliency detection."}, {"title": "Deep Geometric Functional Maps: Robust Feature Learning for Shape Correspondence", "authors": "Nicolas Donati, Abhishek Sharma, Maks Ovsjanikov", "link": "https://arxiv.org/abs/2003.14286", "summary": "We present a novel learning-based approach for computing correspondences\nbetween non-rigid 3D shapes. Unlike previous methods that either require\nextensive training data or operate on handcrafted input descriptors and thus\ngeneralize poorly across diverse datasets, our approach is both accurate and\nrobust to changes in shape structure. Key to our method is a feature-extraction\nnetwork that learns directly from raw shape geometry, combined with a novel\nregularized map extraction layer and loss, based on the functional map\nrepresentation. We demonstrate through extensive experiments in challenging\nshape matching scenarios that our method can learn from less training data than\nexisting supervised approaches and generalizes significantly better than\ncurrent descriptor-based learning methods. Our source code is available at:\nhttps://github.com/LIX-shape-analysis/GeomFmaps."}, {"title": "Deep Polarization Cues for Transparent Object Segmentation", "authors": "Agastya Kalra, Vage Taamazyan, Supreeth Krishna Rao, Kartik Venkataraman, Ramesh Raskar, Achuta Kadambi"}, {"title": "DualConvMesh-Net: Joint Geodesic and Euclidean Convolutions on 3D Meshes", "authors": "Jonas Schult, Francis Engelmann, Theodora Kontogianni, Bastian Leibe", "link": "https://arxiv.org/abs/2004.01002", "summary": "We propose DualConvMesh-Nets (DCM-Net) a family of deep hierarchical\nconvolutional networks over 3D geometric data that combines two types of\nconvolutions. The first type, geodesic convolutions, defines the kernel weights\nover mesh surfaces or graphs. That is, the convolutional kernel weights are\nmapped to the local surface of a given mesh. The second type, Euclidean\nconvolutions, is independent of any underlying mesh structure. The\nconvolutional kernel is applied on a neighborhood obtained from a local\naffinity representation based on the Euclidean distance between 3D points.\nIntuitively, geodesic convolutions can easily separate objects that are\nspatially close but have disconnected surfaces, while Euclidean convolutions\ncan represent interactions between nearby objects better, as they are oblivious\nto object surfaces. To realize a multi-resolution architecture, we borrow\nwell-established mesh simplification methods from the geometry processing\ndomain and adapt them to define mesh-preserving pooling and unpooling\noperations. We experimentally show that combining both types of convolutions in\nour architecture leads to significant performance gains for 3D semantic\nsegmentation, and we report competitive results on three scene segmentation\nbenchmarks. Our models and code are publicly available."}, {"title": "F-BRS: Rethinking Backpropagating Refinement for Interactive Segmentation", "authors": "Konstantin Sofiiuk, Ilia Petrov, Olga Barinova, Anton Konushin", "link": "https://arxiv.org/abs/2001.10331", "summary": "Deep neural networks have become a mainstream approach to interactive\nsegmentation. As we show in our experiments, while for some images a trained\nnetwork provides accurate segmentation result with just a few clicks, for some\nunknown objects it cannot achieve satisfactory result even with a large amount\nof user input. Recently proposed backpropagating refinement (BRS) scheme\nintroduces an optimization problem for interactive segmentation that results in\nsignificantly better performance for the hard cases. At the same time, BRS\nrequires running forward and backward pass through a deep network several times\nthat leads to significantly increased computational budget per click compared\nto other methods. We propose f-BRS (feature backpropagating refinement scheme)\nthat solves an optimization problem with respect to auxiliary variables instead\nof the network inputs, and requires running forward and backward pass just for\na small part of a network. Experiments on GrabCut, Berkeley, DAVIS and SBD\ndatasets set new state-of-the-art at an order of magnitude lower time per click\ncompared to original BRS. The code and trained models are available at\nhttps://github.com/saic-vul/fbrs_interactive_segmentation ."}, {"title": "Approximating shapes in images with low-complexity polygons", "authors": "Muxingzi Li, Florent Lafarge, Renaud Marlet"}, {"title": "Towards Visually Explaining Variational Autoencoders", "authors": "Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, Richard J. Radke, Octavia Camps", "link": "https://arxiv.org/abs/1911.07389", "summary": "Recent advances in Convolutional Neural Network (CNN) model interpretability\nhave led to impressive progress in visualizing and understanding model\npredictions. In particular, gradient-based visual attention methods have driven\nmuch recent effort in using visual attention maps as a means for visual\nexplanations. A key problem, however, is these methods are designed for\nclassification and categorization tasks, and their extension to explaining\ngenerative models, e.g. variational autoencoders (VAE) is not trivial. In this\nwork, we take a step towards bridging this crucial gap, proposing the first\ntechnique to visually explain VAEs by means of gradient-based attention. We\npresent methods to generate visual attention from the learned latent space, and\nalso demonstrate such attention explanations serve more than just explaining\nVAE predictions. We show how these attention maps can be used to localize\nanomalies in images, demonstrating state-of-the-art performance on the MVTec-AD\ndataset. We also show how they can be infused into model training, helping\nbootstrap the VAE into learning improved latent space disentanglement,\ndemonstrated on the Dsprites dataset."}, {"title": "Towards Global Explanations of Convolutional Neural Networks With Concept Attribution", "authors": "Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R. Lyu, Yu-Wing Tai"}, {"title": "Interpretable and Accurate Fine-grained Recognition via Region Grouping", "authors": "Zixuan Huang, Yin Li"}, {"title": "SAM: The Sensitivity of Attribution Methods to Hyperparameters", "authors": "Naman Bansal, Chirag Agarwal, Anh Nguyen", "link": "https://arxiv.org/abs/2003.08754", "summary": "Attribution methods can provide powerful insights into the reasons for a\nclassifier's decision. We argue that a key desideratum of an explanation method\nis its robustness to input hyperparameters which are often randomly set or\nempirically tuned. High sensitivity to arbitrary hyperparameter choices does\nnot only impede reproducibility but also questions the correctness of an\nexplanation and impairs the trust of end-users. In this paper, we provide a\nthorough empirical study on the sensitivity of existing attribution methods. We\nfound an alarming trend that many methods are highly sensitive to changes in\ntheir common hyperparameters e.g. even changing a random seed can yield a\ndifferent explanation! Interestingly, such sensitivity is not reflected in the\naverage explanation accuracy scores over the dataset as commonly reported in\nthe literature. In addition, explanations generated for robust classifiers\n(i.e. which are trained to be invariant to pixel-wise perturbations) are\nsurprisingly more robust than those generated for regular classifiers."}, {"title": "High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks", "authors": "Haohan Wang, Xindi Wu, Zeyi Huang, Eric P. Xing", "link": "https://arxiv.org/abs/1905.13545", "summary": "We investigate the relationship between the frequency spectrum of image data\nand the generalization behavior of convolutional neural networks (CNN). We\nfirst notice CNN's ability in capturing the high-frequency components of\nimages. These high-frequency components are almost imperceptible to a human.\nThus the observation leads to multiple hypotheses that are related to the\ngeneralization behaviors of CNN, including a potential explanation for\nadversarial examples, a discussion of CNN's trade-off between robustness and\naccuracy, and some evidence in understanding training heuristics."}, {"title": "CNN-Generated Images Are Surprisingly Easy to Spot\u2026 for Now", "authors": "Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, Alexei A. Efros"}, {"title": "FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions", "authors": "Shaohua Li, Kaiping Xue, Bin Zhu, Chenkai Ding, Xindi Gao, David Wei, Tao Wan", "link": "https://arxiv.org/abs/1811.08257", "summary": "Machine learning as a service has been widely deployed to utilize deep neural\nnetwork models to provide prediction services. However, this raises privacy\nconcerns since clients need to send sensitive information to servers. In this\npaper, we focus on the scenario where clients want to classify private images\nwith a convolutional neural network model hosted in the server, while both\nparties keep their data private. We present FALCON, a fast and secure approach\nfor CNN predictions based on Fourier Transform. Our solution enables linear\nlayers of a CNN model to be evaluated simply and efficiently with fully\nhomomorphic encryption. We also introduce the first efficient and\nprivacy-preserving protocol for softmax function, which is an indispensable\ncomponent in CNNs and has not yet been evaluated in previous works due to its\nhigh complexity. We implemented the FALCON and evaluated the performance on\nreal-world CNN models. The experimental results show that FALCON outperforms\nthe best known works in both computation and communication cost."}, {"title": "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion", "authors": "Hongxu Yin, Pavlo Molchanov, Jose M. Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K. Jha, Jan Kautz", "link": "https://arxiv.org/abs/1912.08795", "summary": "We introduce DeepInversion, a new method for synthesizing images from the\nimage distribution used to train a deep neural network. We 'invert' a trained\nnetwork (teacher) to synthesize class-conditional input images starting from\nrandom noise, without using any additional information about the training\ndataset. Keeping the teacher fixed, our method optimizes the input while\nregularizing the distribution of intermediate feature maps using information\nstored in the batch normalization layers of the teacher. Further, we improve\nthe diversity of synthesized images using Adaptive DeepInversion, which\nmaximizes the Jensen-Shannon divergence between the teacher and student network\nlogits. The resulting synthesized images from networks trained on the CIFAR-10\nand ImageNet datasets demonstrate high fidelity and degree of realism, and help\nenable a new breed of data-free applications - ones that do not require any\nreal images or labeled data. We demonstrate the applicability of our proposed\nmethod to three tasks of immense practical importance -- (i) data-free network\npruning, (ii) data-free knowledge transfer, and (iii) data-free continual\nlearning."}, {"title": "Unsupervised Domain Adaptation via Structurally Regularized Deep Clustering", "authors": "Hui Tang, Ke Chen, Kui Jia", "link": "https://arxiv.org/abs/2003.08607", "summary": "Unsupervised domain adaptation (UDA) is to make predictions for unlabeled\ndata on a target domain, given labeled data on a source domain whose\ndistribution shifts from the target one. Mainstream UDA methods learn aligned\nfeatures between the two domains, such that a classifier trained on the source\nfeatures can be readily applied to the target ones. However, such a\ntransferring strategy has a potential risk of damaging the intrinsic\ndiscrimination of target data. To alleviate this risk, we are motivated by the\nassumption of structural domain similarity, and propose to directly uncover the\nintrinsic target discrimination via discriminative clustering of target data.\nWe constrain the clustering solutions using structural source regularization\nthat hinges on our assumed structural domain similarity. Technically, we use a\nflexible framework of deep network based discriminative clustering that\nminimizes the KL divergence between predictive label distribution of the\nnetwork and an introduced auxiliary one; replacing the auxiliary distribution\nwith that formed by ground-truth labels of source data implements the\nstructural source regularization via a simple strategy of joint network\ntraining. We term our proposed method as Structurally Regularized Deep\nClustering (SRDC), where we also enhance target discrimination with clustering\nof intermediate network features, and enhance structural regularization with\nsoft selection of less divergent source examples. Careful ablation studies show\nthe efficacy of our proposed SRDC. Notably, with no explicit domain alignment,\nSRDC outperforms all existing methods on three UDA benchmarks."}, {"title": "HyperSTAR: Task-Aware Hyperparameters for Deep Networks", "authors": "Gaurav Mittal, Chang Liu, Nikolaos Karianakis, Victor Fragoso, Mei Chen, Yun Fu", "link": "https://arxiv.org/abs/2005.10524", "summary": "While deep neural networks excel in solving visual recognition tasks, they\nrequire significant effort to find hyperparameters that make them work\noptimally. Hyperparameter Optimization (HPO) approaches have automated the\nprocess of finding good hyperparameters but they do not adapt to a given task\n(task-agnostic), making them computationally inefficient. To reduce HPO time,\nwe present HyperSTAR (System for Task Aware Hyperparameter Recommendation), a\ntask-aware method to warm-start HPO for deep neural networks. HyperSTAR ranks\nand recommends hyperparameters by predicting their performance conditioned on a\njoint dataset-hyperparameter space. It learns a dataset (task) representation\nalong with the performance predictor directly from raw images in an end-to-end\nfashion. The recommendations, when integrated with an existing HPO method, make\nit task-aware and significantly reduce the time to achieve optimal performance.\nWe conduct extensive experiments on 10 publicly available large-scale image\nclassification datasets over two different network architectures, validating\nthat HyperSTAR evaluates 50% less configurations to achieve the best\nperformance compared to existing methods. We further demonstrate that HyperSTAR\nmakes Hyperband (HB) task-aware, achieving the optimal accuracy in just 25% of\nthe budget required by both vanilla HB and Bayesian Optimized HB~(BOHB)."}, {"title": "ActBERT: Learning Global-Local Video-Text Representations", "authors": "Linchao Zhu, Yi Yang"}, {"title": "State-Relabeling Adversarial Active Learning", "authors": "Beichen Zhang, Liang Li, Shijie Yang, Shuhui Wang, Zheng-Jun Zha, Qingming Huang", "link": "https://arxiv.org/abs/2004.04943", "summary": "Active learning is to design label-efficient algorithms by sampling the most\nrepresentative samples to be labeled by an oracle. In this paper, we propose a\nstate relabeling adversarial active learning model (SRAAL), that leverages both\nthe annotation and the labeled/unlabeled state information for deriving the\nmost informative unlabeled samples. The SRAAL consists of a representation\ngenerator and a state discriminator. The generator uses the complementary\nannotation information with traditional reconstruction information to generate\nthe unified representation of samples, which embeds the semantic into the whole\ndata representation. Then, we design an online uncertainty indicator in the\ndiscriminator, which endues unlabeled samples with different importance. As a\nresult, we can select the most informative samples based on the discriminator's\npredicted state. We also design an algorithm to initialize the labeled pool,\nwhich makes subsequent sampling more efficient. The experiments conducted on\nvarious datasets show that our model outperforms the previous state-of-art\nactive learning methods and our initially sampling algorithm achieves better\nperformance."}, {"title": "Erasing Integrated Learning: A Simple Yet Effective Approach for Weakly Supervised Object Localization", "authors": "Jinjie Mai, Meng Yang, Wenfeng Luo"}, {"title": "A Shared Multi-Attention Framework for Multi-Label Zero-Shot Learning", "authors": "Dat Huynh, Ehsan Elhamifar"}, {"title": "Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos", "authors": "Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi"}, {"title": "Few-Shot Open-Set Recognition Using Meta-Learning", "authors": "Bo Liu, Hao Kang, Haoxiang Li, Gang Hua, Nuno Vasconcelos", "link": "https://arxiv.org/abs/2005.13713", "summary": "The problem of open-set recognition is considered. While previous approaches\nonly consider this problem in the context of large-scale classifier training,\nwe seek a unified solution for this and the low-shot classification setting. It\nis argued that the classic softmax classifier is a poor solution for open-set\nrecognition, since it tends to overfit on the training classes. Randomization\nis then proposed as a solution to this problem. This suggests the use of\nmeta-learning techniques, commonly used for few-shot classification, for the\nsolution of open-set recognition. A new oPen sEt mEta LEaRning (PEELER)\nalgorithm is then introduced. This combines the random selection of a set of\nnovel classes per episode, a loss that maximizes the posterior entropy for\nexamples of those classes, and a new metric learning formulation based on the\nMahalanobis distance. Experimental results show that PEELER achieves state of\nthe art open set recognition performance for both few-shot and large-scale\nrecognition. On CIFAR and miniImageNet, it achieves substantial gains in\nseen/unseen class detection AUROC for a given seen-class classification\naccuracy."}, {"title": "Few-Shot Learning via Embedding Adaptation With Set-to-Set Functions", "authors": "Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, Fei Sha", "link": "https://arxiv.org/abs/1812.03664", "summary": "Learning with limited data is a key challenge for visual recognition. Many\nfew-shot learning methods address this challenge by learning an instance\nembedding function from seen classes and apply the function to instances from\nunseen classes with limited labels. This style of transfer learning is\ntask-agnostic: the embedding function is not learned optimally discriminative\nwith respect to the unseen classes, where discerning among them leads to the\ntarget task. In this paper, we propose a novel approach to adapt the instance\nembeddings to the target classification task with a set-to-set function,\nyielding embeddings that are task-specific and are discriminative. We\nempirically investigated various instantiations of such set-to-set functions\nand observed the Transformer is most effective -- as it naturally satisfies key\nproperties of our desired model. We denote this model as FEAT (few-shot\nembedding adaptation w/ Transformer) and validate it on both the standard\nfew-shot classification benchmark and four extended few-shot learning settings\nwith essential use cases, i.e., cross-domain, transductive, generalized\nfew-shot learning, and low-shot learning. It archived consistent improvements\nover baseline models as well as previous methods and established the new\nstate-of-the-art results on two benchmarks."}, {"title": "Temporally Distributed Networks for Fast Video Semantic Segmentation", "authors": "Ping Hu, Fabian Caba, Oliver Wang, Zhe Lin, Stan Sclaroff, Federico Perazzi", "link": "https://arxiv.org/abs/2004.01800", "summary": "We present TDNet, a temporally distributed network designed for fast and\naccurate video semantic segmentation. We observe that features extracted from a\ncertain high-level layer of a deep CNN can be approximated by composing\nfeatures extracted from several shallower sub-networks. Leveraging the inherent\ntemporal continuity in videos, we distribute these sub-networks over sequential\nframes. Therefore, at each time step, we only need to perform a lightweight\ncomputation to extract a sub-features group from a single sub-network. The full\nfeatures used for segmentation are then recomposed by application of a novel\nattention propagation module that compensates for geometry deformation between\nframes. A grouped knowledge distillation loss is also introduced to further\nimprove the representation power at both full and sub-feature levels.\nExperiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method\nachieves state-of-the-art accuracy with significantly faster speed and lower\nlatency."}, {"title": "Benchmarking the Robustness of Semantic Segmentation Models", "authors": "Christoph Kamann, Carsten Rother", "link": "https://arxiv.org/abs/1908.05005", "summary": "When designing a semantic segmentation module for a practical application,\nsuch as autonomous driving, it is crucial to understand the robustness of the\nmodule with respect to a wide range of image corruptions. While there are\nrecent robustness studies for full-image classification, we are the first to\npresent an exhaustive study for semantic segmentation, based on the\nstate-of-the-art model DeepLabv3+. To increase the realism of our study, we\nutilize almost 400,000 images generated from PASCAL VOC 2012, Cityscapes, and\nADE20K. Based on the benchmark study, we gain several new insights. Firstly,\nmodel robustness increases with model performance, in most cases. Secondly,\nsome architecture properties affect robustness significantly, such as a Dense\nPrediction Cell, which was designed to maximize performance on clean data only."}, {"title": "There and Back Again: Revisiting Backpropagation Saliency Methods", "authors": "Sylvestre-Alvise Rebuffi, Ruth Fong, Xu Ji, Andrea Vedaldi", "link": "https://arxiv.org/abs/2004.02866", "summary": "Saliency methods seek to explain the predictions of a model by producing an\nimportance map across each input sample. A popular class of such methods is\nbased on backpropagating a signal and analyzing the resulting gradient. Despite\nmuch research on such methods, relatively little work has been done to clarify\nthe differences between such methods as well as the desiderata of these\ntechniques. Thus, there is a need for rigorously understanding the\nrelationships between different methods as well as their failure modes. In this\nwork, we conduct a thorough analysis of backpropagation-based saliency methods\nand propose a single framework under which several such methods can be unified.\nAs a result of our study, we make three additional contributions. First, we use\nour framework to propose NormGrad, a novel saliency method based on the spatial\ncontribution of gradients of convolutional weights. Second, we combine saliency\nmaps at different layers to test the ability of saliency methods to extract\ncomplementary information at different network levels (e.g.~trading off spatial\nresolution and distinctiveness) and we explain why some methods fail at\nspecific layers (e.g., Grad-CAM anywhere besides the last convolutional layer).\nThird, we introduce a class-sensitivity metric and a meta-learning inspired\nparadigm applicable to any saliency method for improving sensitivity to the\noutput class being explained."}, {"title": "Deep Semantic Clustering by Partition Confidence Maximisation", "authors": "Jiabo Huang, Shaogang Gong, Xiatian Zhu"}, {"title": "StructEdit: Learning Structural Shape Variations", "authors": "Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy J. Mitra, Leonidas J. Guibas", "link": "https://arxiv.org/abs/1911.11098", "summary": "Learning to encode differences in the geometry and (topological) structure of\nthe shapes of ordinary objects is key to generating semantically plausible\nvariations of a given shape, transferring edits from one shape to another, and\nmany other applications in 3D content creation. The common approach of encoding\nshapes as points in a high-dimensional latent feature space suggests treating\nshape differences as vectors in that space. Instead, we treat shape differences\nas primary objects in their own right and propose to encode them in their own\nlatent space. In a setting where the shapes themselves are encoded in terms of\nfine-grained part hierarchies, we demonstrate that a separate encoding of shape\ndeltas or differences provides a principled way to deal with inhomogeneities in\nthe shape space due to different combinatorial part structures, while also\nallowing for compactness in the representation, as well as edit abstraction and\ntransfer. Our approach is based on a conditional variational autoencoder for\nencoding and decoding shape deltas, conditioned on a source shape. We\ndemonstrate the effectiveness and robustness of our approach in multiple shape\nmodification and generation tasks, and provide comparison and ablation studies\non the PartNet dataset, one of the largest publicly available 3D datasets."}, {"title": "Harmonizing Transferability and Discriminability for Adapting Object Detectors", "authors": "Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, Qi Dou", "link": "https://arxiv.org/abs/2003.06297", "summary": "Recent advances in adaptive object detection have achieved compelling results\nin virtue of adversarial feature adaptation to mitigate the distributional\nshifts along the detection pipeline. Whilst adversarial adaptation\nsignificantly enhances the transferability of feature representations, the\nfeature discriminability of object detectors remains less investigated.\nMoreover, transferability and discriminability may come at a contradiction in\nadversarial adaptation given the complex combinations of objects and the\ndifferentiated scene layouts between domains. In this paper, we propose a\nHierarchical Transferability Calibration Network (HTCN) that hierarchically\n(local-region/image/instance) calibrates the transferability of feature\nrepresentations for harmonizing transferability and discriminability. The\nproposed model consists of three components: (1) Importance Weighted\nAdversarial Training with input Interpolation (IWAT-I), which strengthens the\nglobal discriminability by re-weighting the interpolated image-level features;\n(2) Context-aware Instance-Level Alignment (CILA) module, which enhances the\nlocal discriminability by capturing the underlying complementary effect between\nthe instance-level feature and the global context information for the\ninstance-level feature alignment; (3) local feature masks that calibrate the\nlocal transferability to provide semantic guidance for the following\ndiscriminative pattern alignment. Experimental results show that HTCN\nsignificantly outperforms the state-of-the-art methods on benchmark datasets."}, {"title": "Fast Video Object Segmentation With Temporal Aggregation Network and Dynamic Template Matching", "authors": "Xuhua Huang, Jiarui Xu, Yu-Wing Tai, Chi-Keung Tang"}, {"title": "CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation via Global and Local Refinement", "authors": "Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, Chi-Keung Tang", "link": "https://arxiv.org/abs/2005.02551", "summary": "State-of-the-art semantic segmentation methods were almost exclusively\ntrained on images within a fixed resolution range. These segmentations are\ninaccurate for very high-resolution images since using bicubic upsampling of\nlow-resolution segmentation does not adequately capture high-resolution details\nalong object boundaries. In this paper, we propose a novel approach to address\nthe high-resolution segmentation problem without using any high-resolution\ntraining data. The key insight is our CascadePSP network which refines and\ncorrects local boundaries whenever possible. Although our network is trained\nwith low-resolution segmentation data, our method is applicable to any\nresolution even for very high-resolution images larger than 4K. We present\nquantitative and qualitative studies on different datasets to show that\nCascadePSP can reveal pixel-accurate segmentation boundaries using our novel\nrefinement module without any finetuning. Thus, our method can be regarded as\nclass-agnostic. Finally, we demonstrate the application of our model to scene\nparsing in multi-class segmentation."}, {"title": "Correlating Edge, Pose With Parsing", "authors": "Ziwei Zhang, Chi Su, Liang Zheng, Xiaodong Xie", "link": "https://arxiv.org/abs/2005.01431", "summary": "According to existing studies, human body edge and pose are two beneficial\nfactors to human parsing. The effectiveness of each of the high-level features\n(edge and pose) is confirmed through the concatenation of their features with\nthe parsing features. Driven by the insights, this paper studies how human\nsemantic boundaries and keypoint locations can jointly improve human parsing.\nCompared with the existing practice of feature concatenation, we find that\nuncovering the correlation among the three factors is a superior way of\nleveraging the pivotal contextual cues provided by edges and poses. To capture\nsuch correlations, we propose a Correlation Parsing Machine (CorrPM) employing\na heterogeneous non-local block to discover the spatial affinity among feature\nmaps from the edge, pose and parsing. The proposed CorrPM allows us to report\nnew state-of-the-art accuracy on three human parsing datasets. Importantly,\ncomparative studies confirm the advantages of feature correlation over the\nconcatenation."}, {"title": "VecRoad: Point-Based Iterative Graph Exploration for Road Graphs Extraction", "authors": "Yong-Qiang Tan, Shang-Hua Gao, Xuan-Yi Li, Ming-Ming Cheng, Bo Ren"}, {"title": "Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation", "authors": "Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, Olga Russakovsky", "link": "https://arxiv.org/abs/1911.11834", "summary": "Computer vision models learn to perform a task by capturing relevant\nstatistics from training data. It has been shown that models learn spurious\nage, gender, and race correlations when trained for seemingly unrelated tasks\nlike activity recognition or image captioning. Various mitigation techniques\nhave been presented to prevent models from utilizing or learning such biases.\nHowever, there has been little systematic comparison between these techniques.\nWe design a simple but surprisingly effective visual recognition benchmark for\nstudying bias mitigation. Using this benchmark, we provide a thorough analysis\nof a wide range of techniques. We highlight the shortcomings of popular\nadversarial training approaches for bias mitigation, propose a simple but\nsimilarly effective alternative to the inference-time Reducing Bias\nAmplification method of Zhao et al., and design a domain-independent training\ntechnique that outperforms all other methods. Finally, we validate our findings\non the attribute classification task in the CelebA dataset, where attribute\npresence is known to be correlated with the gender of people in the image, and\ndemonstrate that the proposed technique is effective at mitigating real-world\ngender bias."}, {"title": "Hierarchical Human Parsing With Typed Part-Relation Reasoning", "authors": "Wenguan Wang, Hailong Zhu, Jifeng Dai, Yanwei Pang, Jianbing Shen, Ling Shao", "link": "https://arxiv.org/abs/2003.04845", "summary": "Human parsing is for pixel-wise human semantic understanding. As human bodies\nare underlying hierarchically structured, how to model human structures is the\ncentral theme in this task. Focusing on this, we seek to simultaneously exploit\nthe representational capacity of deep graph networks and the hierarchical human\nstructures. In particular, we provide following two contributions. First, three\nkinds of part relations, i.e., decomposition, composition, and dependency, are,\nfor the first time, completely and precisely described by three distinct\nrelation networks. This is in stark contrast to previous parsers, which only\nfocus on a portion of the relations and adopt a type-agnostic relation modeling\nstrategy. More expressive relation information can be captured by explicitly\nimposing the parameters in the relation networks to satisfy the specific\ncharacteristics of different relations. Second, previous parsers largely ignore\nthe need for an approximation algorithm over the loopy human hierarchy, while\nwe instead address an iterative reasoning process, by assimilating generic\nmessage-passing networks with their edge-typed, convolutional counterparts.\nWith these efforts, our parser lays the foundation for more sophisticated and\nflexible human relation patterns of reasoning. Comprehensive experiments on\nfive datasets demonstrate that our parser sets a new state-of-the-art on each."}, {"title": "Compositional Convolutional Neural Networks: A Deep Architecture With Innate Robustness to Partial Occlusion", "authors": "Adam Kortylewski, Ju He, Qing Liu, Alan L. Yuille", "link": "https://arxiv.org/abs/2003.04490", "summary": "Recent findings show that deep convolutional neural networks (DCNNs) do not\ngeneralize well under partial occlusion. Inspired by the success of\ncompositional models at classifying partially occluded objects, we propose to\nintegrate compositional models and DCNNs into a unified deep model with innate\nrobustness to partial occlusion. We term this architecture Compositional\nConvolutional Neural Network. In particular, we propose to replace the fully\nconnected classification head of a DCNN with a differentiable compositional\nmodel. The generative nature of the compositional model enables it to localize\noccluders and subsequently focus on the non-occluded parts of the object. We\nconduct classification experiments on artificially occluded images as well as\nreal images of partially occluded objects from the MS-COCO dataset. The results\nshow that DCNNs do not classify occluded objects robustly, even when trained\nwith data that is strongly augmented with partial occlusions. Our proposed\nmodel outperforms standard DCNNs by a large margin at classifying partially\noccluded objects, even when it has not been exposed to occluded objects during\ntraining. Additional experiments demonstrate that CompositionalNets can also\nlocalize the occluders accurately, despite being trained with class labels\nonly. The code used in this work is publicly available."}, {"title": "Spatial Pyramid Based Graph Reasoning for Semantic Segmentation", "authors": "Xia Li, Yibo Yang, Qijie Zhao, Tiancheng Shen, Zhouchen Lin, Hong Liu", "link": "https://arxiv.org/abs/2003.10211", "summary": "The convolution operation suffers from a limited receptive filed, while\nglobal modeling is fundamental to dense prediction tasks, such as semantic\nsegmentation. In this paper, we apply graph convolution into the semantic\nsegmentation task and propose an improved Laplacian. The graph reasoning is\ndirectly performed in the original feature space organized as a spatial\npyramid. Different from existing methods, our Laplacian is data-dependent and\nwe introduce an attention diagonal matrix to learn a better distance metric. It\ngets rid of projecting and re-projecting processes, which makes our proposed\nmethod a light-weight module that can be easily plugged into current computer\nvision architectures. More importantly, performing graph reasoning directly in\nthe feature space retains spatial relationships and makes spatial pyramid\npossible to explore multiple long-range contextual patterns from different\nscales. Experiments on Cityscapes, COCO Stuff, PASCAL Context and PASCAL VOC\ndemonstrate the effectiveness of our proposed methods on semantic segmentation.\nWe achieve comparable performance with advantages in computational and memory\noverhead."}, {"title": "Learning Video Object Segmentation From Unlabeled Videos", "authors": "Xiankai Lu, Wenguan Wang, Jianbing Shen, Yu-Wing Tai, David J. Crandall, Steven C. H. Hoi", "link": "https://arxiv.org/abs/2003.05020", "summary": "We propose a new method for video object segmentation (VOS) that addresses\nobject pattern learning from unlabeled videos, unlike most existing methods\nwhich rely heavily on extensive annotated data. We introduce a unified\nunsupervised/weakly supervised learning framework, called MuG, that\ncomprehensively captures intrinsic properties of VOS at multiple granularities.\nOur approach can help advance understanding of visual patterns in VOS and\nsignificantly reduce annotation burden. With a carefully-designed architecture\nand strong representation learning ability, our learned model can be applied to\ndiverse VOS settings, including object-level zero-shot VOS, instance-level\nzero-shot VOS, and one-shot VOS. Experiments demonstrate promising performance\nin these settings, as well as the potential of MuG in leveraging unlabeled data\nto further improve the segmentation accuracy."}, {"title": "Part-Aware Context Network for Human Parsing", "authors": "Xiaomei Zhang, Yingying Chen, Bingke Zhu, Jinqiao Wang, Ming Tang", "link": "", "summary": ""}, {"title": "SCOUT: Self-Aware Discriminant Counterfactual Explanations", "authors": "Pei Wang, Nuno Vasconcelos", "link": "https://arxiv.org/abs/2004.07769", "summary": "The problem of counterfactual visual explanations is considered. A new family\nof discriminant explanations is introduced. These produce heatmaps that\nattribute high scores to image regions informative of a classifier prediction\nbut not of a counter class. They connect attributive explanations, which are\nbased on a single heat map, to counterfactual explanations, which account for\nboth predicted class and counter class. The latter are shown to be computable\nby combination of two discriminant explanations, with reversed class pairs. It\nis argued that self-awareness, namely the ability to produce classification\nconfidence scores, is important for the computation of discriminant\nexplanations, which seek to identify regions where it is easy to discriminate\nbetween prediction and counter class. This suggests the computation of\ndiscriminant explanations by the combination of three attribution maps. The\nresulting counterfactual explanations are optimization free and thus much\nfaster than previous methods. To address the difficulty of their evaluation, a\nproxy task and set of quantitative metrics are also proposed. Experiments under\nthis protocol show that the proposed counterfactual explanations outperform the\nstate of the art while achieving much higher speeds, for popular networks. In a\nhuman-learning machine teaching experiment, they are also shown to improve mean\nstudent accuracy from chance level to 95\\%."}, {"title": "Weakly-Supervised Semantic Segmentation via Sub-Category Exploration", "authors": "Yu-Ting Chang, Qiaosong Wang, Wei-Chih Hung, Robinson Piramuthu, Yi-Hsuan Tsai, Ming-Hsuan Yang"}, {"title": "Continual Learning With Extended Kronecker-Factored Approximate Curvature", "authors": "Janghyeon Lee, Hyeong Gwon Hong, Donggyu Joo, Junmo Kim", "link": "https://arxiv.org/abs/2004.07507", "summary": "We propose a quadratic penalty method for continual learning of neural\nnetworks that contain batch normalization (BN) layers. The Hessian of a loss\nfunction represents the curvature of the quadratic penalty function, and a\nKronecker-factored approximate curvature (K-FAC) is used widely to practically\ncompute the Hessian of a neural network. However, the approximation is not\nvalid if there is dependence between examples, typically caused by BN layers in\ndeep network architectures. We extend the K-FAC method so that the\ninter-example relations are taken into account and the Hessian of deep neural\nnetworks can be properly approximated under practical assumptions. We also\npropose a method of weight merging and reparameterization to properly handle\nstatistical parameters of BN, which plays a critical role for continual\nlearning with BN, and a method that selects hyperparameters without source task\ndata. Our method shows better performance than baselines in the permuted MNIST\ntask with BN layers and in sequential learning from the ImageNet classification\ntask to fine-grained classification tasks with ResNet-50, without any explicit\nor implicit use of source task data for hyperparameter selection."}, {"title": "Phase Consistent Ecological Domain Adaptation", "authors": "Yanchao Yang, Dong Lao, Ganesh Sundaramoorthi, Stefano Soatto", "link": "http://arxiv.org/abs/2004.04923", "summary": "We introduce two criteria to regularize the optimization involved in learning\na classifier in a domain where no annotated data are available, leveraging\nannotated data in a different domain, a problem known as unsupervised domain\nadaptation. We focus on the task of semantic segmentation, where annotated\nsynthetic data are aplenty, but annotating real data is laborious. The first\ncriterion, inspired by visual psychophysics, is that the map between the two\nimage domains be phase-preserving. This restricts the set of possible learned\nmaps, while enabling enough flexibility to transfer semantic information. The\nsecond criterion aims to leverage ecological statistics, or regularities in the\nscene which are manifest in any image of it, regardless of the characteristics\nof the illuminant or the imaging sensor. It is implemented using a deep neural\nnetwork that scores the likelihood of each possible segmentation given a single\nun-annotated image. Incorporating these two priors in a standard domain\nadaptation framework improves performance across the board in the most common\nunsupervised domain adaptation benchmarks for semantic segmentation."}, {"title": "AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-Identification", "authors": "Yunpeng Zhai, Shijian Lu, Qixiang Ye, Xuebo Shan, Jie Chen, Rongrong Ji, Yonghong Tian", "link": "https://arxiv.org/abs/2004.08787", "summary": "Domain adaptive person re-identification (re-ID) is a challenging task,\nespecially when person identities in target domains are unknown. Existing\nmethods attempt to address this challenge by transferring image styles or\naligning feature distributions across domains, whereas the rich unlabeled\nsamples in target domains are not sufficiently exploited. This paper presents a\nnovel augmented discriminative clustering (AD-Cluster) technique that estimates\nand augments person clusters in target domains and enforces the discrimination\nability of re-ID models with the augmented clusters. AD-Cluster is trained by\niterative density-based clustering, adaptive sample augmentation, and\ndiscriminative feature learning. It learns an image generator and a feature\nencoder which aim to maximize the intra-cluster diversity in the sample space\nand minimize the intra-cluster distance in the feature space in an adversarial\nmin-max manner. Finally, AD-Cluster increases the diversity of sample clusters\nand improves the discrimination capability of re-ID models greatly. Extensive\nexperiments over Market-1501 and DukeMTMC-reID show that AD-Cluster outperforms\nthe state-of-the-art with large margins."}, {"title": "3D-MPA: Multi-Proposal Aggregation for 3D Semantic Instance Segmentation", "authors": "Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, Matthias Nie\u00dfner", "link": "https://arxiv.org/abs/2003.13867", "summary": "We present 3D-MPA, a method for instance segmentation on 3D point clouds.\nGiven an input point cloud, we propose an object-centric approach where each\npoint votes for its object center. We sample object proposals from the\npredicted object centers. Then, we learn proposal features from grouped point\nfeatures that voted for the same object center. A graph convolutional network\nintroduces inter-proposal relations, providing higher-level feature learning in\naddition to the lower-level point features. Each proposal comprises a semantic\nlabel, a set of associated points over which we define a foreground-background\nmask, an objectness score and aggregation features. Previous works usually\nperform non-maximum-suppression (NMS) over proposals to obtain the final object\ndetections or semantic instances. However, NMS can discard potentially correct\npredictions. Instead, our approach keeps all proposals and groups them together\nbased on the learned aggregation features. We show that grouping proposals\nimproves over NMS and outperforms previous state-of-the-art methods on the\ntasks of 3D object detection and semantic instance segmentation on the\nScanNetV2 benchmark and the S3DIS dataset."}, {"title": "Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision", "authors": "Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, Sotaro Tsukizawa", "link": "https://arxiv.org/abs/2003.00393", "summary": "Active learning (AL) aims to minimize labeling efforts for data-demanding\ndeep neural networks (DNNs) by selecting the most representative data points\nfor annotation. However, currently used methods are ill-equipped to deal with\nbiased data. The main motivation of this paper is to consider a realistic\nsetting for pool-based semi-supervised AL, where the unlabeled collection of\ntrain data is biased. We theoretically derive an optimal acquisition function\nfor AL in this setting. It can be formulated as distribution shift minimization\nbetween unlabeled train data and weakly-labeled validation dataset. To\nimplement such acquisition function, we propose a low-complexity method for\nfeature density matching using self-supervised Fisher kernel (FK) as well as\nseveral novel pseudo-label estimators. Our FK-based method outperforms\nstate-of-the-art methods on MNIST, SVHN, and ImageNet classification while\nrequiring only 1/10th of processing. The conducted experiments show at least\n40% drop in labeling efforts for the biased class-imbalanced data compared to\nexisting methods."}, {"title": "Adaptive Graph Convolutional Network With Attention Graph Clustering for Co-Saliency Detection", "authors": "Kaihua Zhang, Tengpeng Li, Shiwen Shen, Bo Liu, Jin Chen, Qingshan Liu", "link": "https://arxiv.org/abs/2003.06167", "summary": "Co-saliency detection aims to discover the common and salient foregrounds\nfrom a group of relevant images. For this task, we present a novel adaptive\ngraph convolutional network with attention graph clustering (GCAGC). Three\nmajor contributions have been made, and are experimentally shown to have\nsubstantial practical merits. First, we propose a graph convolutional network\ndesign to extract information cues to characterize the intra- and interimage\ncorrespondence. Second, we develop an attention graph clustering algorithm to\ndiscriminate the common objects from all the salient foreground objects in an\nunsupervised fashion. Third, we present a unified framework with\nencoder-decoder structure to jointly train and optimize the graph convolutional\nnetwork, attention graph cluster, and co-saliency detection decoder in an\nend-to-end manner. We evaluate our proposed GCAGC method on three cosaliency\ndetection benchmark datasets (iCoseg, Cosal2015 and COCO-SEG). Our GCAGC method\nobtains significant improvements over the state-of-the-arts on most of them."}, {"title": "A2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection", "authors": "Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren, Huchuan Lu"}, {"title": "Deep Fair Clustering for Visual Learning", "authors": "Peizhao Li, Han Zhao, Hongfu Liu", "link": "", "summary": ""}, {"title": "Bidirectional Graph Reasoning Network for Panoptic Segmentation", "authors": "Yangxin Wu, Gengwei Zhang, Yiming Gao, Xiajun Deng, Ke Gong, Xiaodan Liang, Liang Lin", "link": "https://arxiv.org/abs/2004.06272", "summary": "Recent researches on panoptic segmentation resort to a single end-to-end\nnetwork to combine the tasks of instance segmentation and semantic\nsegmentation. However, prior models only unified the two related tasks at the\narchitectural level via a multi-branch scheme or revealed the underlying\ncorrelation between them by unidirectional feature fusion, which disregards the\nexplicit semantic and co-occurrence relations among objects and background.\nInspired by the fact that context information is critical to recognize and\nlocalize the objects, and inclusive object details are significant to parse the\nbackground scene, we thus investigate on explicitly modeling the correlations\nbetween object and background to achieve a holistic understanding of an image\nin the panoptic segmentation task. We introduce a Bidirectional Graph Reasoning\nNetwork (BGRNet), which incorporates graph structure into the conventional\npanoptic segmentation network to mine the intra-modular and intermodular\nrelations within and between foreground things and background stuff classes. In\nparticular, BGRNet first constructs image-specific graphs in both instance and\nsemantic segmentation branches that enable flexible reasoning at the proposal\nlevel and class level, respectively. To establish the correlations between\nseparate branches and fully leverage the complementary relations between things\nand stuff, we propose a Bidirectional Graph Connection Module to diffuse\ninformation across branches in a learnable fashion. Experimental results\ndemonstrate the superiority of our BGRNet that achieves the new\nstate-of-the-art performance on challenging COCO and ADE20K panoptic\nsegmentation benchmarks."}, {"title": "Exploit Clues From Views: Self-Supervised and Regularized Learning for Multiview Object Recognition", "authors": "Chih-Hui Ho, Bo Liu, Tz-Ying Wu, Nuno Vasconcelos", "link": "https://arxiv.org/abs/2003.12735", "summary": "Multiview recognition has been well studied in the literature and achieves\ndecent performance in object recognition and retrieval task. However, most\nprevious works rely on supervised learning and some impractical underlying\nassumptions, such as the availability of all views in training and inference\ntime. In this work, the problem of multiview self-supervised learning (MV-SSL)\nis investigated, where only image to object association is given. Given this\nsetup, a novel surrogate task for self-supervised learning is proposed by\npursuing \"object invariant\" representation. This is solved by randomly\nselecting an image feature of an object as object prototype, accompanied with\nmultiview consistency regularization, which results in view invariant\nstochastic prototype embedding (VISPE). Experiments shows that the recognition\nand retrieval results using VISPE outperform that of other self-supervised\nlearning methods on seen and unseen data. VISPE can also be applied to\nsemi-supervised scenario and demonstrates robust performance with limited data\navailable. Code is available at https://github.com/chihhuiho/VISPE"}, {"title": "Spherical Space Domain Adaptation With Robust Pseudo-Label Loss", "authors": "Xiang Gu, Jian Sun, Zongben Xu"}, {"title": "Stochastic Classifiers for Unsupervised Domain Adaptation", "authors": "Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, Tao Xiang", "link": "", "summary": ""}, {"title": "Unsupervised Learning of Intrinsic Structural Representation Points", "authors": "Nenglun Chen, Lingjie Liu, Zhiming Cui, Runnan Chen, Duygu Ceylan, Changhe Tu, Wenping Wang", "link": "http://arxiv.org/abs/2003.01661", "summary": "Learning structures of 3D shapes is a fundamental problem in the field of\ncomputer graphics and geometry processing. We present a simple yet\ninterpretable unsupervised method for learning a new structural representation\nin the form of 3D structure points. The 3D structure points produced by our\nmethod encode the shape structure intrinsically and exhibit semantic\nconsistency across all the shape instances with similar structures. This is a\nchallenging goal that has not fully been achieved by other methods.\nSpecifically, our method takes a 3D point cloud as input and encodes it as a\nset of local features. The local features are then passed through a novel point\nintegration module to produce a set of 3D structure points. The chamfer\ndistance is used as reconstruction loss to ensure the structure points lie\nclose to the input point cloud. Extensive experiments have shown that our\nmethod outperforms the state-of-the-art on the semantic shape correspondence\ntask and achieves comparable performance with the state-of-the-art on the\nsegmentation label transfer task. Moreover, the PCA based shape embedding built\nupon consistent structure points demonstrates good performance in preserving\nthe shape structures. Code is available at\nhttps://github.com/NolenChen/3DStructurePoints"}, {"title": "PolyTransform: Deep Polygon Transformer for Instance Segmentation", "authors": "Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen Xiong, Rui Hu, Raquel Urtasun", "link": "https://arxiv.org/abs/1912.02801", "summary": "In this paper, we propose PolyTransform, a novel instance segmentation\nalgorithm that produces precise, geometry-preserving masks by combining the\nstrengths of prevailing segmentation approaches and modern polygon-based\nmethods. In particular, we first exploit a segmentation network to generate\ninstance masks. We then convert the masks into a set of polygons that are then\nfed to a deforming network that transforms the polygons such that they better\nfit the object boundaries. Our experiments on the challenging Cityscapes\ndataset show that our PolyTransform significantly improves the performance of\nthe backbone instance segmentation network and ranks 1st on the Cityscapes\ntest-set leaderboard. We also show impressive gains in the interactive\nannotation setting."}, {"title": "Interactive Two-Stream Decoder for Accurate and Fast Saliency Detection", "authors": "Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, Lingxiao Yang"}, {"title": "Towards Better Generalization: Joint Depth-Pose Learning Without PoseNet", "authors": "Wang Zhao, Shaohui Liu, Yezhi Shu, Yong-Jin Liu", "link": "https://arxiv.org/abs/2004.01314", "summary": "In this work, we tackle the essential problem of scale inconsistency for\nself-supervised joint depth-pose learning. Most existing methods assume that a\nconsistent scale of depth and pose can be learned across all input samples,\nwhich makes the learning problem harder, resulting in degraded performance and\nlimited generalization in indoor environments and long-sequence visual odometry\napplication. To address this issue, we propose a novel system that explicitly\ndisentangles scale from the network estimation. Instead of relying on PoseNet\narchitecture, our method recovers relative pose by directly solving fundamental\nmatrix from dense optical flow correspondence and makes use of a two-view\ntriangulation module to recover an up-to-scale 3D structure. Then, we align the\nscale of the depth prediction with the triangulated point cloud and use the\ntransformed depth map for depth error computation and dense reprojection check.\nOur whole system can be jointly trained end-to-end. Extensive experiments show\nthat our system not only reaches state-of-the-art performance on KITTI depth\nand flow estimation, but also significantly improves the generalization ability\nof existing self-supervised depth-pose learning methods under a variety of\nchallenging scenarios, and achieves state-of-the-art results among\nself-supervised learning-based methods on KITTI Odometry and NYUv2 dataset.\nFurthermore, we present some interesting findings on the limitation of\nPoseNet-based relative pose estimation methods in terms of generalization\nability. Code is available at https://github.com/B1ueber2y/TrianFlow."}, {"title": "LT-Net: Label Transfer by Learning Reversible Voxel-Wise Correspondence for One-Shot Medical Image Segmentation", "authors": "Shuxin Wang, Shilei Cao, Dong Wei, Renzhen Wang, Kai Ma, Liansheng Wang, Deyu Meng, Yefeng Zheng", "link": "http://arxiv.org/abs/2003.07072", "summary": "We introduce a one-shot segmentation method to alleviate the burden of manual\nannotation for medical images. The main idea is to treat one-shot segmentation\nas a classical atlas-based segmentation problem, where voxel-wise\ncorrespondence from the atlas to the unlabelled data is learned. Subsequently,\nsegmentation label of the atlas can be transferred to the unlabelled data with\nthe learned correspondence. However, since ground truth correspondence between\nimages is usually unavailable, the learning system must be well-supervised to\navoid mode collapse and convergence failure. To overcome this difficulty, we\nresort to the forward-backward consistency, which is widely used in\ncorrespondence problems, and additionally learn the backward correspondences\nfrom the warped atlases back to the original atlas. This cycle-correspondence\nlearning design enables a variety of extra, cycle-consistency-based supervision\nsignals to make the training process stable, while also boost the performance.\nWe demonstrate the superiority of our method over both deep learning-based\none-shot segmentation methods and a classical multi-atlas segmentation method\nvia thorough experiments."}, {"title": "FGN: Fully Guided Network for Few-Shot Instance Segmentation", "authors": "Zhibo Fan, Jin-Gang Yu, Zhihao Liang, Jiarong Ou, Changxin Gao, Gui-Song Xia, Yuanqing Li", "link": "https://arxiv.org/abs/2003.13954", "summary": "Few-shot instance segmentation (FSIS) conjoins the few-shot learning paradigm\nwith general instance segmentation, which provides a possible way of tackling\ninstance segmentation in the lack of abundant labeled data for training. This\npaper presents a Fully Guided Network (FGN) for few-shot instance segmentation.\nFGN perceives FSIS as a guided model where a so-called support set is encoded\nand utilized to guide the predictions of a base instance segmentation network\n(i.e., Mask R-CNN), critical to which is the guidance mechanism. In this view,\nFGN introduces different guidance mechanisms into the various key components in\nMask R-CNN, including Attention-Guided RPN, Relation-Guided Detector, and\nAttention-Guided FCN, in order to make full use of the guidance effect from the\nsupport set and adapt better to the inter-class generalization. Experiments on\npublic datasets demonstrate that our proposed FGN can outperform the\nstate-of-the-art methods."}, {"title": "A Quantum Computational Approach to Correspondence Problems on Point Sets", "authors": "Vladislav Golyanik, Christian Theobalt", "link": "https://arxiv.org/abs/1912.12296", "summary": "Modern adiabatic quantum computers (AQC) are already used to solve difficult\ncombinatorial optimisation problems in various domains of science. Currently,\nonly a few applications of AQC in computer vision have been demonstrated. We\nreview AQC and derive a new algorithm for correspondence problems on point sets\nsuitable for execution on AQC. Our algorithm has a subquadratic computational\ncomplexity of the state preparation. Examples of successful transformation\nestimation and point set alignment by simulated sampling are shown in the\nsystematic experimental evaluation. Finally, we analyse the differences in the\nsolutions and the corresponding energy values."}, {"title": "Data-Efficient Semi-Supervised Learning by Reliable Edge Mining", "authors": "Peibin Chen, Tao Ma, Xu Qin, Weidi Xu, Shuchang Zhou"}, {"title": "NestedVAE: Isolating Common Factors via Weak Supervision", "authors": "Matthew J. Vowels, Necati Cihan Camg\u00f6z, Richard Bowden", "link": "https://arxiv.org/abs/2002.11576", "summary": "Fair and unbiased machine learning is an important and active field of\nresearch, as decision processes are increasingly driven by models that learn\nfrom data. Unfortunately, any biases present in the data may be learned by the\nmodel, thereby inappropriately transferring that bias into the decision making\nprocess. We identify the connection between the task of bias reduction and that\nof isolating factors common between domains whilst encouraging domain specific\ninvariance. To isolate the common factors we combine the theory of deep latent\nvariable models with information bottleneck theory for scenarios whereby data\nmay be naturally paired across domains and no additional supervision is\nrequired. The result is the Nested Variational AutoEncoder (NestedVAE). Two\nouter VAEs with shared weights attempt to reconstruct the input and infer a\nlatent space, whilst a nested VAE attempts to reconstruct the latent\nrepresentation of one image, from the latent representation of its paired\nimage. In so doing, the nested VAE isolates the common latent factors/causes\nand becomes invariant to unwanted factors that are not shared between paired\nimages. We also propose a new metric to provide a balanced method of evaluating\nconsistency and classifier performance across domains which we refer to as the\nAdjusted Parity metric. An evaluation of NestedVAE on both domain and attribute\ninvariance, change detection, and learning common factors for the prediction of\nbiological sex demonstrates that NestedVAE significantly outperforms\nalternative methods."}, {"title": "Progressive Adversarial Networks for Fine-Grained Domain Adaptation", "authors": "Sinan Wang, Xinyang Chen, Yunbo Wang, Mingsheng Long, Jianmin Wang"}, {"title": "A Disentangling Invertible Interpretation Network for Explaining Latent Representations", "authors": "Patrick Esser, Robin Rombach, Bj\u00f6rn Ommer", "link": "https://arxiv.org/abs/2004.13166", "summary": "Neural networks have greatly boosted performance in computer vision by\nlearning powerful representations of input data. The drawback of end-to-end\ntraining for maximal overall performance are black-box models whose hidden\nrepresentations are lacking interpretability: Since distributed coding is\noptimal for latent layers to improve their robustness, attributing meaning to\nparts of a hidden feature vector or to individual neurons is hindered. We\nformulate interpretation as a translation of hidden representations onto\nsemantic concepts that are comprehensible to the user. The mapping between both\ndomains has to be bijective so that semantic modifications in the target domain\ncorrectly alter the original representation. The proposed invertible\ninterpretation network can be transparently applied on top of existing\narchitectures with no need to modify or retrain them. Consequently, we\ntranslate an original representation to an equivalent yet interpretable one and\nbackwards without affecting the expressiveness and performance of the original.\nThe invertible interpretation network disentangles the hidden representation\ninto separate, semantically meaningful concepts. Moreover, we present an\nefficient approach to define semantic concepts by only sketching two images and\nalso an unsupervised strategy. Experimental evaluation demonstrates the wide\napplicability to interpretation of existing classification and image generation\nnetworks as well as to semantically guided image manipulation."}, {"title": "Modeling the Background for Incremental Learning in Semantic Segmentation", "authors": "Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bul\u00f2, Elisa Ricci, Barbara Caputo", "link": "https://arxiv.org/abs/2002.00718", "summary": "Despite their effectiveness in a wide range of tasks, deep architectures\nsuffer from some important limitations. In particular, they are vulnerable to\ncatastrophic forgetting, i.e. they perform poorly when they are required to\nupdate their model as new classes are available but the original training set\nis not retained. This paper addresses this problem in the context of semantic\nsegmentation. Current strategies fail on this task because they do not consider\na peculiar aspect of semantic segmentation: since each training step provides\nannotation only for a subset of all possible classes, pixels of the background\nclass (i.e. pixels that do not belong to any other classes) exhibit a semantic\ndistribution shift. In this work we revisit classical incremental learning\nmethods, proposing a new distillation-based framework which explicitly accounts\nfor this shift. Furthermore, we introduce a novel strategy to initialize\nclassifier's parameters, thus preventing biased predictions toward the\nbackground class. We demonstrate the effectiveness of our approach with an\nextensive evaluation on the Pascal-VOC 2012 and ADE20K datasets, significantly\noutperforming state of the art incremental learning methods."}, {"title": "Interpreting the Latent Space of GANs for Semantic Face Editing", "authors": "Yujun Shen, Jinjin Gu, Xiaoou Tang, Bolei Zhou", "link": "https://arxiv.org/abs/1907.10786", "summary": "Despite the recent advance of Generative Adversarial Networks (GANs) in\nhigh-fidelity image synthesis, there lacks enough understanding of how GANs are\nable to map a latent code sampled from a random distribution to a\nphoto-realistic image. Previous work assumes the latent space learned by GANs\nfollows a distributed representation but observes the vector arithmetic\nphenomenon. In this work, we propose a novel framework, called InterFaceGAN,\nfor semantic face editing by interpreting the latent semantics learned by GANs.\nIn this framework, we conduct a detailed study on how different semantics are\nencoded in the latent space of GANs for face synthesis. We find that the latent\ncode of well-trained generative models actually learns a disentangled\nrepresentation after linear transformations. We explore the disentanglement\nbetween various semantics and manage to decouple some entangled semantics with\nsubspace projection, leading to more precise control of facial attributes.\nBesides manipulating gender, age, expression, and the presence of eyeglasses,\nwe can even vary the face pose as well as fix the artifacts accidentally\ngenerated by GAN models. The proposed method is further applied to achieve real\nimage manipulation when combined with GAN inversion methods or some\nencoder-involved models. Extensive results suggest that learning to synthesize\nfaces spontaneously brings a disentangled and controllable facial attribute\nrepresentation."}, {"title": "Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation", "authors": "Jianqiang Wan, Yang Liu, Donglai Wei, Xiang Bai, Yongchao Xu"}, {"title": "Self-Learning With Rectification Strategy for Human Parsing", "authors": "Tao Li, Zhiyuan Liang, Sanyuan Zhao, Jiahao Gong, Jianbing Shen", "link": "https://arxiv.org/abs/2004.08055", "summary": "In this paper, we solve the sample shortage problem in the human parsing\ntask. We begin with the self-learning strategy, which generates pseudo-labels\nfor unlabeled data to retrain the model. However, directly using noisy\npseudo-labels will cause error amplification and accumulation. Considering the\ntopology structure of human body, we propose a trainable graph reasoning method\nthat establishes internal structural connections between graph nodes to correct\ntwo typical errors in the pseudo-labels, i.e., the global structural error and\nthe local consistency error. For the global error, we first transform\ncategory-wise features into a high-level graph model with coarse-grained\nstructural information, and then decouple the high-level graph to reconstruct\nthe category features. The reconstructed features have a stronger ability to\nrepresent the topology structure of the human body. Enlarging the receptive\nfield of features can effectively reducing the local error. We first project\nfeature pixels into a local graph model to capture pixel-wise relations in a\nhierarchical graph manner, then reverse the relation information back to the\npixels. With the global structural and local consistency modules, these errors\nare rectified and confident pseudo-labels are generated for retraining.\nExtensive experiments on the LIP and the ATR datasets demonstrate the\neffectiveness of our global and local rectification modules. Our method\noutperforms other state-of-the-art methods in supervised human parsing tasks."}, {"title": "Hyperbolic Visual Embedding Learning for Zero-Shot Recognition", "authors": "Shaoteng Liu, Jingjing Chen, Liangming Pan, Chong-Wah Ngo, Tat-Seng Chua, Yu-Gang Jiang"}, {"title": "Sequential Mastery of Multiple Visual Tasks: Networks Naturally Learn to Learn and Forget to Forget", "authors": "Guy Davidson, Michael C. Mozer", "link": "https://arxiv.org/abs/1905.10837", "summary": "We explore the behavior of a standard convolutional neural net in a\ncontinual-learning setting that introduces visual classification tasks\nsequentially and requires the net to master new tasks while preserving mastery\nof previously learned tasks. This setting corresponds to that which human\nlearners face as they acquire domain expertise serially, for example, as an\nindividual studies a textbook. Through simulations involving sequences of ten\nrelated visual tasks, we find reason for optimism that nets will scale well as\nthey advance from having a single skill to becoming multi-skill domain experts.\nWe observe two key phenomena. First, \\emph{forward facilitation}---the\naccelerated learning of task $n+1$ having learned $n$ previous tasks---grows\nwith $n$. Second, \\emph{backward interference}---the forgetting of the $n$\nprevious tasks when learning task $n+1$---diminishes with $n$. Amplifying\nforward facilitation is the goal of research on metalearning, and attenuating\nbackward interference is the goal of research on catastrophic forgetting. We\nfind that both of these goals are attained simply through broader exposure to a\ndomain."}, {"title": "Distilling Effective Supervision From Severe Label Noise", "authors": "Zizhao Zhang, Han Zhang, Sercan \u00d6. Arik, Honglak Lee, Tomas Pfister", "link": "https://arxiv.org/abs/1910.00701", "summary": "Collecting large-scale data with clean labels for supervised training of\nneural networks is practically challenging. Although noisy labels are usually\ncheap to acquire, existing methods suffer a lot from label noise. This paper\ntargets at the challenge of robust training at high label noise regimes. The\nkey insight to achieve this goal is to wisely leverage a small trusted set to\nestimate exemplar weights and pseudo labels for noisy data in order to reuse\nthem for supervised training. We present a holistic framework to train deep\nneural networks in a way that is highly invulnerable to label noise. Our method\nsets the new state of the art on various types of label noise and achieves\nexcellent performance on large-scale datasets with real-world label noise. For\ninstance, on CIFAR100 with a $40\\%$ uniform noise ratio and only 10 trusted\nlabeled data per class, our method achieves $80.2{\\pm}0.3\\%$ classification\naccuracy, where the error rate is only $1.4\\%$ higher than a neural network\ntrained without label noise. Moreover, increasing the noise ratio to $80\\%$,\nour method still maintains a high accuracy of $75.5{\\pm}0.2\\%$, compared to the\nprevious best accuracy $48.2\\%$.\n  Source code available:\nhttps://github.com/google-research/google-research/tree/master/ieg"}, {"title": "Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks", "authors": "Aditya Golatkar, Alessandro Achille, Stefano Soatto", "link": "https://arxiv.org/abs/1911.04933", "summary": "We explore the problem of selectively forgetting a particular subset of the\ndata used for training a deep neural network. While the effects of the data to\nbe forgotten can be hidden from the output of the network, insights may still\nbe gleaned by probing deep into its weights. We propose a method for\n\"scrubbing'\" the weights clean of information about a particular set of\ntraining data. The method does not require retraining from scratch, nor access\nto the data originally used for training. Instead, the weights are modified so\nthat any probing function of the weights is indistinguishable from the same\nfunction applied to the weights of a network trained without the data to be\nforgotten. This condition is a generalized and weaker form of Differential\nPrivacy. Exploiting ideas related to the stability of stochastic gradient\ndescent, we introduce an upper-bound on the amount of information remaining in\nthe weights, which can be estimated efficiently even for deep neural networks."}, {"title": "CenterMask: Single Shot Instance Segmentation With Point Representation", "authors": "Yuqing Wang, Zhaoliang Xu, Hao Shen, Baoshan Cheng, Lirong Yang", "link": "https://arxiv.org/abs/2004.04446", "summary": "In this paper, we propose a single-shot instance segmentation method, which\nis simple, fast and accurate. There are two main challenges for one-stage\ninstance segmentation: object instances differentiation and pixel-wise feature\nalignment. Accordingly, we decompose the instance segmentation into two\nparallel subtasks: Local Shape prediction that separates instances even in\noverlapping conditions, and Global Saliency generation that segments the whole\nimage in a pixel-to-pixel manner. The outputs of the two branches are assembled\nto form the final instance masks. To realize that, the local shape information\nis adopted from the representation of object center points. Totally trained\nfrom scratch and without any bells and whistles, the proposed CenterMask\nachieves 34.5 mask AP with a speed of 12.3 fps, using a single-model with\nsingle-scale training/testing on the challenging COCO dataset. The accuracy is\nhigher than all other one-stage instance segmentation methods except the 5\ntimes slower TensorMask, which shows the effectiveness of CenterMask. Besides,\nour method can be easily embedded to other one-stage object detectors such as\nFCOS and performs well, showing the generalization of CenterMask."}, {"title": "Mitigating Bias in Face Recognition Using Skewness-Aware Reinforcement Learning", "authors": "Mei Wang, Weihong Deng", "link": "https://arxiv.org/abs/1911.10692", "summary": "Racial equality is an important theme of international human rights law, but\nit has been largely obscured when the overall face recognition accuracy is\npursued blindly. More facts indicate racial bias indeed degrades the fairness\nof recognition system and the error rates on non-Caucasians are usually much\nhigher than Caucasians. To encourage fairness, we introduce the idea of\nadaptive margin to learn balanced performance for different races based on\nlarge margin losses. A reinforcement learning based race balance network\n(RL-RBN) is proposed. We formulate the process of finding the optimal margins\nfor non-Caucasians as a Markov decision process and employ deep Q-learning to\nlearn policies for an agent to select appropriate margin by approximating the\nQ-value function. Guided by the agent, the skewness of feature scatter between\nraces can be reduced. Besides, we provide two ethnicity aware training\ndatasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be\nutilized to study racial bias from both data and algorithm aspects. Extensive\nexperiments on RFW database show that RL-RBN successfully mitigates racial bias\nand learns more balanced performance for different races."}, {"title": "MineGAN: Effective Knowledge Transfer From GANs to Target Domains With Few Images", "authors": "Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, Fahad Shahbaz Khan, Joost van de Weijer", "link": "https://arxiv.org/abs/1912.05270", "summary": "One of the attractive characteristics of deep neural networks is their\nability to transfer knowledge obtained in one domain to other related domains.\nAs a result, high-quality networks can be trained in domains with relatively\nlittle training data. This property has been extensively studied for\ndiscriminative networks but has received significantly less attention for\ngenerative models. Given the often enormous effort required to train GANs, both\ncomputationally as well as in the dataset collection, the re-use of pretrained\nGANs is a desirable objective. We propose a novel knowledge transfer method for\ngenerative models based on mining the knowledge that is most beneficial to a\nspecific target domain, either from a single or multiple pretrained GANs. This\nis done using a miner network that identifies which part of the generative\ndistribution of each pretrained GAN outputs samples closest to the target\ndomain. Mining effectively steers GAN sampling towards suitable regions of the\nlatent space, which facilitates the posterior finetuning and avoids pathologies\nof other methods such as mode collapse and lack of flexibility. We perform\nexperiments on several complex datasets using various GAN architectures\n(BigGAN, Progressive GAN) and show that the proposed method, called MineGAN,\neffectively transfers knowledge to domains with few target images,\noutperforming existing methods. In addition, MineGAN can successfully transfer\nknowledge from multiple pretrained GANs. Our code is available at:\nhttps://github.com/yaxingwang/MineGAN."}, {"title": "DLWL: Improving Detection for Lowshot Classes With Weakly Labelled Data", "authors": "Vignesh Ramanathan, Rui Wang, Dhruv Mahajan"}, {"title": "Unsupervised Deep Shape Descriptor With Point Distribution Learning", "authors": "Yi Shi, Mengchen Xu, Shuaihang Yuan, Yi Fang"}, {"title": "Stylization-Based Architecture for Fast Deep Exemplar Colorization", "authors": "Zhongyou Xu, Tingting Wang, Faming Fang, Yun Sheng, Guixu Zhang", "link": "", "summary": ""}, {"title": "Cars Can\u2019t Fly Up in the Sky: Improving Urban-Scene Segmentation via Height-Driven Attention Networks", "authors": "Sungha Choi, Joanne T. Kim, Jaegul Choo", "link": "https://arxiv.org/abs/2003.05128", "summary": "This paper exploits the intrinsic features of urban-scene images and proposes\na general add-on module, called height-driven attention networks (HANet), for\nimproving semantic segmentation for urban-scene images. It emphasizes\ninformative features or classes selectively according to the vertical position\nof a pixel. The pixel-wise class distributions are significantly different from\neach other among horizontally segmented sections in the urban-scene images.\nLikewise, urban-scene images have their own distinct characteristics, but most\nsemantic segmentation networks do not reflect such unique attributes in the\narchitecture. The proposed network architecture incorporates the capability\nexploiting the attributes to handle the urban scene dataset effectively. We\nvalidate the consistent performance (mIoU) increase of various semantic\nsegmentation models on two datasets when HANet is adopted. This extensive\nquantitative analysis demonstrates that adding our module to existing models is\neasy and cost-effective. Our method achieves a new state-of-the-art performance\non the Cityscapes benchmark with a large margin among ResNet-101 based\nsegmentation models. Also, we show that the proposed model is coherent with the\nfacts observed in the urban scene by visualizing and interpreting the attention\nmap. Our code and trained models are publicly available at\nhttps://github.com/shachoi/HANet"}, {"title": "State-Aware Tracker for Real-Time Video Object Segmentation", "authors": "Xi Chen, Zuoxin Li, Ye Yuan, Gang Yu, Jianxin Shen, Donglian Qi", "link": "https://arxiv.org/abs/2003.00482", "summary": "In this work, we address the task of semi-supervised video object\nsegmentation(VOS) and explore how to make efficient use of video property to\ntackle the challenge of semi-supervision. We propose a novel pipeline called\nState-Aware Tracker(SAT), which can produce accurate segmentation results with\nreal-time speed. For higher efficiency, SAT takes advantage of the inter-frame\nconsistency and deals with each target object as a tracklet. For more stable\nand robust performance over video sequences, SAT gets awareness for each state\nand makes self-adaptation via two feedback loops. One loop assists SAT in\ngenerating more stable tracklets. The other loop helps to construct a more\nrobust and holistic target representation. SAT achieves a promising result of\n72.3% J&F mean with 39 FPS on DAVIS2017-Val dataset, which shows a decent\ntrade-off between efficiency and accuracy. Code will be released at\ngithub.com/MegviiDetection/video_analyst."}, {"title": "Iteratively-Refined Interactive 3D Medical Image Segmentation With Multi-Agent Reinforcement Learning", "authors": "Xuan Liao, Wenhao Li, Qisen Xu, Xiangfeng Wang, Bo Jin, Xiaoyun Zhang, Yanfeng Wang, Ya Zhang", "link": "https://arxiv.org/abs/1911.10334", "summary": "Existing automatic 3D image segmentation methods usually fail to meet the\nclinic use. Many studies have explored an interactive strategy to improve the\nimage segmentation performance by iteratively incorporating user hints.\nHowever, the dynamic process for successive interactions is largely ignored. We\nhere propose to model the dynamic process of iterative interactive image\nsegmentation as a Markov decision process (MDP) and solve it with reinforcement\nlearning (RL). Unfortunately, it is intractable to use single-agent RL for\nvoxel-wise prediction due to the large exploration space. To reduce the\nexploration space to a tractable size, we treat each voxel as an agent with a\nshared voxel-level behavior strategy so that it can be solved with multi-agent\nreinforcement learning. An additional advantage of this multi-agent model is to\ncapture the dependency among voxels for segmentation task. Meanwhile, to enrich\nthe information of previous segmentations, we reserve the prediction\nuncertainty in the state space of MDP and derive an adjustment action space\nleading to a more precise and finer segmentation. In addition, to improve the\nefficiency of exploration, we design a relative cross-entropy gain-based reward\nto update the policy in a constrained direction. Experimental results on\nvarious medical datasets have shown that our method significantly outperforms\nexisting state-of-the-art methods, with the advantage of fewer interactions and\na faster convergence."}, {"title": "ENSEI: Efficient Secure Inference via Frequency-Domain Homomorphic Convolution for Privacy-Preserving Visual Recognition", "authors": "Song Bian, Tianchen Wang, Masayuki Hiromoto, Yiyu Shi, Takashi Sato", "link": "https://arxiv.org/abs/2003.05328", "summary": "In this work, we propose ENSEI, a secure inference (SI) framework based on\nthe frequency-domain secure convolution (FDSC) protocol for the efficient\nexecution of privacy-preserving visual recognition. Our observation is that,\nunder the combination of homomorphic encryption and secret sharing, homomorphic\nconvolution can be obliviously carried out in the frequency domain,\nsignificantly simplifying the related computations. We provide protocol designs\nand parameter derivations for number-theoretic transform (NTT) based FDSC. In\nthe experiment, we thoroughly study the accuracy-efficiency trade-offs between\ntime- and frequency-domain homomorphic convolution. With ENSEI, compared to the\nbest known works, we achieve 5--11x online time reduction, up to 33x setup time\nreduction, and up to 10x reduction in the overall inference time. A further 33%\nof bandwidth reductions can be obtained on binary neural networks with only 1%\nof accuracy degradation on the CIFAR-10 dataset."}, {"title": "Multi-Scale Interactive Network for Salient Object Detection", "authors": "Youwei Pang, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu"}, {"title": "Interactive Multi-Label CNN Learning With Partial Labels", "authors": "Dat Huynh, Ehsan Elhamifar"}, {"title": "ViewAL: Active Learning With Viewpoint Entropy for Semantic Segmentation", "authors": "Yawar Siddiqui, Julien Valentin, Matthias Nie\u00dfner", "link": "https://arxiv.org/abs/1911.11789", "summary": "We propose ViewAL, a novel active learning strategy for semantic segmentation\nthat exploits viewpoint consistency in multi-view datasets. Our core idea is\nthat inconsistencies in model predictions across viewpoints provide a very\nreliable measure of uncertainty and encourage the model to perform well\nirrespective of the viewpoint under which objects are observed. To incorporate\nthis uncertainty measure, we introduce a new viewpoint entropy formulation,\nwhich is the basis of our active learning strategy. In addition, we propose\nuncertainty computations on a superpixel level, which exploits inherently\nlocalized signal in the segmentation task, directly lowering the annotation\ncosts. This combination of viewpoint entropy and the use of superpixels allows\nto efficiently select samples that are highly informative for improving the\nnetwork. We demonstrate that our proposed active learning strategy not only\nyields the best-performing models for the same amount of required labeled data,\nbut also significantly reduces labeling effort. For instance, our method\nachieves 95% of maximum achievable network performance using only 7%, 17%, and\n24% labeled data on SceneNet-RGBD, ScanNet, and Matterport3D, respectively. On\nthese datasets, the best state-of-the-art method achieves the same performance\nwith 14%, 27% and 33% labeled data. Finally, we demonstrate that labeling using\nsuperpixels yields the same quality of ground-truth compared to labeling whole\nimages, but requires 25% less time."}, {"title": "Scene-Adaptive Video Frame Interpolation via Meta-Learning", "authors": "Myungsub Choi, Janghoon Choi, Sungyong Baik, Tae Hyun Kim, Kyoung Mu Lee", "link": "https://arxiv.org/abs/2004.00779", "summary": "Video frame interpolation is a challenging problem because there are\ndifferent scenarios for each video depending on the variety of foreground and\nbackground motion, frame rate, and occlusion. It is therefore difficult for a\nsingle network with fixed parameters to generalize across different videos.\nIdeally, one could have a different network for each scenario, but this is\ncomputationally infeasible for practical applications. In this work, we propose\nto adapt the model to each video by making use of additional information that\nis readily available at test time and yet has not been exploited in previous\nworks. We first show the benefits of `test-time adaptation' through simple\nfine-tuning of a network, then we greatly improve its efficiency by\nincorporating meta-learning. We obtain significant performance gains with only\na single gradient update without any additional parameters. Finally, we show\nthat our meta-learning framework can be easily employed to any video frame\ninterpolation network and can consistently improve its performance on multiple\nbenchmark datasets."}, {"title": "Action Segmentation With Joint Self-Supervised Temporal Domain Adaptation", "authors": "Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib, Zsolt Kira", "link": "https://arxiv.org/abs/2003.02824", "summary": "Despite the recent progress of fully-supervised action segmentation\ntechniques, the performance is still not fully satisfactory. One main challenge\nis the problem of spatiotemporal variations (e.g. different people may perform\nthe same activity in various ways). Therefore, we exploit unlabeled videos to\naddress this problem by reformulating the action segmentation task as a\ncross-domain problem with domain discrepancy caused by spatio-temporal\nvariations. To reduce the discrepancy, we propose Self-Supervised Temporal\nDomain Adaptation (SSTDA), which contains two self-supervised auxiliary tasks\n(binary and sequential domain prediction) to jointly align cross-domain feature\nspaces embedded with local and global temporal dynamics, achieving better\nperformance than other Domain Adaptation (DA) approaches. On three challenging\nbenchmark datasets (GTEA, 50Salads, and Breakfast), SSTDA outperforms the\ncurrent state-of-the-art method by large margins (e.g. for the F1@25 score,\nfrom 59.6% to 69.1% on Breakfast, from 73.4% to 81.5% on 50Salads, and from\n83.6% to 89.1% on GTEA), and requires only 65% of the labeled training data for\ncomparable performance, demonstrating the usefulness of adapting to unlabeled\ntarget videos across variations. The source code is available at\nhttps://github.com/cmhungsteve/SSTDA."}, {"title": "Pixel Consensus Voting for Panoptic Segmentation", "authors": "Haochen Wang, Ruotian Luo, Michael Maire, Greg Shakhnarovich", "link": "https://arxiv.org/abs/2004.01849", "summary": "The core of our approach, Pixel Consensus Voting, is a framework for instance\nsegmentation based on the Generalized Hough transform. Pixels cast discretized,\nprobabilistic votes for the likely regions that contain instance centroids. At\nthe detected peaks that emerge in the voting heatmap, backprojection is applied\nto collect pixels and produce instance masks. Unlike a sliding window detector\nthat densely enumerates object proposals, our method detects instances as a\nresult of the consensus among pixel-wise votes. We implement vote aggregation\nand backprojection using native operators of a convolutional neural network.\nThe discretization of centroid voting reduces the training of instance\nsegmentation to pixel labeling, analogous and complementary to FCN-style\nsemantic segmentation, leading to an efficient and unified architecture that\njointly models things and stuff. We demonstrate the effectiveness of our\npipeline on COCO and Cityscapes Panoptic Segmentation and obtain competitive\nresults. Code will be open-sourced."}, {"title": "Minimizing Discrete Total Curvature for Image Processing", "authors": "Qiuxiang Zhong, Yutong Li, Yijie Yang, Yuping Duan"}, {"title": "Towards Robust Image Classification Using Sequential Attention Models", "authors": "Daniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, Alex Mott, Pushmeet Kohli", "link": "https://arxiv.org/abs/1912.02184", "summary": "In this paper we propose to augment a modern neural-network architecture with\nan attention model inspired by human perception. Specifically, we adversarially\ntrain and analyze a neural model incorporating a human inspired, visual\nattention component that is guided by a recurrent top-down sequential process.\nOur experimental evaluation uncovers several notable findings about the\nrobustness and behavior of this new model. First, introducing attention to the\nmodel significantly improves adversarial robustness resulting in\nstate-of-the-art ImageNet accuracies under a wide range of random targeted\nattack strengths. Second, we show that by varying the number of attention steps\n(glances/fixations) for which the model is unrolled, we are able to make its\ndefense capabilities stronger, even in light of stronger attacks --- resulting\nin a \"computational race\" between the attacker and the defender. Finally, we\nshow that some of the adversarial examples generated by attacking our model are\nquite different from conventional adversarial examples --- they contain global,\nsalient and spatially coherent structures coming from the target class that\nwould be recognizable even to a human, and work by distracting the attention of\nthe model away from the main object in the original image."}, {"title": "Discovering Synchronized Subsets of Sequences: A Large Scale Solution", "authors": "Evangelos Sariyanidi, Casey J. Zampella, Keith G. Bartley, John D. Herrington, Theodore D. Satterthwaite, Robert T. Schultz, Birkan Tunc"}, {"title": "Going Deeper With Lean Point Networks", "authors": "Eric-Tuan Le, Iasonas Kokkinos, Niloy J. Mitra"}, {"title": "Efficient and Robust Shape Correspondence via Sparsity-Enforced Quadratic Assignment", "authors": "Rui Xiang, Rongjie Lai, Hongkai Zhao", "link": "https://arxiv.org/abs/2003.08680", "summary": "In this work, we introduce a novel local pairwise descriptor and then develop\na simple, effective iterative method to solve the resulting quadratic\nassignment through sparsity control for shape correspondence between two\napproximate isometric surfaces. Our pairwise descriptor is based on the\nstiffness and mass matrix of finite element approximation of the\nLaplace-Beltrami differential operator, which is local in space, sparse to\nrepresent, and extremely easy to compute while containing global information.\nIt allows us to deal with open surfaces, partial matching, and topological\nperturbations robustly. To solve the resulting quadratic assignment problem\nefficiently, the two key ideas of our iterative algorithm are: 1) select pairs\nwith good (approximate) correspondence as anchor points, 2) solve a regularized\nquadratic assignment problem only in the neighborhood of selected anchor points\nthrough sparsity control. These two ingredients can improve and increase the\nnumber of anchor points quickly while reducing the computation cost in each\nquadratic assignment iteration significantly. With enough high-quality anchor\npoints, one may use various pointwise global features with reference to these\nanchor points to further improve the dense shape correspondence. We use various\nexperiments to show the efficiency, quality, and versatility of our method on\nlarge data sets, patches, and point clouds (without global meshes)."}, {"title": "Explainable Object-Induced Action Decision for Autonomous Vehicles", "authors": "Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu, Yunsheng Li, Nuno Vasconcelos", "link": "http://arxiv.org/abs/2003.09405", "summary": "A new paradigm is proposed for autonomous driving. The new paradigm lies\nbetween the end-to-end and pipelined approaches, and is inspired by how humans\nsolve the problem. While it relies on scene understanding, the latter only\nconsiders objects that could originate hazard. These are denoted as\naction-inducing, since changes in their state should trigger vehicle actions.\nThey also define a set of explanations for these actions, which should be\nproduced jointly with the latter. An extension of the BDD100K dataset,\nannotated for a set of 4 actions and 21 explanations, is proposed. A new\nmulti-task formulation of the problem, which optimizes the accuracy of both\naction commands and explanations, is then introduced. A CNN architecture is\nfinally proposed to solve this problem, by combining reasoning about action\ninducing objects and global scene context. Experimental results show that the\nrequirement of explanations improves the recognition of action-inducing\nobjects, which in turn leads to better action predictions."}, {"title": "Spatially Attentive Output Layer for Image Classification", "authors": "Ildoo Kim, Woonhyuk Baek, Sungwoong Kim", "link": "https://arxiv.org/abs/2004.07570", "summary": "Most convolutional neural networks (CNNs) for image classification use a\nglobal average pooling (GAP) followed by a fully-connected (FC) layer for\noutput logits. However, this spatial aggregation procedure inherently restricts\nthe utilization of location-specific information at the output layer, although\nthis spatial information can be beneficial for classification. In this paper,\nwe propose a novel spatial output layer on top of the existing convolutional\nfeature maps to explicitly exploit the location-specific output information. In\nspecific, given the spatial feature maps, we replace the previous GAP-FC layer\nwith a spatially attentive output layer (SAOL) by employing a attention mask on\nspatial logits. The proposed location-specific attention selectively aggregates\nspatial logits within a target region, which leads to not only the performance\nimprovement but also spatially interpretable outputs. Moreover, the proposed\nSAOL also permits to fully exploit location-specific self-supervision as well\nas self-distillation to enhance the generalization ability during training. The\nproposed SAOL with self-supervision and self-distillation can be easily plugged\ninto existing CNNs. Experimental results on various classification tasks with\nrepresentative architectures show consistent performance improvements by SAOL\nat almost the same computational cost."}, {"title": "Attack to Explain Deep Representation", "authors": "Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal Mian", "link": "", "summary": ""}, {"title": "Computing Valid P-Values for Image Segmentation by Selective Inference", "authors": "Kosuke Tanizaki, Noriaki Hashimoto, Yu Inatsu, Hidekata Hontani, Ichiro Takeuchi", "link": "https://arxiv.org/abs/1906.00629", "summary": "Image segmentation is one of the most fundamental tasks of computer vision.\nIn many practical applications, it is essential to properly evaluate the\nreliability of individual segmentation results. In this study, we propose a\nnovel framework to provide the statistical significance of segmentation results\nin the form of p-values. Specifically, we consider a statistical hypothesis\ntest for determining the difference between the object and the background\nregions. This problem is challenging because the difference can be deceptively\nlarge (called segmentation bias) due to the adaptation of the segmentation\nalgorithm to the data. To overcome this difficulty, we introduce a statistical\napproach called selective inference, and develop a framework to compute valid\np-values in which the segmentation bias is properly accounted for. Although the\nproposed framework is potentially applicable to various segmentation\nalgorithms, we focus in this paper on graph cut-based and threshold-based\nsegmentation algorithms, and develop two specific methods to compute valid\np-values for the segmentation results obtained by these algorithms. We prove\nthe theoretical validity of these two methods and demonstrate their\npracticality by applying them to segmentation problems for medical images."}, {"title": "Unsupervised Learning From Video With Deep Neural Embeddings", "authors": "Chengxu Zhuang, Tianwei She, Alex Andonian, Max Sobol Mark, Daniel Yamins", "link": "https://arxiv.org/abs/1905.11954", "summary": "Because of the rich dynamical structure of videos and their ubiquity in\neveryday life, it is a natural idea that video data could serve as a powerful\nunsupervised learning signal for training visual representations in deep neural\nnetworks. However, instantiating this idea, especially at large scale, has\nremained a significant artificial intelligence challenge. Here we present the\nVideo Instance Embedding (VIE) framework, which extends powerful recent\nunsupervised loss functions for learning deep nonlinear embeddings to\nmulti-stream temporal processing architectures on large-scale video datasets.\nWe show that VIE-trained networks substantially advance the state of the art in\nunsupervised learning from video datastreams, both for action recognition in\nthe Kinetics dataset, and object recognition in the ImageNet dataset. We show\nthat a hybrid model with both static and dynamic processing pathways is optimal\nfor both transfer tasks, and provide analyses indicating how the pathways\ndiffer. Taken in context, our results suggest that deep neural embeddings are a\npromising approach to unsupervised visual learning across a wide variety of\ndomains."}, {"title": "Partial Weight Adaptation for Robust DNN Inference", "authors": "Xiufeng Xie, Kyu-Han Kim", "link": "https://arxiv.org/abs/2003.06131", "summary": "Mainstream video analytics uses a pre-trained DNN model with an assumption\nthat inference input and training data follow the same probability\ndistribution. However, this assumption does not always hold in the wild:\nautonomous vehicles may capture video with varying brightness; unstable\nwireless bandwidth calls for adaptive bitrate streaming of video; and,\ninference servers may serve inputs from heterogeneous IoT devices/cameras. In\nsuch situations, the level of input distortion changes rapidly, thus reshaping\nthe probability distribution of the input.\n  We present GearNN, an adaptive inference architecture that accommodates\nheterogeneous DNN inputs. GearNN employs an optimization algorithm to identify\na small set of \"distortion-sensitive\" DNN parameters, given a memory budget.\nBased on the distortion level of the input, GearNN then adapts only the\ndistortion-sensitive parameters, while reusing the rest of constant parameters\nacross all input qualities. In our evaluation of DNN inference with dynamic\ninput distortions, GearNN improves the accuracy (mIoU) by an average of 18.12%\nover a DNN trained with the undistorted dataset and 4.84% over stability\ntraining from Google, with only 1.8% extra memory overhead."}, {"title": "Probability Weighted Compact Feature for Domain Adaptive Retrieval", "authors": "Fuxiang Huang, Lei Zhang, Yang Yang, Xichuan Zhou", "link": "https://arxiv.org/abs/2003.03293", "summary": "Domain adaptive image retrieval includes single-domain retrieval and\ncross-domain retrieval. Most of the existing image retrieval methods only focus\non single-domain retrieval, which assumes that the distributions of retrieval\ndatabases and queries are similar. However, in practical application, the\ndiscrepancies between retrieval databases often taken in ideal\nillumination/pose/background/camera conditions and queries usually obtained in\nuncontrolled conditions are very large. In this paper, considering the\npractical application, we focus on challenging cross-domain retrieval. To\naddress the problem, we propose an effective method named Probability Weighted\nCompact Feature Learning (PWCF), which provides inter-domain correlation\nguidance to promote cross-domain retrieval accuracy and learns a series of\ncompact binary codes to improve the retrieval speed. First, we derive our loss\nfunction through the Maximum A Posteriori Estimation (MAP): Bayesian\nPerspective (BP) induced focal-triplet loss, BP induced quantization loss and\nBP induced classification loss. Second, we propose a common manifold structure\nbetween domains to explore the potential correlation across domains.\nConsidering the original feature representation is biased due to the\ninter-domain discrepancy, the manifold structure is difficult to be\nconstructed. Therefore, we propose a new feature named Histogram Feature of\nNeighbors (HFON) from the sample statistics perspective. Extensive experiments\non various benchmark databases validate that our method outperforms many\nstate-of-the-art image retrieval methods for domain adaptive image retrieval.\nThe source code is available at https://github.com/fuxianghuang1/PWCF"}, {"title": "Where Does It End? \u2013 Reasoning About Hidden Surfaces by Object Intersection Constraints", "authors": "Michael Strecke, J\u00f6rg St\u00fcckler", "link": "http://arxiv.org/abs/2004.04630", "summary": "Dynamic scene understanding is an essential capability in robotics and VR/AR.\nIn this paper we propose Co-Section, an optimization-based approach to 3D\ndynamic scene reconstruction, which infers hidden shape information from\nintersection constraints. An object-level dynamic SLAM frontend detects,\nsegments, tracks and maps dynamic objects in the scene. Our optimization\nbackend completes the shapes using hull and intersection constraints between\nthe objects. In experiments, we demonstrate our approach on real and synthetic\ndynamic scene datasets. We also assess the shape completion performance of our\nmethod quantitatively. To the best of our knowledge, our approach is the first\nmethod to incorporate such physical plausibility constraints on object\nintersections for shape completion of dynamic objects in an energy minimization\nframework."}, {"title": "PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation", "authors": "Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Zerong Xi, Boqing Gong, Hassan Foroosh", "link": "https://arxiv.org/abs/2003.14032", "summary": "The need for fine-grained perception in autonomous driving systems has\nresulted in recently increased research on online semantic segmentation of\nsingle-scan LiDAR. Despite the emerging datasets and technological\nadvancements, it remains challenging due to three reasons: (1) the need for\nnear-real-time latency with limited hardware; (2) uneven or even long-tailed\ndistribution of LiDAR points across space; and (3) an increasing number of\nextremely fine-grained semantic classes. In an attempt to jointly tackle all\nthe aforementioned challenges, we propose a new LiDAR-specific,\nnearest-neighbor-free segmentation algorithm - PolarNet. Instead of using\ncommon spherical or bird's-eye-view projection, our polar bird's-eye-view\nrepresentation balances the points across grid cells in a polar coordinate\nsystem, indirectly aligning a segmentation network's attention with the\nlong-tailed distribution of the points along the radial axis. We find that our\nencoding scheme greatly increases the mIoU in three drastically different\nsegmentation datasets of real urban LiDAR single scans while retaining near\nreal-time throughput."}, {"title": "Pathological Retinal Region Segmentation From OCT Images Using Geometric Relation Based Augmentation", "authors": "Dwarikanath Mahapatra, Behzad Bozorgtabar, Ling Shao", "link": "http://arxiv.org/abs/2003.14119", "summary": "Medical image segmentation is an important task for computer aided diagnosis.\nPixelwise manual annotations of large datasets require high expertise and is\ntime consuming. Conventional data augmentations have limited benefit by not\nfully representing the underlying distribution of the training set, thus\naffecting model robustness when tested on images captured from different\nsources. Prior work leverages synthetic images for data augmentation ignoring\nthe interleaved geometric relationship between different anatomical labels. We\npropose improvements over previous GAN-based medical image synthesis methods by\njointly encoding the intrinsic relationship of geometry and shape. Latent space\nvariable sampling results in diverse generated images from a base image and\nimproves robustness. Given those augmented images generated by our method, we\ntrain the segmentation network to enhance the segmentation performance of\nretinal optical coherence tomography (OCT) images. The proposed method\noutperforms state-of-the-art segmentation methods on the public RETOUCH dataset\nhaving images captured from different acquisition procedures. Ablation studies\nand visual analysis also demonstrate benefits of integrating geometry and\ndiversity."}, {"title": "Transferring and Regularizing Prediction for Semantic Segmentation", "authors": "Yiheng Zhang, Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Dong Liu, Tao Mei"}, {"title": "PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition", "authors": "Kun Su, Xiulong Liu, Eli Shlizerman", "link": "", "summary": ""}, {"title": "Model Adaptation: Unsupervised Domain Adaptation Without Source Data", "authors": "Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, Si Wu"}, {"title": "Evade Deep Image Retrieval by Stashing Private Images in the Hash Space", "authors": "Yanru Xiao, Cong Wang, Xing Gao"}, {"title": "Advisable Learning for Self-Driving Vehicles by Internalizing Observation-to-Action Rules", "authors": "Jinkyu Kim, Suhong Moon, Anna Rohrbach, Trevor Darrell, John Canny"}, {"title": "ProAlignNet: Unsupervised Learning for Progressively Aligning Noisy Contours", "authors": "VSR Veeravasarapu, Abhishek Goel, Deepak Mittal, Maneesh Singh", "link": "https://arxiv.org/abs/2005.11546", "summary": "Contour shape alignment is a fundamental but challenging problem in computer\nvision, especially when the observations are partial, noisy, and largely\nmisaligned. Recent ConvNet-based architectures that were proposed to align\nimage structures tend to fail with contour representation of shapes, mostly due\nto the use of proximity-insensitive pixel-wise similarity measures as loss\nfunctions in their training processes. This work presents a novel ConvNet,\n\"ProAlignNet\" that accounts for large scale misalignments and complex\ntransformations between the contour shapes. It infers the warp parameters in a\nmulti-scale fashion with progressively increasing complex transformations over\nincreasing scales. It learns --without supervision-- to align contours,\nagnostic to noise and missing parts, by training with a novel loss function\nwhich is derived an upperbound of a proximity-sensitive and local\nshape-dependent similarity metric that uses classical Morphological Chamfer\nDistance Transform. We evaluate the reliability of these proposals on a\nsimulated MNIST noisy contours dataset via some basic sanity check experiments.\nNext, we demonstrate the effectiveness of the proposed models in two real-world\napplications of (i) aligning geo-parcel data to aerial image maps and (ii)\nrefining coarsely annotated segmentation labels. In both applications, the\nproposed models consistently perform superior to state-of-the-art methods."}, {"title": "Attribution in Scale and Space", "authors": "Shawn Xu, Subhashini Venugopalan, Mukund Sundararajan", "link": "https://arxiv.org/abs/2004.03383", "summary": "We study the attribution problem [28] for deep networks applied to perception\ntasks. For vision tasks, attribution techniques attribute the prediction of a\nnetwork to the pixels of the input image. We propose a new technique called\n\\emph{Blur Integrated Gradients}. This technique has several advantages over\nother methods. First, it can tell at what scale a network recognizes an object.\nIt produces scores in the scale/frequency dimension, that we find captures\ninteresting phenomena. Second, it satisfies the scale-space axioms [14], which\nimply that it employs perturbations that are free of artifact. We therefore\nproduce explanations that are cleaner and consistent with the operation of deep\nnetworks. Third, it eliminates the need for a 'baseline' parameter for\nIntegrated Gradients [31] for perception tasks. This is desirable because the\nchoice of baseline has a significant effect on the explanations. We compare the\nproposed technique against previous techniques and demonstrate application on\nthree tasks: ImageNet object recognition, Diabetic Retinopathy prediction, and\nAudioSet audio event identification."}, {"title": "Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing", "authors": "Vedika Agarwal, Rakshith Shetty, Mario Fritz", "link": "https://arxiv.org/abs/1912.07538", "summary": "Despite significant success in Visual Question Answering (VQA), VQA models\nhave been shown to be notoriously brittle to linguistic variations in the\nquestions. Due to deficiencies in models and datasets, today's models often\nrely on correlations rather than predictions that are causal w.r.t. data. In\nthis paper, we propose a novel way to analyze and measure the robustness of the\nstate of the art models w.r.t semantic visual variations as well as propose\nways to make models more robust against spurious correlations. Our method\nperforms automated semantic image manipulations and tests for consistency in\nmodel predictions to quantify the model robustness as well as generate\nsynthetic data to counter these problems. We perform our analysis on three\ndiverse, state of the art VQA models and diverse question types with a\nparticular focus on challenging counting questions. In addition, we show that\nmodels can be made significantly more robust against inconsistent predictions\nusing our edited data. Finally, we show that results also translate to\nreal-world error cases of state of the art models, which results in improved\noverall performance."}, {"title": "Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection", "authors": "Shi-Xue Zhang, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa Wang, Xu-Cheng Yin", "link": "https://arxiv.org/abs/2003.07493", "summary": "Arbitrary shape text detection is a challenging task due to the high variety\nand complexity of scenes texts. In this paper, we propose a novel unified\nrelational reasoning graph network for arbitrary shape text detection. In our\nmethod, an innovative local graph bridges a text proposal model via\nConvolutional Neural Network (CNN) and a deep relational reasoning network via\nGraph Convolutional Network (GCN), making our network end-to-end trainable. To\nbe concrete, every text instance will be divided into a series of small\nrectangular components, and the geometry attributes (e.g., height, width, and\norientation) of the small components will be estimated by our text proposal\nmodel. Given the geometry attributes, the local graph construction model can\nroughly establish linkages between different text components. For further\nreasoning and deducing the likelihood of linkages between the component and its\nneighbors, we adopt a graph-based network to perform deep relational reasoning\non local graphs. Experiments on public available datasets demonstrate the\nstate-of-the-art performance of our method."}, {"title": "Large-Scale Object Detection in the Wild From Imbalanced Multi-Labels", "authors": "Junran Peng, Xingyuan Bu, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, Junjie Yan", "link": "https://arxiv.org/abs/2005.08455", "summary": "Training with more data has always been the most stable and effective way of\nimproving performance in deep learning era. As the largest object detection\ndataset so far, Open Images brings great opportunities and challenges for\nobject detection in general and sophisticated scenarios. However, owing to its\nsemi-automatic collecting and labeling pipeline to deal with the huge data\nscale, Open Images dataset suffers from label-related problems that objects may\nexplicitly or implicitly have multiple labels and the label distribution is\nextremely imbalanced. In this work, we quantitatively analyze these label\nproblems and provide a simple but effective solution. We design a concurrent\nsoftmax to handle the multi-label problems in object detection and propose a\nsoft-sampling methods with hybrid training scheduler to deal with the label\nimbalance. Overall, our method yields a dramatic improvement of 3.34 points,\nleading to the best single model with 60.90 mAP on the public object detection\ntest set of Open Images. And our ensembling result achieves 67.17 mAP, which is\n4.29 points higher than the best result of Open Images public test 2018."}, {"title": "BBN: Bilateral-Branch Network With Cumulative Learning for Long-Tailed Visual Recognition", "authors": "Boyan Zhou, Quan Cui, Xiu-Shen Wei, Zhao-Min Chen", "link": "https://arxiv.org/abs/1912.02413", "summary": "Our work focuses on tackling the challenging but natural visual recognition\ntask of long-tailed data distribution (i.e., a few classes occupy most of the\ndata, while most classes have rarely few samples). In the literature, class\nre-balancing strategies (e.g., re-weighting and re-sampling) are the prominent\nand effective methods proposed to alleviate the extreme imbalance for dealing\nwith long-tailed problems. In this paper, we firstly discover that these\nre-balancing methods achieving satisfactory recognition accuracy owe to that\nthey could significantly promote the classifier learning of deep networks.\nHowever, at the same time, they will unexpectedly damage the representative\nability of the learned deep features to some extent. Therefore, we propose a\nunified Bilateral-Branch Network (BBN) to take care of both representation\nlearning and classifier learning simultaneously, where each branch does perform\nits own duty separately. In particular, our BBN model is further equipped with\na novel cumulative learning strategy, which is designed to first learn the\nuniversal patterns and then pay attention to the tail data gradually. Extensive\nexperiments on four benchmark datasets, including the large-scale iNaturalist\nones, justify that the proposed BBN can significantly outperform\nstate-of-the-art methods. Furthermore, validation experiments can demonstrate\nboth our preliminary discovery and effectiveness of tailored designs in BBN for\nlong-tailed problems. Our method won the first place in the iNaturalist 2019\nlarge scale species classification competition, and our code is open-source and\navailable at https://github.com/Megvii-Nanjing/BBN."}, {"title": "Momentum Contrast for Unsupervised Visual Representation Learning", "authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick", "link": "https://arxiv.org/abs/1911.05722", "summary": "We present Momentum Contrast (MoCo) for unsupervised visual representation\nlearning. From a perspective on contrastive learning as dictionary look-up, we\nbuild a dynamic dictionary with a queue and a moving-averaged encoder. This\nenables building a large and consistent dictionary on-the-fly that facilitates\ncontrastive unsupervised learning. MoCo provides competitive results under the\ncommon linear protocol on ImageNet classification. More importantly, the\nrepresentations learned by MoCo transfer well to downstream tasks. MoCo can\noutperform its supervised pre-training counterpart in 7 detection/segmentation\ntasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large\nmargins. This suggests that the gap between unsupervised and supervised\nrepresentation learning has been largely closed in many vision tasks."}, {"title": "Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation", "authors": "Gedas Bertasius, Lorenzo Torresani", "link": "https://arxiv.org/abs/1912.04573", "summary": "We introduce a method for simultaneously classifying, segmenting and tracking\nobject instances in a video sequence. Our method, named MaskProp, adapts the\npopular Mask R-CNN to video by adding a mask propagation branch that propagates\nframe-level object instance masks from each video frame to all the other frames\nin a video clip. This allows our system to predict clip-level instance tracks\nwith respect to the object instances segmented in the middle frame of the clip.\nClip-level instance tracks generated densely for each frame in the sequence are\nfinally aggregated to produce video-level object instance segmentation and\nclassification. Our experiments demonstrate that our clip-level instance\nsegmentation makes our approach robust to motion blur and object occlusions in\nvideo. MaskProp achieves the best reported accuracy on the YouTube-VIS dataset,\noutperforming the ICCV 2019 video instance segmentation challenge winner\ndespite being much simpler and using orders of magnitude less labeled data\n(1.3M vs 1B images and 860K vs 14M bounding boxes). The project page is at:\nhttps://gberta.github.io/maskprop/."}, {"title": "Weakly Supervised Fine-Grained Image Classification via Guassian Mixture Model Oriented Discriminative Learning", "authors": "Zhihui Wang, Shijie Wang, Shuhui Yang, Haojie Li, Jianjun Li, Zezhou Li"}, {"title": "Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection", "authors": "Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, Stan Z. Li", "link": "https://arxiv.org/abs/1912.02424", "summary": "Object detection has been dominated by anchor-based detectors for several\nyears. Recently, anchor-free detectors have become popular due to the proposal\nof FPN and Focal Loss. In this paper, we first point out that the essential\ndifference between anchor-based and anchor-free detection is actually how to\ndefine positive and negative training samples, which leads to the performance\ngap between them. If they adopt the same definition of positive and negative\nsamples during training, there is no obvious difference in the final\nperformance, no matter regressing from a box or a point. This shows that how to\nselect positive and negative training samples is important for current object\ndetectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to\nautomatically select positive and negative samples according to statistical\ncharacteristics of object. It significantly improves the performance of\nanchor-based and anchor-free detectors and bridges the gap between them.\nFinally, we discuss the necessity of tiling multiple anchors per location on\nthe image to detect objects. Extensive experiments conducted on MS COCO support\nour aforementioned analysis and conclusions. With the newly introduced ATSS, we\nimprove state-of-the-art detectors by a large margin to $50.7\\%$ AP without\nintroducing any overhead. The code is available at\nhttps://github.com/sfzhang15/ATSS"}, {"title": "Learning User Representations for Open Vocabulary Image Hashtag Prediction", "authors": "Thibaut Durand"}, {"title": "Sketch Less for More: On-the-Fly Fine-Grained Sketch-Based Image Retrieval", "authors": "Ayan Kumar Bhunia, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Yi-Zhe Song", "link": "https://arxiv.org/abs/2002.10310", "summary": "Fine-grained sketch-based image retrieval (FG-SBIR) addresses the problem of\nretrieving a particular photo instance given a user's query sketch. Its\nwidespread applicability is however hindered by the fact that drawing a sketch\ntakes time, and most people struggle to draw a complete and faithful sketch. In\nthis paper, we reformulate the conventional FG-SBIR framework to tackle these\nchallenges, with the ultimate goal of retrieving the target photo with the\nleast number of strokes possible. We further propose an on-the-fly design that\nstarts retrieving as soon as the user starts drawing. To accomplish this, we\ndevise a reinforcement learning-based cross-modal retrieval framework that\ndirectly optimizes rank of the ground-truth photo over a complete sketch\ndrawing episode. Additionally, we introduce a novel reward scheme that\ncircumvents the problems related to irrelevant sketch strokes, and thus\nprovides us with a more consistent rank list during the retrieval. We achieve\nsuperior early-retrieval efficiency over state-of-the-art methods and\nalternative baselines on two publicly available fine-grained sketch retrieval\ndatasets."}, {"title": "Few-Shot Pill Recognition", "authors": "Suiyi Ling, Andr\u00e9as Pastor, Jing Li, Zhaohui Che, Junle Wang, Jieun Kim, Patrick Le Callet"}, {"title": "PointRend: Image Segmentation As Rendering", "authors": "Alexander Kirillov, Yuxin Wu, Kaiming He, Ross Girshick", "link": "https://arxiv.org/abs/1912.08193", "summary": "We present a new method for efficient high-quality image segmentation of\nobjects and scenes. By analogizing classical computer graphics methods for\nefficient rendering with over- and undersampling challenges faced in pixel\nlabeling tasks, we develop a unique perspective of image segmentation as a\nrendering problem. From this vantage, we present the PointRend (Point-based\nRendering) neural network module: a module that performs point-based\nsegmentation predictions at adaptively selected locations based on an iterative\nsubdivision algorithm. PointRend can be flexibly applied to both instance and\nsemantic segmentation tasks by building on top of existing state-of-the-art\nmodels. While many concrete implementations of the general idea are possible,\nwe show that a simple design already achieves excellent results. Qualitatively,\nPointRend outputs crisp object boundaries in regions that are over-smoothed by\nprevious methods. Quantitatively, PointRend yields significant gains on COCO\nand Cityscapes, for both instance and semantic segmentation. PointRend's\nefficiency enables output resolutions that are otherwise impractical in terms\nof memory or computation compared to existing approaches. Code has been made\navailable at\nhttps://github.com/facebookresearch/detectron2/tree/master/projects/PointRend."}, {"title": "ABCNet: Real-Time Scene Text Spotting With Adaptive Bezier-Curve Network", "authors": "Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, Liangwei Wang", "link": "https://arxiv.org/abs/2002.10200", "summary": "Scene text detection and recognition has received increasing research\nattention. Existing methods can be roughly categorized into two groups:\ncharacter-based and segmentation-based. These methods either are costly for\ncharacter annotation or need to maintain a complex pipeline, which is often not\nsuitable for real-time applications. Here we address the problem by proposing\nthe Adaptive Bezier-Curve Network (ABCNet). Our contributions are three-fold:\n1) For the first time, we adaptively fit arbitrarily-shaped text by a\nparameterized Bezier curve. 2) We design a novel BezierAlign layer for\nextracting accurate convolution features of a text instance with arbitrary\nshapes, significantly improving the precision compared with previous methods.\n3) Compared with standard bounding box detection, our Bezier curve detection\nintroduces negligible computation overhead, resulting in superiority of our\nmethod in both efficiency and accuracy. Experiments on arbitrarily-shaped\nbenchmark datasets, namely Total-Text and CTW1500, demonstrate that ABCNet\nachieves state-of-the-art accuracy, meanwhile significantly improving the\nspeed. In particular, on Total-Text, our realtime version is over 10 times\nfaster than recent state-of-the-art methods with a competitive recognition\naccuracy. Code is available at https://tinyurl.com/AdelaiDet"}, {"title": "Learning Temporal Co-Attention Models for Unsupervised Video Action Localization", "authors": "Guoqiang Gong, Xinghan Wang, Yadong Mu, Qi Tian", "link": "", "summary": ""}, {"title": "Spatiotemporal Fusion in 3D CNNs: A Probabilistic View", "authors": "Yizhou Zhou, Xiaoyan Sun, Chong Luo, Zheng-Jun Zha, Wenjun Zeng", "link": "https://arxiv.org/abs/2004.04981", "summary": "Despite the success in still image recognition, deep neural networks for\nspatiotemporal signal tasks (such as human action recognition in videos) still\nsuffers from low efficacy and inefficiency over the past years. Recently, human\nexperts have put more efforts into analyzing the importance of different\ncomponents in 3D convolutional neural networks (3D CNNs) to design more\npowerful spatiotemporal learning backbones. Among many others, spatiotemporal\nfusion is one of the essentials. It controls how spatial and temporal signals\nare extracted at each layer during inference. Previous attempts usually start\nby ad-hoc designs that empirically combine certain convolutions and then draw\nconclusions based on the performance obtained by training the corresponding\nnetworks. These methods only support network-level analysis on limited number\nof fusion strategies. In this paper, we propose to convert the spatiotemporal\nfusion strategies into a probability space, which allows us to perform\nnetwork-level evaluations of various fusion strategies without having to train\nthem separately. Besides, we can also obtain fine-grained numerical information\nsuch as layer-level preference on spatiotemporal fusion within the probability\nspace. Our approach greatly boosts the efficiency of analyzing spatiotemporal\nfusion. Based on the probability space, we further generate new fusion\nstrategies which achieve the state-of-the-art performance on four well-known\naction recognition datasets."}, {"title": "Uncertainty-Aware Score Distribution Learning for Action Quality Assessment", "authors": "Yansong Tang, Zanlin Ni, Jiahuan Zhou, Danyang Zhang, Jiwen Lu, Ying Wu, Jie Zhou"}, {"title": "Learning Interactions and Relationships Between Movie Characters", "authors": "Anna Kukleva, Makarand Tapaswi, Ivan Laptev", "link": "https://arxiv.org/abs/2003.13158", "summary": "Interactions between people are often governed by their relationships. On the\nflip side, social relationships are built upon several interactions. Two\nstrangers are more likely to greet and introduce themselves while becoming\nfriends over time. We are fascinated by this interplay between interactions and\nrelationships, and believe that it is an important aspect of understanding\nsocial situations. In this work, we propose neural models to learn and jointly\npredict interactions, relationships, and the pair of characters that are\ninvolved. We note that interactions are informed by a mixture of visual and\ndialog cues, and present a multimodal architecture to extract meaningful\ninformation from them. Localizing the pair of interacting characters in video\nis a time-consuming process, instead, we train our model to learn from\nclip-level weak labels. We evaluate our models on the MovieGraphs dataset and\nshow the impact of modalities, use of longer temporal context for predicting\nrelationships, and achieve encouraging performance using weak labels as\ncompared with ground-truth labels. Code is online."}, {"title": "Video Panoptic Segmentation", "authors": "Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon"}, {"title": "Understanding Human Hands in Contact at Internet Scale", "authors": "Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey"}, {"title": "End-to-End Learning of Visual Representations From Uncurated Instructional Videos", "authors": "Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, Andrew Zisserman", "link": "https://arxiv.org/abs/1912.06430", "summary": "Annotating videos is cumbersome, expensive and not scalable. Yet, many strong\nvideo models still rely on manually annotated data. With the recent\nintroduction of the HowTo100M dataset, narrated videos now offer the\npossibility of learning video representations without manual supervision. In\nthis work we propose a new learning approach, MIL-NCE, capable of addressing\nmisalignments inherent to narrated videos. With this approach we are able to\nlearn strong video representations from scratch, without the need for any\nmanual annotation. We evaluate our representations on a wide range of four\ndownstream tasks over eight datasets: action recognition (HMDB-51, UCF-101,\nKinetics-700), text-to-video retrieval (YouCook2, MSR-VTT), action localization\n(YouTube-8M Segments, CrossTask) and action segmentation (COIN). Our method\noutperforms all published self-supervised approaches for these tasks as well as\nseveral fully supervised baselines."}, {"title": "You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions", "authors": "Evonne Ng, Donglai Xiang, Hanbyul Joo, Kristen Grauman", "link": "https://arxiv.org/abs/1904.09882", "summary": "The body pose of a person wearing a camera is of great interest for\napplications in augmented reality, healthcare, and robotics, yet much of the\nperson's body is out of view for a typical wearable camera. We propose a\nlearning-based approach to estimate the camera wearer's 3D body pose from\negocentric video sequences. Our key insight is to leverage interactions with\nanother person---whose body pose we can directly observe---as a signal\ninherently linked to the body pose of the first-person subject. We show that\nsince interactions between individuals often induce a well-ordered series of\nback-and-forth responses, it is possible to learn a temporal model of the\ninterlinked poses even though one party is largely out of view. We demonstrate\nour idea on a variety of domains with dyadic interaction and show the\nsubstantial impact on egocentric body pose estimation, which improves the state\nof the art. Video results are available at\nhttp://vision.cs.utexas.edu/projects/you2me/"}, {"title": "Learning a Weakly-Supervised Video Actor-Action Segmentation Model With a Wise Selection", "authors": "Jie Chen, Zhiheng Li, Jiebo Luo, Chenliang Xu", "link": "https://arxiv.org/abs/2003.13141", "summary": "We address weakly-supervised video actor-action segmentation (VAAS), which\nextends general video object segmentation (VOS) to additionally consider action\nlabels of the actors. The most successful methods on VOS synthesize a pool of\npseudo-annotations (PAs) and then refine them iteratively. However, they face\nchallenges as to how to select from a massive amount of PAs high-quality ones,\nhow to set an appropriate stop condition for weakly-supervised training, and\nhow to initialize PAs pertaining to VAAS. To overcome these challenges, we\npropose a general Weakly-Supervised framework with a Wise Selection of training\nsamples and model evaluation criterion (WS^2). Instead of blindly trusting\nquality-inconsistent PAs, WS^2 employs a learning-based selection to select\neffective PAs and a novel region integrity criterion as a stopping condition\nfor weakly-supervised training. In addition, a 3D-Conv GCAM is devised to adapt\nto the VAAS task. Extensive experiments show that WS^2 achieves\nstate-of-the-art performance on both weakly-supervised VOS and VAAS tasks and\nis on par with the best fully-supervised method on VAAS."}, {"title": "Learning to Measure the Static Friction Coefficient in Cloth Contact", "authors": "Abdullah Haroon Rasheed, Victor Romero, Florence Bertails-Descoubes, Stefanie Wuhrer, Jean-Sebastien Franco, Arnaud Lazarus"}, {"title": "SpeedNet: Learning the Speediness in Videos", "authors": "Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T. Freeman, Michael Rubinstein, Michal Irani, Tali Dekel", "link": "https://arxiv.org/abs/2004.06130", "summary": "We wish to automatically predict the \"speediness\" of moving objects in\nvideos---whether they move faster, at, or slower than their \"natural\" speed.\nThe core component in our approach is SpeedNet---a novel deep network trained\nto detect if a video is playing at normal rate, or if it is sped up. SpeedNet\nis trained on a large corpus of natural videos in a self-supervised manner,\nwithout requiring any manual annotations. We show how this single, binary\nclassification network can be used to detect arbitrary rates of speediness of\nobjects. We demonstrate prediction results by SpeedNet on a wide range of\nvideos containing complex natural motions, and examine the visual cues it\nutilizes for making those predictions. Importantly, we show that through\npredicting the speed of videos, the model learns a powerful and meaningful\nspace-time representation that goes beyond simple motion cues. We demonstrate\nhow those learned features can boost the performance of self-supervised action\nrecognition, and can be used for video retrieval. Furthermore, we also apply\nSpeedNet for generating time-varying, adaptive video speedups, which can allow\nviewers to watch videos faster, but with less of the jittery, unnatural motions\ntypical to videos that are sped up uniformly."}, {"title": "Telling Left From Right: Learning Spatial Correspondence of Sight and Sound", "authors": "Karren Yang, Bryan Russell, Justin Salamon"}, {"title": "Visual-Textual Capsule Routing for Text-Based Video Segmentation", "authors": "Bruce McIntosh, Kevin Duarte, Yogesh S Rawat, Mubarak Shah"}, {"title": "Graph-Structured Referring Expression Reasoning in the Wild", "authors": "Sibei Yang, Guanbin Li, Yizhou Yu", "link": "https://arxiv.org/abs/2004.08814", "summary": "Grounding referring expressions aims to locate in an image an object referred\nto by a natural language expression. The linguistic structure of a referring\nexpression provides a layout of reasoning over the visual contents, and it is\noften crucial to align and jointly understand the image and the referring\nexpression. In this paper, we propose a scene graph guided modular network\n(SGMN), which performs reasoning over a semantic graph and a scene graph with\nneural modules under the guidance of the linguistic structure of the\nexpression. In particular, we model the image as a structured semantic graph,\nand parse the expression into a language scene graph. The language scene graph\nnot only decodes the linguistic structure of the expression, but also has a\nconsistent representation with the image semantic graph. In addition to\nexploring structured solutions to grounding referring expressions, we also\npropose Ref-Reasoning, a large-scale real-world dataset for structured\nreferring expression reasoning. We automatically generate referring expressions\nover the scene graphs of images using diverse expression templates and\nfunctional programs. This dataset is equipped with real-world visual contents\nas well as semantically rich expressions with different reasoning layouts.\nExperimental results show that our SGMN not only significantly outperforms\nexisting state-of-the-art algorithms on the new Ref-Reasoning dataset, but also\nsurpasses state-of-the-art structured methods on commonly used benchmark\ndatasets. It can also provide interpretable visual evidences of reasoning. Data\nand code are available at https://github.com/sibeiyang/sgmn"}, {"title": "Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs", "authors": "Shizhe Chen, Qin Jin, Peng Wang, Qi Wu", "link": "https://arxiv.org/abs/2003.00387", "summary": "Humans are able to describe image contents with coarse to fine details as\nthey wish. However, most image captioning models are intention-agnostic which\ncan not generate diverse descriptions according to different user intentions\ninitiatively. In this work, we propose the Abstract Scene Graph (ASG) structure\nto represent user intention in fine-grained level and control what and how\ndetailed the generated description should be. The ASG is a directed graph\nconsisting of three types of \\textbf{abstract nodes} (object, attribute,\nrelationship) grounded in the image without any concrete semantic labels. Thus\nit is easy to obtain either manually or automatically. From the ASG, we propose\na novel ASG2Caption model, which is able to recognise user intentions and\nsemantics in the graph, and therefore generate desired captions according to\nthe graph structure. Our model achieves better controllability conditioning on\nASGs than carefully designed baselines on both VisualGenome and MSCOCO\ndatasets. It also significantly improves the caption diversity via\nautomatically sampling diverse ASGs as control signals."}, {"title": "Hierarchical Conditional Relation Networks for Video Question Answering", "authors": "Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran", "link": "https://arxiv.org/abs/2002.10698", "summary": "Video question answering (VideoQA) is challenging as it requires modeling\ncapacity to distill dynamic visual artifacts and distant relations and to\nassociate them with linguistic concepts. We introduce a general-purpose\nreusable neural unit called Conditional Relation Network (CRN) that serves as a\nbuilding block to construct more sophisticated structures for representation\nand reasoning over video. CRN takes as input an array of tensorial objects and\na conditioning feature, and computes an array of encoded output objects. Model\nbuilding becomes a simple exercise of replication, rearrangement and stacking\nof these reusable units for diverse modalities and contextual information. This\ndesign thus supports high-order relational and multi-step reasoning. The\nresulting architecture for VideoQA is a CRN hierarchy whose branches represent\nsub-videos or clips, all sharing the same question as the contextual condition.\nOur evaluations on well-known datasets achieved new SoTA results, demonstrating\nthe impact of building a general-purpose reasoning unit on complex domains such\nas VideoQA."}, {"title": "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments", "authors": "Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton van den Hengel", "link": "https://arxiv.org/abs/1904.10151", "summary": "One of the long-term challenges of robotics is to enable robots to interact\nwith humans in the visual world via natural language, as humans are visual\nanimals that communicate through language. Overcoming this challenge requires\nthe ability to perform a wide variety of complex tasks in response to\nmultifarious instructions from humans. In the hope that it might drive progress\ntowards more flexible and powerful human interactions with robots, we propose a\ndataset of varied and complex robot tasks, described in natural language, in\nterms of objects visible in a large set of real images. Given an instruction,\nsuccess requires navigating through a previously-unseen environment to identify\nan object. This represents a practical challenge, but one that closely reflects\none of the core visual problems in robotics. Several state-of-the-art\nvision-and-language navigation, and referring-expression models are tested to\nverify the difficulty of this new task, but none of them show promising results\nbecause there are many fundamental differences between our task and previous\nones. A novel Interactive Navigator-Pointer model is also proposed that\nprovides a strong baseline on the task. The proposed model especially achieves\nthe best performance on the unseen test split, but still leaves substantial\nroom for improvement compared to the human performance."}, {"title": "Iterative Answer Prediction With Pointer-Augmented Multimodal Transformers for TextVQA", "authors": "Ronghang Hu, Amanpreet Singh, Trevor Darrell, Marcus Rohrbach", "link": "https://arxiv.org/abs/1911.06258", "summary": "Many visual scenes contain text that carries crucial information, and it is\nthus essential to understand text in images for downstream reasoning tasks. For\nexample, a deep water label on a warning sign warns people about the danger in\nthe scene. Recent work has explored the TextVQA task that requires reading and\nunderstanding text in images to answer a question. However, existing approaches\nfor TextVQA are mostly based on custom pairwise fusion mechanisms between a\npair of two modalities and are restricted to a single prediction step by\ncasting TextVQA as a classification task. In this work, we propose a novel\nmodel for the TextVQA task based on a multimodal transformer architecture\naccompanied by a rich representation for text in images. Our model naturally\nfuses different modalities homogeneously by embedding them into a common\nsemantic space where self-attention is applied to model inter- and intra-\nmodality context. Furthermore, it enables iterative answer decoding with a\ndynamic pointer network, allowing the model to form an answer through\nmulti-step prediction instead of one-step classification. Our model outperforms\nexisting approaches on three benchmark datasets for the TextVQA task by a large\nmargin."}, {"title": "SQuINTing at VQA Models: Introspecting VQA Models With Sub-Questions", "authors": "Ramprasaath R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz, Marco Tulio Ribeiro, Besmira Nushi, Ece Kamar", "link": "", "summary": ""}, {"title": "Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks", "authors": "Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang", "link": "https://arxiv.org/abs/1911.07883", "summary": "Vision-Language Navigation (VLN) is a task where agents learn to navigate\nfollowing natural language instructions. The key to this task is to perceive\nboth the visual scene and natural language sequentially. Conventional\napproaches exploit the vision and language features in cross-modal grounding.\nHowever, the VLN task remains challenging, since previous works have neglected\nthe rich semantic information contained in the environment (such as implicit\nnavigation graphs or sub-trajectory semantics). In this paper, we introduce\nAuxiliary Reasoning Navigation (AuxRN), a framework with four self-supervised\nauxiliary reasoning tasks to take advantage of the additional training signals\nderived from the semantic information. The auxiliary tasks have four reasoning\nobjectives: explaining the previous actions, estimating the navigation\nprogress, predicting the next orientation, and evaluating the trajectory\nconsistency. As a result, these additional training signals help the agent to\nacquire knowledge of semantic representations in order to reason about its\nactivity and build a thorough perception of the environment. Our experiments\nindicate that auxiliary reasoning tasks improve both the performance of the\nmain task and the model generalizability by a large margin. Empirically, we\ndemonstrate that an agent trained with self-supervised auxiliary reasoning\ntasks substantially outperforms the previous state-of-the-art method, being the\nbest existing approach on the standard benchmark."}, {"title": "Sign Language Transformers: Joint End-to-End Sign Language Recognition and Translation", "authors": "Necati Cihan Camg\u00f6z, Oscar Koller, Simon Hadfield, Richard Bowden", "link": "https://arxiv.org/abs/2003.13830", "summary": "Prior work on Sign Language Translation has shown that having a mid-level\nsign gloss representation (effectively recognizing the individual signs)\nimproves the translation performance drastically. In fact, the current\nstate-of-the-art in translation requires gloss level tokenization in order to\nwork. We introduce a novel transformer based architecture that jointly learns\nContinuous Sign Language Recognition and Translation while being trainable in\nan end-to-end manner. This is achieved by using a Connectionist Temporal\nClassification (CTC) loss to bind the recognition and translation problems into\na single unified architecture. This joint approach does not require any\nground-truth timing information, simultaneously solving two co-dependant\nsequence-to-sequence learning problems and leads to significant performance\ngains.\n  We evaluate the recognition and translation performances of our approaches on\nthe challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report\nstate-of-the-art sign language recognition and translation results achieved by\nour Sign Language Transformers. Our translation networks outperform both sign\nvideo to spoken language and gloss to spoken language translation models, in\nsome cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We\nalso share new baseline translation results using transformer networks for\nseveral other text-to-text sign language translation tasks."}, {"title": "Multi-Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation", "authors": "Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, Rongrong Ji", "link": "https://arxiv.org/abs/2003.08813", "summary": "Referring expression comprehension (REC) and segmentation (RES) are two\nhighly-related tasks, which both aim at identifying the referent according to a\nnatural language expression. In this paper, we propose a novel Multi-task\nCollaborative Network (MCN) to achieve a joint learning of REC and RES for the\nfirst time. In MCN, RES can help REC to achieve better language-vision\nalignment, while REC can help RES to better locate the referent. In addition,\nwe address a key challenge in this multi-task setup, i.e., the prediction\nconflict, with two innovative designs namely, Consistency Energy Maximization\n(CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM\nenables REC and RES to focus on similar visual regions by maximizing the\nconsistency energy between two tasks. ASNLS supresses the response of unrelated\nregions in RES based on the prediction of REC. To validate our model, we\nconduct extensive experiments on three benchmark datasets of REC and RES, i.e.,\nRefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant\nperformance gains of MCN over all existing methods, i.e., up to +7.13% for REC\nand +11.50% for RES over SOTA, which well confirm the validity of our model for\njoint REC and RES learning."}, {"title": "Counterfactual Vision and Language Learning", "authors": "Ehsan Abbasnejad, Damien Teney, Amin Parvaneh, Javen Shi, Anton van den Hengel", "link": "", "summary": ""}, {"title": "Iterative Context-Aware Graph Inference for Visual Dialog", "authors": "Dan Guo, Hui Wang, Hanwang Zhang, Zheng-Jun Zha, Meng Wang", "link": "https://arxiv.org/abs/2004.02194", "summary": "Visual dialog is a challenging task that requires the comprehension of the\nsemantic dependencies among implicit visual and textual contexts. This task can\nrefer to the relation inference in a graphical model with sparse contexts and\nunknown graph structure (relation descriptor), and how to model the underlying\ncontext-aware relation inference is critical. To this end, we propose a novel\nContext-Aware Graph (CAG) neural network. Each node in the graph corresponds to\na joint semantic feature, including both object-based (visual) and\nhistory-related (textual) context representations. The graph structure\n(relations in dialog) is iteratively updated using an adaptive top-$K$ message\npassing mechanism. Specifically, in every message passing step, each node\nselects the most $K$ relevant nodes, and only receives messages from them.\nThen, after the update, we impose graph attention on all the nodes to get the\nfinal graph embedding and infer the answer. In CAG, each node has dynamic\nrelations in the graph (different related $K$ neighbor nodes), and only the\nmost relevant nodes are attributive to the context-aware relational graph\ninference. Experimental results on VisDial v0.9 and v1.0 datasets show that CAG\noutperforms comparative methods. Visualization results further validate the\ninterpretability of our method."}, {"title": "TA-Student VQA: Multi-Agents Training by Self-Questioning", "authors": "Peixi Xiong, Ying Wu", "link": "", "summary": ""}, {"title": "Exploring Self-Attention for Image Recognition", "authors": "Hengshuang Zhao, Jiaya Jia, Vladlen Koltun", "link": "https://arxiv.org/abs/2004.13621", "summary": "Recent work has shown that self-attention can serve as a basic building block\nfor image recognition models. We explore variations of self-attention and\nassess their effectiveness for image recognition. We consider two forms of\nself-attention. One is pairwise self-attention, which generalizes standard\ndot-product attention and is fundamentally a set operator. The other is\npatchwise self-attention, which is strictly more powerful than convolution. Our\npairwise self-attention networks match or outperform their convolutional\ncounterparts, and the patchwise models substantially outperform the\nconvolutional baselines. We also conduct experiments that probe the robustness\nof learned representations and conclude that self-attention networks may have\nsignificant benefits in terms of robustness and generalization."}, {"title": "Cops-Ref: A New Dataset and Task on Compositional Referring Expression Comprehension", "authors": "Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K. Wong, Qi Wu", "link": "https://arxiv.org/abs/2003.00403", "summary": "Referring expression comprehension (REF) aims at identifying a particular\nobject in a scene by a natural language expression. It requires joint reasoning\nover the textual and visual domains to solve the problem. Some popular\nreferring expression datasets, however, fail to provide an ideal test bed for\nevaluating the reasoning ability of the models, mainly because 1) their\nexpressions typically describe only some simple distinctive properties of the\nobject and 2) their images contain limited distracting information. To bridge\nthe gap, we propose a new dataset for visual reasoning in context of referring\nexpression comprehension with two main features. First, we design a novel\nexpression engine rendering various reasoning logics that can be flexibly\ncombined with rich visual properties to generate expressions with varying\ncompositionality. Second, to better exploit the full reasoning chain embodied\nin an expression, we propose a new test setting by adding additional\ndistracting images containing objects sharing similar properties with the\nreferent, thus minimising the success rate of reasoning-free cross-domain\nalignment. We evaluate several state-of-the-art REF models, but find none of\nthem can achieve promising performance. A proposed modular hard mining strategy\nperforms the best but still leaves substantial room for improvement. We hope\nthis new dataset and task can serve as a benchmark for deeper visual reasoning\nanalysis and foster the research on referring expression comprehension."}, {"title": "Improving Convolutional Networks With Self-Calibrated Convolutions", "authors": "Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Changhu Wang, Jiashi Feng"}, {"title": "Modality Shifting Attention Network for Multi-Modal Video Question Answering", "authors": "Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, Chang D. Yoo"}, {"title": "Learning to Structure an Image With Few Colors", "authors": "Yunzhong Hou, Liang Zheng, Stephen Gould", "link": "https://arxiv.org/abs/2003.07848", "summary": "Color and structure are the two pillars that construct an image. Usually, the\nstructure is well expressed through a rich spectrum of colors, allowing objects\nin an image to be recognized by neural networks. However, under extreme\nlimitations of color space, the structure tends to vanish, and thus a neural\nnetwork might fail to understand the image. Interested in exploring this\ninterplay between color and structure, we study the scientific problem of\nidentifying and preserving the most informative image structures while\nconstraining the color space to just a few bits, such that the resulting image\ncan be recognized with possibly high accuracy. To this end, we propose a color\nquantization network, ColorCNN, which learns to structure the images from the\nclassification loss in an end-to-end manner. Given a color space size, ColorCNN\nquantizes colors in the original image by generating a color index map and an\nRGB color palette. Then, this color-quantized image is fed to a pre-trained\ntask network to evaluate its performance. In our experiment, with only a 1-bit\ncolor space (i.e., two colors), the proposed network achieves 82.1% top-1\naccuracy on the CIFAR10 dataset, outperforming traditional color quantization\nmethods by a large margin. For applications, when encoded with PNG, the\nproposed color quantization shows superiority over other image compression\nmethods in the extremely low bit-rate regime. The code is available at:\nhttps://github.com/hou-yz/color_distillation."}, {"title": "On the General Value of Evidence, and Bilingual Scene-Text Visual Question Answering", "authors": "Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, Liangwei Wang", "link": "https://arxiv.org/abs/2002.10215", "summary": "Visual Question Answering (VQA) methods have made incredible progress, but\nsuffer from a failure to generalize. This is visible in the fact that they are\nvulnerable to learning coincidental correlations in the data rather than deeper\nrelations between image content and ideas expressed in language. We present a\ndataset that takes a step towards addressing this problem in that it contains\nquestions expressed in two languages, and an evaluation process that co-opts a\nwell understood image-based metric to reflect the method's ability to reason.\nMeasuring reasoning directly encourages generalization by penalizing answers\nthat are coincidentally correct. The dataset reflects the scene-text version of\nthe VQA problem, and the reasoning evaluation can be seen as a text-based\nversion of a referring expression challenge. Experiments and analysis are\nprovided that show the value of the dataset."}, {"title": "From Paris to Berlin: Discovering Fashion Style Influences Around the World", "authors": "Ziad Al-Halah, Kristen Grauman", "link": "https://arxiv.org/abs/2004.01316", "summary": "The evolution of clothing styles and their migration across the world is\nintriguing, yet difficult to describe quantitatively. We propose to discover\nand quantify fashion influences from everyday images of people wearing clothes.\nWe introduce an approach that detects which cities influence which other cities\nin terms of propagating their styles. We then leverage the discovered influence\npatterns to inform a forecasting model that predicts the popularity of any\ngiven style at any given city into the future. Demonstrating our idea with\nGeoStyle---a large-scale dataset of 7.7M images covering 44 major world cities,\nwe present the discovered influence relationships, revealing how cities exert\nand receive fashion influence for an array of 50 observed visual styles.\nFurthermore, the proposed forecasting model achieves state-of-the-art results\nfor a challenging style forecasting task, showing the advantage of grounding\nvisual style evolution both spatially and temporally."}, {"title": "A Local-to-Global Approach to Multi-Modal Movie Scene Segmentation", "authors": "Anyi Rao, Linning Xu, Yu Xiong, Guodong Xu, Qingqiu Huang, Bolei Zhou, Dahua Lin", "link": "https://arxiv.org/abs/2004.02678", "summary": "Scene, as the crucial unit of storytelling in movies, contains complex\nactivities of actors and their interactions in a physical environment.\nIdentifying the composition of scenes serves as a critical step towards\nsemantic understanding of movies. This is very challenging -- compared to the\nvideos studied in conventional vision problems, e.g. action recognition, as\nscenes in movies usually contain much richer temporal structures and more\ncomplex semantic information. Towards this goal, we scale up the scene\nsegmentation task by building a large-scale video dataset MovieScenes, which\ncontains 21K annotated scene segments from 150 movies. We further propose a\nlocal-to-global scene segmentation framework, which integrates multi-modal\ninformation across three levels, i.e. clip, segment, and movie. This framework\nis able to distill complex semantics from hierarchical temporal structures over\na long movie, providing top-down guidance for scene segmentation. Our\nexperiments show that the proposed network is able to segment a movie into\nscenes with high accuracy, consistently outperforming previous methods. We also\nfound that pretraining on our MovieScenes can bring significant improvements to\nthe existing approaches."}, {"title": "G-TAD: Sub-Graph Localization for Temporal Action Detection", "authors": "Mengmeng Xu, Chen Zhao, David S. Rojas, Ali Thabet, Bernard Ghanem", "link": "https://arxiv.org/abs/1911.11462", "summary": "Temporal action detection is a fundamental yet challenging task in video\nunderstanding. Video context is a critical cue to effectively detect actions,\nbut current works mainly focus on temporal context, while neglecting semantic\ncontext as well as other important context properties. In this work, we propose\na graph convolutional network (GCN) model to adaptively incorporate multi-level\nsemantic context into video features and cast temporal action detection as a\nsub-graph localization problem. Specifically, we formulate video snippets as\ngraph nodes, snippet-snippet correlations as edges, and actions associated with\ncontext as target sub-graphs. With graph convolution as the basic operation, we\ndesign a GCN block called GCNeXt, which learns the features of each node by\naggregating its context and dynamically updates the edges in the graph. To\nlocalize each sub-graph, we also design an SGAlign layer to embed each\nsub-graph into the Euclidean space. Extensive experiments show that G-TAD is\ncapable of finding effective video context without extra supervision and\nachieves state-of-the-art performance on two detection benchmarks. On\nActivityNet-1.3, it obtains an average mAP of 34.09%; on THUMOS14, it reaches\n51.6% at IoU@0.5 when combined with a proposal processing method. G-TAD code is\npublicly available at https://github.com/frostinassiky/gtad."}, {"title": "Detailed 2D-3D Joint Representation for Human-Object Interaction", "authors": "Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu", "link": "https://arxiv.org/abs/2004.08154", "summary": "Human-Object Interaction (HOI) detection lies at the core of action\nunderstanding. Besides 2D information such as human/object appearance and\nlocations, 3D pose is also usually utilized in HOI learning since its\nview-independence. However, rough 3D body joints just carry sparse body\ninformation and are not sufficient to understand complex interactions. Thus, we\nneed detailed 3D body shape to go further. Meanwhile, the interacted object in\n3D is also not fully studied in HOI learning. In light of these, we propose a\ndetailed 2D-3D joint representation learning method. First, we utilize the\nsingle-view human body capture method to obtain detailed 3D body, face and hand\nshapes. Next, we estimate the 3D object location and size with reference to the\n2D human-object spatial configuration and object category priors. Finally, a\njoint learning framework and cross-modal consistency tasks are proposed to\nlearn the joint HOI representation. To better evaluate the 2D ambiguity\nprocessing capacity of models, we propose a new benchmark named Ambiguous-HOI\nconsisting of hard ambiguous images. Extensive experiments in large-scale HOI\nbenchmark and Ambiguous-HOI show impressive effectiveness of our method. Code\nand data are available at https://github.com/DirtyHarryLYL/DJ-RN."}, {"title": "One-Shot Adversarial Attacks on Visual Tracking With Dual Attention", "authors": "Xuesong Chen, Xiyu Yan, Feng Zheng, Yong Jiang, Shu-Tao Xia, Yong Zhao, Rongrong Ji"}, {"title": "Rethinking Classification and Localization for Object Detection", "authors": "Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan Wang, Hongzhi Li, Yun Fu", "link": "https://arxiv.org/abs/1904.06493", "summary": "Two head structures (i.e. fully connected head and convolution head) have\nbeen widely used in R-CNN based detectors for classification and localization\ntasks. However, there is a lack of understanding of how does these two head\nstructures work for these two tasks. To address this issue, we perform a\nthorough analysis and find an interesting fact that the two head structures\nhave opposite preferences towards the two tasks. Specifically, the fully\nconnected head (fc-head) is more suitable for the classification task, while\nthe convolution head (conv-head) is more suitable for the localization task.\nFurthermore, we examine the output feature maps of both heads and find that\nfc-head has more spatial sensitivity than conv-head. Thus, fc-head has more\ncapability to distinguish a complete object from part of an object, but is not\nrobust to regress the whole object. Based upon these findings, we propose a\nDouble-Head method, which has a fully connected head focusing on classification\nand a convolution head for bounding box regression. Without bells and whistles,\nour method gains +3.5 and +2.8 AP on MS COCO dataset from Feature Pyramid\nNetwork (FPN) baselines with ResNet-50 and ResNet-101 backbones, respectively."}, {"title": "Correspondence Networks With Adaptive Neighbourhood Consensus", "authors": "Shuda Li, Kai Han, Theo W. Costain, Henry Howard-Jenkins, Victor Prisacariu", "link": "https://arxiv.org/abs/2003.12059", "summary": "In this paper, we tackle the task of establishing dense visual\ncorrespondences between images containing objects of the same category. This is\na challenging task due to large intra-class variations and a lack of dense\npixel level annotations. We propose a convolutional neural network\narchitecture, called adaptive neighbourhood consensus network (ANC-Net), that\ncan be trained end-to-end with sparse key-point annotations, to handle this\nchallenge. At the core of ANC-Net is our proposed non-isotropic 4D convolution\nkernel, which forms the building block for the adaptive neighbourhood consensus\nmodule for robust matching. We also introduce a simple and efficient\nmulti-scale self-similarity module in ANC-Net to make the learned feature\nrobust to intra-class variations. Furthermore, we propose a novel orthogonal\nloss that can enforce the one-to-one matching constraint. We thoroughly\nevaluate the effectiveness of our method on various benchmarks, where it\nsubstantially outperforms state-of-the-art methods."}, {"title": "Multiple Anchor Learning for Visual Object Detection", "authors": "Wei Ke, Tianliang Zhang, Zeyi Huang, Qixiang Ye, Jianzhuang Liu, Dong Huang", "link": "https://arxiv.org/abs/1912.02252", "summary": "Classification and localization are two pillars of visual object detectors.\nHowever, in CNN-based detectors, these two modules are usually optimized under\na fixed set of candidate (or anchor) bounding boxes. This configuration\nsignificantly limits the possibility to jointly optimize classification and\nlocalization. In this paper, we propose a Multiple Instance Learning (MIL)\napproach that selects anchors and jointly optimizes the two modules of a\nCNN-based object detector. Our approach, referred to as Multiple Anchor\nLearning (MAL), constructs anchor bags and selects the most representative\nanchors from each bag. Such an iterative selection process is potentially\nNP-hard to optimize. To address this issue, we solve MAL by repetitively\ndepressing the confidence of selected anchors by perturbing their corresponding\nfeatures. In an adversarial selection-depression manner, MAL not only pursues\noptimal solutions but also fully leverages multiple anchors/features to learn a\ndetection model. Experiments show that MAL improves the baseline RetinaNet with\nsignificant margins on the commonly used MS-COCO object detection benchmark and\nachieves new state-of-the-art detection performance compared with recent\nmethods."}, {"title": "PhraseCut: Language-Based Image Segmentation in the Wild", "authors": "Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, Subhransu Maji"}, {"title": "Mask Encoding for Single Shot Instance Segmentation", "authors": "Rufeng Zhang, Zhi Tian, Chunhua Shen, Mingyu You, Youliang Yan", "link": "https://arxiv.org/abs/2003.11712", "summary": "To date, instance segmentation is dominated by twostage methods, as pioneered\nby Mask R-CNN. In contrast, one-stage alternatives cannot compete with Mask\nR-CNN in mask AP, mainly due to the difficulty of compactly representing masks,\nmaking the design of one-stage methods very challenging. In this work, we\npropose a simple singleshot instance segmentation framework, termed mask\nencoding based instance segmentation (MEInst). Instead of predicting the\ntwo-dimensional mask directly, MEInst distills it into a compact and\nfixed-dimensional representation vector, which allows the instance segmentation\ntask to be incorporated into one-stage bounding-box detectors and results in a\nsimple yet efficient instance segmentation framework. The proposed one-stage\nMEInst achieves 36.4% in mask AP with single-model (ResNeXt-101-FPN backbone)\nand single-scale testing on the MS-COCO benchmark. We show that the much\nsimpler and flexible one-stage instance segmentation method, can also achieve\ncompetitive performance. This framework can be easily adapted for other\ninstance-level recognition tasks. Code is available at:\nhttps://git.io/AdelaiDet"}, {"title": "Action Genome: Actions As Compositions of Spatio-Temporal Scene Graphs", "authors": "Jingwei Ji, Ranjay Krishna, Li Fei-Fei, Juan Carlos Niebles", "link": "https://arxiv.org/abs/1912.06992", "summary": "Action recognition has typically treated actions and activities as monolithic\nevents that occur in videos. However, there is evidence from Cognitive Science\nand Neuroscience that people actively encode activities into consistent\nhierarchical part structures. However in Computer Vision, few explorations on\nrepresentations encoding event partonomies have been made. Inspired by evidence\nthat the prototypical unit of an event is an action-object interaction, we\nintroduce Action Genome, a representation that decomposes actions into\nspatio-temporal scene graphs. Action Genome captures changes between objects\nand their pairwise relationships while an action occurs. It contains 10K videos\nwith 0.4M objects and 1.7M visual relationships annotated. With Action Genome,\nwe extend an existing action recognition model by incorporating scene graphs as\nspatio-temporal feature banks to achieve better performance on the Charades\ndataset. Next, by decomposing and learning the temporal changes in visual\nrelationships that result in an action, we demonstrate the utility of a\nhierarchical event decomposition by enabling few-shot action recognition,\nachieving 42.7% mAP using as few as 10 examples. Finally, we benchmark existing\nscene graph models on the new task of spatio-temporal scene graph prediction."}, {"title": "Learning Unseen Concepts via Hierarchical Decomposition and Composition", "authors": "Muli Yang, Cheng Deng, Junchi Yan, Xianglong Liu, Dacheng Tao"}, {"title": "Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identification", "authors": "Seokeon Choi, Sumin Lee, Youngeun Kim, Taekyung Kim, Changick Kim", "link": "https://arxiv.org/abs/1912.01230", "summary": "Visible-infrared person re-identification (VI-ReID) is an important task in\nnight-time surveillance applications, since visible cameras are difficult to\ncapture valid appearance information under poor illumination conditions.\nCompared to traditional person re-identification that handles only the\nintra-modality discrepancy, VI-ReID suffers from additional cross-modality\ndiscrepancy caused by different types of imaging systems. To reduce both intra-\nand cross-modality discrepancies, we propose a Hierarchical Cross-Modality\nDisentanglement (Hi-CMD) method, which automatically disentangles\nID-discriminative factors and ID-excluded factors from visible-thermal images.\nWe only use ID-discriminative factors for robust cross-modality matching\nwithout ID-excluded factors such as pose or illumination. To implement our\napproach, we introduce an ID-preserving person image generation network and a\nhierarchical feature learning module. Our generation network learns the\ndisentangled representation by generating a new cross-modality image with\ndifferent poses and illuminations while preserving a person's identity. At the\nsame time, the feature learning module enables our model to explicitly extract\nthe common ID-discriminative characteristic between visible-infrared images.\nExtensive experimental results demonstrate that our method outperforms the\nstate-of-the-art methods on two VI-ReID datasets. The source code is available\nat: https://github.com/bismex/HiCMD."}, {"title": "In Defense of Grid Features for Visual Question Answering", "authors": "Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, Xinlei Chen", "link": "https://arxiv.org/abs/2001.03615", "summary": "Popularized as 'bottom-up' attention, bounding box (or region) based visual\nfeatures have recently surpassed vanilla grid-based convolutional features as\nthe de facto standard for vision and language tasks like visual question\nanswering (VQA). However, it is not clear whether the advantages of regions\n(e.g. better localization) are the key reasons for the success of bottom-up\nattention. In this paper, we revisit grid features for VQA, and find they can\nwork surprisingly well - running more than an order of magnitude faster with\nthe same accuracy (e.g. if pre-trained in a similar fashion). Through extensive\nexperiments, we verify that this observation holds true across different VQA\nmodels (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71),\ndatasets, and generalizes well to other tasks like image captioning. As grid\nfeatures make the model design and training process much simpler, this enables\nus to train them end-to-end and also use a more flexible network design. We\nlearn VQA models end-to-end, from pixels directly to answers, and show that\nstrong performance is achievable without using any region annotations in\npre-training. We hope our findings help further improve the scientific\nunderstanding and the practical application of VQA. Code and features will be\nmade available."}, {"title": "Multi-Mutual Consistency Induced Transfer Subspace Learning for Human Motion Segmentation", "authors": "Tao Zhou, Huazhu Fu, Chen Gong, Jianbing Shen, Ling Shao, Fatih Porikli"}, {"title": "Dense Regression Network for Video Grounding", "authors": "Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, Chuang Gan", "link": "https://arxiv.org/abs/2004.03545", "summary": "We address the problem of video grounding from natural language queries. The\nkey challenge in this task is that one training video might only contain a few\nannotated starting/ending frames that can be used as positive examples for\nmodel training. Most conventional approaches directly train a binary classifier\nusing such imbalance data, thus achieving inferior results. The key idea of\nthis paper is to use the distances between the frame within the ground truth\nand the starting (ending) frame as dense supervisions to improve the video\ngrounding accuracy. Specifically, we design a novel dense regression network\n(DRN) to regress the distances from each frame to the starting (ending) frame\nof the video segment described by the query. We also propose a simple but\neffective IoU regression head module to explicitly consider the localization\nquality of the grounding results (i.e., the IoU between the predicted location\nand the ground truth). Experimental results show that our approach\nsignificantly outperforms state-of-the-arts on three datasets (i.e.,\nCharades-STA, ActivityNet-Captions, and TACoS)."}, {"title": "Neural Architecture Search for Lightweight Non-Local Networks", "authors": "Yingwei Li, Xiaojie Jin, Jieru Mei, Xiaochen Lian, Linjie Yang, Cihang Xie, Qihang Yu, Yuyin Zhou, Song Bai, Alan L. Yuille", "link": "https://arxiv.org/abs/2004.01961", "summary": "Non-Local (NL) blocks have been widely studied in various vision tasks.\nHowever, it has been rarely explored to embed the NL blocks in mobile neural\nnetworks, mainly due to the following challenges: 1) NL blocks generally have\nheavy computation cost which makes it difficult to be applied in applications\nwhere computational resources are limited, and 2) it is an open problem to\ndiscover an optimal configuration to embed NL blocks into mobile neural\nnetworks. We propose AutoNL to overcome the above two obstacles. Firstly, we\npropose a Lightweight Non-Local (LightNL) block by squeezing the transformation\noperations and incorporating compact features. With the novel design choices,\nthe proposed LightNL block is 400x computationally cheaper} than its\nconventional counterpart without sacrificing the performance. Secondly, by\nrelaxing the structure of the LightNL block to be differentiable during\ntraining, we propose an efficient neural architecture search algorithm to learn\nan optimal configuration of LightNL blocks in an end-to-end manner. Notably,\nusing only 32 GPU hours, the searched AutoNL model achieves 77.7% top-1\naccuracy on ImageNet under a typical mobile setting (350M FLOPs), significantly\noutperforming previous mobile models including MobileNetV2 (+5.7%), FBNet\n(+2.8%) and MnasNet (+2.1%). Code and models are available at\nhttps://github.com/LiYingwei/AutoNL."}, {"title": "Learning Saliency Propagation for Semi-Supervised Instance Segmentation", "authors": "Yanzhao Zhou, Xin Wang, Jianbin Jiao, Trevor Darrell, Fisher Yu"}, {"title": "Speech2Action: Cross-Modal Supervision for Action Recognition", "authors": "Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar, Cordelia Schmid, Andrew Zisserman", "link": "http://arxiv.org/abs/2003.13594", "summary": "Is it possible to guess human action from dialogue alone? In this work we\ninvestigate the link between spoken words and actions in movies. We note that\nmovie screenplays describe actions, as well as contain the speech of characters\nand hence can be used to learn this correlation with no additional supervision.\nWe train a BERT-based Speech2Action classifier on over a thousand movie\nscreenplays, to predict action labels from transcribed speech segments. We then\napply this model to the speech segments of a large unlabelled movie corpus\n(188M speech segments from 288K movies). Using the predictions of this model,\nwe obtain weak action labels for over 800K video clips. By training on these\nvideo clips, we demonstrate superior action recognition performance on standard\naction recognition benchmarks, without using a single manually labelled action\nexample."}, {"title": "Normalized and Geometry-Aware Self-Attention Network for Image Captioning", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, Hanqing Lu", "link": "https://arxiv.org/abs/2003.08897", "summary": "Self-attention (SA) network has shown profound value in image captioning. In\nthis paper, we improve SA from two aspects to promote the performance of image\ncaptioning. First, we propose Normalized Self-Attention (NSA), a\nreparameterization of SA that brings the benefits of normalization inside SA.\nWhile normalization is previously only applied outside SA, we introduce a novel\nnormalization method and demonstrate that it is both possible and beneficial to\nperform it on the hidden activations inside SA. Second, to compensate for the\nmajor limit of Transformer that it fails to model the geometry structure of the\ninput objects, we propose a class of Geometry-aware Self-Attention (GSA) that\nextends SA to explicitly and efficiently consider the relative geometry\nrelations between the objects in the image. To construct our image captioning\nmodel, we combine the two modules and apply it to the vanilla self-attention\nnetwork. We extensively evaluate our proposals on MS-COCO image captioning\ndataset and superior results are achieved when comparing to state-of-the-art\napproaches. Further experiments on three challenging tasks, i.e. video\ncaptioning, machine translation, and visual question answering, show the\ngenerality of our methods."}, {"title": "Memory Enhanced Global-Local Aggregation for Video Object Detection", "authors": "Yihong Chen, Yue Cao, Han Hu, Liwei Wang", "link": "https://arxiv.org/abs/2003.12063", "summary": "How do humans recognize an object in a piece of video? Due to the\ndeteriorated quality of single frame, it may be hard for people to identify an\noccluded object in this frame by just utilizing information within one image.\nWe argue that there are two important cues for humans to recognize objects in\nvideos: the global semantic information and the local localization information.\nRecently, plenty of methods adopt the self-attention mechanisms to enhance the\nfeatures in key frame with either global semantic information or local\nlocalization information. In this paper we introduce memory enhanced\nglobal-local aggregation (MEGA) network, which is among the first trials that\ntakes full consideration of both global and local information. Furthermore,\nempowered by a novel and carefully-designed Long Range Memory (LRM) module, our\nproposed MEGA could enable the key frame to get access to much more content\nthan any previous methods. Enhanced by these two sources of information, our\nmethod achieves state-of-the-art performance on ImageNet VID dataset. Code is\navailable at \\url{https://github.com/Scalsol/mega.pytorch}."}, {"title": "Solving Mixed-Modal Jigsaw Puzzle for Fine-Grained Sketch-Based Image Retrieval", "authors": "Kaiyue Pang, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Yi-Zhe Song"}, {"title": "LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of Point Cloud Based Deep Networks", "authors": "Hang Zhou, Dongdong Chen, Jing Liao, Kejiang Chen, Xiaoyi Dong, Kunlin Liu, Weiming Zhang, Gang Hua, Nenghai Yu"}, {"title": "Memory Aggregation Networks for Efficient Interactive Video Object Segmentation", "authors": "Jiaxu Miao, Yunchao Wei, Yi Yang", "link": "https://arxiv.org/abs/2003.13246", "summary": "Interactive video object segmentation (iVOS) aims at efficiently harvesting\nhigh-quality segmentation masks of the target object in a video with user\ninteractions. Most previous state-of-the-arts tackle the iVOS with two\nindependent networks for conducting user interaction and temporal propagation,\nrespectively, leading to inefficiencies during the inference stage. In this\nwork, we propose a unified framework, named Memory Aggregation Networks\n(MA-Net), to address the challenging iVOS in a more efficient way. Our MA-Net\nintegrates the interaction and the propagation operations into a single\nnetwork, which significantly promotes the efficiency of iVOS in the scheme of\nmulti-round interactions. More importantly, we propose a simple yet effective\nmemory aggregation mechanism to record the informative knowledge from the\nprevious interaction rounds, improving the robustness in discovering\nchallenging objects of interest greatly. We conduct extensive experiments on\nthe validation set of DAVIS Challenge 2018 benchmark. In particular, our MA-Net\nachieves the J@60 score of 76.1% without any bells and whistles, outperforming\nthe state-of-the-arts with more than 2.7%."}, {"title": "VQA With No Questions-Answers Training", "authors": "Ben-Zion Vatashsky, Shimon Ullman", "link": "", "summary": ""}, {"title": "Counting Out Time: Class Agnostic Video Repetition Counting in the Wild", "authors": "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, Andrew Zisserman"}, {"title": "SaccadeNet: A Fast and Accurate Object Detector", "authors": "Shiyi Lan, Zhou Ren, Yi Wu, Larry S. Davis, Gang Hua", "link": "https://arxiv.org/abs/2003.12125", "summary": "Object detection is an essential step towards holistic scene understanding.\nMost existing object detection algorithms attend to certain object areas once\nand then predict the object locations. However, neuroscientists have revealed\nthat humans do not look at the scene in fixed steadiness. Instead, human eyes\nmove around, locating informative parts to understand the object location. This\nactive perceiving movement process is called \\textit{saccade}.\n  %In this paper, Inspired by such mechanism, we propose a fast and accurate\nobject detector called \\textit{SaccadeNet}. It contains four main modules, the\n\\cenam, the \\coram, the \\atm, and the \\aggatt, which allows it to attend to\ndifferent informative object keypoints, and predict object locations from\ncoarse to fine. The \\coram~is used only during training to extract more\ninformative corner features which brings free-lunch performance boost. On the\nMS COCO dataset, we achieve the performance of 40.4\\% mAP at 28 FPS and 30.5\\%\nmAP at 118 FPS. Among all the real-time object detectors, %that can run faster\nthan 25 FPS, our SaccadeNet achieves the best detection performance, which\ndemonstrates the effectiveness of the proposed detection mechanism."}, {"title": "Multi-Granularity Reference-Aided Attentive Feature Aggregation for Video-Based Person Re-Identification", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "link": "https://arxiv.org/abs/2003.12224", "summary": "Video-based person re-identification (reID) aims at matching the same person\nacross video clips. It is a challenging task due to the existence of redundancy\namong frames, newly revealed appearance, occlusion, and motion blurs. In this\npaper, we propose an attentive feature aggregation module, namely\nMulti-Granularity Reference-aided Attentive Feature Aggregation (MG-RAFA), to\ndelicately aggregate spatio-temporal features into a discriminative video-level\nfeature representation. In order to determine the contribution/importance of a\nspatial-temporal feature node, we propose to learn the attention from a global\nview with convolutional operations. Specifically, we stack its relations, i.e.,\npairwise correlations with respect to a representative set of reference feature\nnodes (S-RFNs) that represents global video information, together with the\nfeature itself to infer the attention. Moreover, to exploit the semantics of\ndifferent levels, we propose to learn multi-granularity attentions based on the\nrelations captured at different granularities. Extensive ablation studies\ndemonstrate the effectiveness of our attentive feature aggregation module\nMG-RAFA. Our framework achieves the state-of-the-art performance on three\nbenchmark datasets."}, {"title": "Video Object Grounding Using Semantic Roles in Language Description", "authors": "Arka Sadhu, Kan Chen, Ram Nevatia", "link": "https://arxiv.org/abs/2003.10606", "summary": "We explore the task of Video Object Grounding (VOG), which grounds objects in\nvideos referred to in natural language descriptions. Previous methods apply\nimage grounding based algorithms to address VOG, fail to explore the object\nrelation information and suffer from limited generalization. Here, we\ninvestigate the role of object relations in VOG and propose a novel framework\nVOGNet to encode multi-modal object relations via self-attention with relative\nposition encoding. To evaluate VOGNet, we propose novel contrasting sampling\nmethods to generate more challenging grounding input samples, and construct a\nnew dataset called ActivityNet-SRL (ASRL) based on existing caption and\ngrounding datasets. Experiments on ASRL validate the need of encoding object\nrelations in VOG, and our VOGNet outperforms competitive baselines by a\nsignificant margin."}, {"title": "Designing Network Design Spaces", "authors": "Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Doll\u00e1r", "link": "https://arxiv.org/abs/2003.13678", "summary": "In this work, we present a new network design paradigm. Our goal is to help\nadvance the understanding of network design and discover design principles that\ngeneralize across settings. Instead of focusing on designing individual network\ninstances, we design network design spaces that parametrize populations of\nnetworks. The overall process is analogous to classic manual design of\nnetworks, but elevated to the design space level. Using our methodology we\nexplore the structure aspect of network design and arrive at a low-dimensional\ndesign space consisting of simple, regular networks that we call RegNet. The\ncore insight of the RegNet parametrization is surprisingly simple: widths and\ndepths of good networks can be explained by a quantized linear function. We\nanalyze the RegNet design space and arrive at interesting findings that do not\nmatch the current practice of network design. The RegNet design space provides\nsimple and fast networks that work well across a wide range of flop regimes.\nUnder comparable training settings and flops, the RegNet models outperform the\npopular EfficientNet models while being up to 5x faster on GPUs."}, {"title": "12-in-1: Multi-Task Vision and Language Representation Learning", "authors": "Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee", "link": "https://arxiv.org/abs/1912.02315", "summary": "Much of vision-and-language research focuses on a small but diverse set of\nindependent tasks and supporting datasets often studied in isolation; however,\nthe visually-grounded language understanding skills required for success at\nthese tasks overlap significantly. In this work, we investigate these\nrelationships between vision-and-language tasks by developing a large-scale,\nmulti-task training regime. Our approach culminates in a single model on 12\ndatasets from four broad categories of task including visual question\nanswering, caption-based image retrieval, grounding referring expressions, and\nmulti-modal verification. Compared to independently trained single-task models,\nthis represents a reduction from approximately 3 billion parameters to 270\nmillion while simultaneously improving performance by 2.05 points on average\nacross tasks. We use our multi-task framework to perform in-depth analysis of\nthe effect of joint training diverse tasks. Further, we show that finetuning\ntask-specific models from our single multi-task model can lead to further\nimprovements, achieving performance at or above the state-of-the-art."}, {"title": "MLCVNet: Multi-Level Context VoteNet for 3D Object Detection", "authors": "Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, Jun Wang", "link": "", "summary": ""}, {"title": "Listen to Look: Action Recognition by Previewing Audio", "authors": "Ruohan Gao, Tae-Hyun Oh, Kristen Grauman, Lorenzo Torresani", "link": "https://arxiv.org/abs/1912.04487", "summary": "In the face of the video data deluge, today's expensive clip-level\nclassifiers are increasingly impractical. We propose a framework for efficient\naction recognition in untrimmed video that uses audio as a preview mechanism to\neliminate both short-term and long-term visual redundancies. First, we devise\nan ImgAud2Vid framework that hallucinates clip-level features by distilling\nfrom lighter modalities---a single frame and its accompanying audio---reducing\nshort-term temporal redundancy for efficient clip-level recognition. Second,\nbuilding on ImgAud2Vid, we further propose ImgAud-Skimming, an attention-based\nlong short-term memory network that iteratively selects useful moments in\nuntrimmed videos, reducing long-term temporal redundancy for efficient\nvideo-level recognition. Extensive experiments on four action recognition\ndatasets demonstrate that our method achieves the state-of-the-art in terms of\nboth recognition accuracy and speed."}, {"title": "Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization", "authors": "Ruyi Ji, Longyin Wen, Libo Zhang, Dawei Du, Yanjun Wu, Chen Zhao, Xianglong Liu, Feiyue Huang", "link": "https://arxiv.org/abs/1909.11378", "summary": "Fine-grained visual categorization (FGVC) is an important but challenging\ntask due to high intra-class variances and low inter-class variances caused by\ndeformation, occlusion, illumination, etc. An attention convolutional binary\nneural tree architecture is presented to address those problems for weakly\nsupervised FGVC. Specifically, we incorporate convolutional operations along\nedges of the tree structure, and use the routing functions in each node to\ndetermine the root-to-leaf computational paths within the tree. The final\ndecision is computed as the summation of the predictions from leaf nodes. The\ndeep convolutional operations learn to capture the representations of objects,\nand the tree structure characterizes the coarse-to-fine hierarchical feature\nlearning process. In addition, we use the attention transformer module to\nenforce the network to capture discriminative features. The negative\nlog-likelihood loss is used to train the entire network in an end-to-end\nfashion by SGD with back-propagation. Several experiments on the CUB-200-2011,\nStanford Cars and Aircraft datasets demonstrate that the proposed method\nperforms favorably against the state-of-the-arts."}, {"title": "Music Gesture for Visual Sound Separation", "authors": "Chuang Gan, Deng Huang, Hang Zhao, Joshua B. Tenenbaum, Antonio Torralba", "link": "https://arxiv.org/abs/2004.09476", "summary": "Recent deep learning approaches have achieved impressive performance on\nvisual sound separation tasks. However, these approaches are mostly built on\nappearance and optical flow like motion feature representations, which exhibit\nlimited abilities to find the correlations between audio signals and visual\npoints, especially when separating multiple instruments of the same types, such\nas multiple violins in a scene. To address this, we propose \"Music Gesture,\" a\nkeypoint-based structured representation to explicitly model the body and\nfinger movements of musicians when they perform music. We first adopt a\ncontext-aware graph network to integrate visual semantic context with body\ndynamics, and then apply an audio-visual fusion model to associate body\nmovements with the corresponding audio signals. Experimental results on three\nmusic performance datasets show: 1) strong improvements upon benchmark metrics\nfor hetero-musical separation tasks (i.e. different instruments); 2) new\nability for effective homo-musical separation for piano, flute, and trumpet\nduets, which to our best knowledge has never been achieved with alternative\nmethods. Project page: http://music-gesture.csail.mit.edu."}, {"title": "Referring Image Segmentation via Cross-Modal Progressive Comprehension", "authors": "Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, Bo Li", "link": "", "summary": ""}, {"title": "Cloth in the Wind: A Case Study of Physical Measurement Through Simulation", "authors": "Tom F. H. Runia, Kirill Gavrilyuk, Cees G. M. Snoek, Arnold W. M. Smeulders", "link": "https://arxiv.org/abs/2003.05065", "summary": "For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, measuring physical properties\nfrom visual observations is challenging due to the high number of causally\nunderlying physical parameters -- including material properties and external\nforces. In this paper, we propose to measure latent physical properties for\ncloth in the wind without ever having seen a real example before. Our solution\nis an iterative refinement procedure with simulation at its core. The algorithm\ngradually updates the physical model parameters by running a simulation of the\nobserved phenomenon and comparing the current simulation to a real-world\nobservation. The correspondence is measured using an embedding function that\nmaps physically similar examples to nearby points. We consider a case study of\ncloth in the wind, with curling flags as our leading example -- a seemingly\nsimple phenomena but physically highly involved. Based on the physics of cloth\nand its visual manifestation, we propose an instantiation of the embedding\nfunction. For this mapping, modeled as a deep network, we introduce a spectral\nlayer that decomposes a video volume into its temporal spectral power and\ncorresponding frequencies. Our experiments demonstrate that the proposed method\ncompares favorably to prior work on the task of measuring cloth material\nproperties and external wind force from a real-world video."}, {"title": "The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction", "authors": "Junwei Liang, Lu Jiang, Kevin Murphy, Ting Yu, Alexander Hauptmann", "link": "https://arxiv.org/abs/1912.06445", "summary": "This paper studies the problem of predicting the distribution over multiple\npossible future paths of people as they move through various visual scenes. We\nmake two main contributions. The first contribution is a new dataset, created\nin a realistic 3D simulator, which is based on real world trajectory data, and\nthen extrapolated by human annotators to achieve different latent goals. This\nprovides the first benchmark for quantitative evaluation of the models to\npredict multi-future trajectories. The second contribution is a new model to\ngenerate multiple plausible future trajectories, which contains novel designs\nof using multi-scale location encodings and convolutional RNNs over graphs. We\nrefer to our model as Multiverse. We show that our model achieves the best\nresults on our dataset, as well as on the real-world VIRAT/ActEV dataset (which\njust contains one possible future)."}, {"title": "CentripetalNet: Pursuing High-Quality Keypoint Pairs for Object Detection", "authors": "Zhiwei Dong, Guoxuan Li, Yue Liao, Fei Wang, Pengju Ren, Chen Qian", "link": "https://arxiv.org/abs/2003.09119", "summary": "Keypoint-based detectors have achieved pretty-well performance. However,\nincorrect keypoint matching is still widespread and greatly affects the\nperformance of the detector. In this paper, we propose CentripetalNet which\nuses centripetal shift to pair corner keypoints from the same instance.\nCentripetalNet predicts the position and the centripetal shift of the corner\npoints and matches corners whose shifted results are aligned. Combining\nposition information, our approach matches corner points more accurately than\nthe conventional embedding approaches do. Corner pooling extracts information\ninside the bounding boxes onto the border. To make this information more aware\nat the corners, we design a cross-star deformable convolution network to\nconduct feature adaption. Furthermore, we explore instance segmentation on\nanchor-free detectors by equipping our CentripetalNet with a mask prediction\nmodule. On MS-COCO test-dev, our CentripetalNet not only outperforms all\nexisting anchor-free detectors with an AP of 48.0% but also achieves comparable\nperformance to the state-of-the-art instance segmentation approaches with a\n40.2% MaskAP. Code will be available at\nhttps://github.com/KiveeDong/CentripetalNet."}, {"title": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "authors": "Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li", "link": "", "summary": ""}, {"title": "Graph Embedded Pose Clustering for Anomaly Detection", "authors": "Amir Markovitz, Gilad Sharir, Itamar Friedman, Lihi Zelnik-Manor, Shai Avidan", "link": "https://arxiv.org/abs/1912.11850", "summary": "We propose a new method for anomaly detection of human actions. Our method\nworks directly on human pose graphs that can be computed from an input video\nsequence. This makes the analysis independent of nuisance parameters such as\nviewpoint or illumination. We map these graphs to a latent space and cluster\nthem. Each action is then represented by its soft-assignment to each of the\nclusters. This gives a kind of \"bag of words\" representation to the data, where\nevery action is represented by its similarity to a group of base action-words.\nThen, we use a Dirichlet process based mixture, that is useful for handling\nproportional data such as our soft-assignment vectors, to determine if an\naction is normal or not.\n  We evaluate our method on two types of data sets. The first is a fine-grained\nanomaly detection data set (e.g. ShanghaiTech) where we wish to detect unusual\nvariations of some action. The second is a coarse-grained anomaly detection\ndata set (e.g., a Kinetics-based data set) where few actions are considered\nnormal, and every other action should be considered abnormal.\n  Extensive experiments on the benchmarks show that our method performs\nconsiderably better than other state of the art methods."}, {"title": "Disp R-CNN: Stereo 3D Object Detection via Shape Prior Guided Instance Disparity Estimation", "authors": "Jiaming Sun, Linghao Chen, Yiming Xie, Siyu Zhang, Qinhong Jiang, Xiaowei Zhou, Hujun Bao", "link": "https://arxiv.org/abs/2004.03572", "summary": "In this paper, we propose a novel system named Disp R-CNN for 3D object\ndetection from stereo images. Many recent works solve this problem by first\nrecovering a point cloud with disparity estimation and then apply a 3D\ndetector. The disparity map is computed for the entire image, which is costly\nand fails to leverage category-specific prior. In contrast, we design an\ninstance disparity estimation network (iDispNet) that predicts disparity only\nfor pixels on objects of interest and learns a category-specific shape prior\nfor more accurate disparity estimation. To address the challenge from scarcity\nof disparity annotation in training, we propose to use a statistical shape\nmodel to generate dense disparity pseudo-ground-truth without the need of LiDAR\npoint clouds, which makes our system more widely applicable. Experiments on the\nKITTI dataset show that, even when LiDAR ground-truth is not available at\ntraining time, Disp R-CNN achieves competitive performance and outperforms\nprevious state-of-the-art methods by 20% in terms of average precision."}, {"title": "Deepstrip: High-Resolution Boundary Refinement", "authors": "Peng Zhou, Brian Price, Scott Cohen, Gregg Wilensky, Larry S. Davis", "link": "https://arxiv.org/abs/2003.11670", "summary": "In this paper, we target refining the boundaries in high resolution images\ngiven low resolution masks. For memory and computation efficiency, we propose\nto convert the regions of interest into strip images and compute a boundary\nprediction in the strip domain. To detect the target boundary, we present a\nframework with two prediction layers. First, all potential boundaries are\npredicted as an initial prediction and then a selection layer is used to pick\nthe target boundary and smooth the result. To encourage accurate prediction, a\nloss which measures the boundary distance in the strip domain is introduced. In\naddition, we enforce a matching consistency and C0 continuity regularization to\nthe network to reduce false alarms. Extensive experiments on both public and a\nnewly created high resolution dataset strongly validate our approach."}, {"title": "Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification", "authors": "Guangcong Wang, Jian-Huang Lai, Wenqi Liang, Guangrun Wang", "link": "", "summary": ""}, {"title": "Meshed-Memory Transformer for Image Captioning", "authors": "Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara", "link": "https://arxiv.org/abs/1912.08226", "summary": "Transformer-based architectures represent the state of the art in sequence\nmodeling tasks like machine translation and language understanding. Their\napplicability to multi-modal contexts like image captioning, however, is still\nlargely under-explored. With the aim of filling this gap, we present M$^2$ - a\nMeshed Transformer with Memory for Image Captioning. The architecture improves\nboth the image encoding and the language generation steps: it learns a\nmulti-level representation of the relationships between image regions\nintegrating learned a priori knowledge, and uses a mesh-like connectivity at\ndecoding stage to exploit low- and high-level features. Experimentally, we\ninvestigate the performance of the M$^2$ Transformer and different\nfully-attentive models in comparison with recurrent ones. When tested on COCO,\nour proposal achieves a new state of the art in single-model and ensemble\nconfigurations on the \"Karpathy\" test split and on the online test server. We\nalso assess its performances when describing objects unseen in the training\nset. Trained models and code for reproducing the experiments are publicly\navailable at: https://github.com/aimagelab/meshed-memory-transformer."}, {"title": "Learning From Noisy Anchors for One-Stage Object Detection", "authors": "Hengduo Li, Zuxuan Wu, Chen Zhu, Caiming Xiong, Richard Socher, Larry S. Davis", "link": "https://arxiv.org/abs/1912.05086", "summary": "State-of-the-art object detectors rely on regressing and classifying an\nextensive list of possible anchors, which are divided into positive and\nnegative samples based on their intersection-over-union (IoU) with\ncorresponding groundtruth objects. Such a harsh split conditioned on IoU\nresults in binary labels that are potentially noisy and challenging for\ntraining. In this paper, we propose to mitigate noise incurred by imperfect\nlabel assignment such that the contributions of anchors are dynamically\ndetermined by a carefully constructed cleanliness score associated with each\nanchor. Exploring outputs from both regression and classification branches, the\ncleanliness scores, estimated without incurring any additional computational\noverhead, are used not only as soft labels to supervise the training of the\nclassification branch but also sample re-weighting factors for improved\nlocalization and classification accuracy. We conduct extensive experiments on\nCOCO, and demonstrate, among other things, the proposed approach steadily\nimproves RetinaNet by ~2% with various backbones."}, {"title": "Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection", "authors": "Zhongzheng Ren, Zhiding Yu, Xiaodong Yang, Ming-Yu Liu, Yong Jae Lee, Alexander G. Schwing, Jan Kautz", "link": "https://arxiv.org/abs/2004.04725", "summary": "Weakly supervised learning has emerged as a compelling tool for object\ndetection by reducing the need for strong supervision during training. However,\nmajor challenges remain: (1) differentiation of object instances can be\nambiguous; (2) detectors tend to focus on discriminative parts rather than\nentire objects; (3) without ground truth, object proposals have to be redundant\nfor high recalls, causing significant memory consumption. Addressing these\nchallenges is difficult, as it often requires to eliminate uncertainties and\ntrivial solutions. To target these issues we develop an instance-aware and\ncontext-focused unified framework. It employs an instance-aware self-training\nalgorithm and a learnable Concrete DropBlock while devising a memory-efficient\nsequential batch back-propagation. Our proposed method achieves\nstate-of-the-art results on COCO ($12.1\\% ~AP$, $24.8\\% ~AP_{50}$), VOC 2007\n($54.9\\% ~AP$), and VOC 2012 ($52.1\\% ~AP$), improving baselines by great\nmargins. In addition, the proposed method is the first to benchmark ResNet\nbased models and weakly supervised video object detection. Refer to our project\npage for code, models, and more details: https://github.com/NVlabs/wetectron."}, {"title": "Density-Based Clustering for 3D Object Detection in Point Clouds", "authors": "Syeda Mariam Ahmed, Chee Meng Chew"}, {"title": "Few-Shot Video Classification via Temporal Alignment", "authors": "Kaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, Juan Carlos Niebles", "link": "https://arxiv.org/abs/1906.11415", "summary": "There is a growing interest in learning a model which could recognize novel\nclasses with only a few labeled examples. In this paper, we propose Temporal\nAlignment Module (TAM), a novel few-shot learning framework that can learn to\nclassify a previous unseen video. While most previous works neglect long-term\ntemporal ordering information, our proposed model explicitly leverages the\ntemporal ordering information in video data through temporal alignment. This\nleads to strong data-efficiency for few-shot learning. In concrete, TAM\ncalculates the distance value of query video with respect to novel class\nproxies by averaging the per frame distances along its alignment path. We\nintroduce continuous relaxation to TAM so the model can be learned in an\nend-to-end fashion to directly optimize the few-shot learning objective. We\nevaluate TAM on two challenging real-world datasets, Kinetics and\nSomething-Something-V2, and show that our model leads to significant\nimprovement of few-shot video classification over a wide range of competitive\nbaselines."}, {"title": "Densely Connected Search Space for More Flexible Neural Architecture Search", "authors": "Jiemin Fang, Yuzhu Sun, Qian Zhang, Yuan Li, Wenyu Liu, Xinggang Wang", "link": "https://arxiv.org/abs/1906.09607", "summary": "Neural architecture search (NAS) has dramatically advanced the development of\nneural network design. We revisit the search space design in most previous NAS\nmethods and find the number and widths of blocks are set manually. However,\nblock counts and block widths determine the network scale (depth and width) and\nmake a great influence on both the accuracy and the model cost (FLOPs/latency).\nIn this paper, we propose to search block counts and block widths by designing\na densely connected search space, i.e., DenseNAS. The new search space is\nrepresented as a dense super network, which is built upon our designed routing\nblocks. In the super network, routing blocks are densely connected and we\nsearch for the best path between them to derive the final architecture. We\nfurther propose a chained cost estimation algorithm to approximate the model\ncost during the search. Both the accuracy and model cost are optimized in\nDenseNAS. For experiments on the MobileNetV2-based search space, DenseNAS\nachieves 75.3% top-1 accuracy on ImageNet with only 361MB FLOPs and 17.9ms\nlatency on a single TITAN-XP. The larger model searched by DenseNAS achieves\n76.1% accuracy with only 479M FLOPs. DenseNAS further promotes the ImageNet\nclassification accuracies of ResNet-18, -34 and -50-B by 1.5%, 0.5% and 0.3%\nwith 200M, 600M and 680M FLOPs reduction respectively. The related code is\navailable at https://github.com/JaminFong/DenseNAS."}, {"title": "Fine-Grained Video-Text Retrieval With Hierarchical Graph Reasoning", "authors": "Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu", "link": "https://arxiv.org/abs/2003.00392", "summary": "Cross-modal retrieval between videos and texts has attracted growing\nattentions due to the rapid emergence of videos on the web. The current\ndominant approach for this problem is to learn a joint embedding space to\nmeasure cross-modal similarities. However, simple joint embeddings are\ninsufficient to represent complicated visual and textual details, such as\nscenes, objects, actions and their compositions. To improve fine-grained\nvideo-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model,\nwhich decomposes video-text matching into global-to-local levels. To be\nspecific, the model disentangles texts into hierarchical semantic graph\nincluding three levels of events, actions, entities and relationships across\nlevels. Attention-based graph reasoning is utilized to generate hierarchical\ntextual embeddings, which can guide the learning of diverse and hierarchical\nvideo representations. The HGR model aggregates matchings from different\nvideo-text levels to capture both global and local details. Experimental\nresults on three video-text datasets demonstrate the advantages of our model.\nSuch hierarchical decomposition also enables better generalization across\ndatasets and improves the ability to distinguish fine-grained semantic\ndifferences."}, {"title": "Warp to the Future: Joint Forecasting of Features and Feature Motion", "authors": "Josip \u0160ari\u0107, Marin Or\u0161i\u0107, Ton\u0107i Antunovi\u0107, Sacha Vra\u017ei\u0107, Sini\u0161a \u0160egvi\u0107"}, {"title": "Network Adjustment: Channel Search Guided by FLOPs Utilization Ratio", "authors": "Zhengsu Chen, Jianwei Niu, Lingxi Xie, Xuefeng Liu, Longhui Wei, Qi Tian", "link": "https://arxiv.org/abs/2004.02767", "summary": "Automatic designing computationally efficient neural networks has received\nmuch attention in recent years. Existing approaches either utilize network\npruning or leverage the network architecture search methods. This paper\npresents a new framework named network adjustment, which considers network\naccuracy as a function of FLOPs, so that under each network configuration, one\ncan estimate the FLOPs utilization ratio (FUR) for each layer and use it to\ndetermine whether to increase or decrease the number of channels on the layer.\nNote that FUR, like the gradient of a non-linear function, is accurate only in\na small neighborhood of the current network. Hence, we design an iterative\nmechanism so that the initial network undergoes a number of steps, each of\nwhich has a small `adjusting rate' to control the changes to the network. The\ncomputational overhead of the entire search process is reasonable, i.e.,\ncomparable to that of re-training the final model from scratch. Experiments on\nstandard image classification datasets and a wide range of base networks\ndemonstrate the effectiveness of our approach, which consistently outperforms\nthe pruning counterpart. The code is available at\nhttps://github.com/danczs/NetworkAdjustment."}, {"title": "Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences", "authors": "Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, Lianli Gao", "link": "https://arxiv.org/abs/2001.06891", "summary": "In this paper, we consider a novel task, Spatio-Temporal Video Grounding for\nMulti-Form Sentences (STVG). Given an untrimmed video and a\ndeclarative/interrogative sentence depicting an object, STVG aims to localize\nthe spatio-temporal tube of the queried object. STVG has two challenging\nsettings: (1) We need to localize spatio-temporal object tubes from untrimmed\nvideos, where the object may only exist in a very small segment of the video;\n(2) We deal with multi-form sentences, including the declarative sentences with\nexplicit objects and interrogative sentences with unknown objects. Existing\nmethods cannot tackle the STVG task due to the ineffective tube pre-generation\nand the lack of object relationship modeling. Thus, we then propose a novel\nSpatio-Temporal Graph Reasoning Network (STGRN) for this task. First, we build\na spatio-temporal region graph to capture the region relationships with\ntemporal object dynamics, which involves the implicit and explicit spatial\nsubgraphs in each frame and the temporal dynamic subgraph across frames. We\nthen incorporate textual clues into the graph and develop the multi-step\ncross-modal graph reasoning. Next, we introduce a spatio-temporal localizer\nwith a dynamic selection method to directly retrieve the spatio-temporal tubes\nwithout tube pre-generation. Moreover, we contribute a large-scale video\ngrounding dataset VidSTG based on video relation dataset VidOR. The extensive\nexperiments demonstrate the effectiveness of our method."}, {"title": "Cross-Modal Cross-Domain Moment Alignment Network for Person Search", "authors": "Ya Jing, Wei Wang, Liang Wang, Tieniu Tan"}, {"title": "Self-Training With Noisy Student Improves ImageNet Classification", "authors": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le", "link": "https://arxiv.org/abs/1911.04252", "summary": "We present Noisy Student Training, a semi-supervised learning approach that\nworks well even when labeled data is abundant. Noisy Student Training achieves\n88.4% top-1 accuracy on ImageNet, which is 2.0% better than the\nstate-of-the-art model that requires 3.5B weakly labeled Instagram images. On\nrobustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to\n83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces\nImageNet-P mean flip rate from 27.8 to 12.2.\n  Noisy Student Training extends the idea of self-training and distillation\nwith the use of equal-or-larger student models and noise added to the student\nduring learning. On ImageNet, we first train an EfficientNet model on labeled\nimages and use it as a teacher to generate pseudo labels for 300M unlabeled\nimages. We then train a larger EfficientNet as a student model on the\ncombination of labeled and pseudo labeled images. We iterate this process by\nputting back the student as the teacher. During the learning of the student, we\ninject noise such as dropout, stochastic depth, and data augmentation via\nRandAugment to the student so that the student generalizes better than the\nteacher. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\nCode is available at https://github.com/google-research/noisystudent."}, {"title": "Learning Longterm Representations for Person Re-Identification Using Radio Signals", "authors": "Lijie Fan, Tianhong Li, Rongyao Fang, Rumen Hristov, Yuan Yuan, Dina Katabi", "link": "http://arxiv.org/abs/2004.01091", "summary": "Person Re-Identification (ReID) aims to recognize a person-of-interest across\ndifferent places and times. Existing ReID methods rely on images or videos\ncollected using RGB cameras. They extract appearance features like clothes,\nshoes, hair, etc. Such features, however, can change drastically from one day\nto the next, leading to inability to identify people over extended time\nperiods. In this paper, we introduce RF-ReID, a novel approach that harnesses\nradio frequency (RF) signals for longterm person ReID. RF signals traverse\nclothes and reflect off the human body; thus they can be used to extract more\npersistent human-identifying features like body size and shape. We evaluate the\nperformance of RF-ReID on longitudinal datasets that span days and weeks, where\nthe person may wear different clothes across days. Our experiments demonstrate\nthat RF-ReID outperforms state-of-the-art RGB-based ReID approaches for long\nterm person ReID. Our results also reveal two interesting features: First since\nRF signals work in the presence of occlusions and poor lighting, RF-ReID allows\nfor person ReID in such scenarios. Second, unlike photos and videos which\nreveal personal and private information, RF signals are more\nprivacy-preserving, and hence can help extend person ReID to privacy-concerned\ndomains, like healthcare."}, {"title": "LatentFusion: End-to-End Differentiable Reconstruction and Rendering for Unseen Object Pose Estimation", "authors": "Keunhong Park, Arsalan Mousavian, Yu Xiang, Dieter Fox", "link": "https://arxiv.org/abs/1912.00416", "summary": "Current 6D object pose estimation methods usually require a 3D model for each\nobject. These methods also require additional training in order to incorporate\nnew objects. As a result, they are difficult to scale to a large number of\nobjects and cannot be directly applied to unseen objects. In this work, we\npropose a novel framework for 6D pose estimation of unseen objects. We design\nan end-to-end neural network that reconstructs a latent 3D representation of an\nobject using a small number of reference views of the object. Using the learned\n3D representation, the network is able to render the object from arbitrary\nviews. Using this neural renderer, we directly optimize for pose given an input\nimage. By training our network with a large number of 3D shapes for\nreconstruction and rendering, our network generalizes well to unseen objects.\nWe present a new dataset for unseen object pose estimation--MOPED. We evaluate\nthe performance of our method for unseen object pose estimation on MOPED as\nwell as the ModelNet dataset."}, {"title": "Learning Instance Occlusion for Panoptic Segmentation", "authors": "Justin Lazarow, Kwonjoon Lee, Kunyu Shi, Zhuowen Tu", "link": "https://arxiv.org/abs/1906.05896", "summary": "Panoptic segmentation requires segments of both \"things\" (countable object\ninstances) and \"stuff\" (uncountable and amorphous regions) within a single\noutput. A common approach involves the fusion of instance segmentation (for\n\"things\") and semantic segmentation (for \"stuff\") into a non-overlapping\nplacement of segments, and resolves overlaps. However, instance ordering with\ndetection confidence do not correlate well with natural occlusion relationship.\nTo resolve this issue, we propose a branch that is tasked with modeling how two\ninstance masks should overlap one another as a binary relation. Our method,\nnamed OCFusion, is lightweight but particularly effective in the instance\nfusion process. OCFusion is trained with the ground truth relation derived\nautomatically from the existing dataset annotations. We obtain state-of-the-art\nresults on COCO and show competitive results on the Cityscapes panoptic\nsegmentation benchmark."}, {"title": "Vision-Dialog Navigation by Exploring Cross-Modal Memory", "authors": "Yi Zhu, Fengda Zhu, Zhaohuan Zhan, Bingqian Lin, Jianbin Jiao, Xiaojun Chang, Xiaodan Liang", "link": "https://arxiv.org/abs/2003.06745", "summary": "Vision-dialog navigation posed as a new holy-grail task in vision-language\ndisciplinary targets at learning an agent endowed with the capability of\nconstant conversation for help with natural language and navigating according\nto human responses. Besides the common challenges faced in visual language\nnavigation, vision-dialog navigation also requires to handle well with the\nlanguage intentions of a series of questions about the temporal context from\ndialogue history and co-reasoning both dialogs and visual scenes. In this\npaper, we propose the Cross-modal Memory Network (CMN) for remembering and\nunderstanding the rich information relevant to historical navigation actions.\nOur CMN consists of two memory modules, the language memory module (L-mem) and\nthe visual memory module (V-mem). Specifically, L-mem learns latent\nrelationships between the current language interaction and a dialog history by\nemploying a multi-head attention mechanism. V-mem learns to associate the\ncurrent visual views and the cross-modal memory about the previous navigation\nactions. The cross-modal memory is generated via a vision-to-language attention\nand a language-to-vision attention. Benefiting from the collaborative learning\nof the L-mem and the V-mem, our CMN is able to explore the memory about the\ndecision making of historical navigation actions which is for the current step.\nExperiments on the CVDN dataset show that our CMN outperforms the previous\nstate-of-the-art model by a significant margin on both seen and unseen\nenvironments."}, {"title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks", "authors": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox", "link": "https://arxiv.org/abs/1912.01734", "summary": "We present ALFRED (Action Learning From Realistic Environments and\nDirectives), a benchmark for learning a mapping from natural language\ninstructions and egocentric vision to sequences of actions for household tasks.\nALFRED includes long, compositional tasks with non-reversible state changes to\nshrink the gap between research benchmarks and real-world applications. ALFRED\nconsists of expert demonstrations in interactive visual environments for 25k\nnatural language directives. These directives contain both high-level goals\nlike \"Rinse off a mug and place it in the coffee maker.\" and low-level language\ninstructions like \"Walk to the coffee maker on the right.\" ALFRED tasks are\nmore complex in terms of sequence length, action space, and language than\nexisting vision-and-language task datasets. We show that a baseline model based\non recent embodied vision-and-language tasks performs poorly on ALFRED,\nsuggesting that there is significant room for developing innovative grounded\nvisual language understanding models with this benchmark."}, {"title": "NMS by Representative Region: Towards Crowded Pedestrian Detection by Proposal Pairing", "authors": "Xin Huang, Zheng Ge, Zequn Jie, Osamu Yoshie", "link": "https://arxiv.org/abs/2003.12729", "summary": "Although significant progress has been made in pedestrian detection recently,\npedestrian detection in crowded scenes is still challenging. The heavy\nocclusion between pedestrians imposes great challenges to the standard\nNon-Maximum Suppression (NMS). A relative low threshold of intersection over\nunion (IoU) leads to missing highly overlapped pedestrians, while a higher one\nbrings in plenty of false positives. To avoid such a dilemma, this paper\nproposes a novel Representative Region NMS approach leveraging the less\noccluded visible parts, effectively removing the redundant boxes without\nbringing in many false positives. To acquire the visible parts, a novel\nPaired-Box Model (PBM) is proposed to simultaneously predict the full and\nvisible boxes of a pedestrian. The full and visible boxes constitute a pair\nserving as the sample unit of the model, thus guaranteeing a strong\ncorrespondence between the two boxes throughout the detection pipeline.\nMoreover, convenient feature integration of the two boxes is allowed for the\nbetter performance on both full and visible pedestrian detection tasks.\nExperiments on the challenging CrowdHuman and CityPersons benchmarks\nsufficiently validate the effectiveness of the proposed approach on pedestrian\ndetection in the crowded situation."}, {"title": "Visual Commonsense R-CNN", "authors": "Tan Wang, Jianqiang Huang, Hanwang Zhang, Qianru Sun", "link": "", "summary": ""}, {"title": "What Deep CNNs Benefit From Global Covariance Pooling: An Optimization Perspective", "authors": "Qilong Wang, Li Zhang, Banggu Wu, Dongwei Ren, Peihua Li, Wangmeng Zuo, Qinghua Hu", "link": "https://arxiv.org/abs/2003.11241", "summary": "Recent works have demonstrated that global covariance pooling (GCP) has the\nability to improve performance of deep convolutional neural networks (CNNs) on\nvisual classification task. Despite considerable advance, the reasons on\neffectiveness of GCP on deep CNNs have not been well studied. In this paper, we\nmake an attempt to understand what deep CNNs benefit from GCP in a viewpoint of\noptimization. Specifically, we explore the effect of GCP on deep CNNs in terms\nof the Lipschitzness of optimization loss and the predictiveness of gradients,\nand show that GCP can make the optimization landscape more smooth and the\ngradients more predictive. Furthermore, we discuss the connection between GCP\nand second-order optimization for deep CNNs. More importantly, above findings\ncan account for several merits of covariance pooling for training deep CNNs\nthat have not been recognized previously or fully explored, including\nsignificant acceleration of network convergence (i.e., the networks trained\nwith GCP can support rapid decay of learning rates, achieving favorable\nperformance while significantly reducing number of training epochs), stronger\nrobustness to distorted examples generated by image corruptions and\nperturbations, and good generalization ability to different vision tasks, e.g.,\nobject detection and instance segmentation. We conduct extensive experiments\nusing various deep CNN models on diversified tasks, and the results provide\nstrong support to our findings."}, {"title": "EfficientDet: Scalable and Efficient Object Detection", "authors": "Mingxing Tan, Ruoming Pang, Quoc V. Le", "link": "https://arxiv.org/abs/1911.09070", "summary": "Model efficiency has become increasingly important in computer vision. In\nthis paper, we systematically study neural network architecture design choices\nfor object detection and propose several key optimizations to improve\nefficiency. First, we propose a weighted bi-directional feature pyramid network\n(BiFPN), which allows easy and fast multi-scale feature fusion; Second, we\npropose a compound scaling method that uniformly scales the resolution, depth,\nand width for all backbone, feature network, and box/class prediction networks\nat the same time. Based on these optimizations and better backbones, we have\ndeveloped a new family of object detectors, we have developed a new family of\nobject detectors, called EfficientDet, which consistently achieve much better\nefficiency than prior art across a wide spectrum of resource constraints. In\nparticular, with single-model and single-scale, our EfficientDet-D7 achieves\nstate-of-the-art 52.6 AP on COCO test-dev with 52M parameters and 325B FLOPs,\nbeing 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors.\nCode is available at https://github.com/google/automl/tree/master/efficientdet."}, {"title": "Fast Template Matching and Update for Video Object Tracking and Segmentation", "authors": "Mingjie Sun, Jimin Xiao, Eng Gee Lim, Bingfeng Zhang, Yao Zhao", "link": "http://arxiv.org/abs/2004.07538", "summary": "In this paper, the main task we aim to tackle is the multi-instance\nsemi-supervised video object segmentation across a sequence of frames where\nonly the first-frame box-level ground-truth is provided. Detection-based\nalgorithms are widely adopted to handle this task, and the challenges lie in\nthe selection of the matching method to predict the result as well as to decide\nwhether to update the target template using the newly predicted result. The\nexisting methods, however, make these selections in a rough and inflexible way,\ncompromising their performance. To overcome this limitation, we propose a novel\napproach which utilizes reinforcement learning to make these two decisions at\nthe same time. Specifically, the reinforcement learning agent learns to decide\nwhether to update the target template according to the quality of the predicted\nresult. The choice of the matching method will be determined at the same time,\nbased on the action history of the reinforcement learning agent. Experiments\nshow that our method is almost 10 times faster than the previous\nstate-of-the-art method with even higher accuracy (region similarity of 69.1%\non DAVIS 2017 dataset)."}, {"title": "Counterfactual Samples Synthesizing for Robust Visual Question Answering", "authors": "Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, Yueting Zhuang", "link": "https://arxiv.org/abs/2003.06576", "summary": "Despite Visual Question Answering (VQA) has realized impressive progress over\nthe last few years, today's VQA models tend to capture superficial linguistic\ncorrelations in the train set and fail to generalize to the test set with\ndifferent QA distributions. To reduce the language biases, several recent works\nintroduce an auxiliary question-only model to regularize the training of\ntargeted VQA model, and achieve dominating performance on VQA-CP. However,\nsince the complexity of design, current methods are unable to equip the\nensemble-based models with two indispensable characteristics of an ideal VQA\nmodel: 1) visual-explainable: the model should rely on the right visual regions\nwhen making decisions. 2) question-sensitive: the model should be sensitive to\nthe linguistic variations in question. To this end, we propose a model-agnostic\nCounterfactual Samples Synthesizing (CSS) training scheme. The CSS generates\nnumerous counterfactual training samples by masking critical objects in images\nor words in questions, and assigning different ground-truth answers. After\ntraining with the complementary samples (ie, the original and generated\nsamples), the VQA models are forced to focus on all critical objects and words,\nwhich significantly improves both visual-explainable and question-sensitive\nabilities. In return, the performance of these models is further boosted.\nExtensive ablations have shown the effectiveness of CSS. Particularly, by\nbuilding on top of the model LMH, we achieve a record-breaking performance of\n58.95% on VQA-CP v2, with 6.5% gains."}, {"title": "Local-Global Video-Text Interactions for Temporal Grounding", "authors": "Jonghwan Mun, Minsu Cho, Bohyung Han", "link": "https://arxiv.org/abs/2004.07514", "summary": "This paper addresses the problem of text-to-video temporal grounding, which\naims to identify the time interval in a video semantically relevant to a text\nquery. We tackle this problem using a novel regression-based model that learns\nto extract a collection of mid-level features for semantic phrases in a text\nquery, which corresponds to important semantic entities described in the query\n(e.g., actors, objects, and actions), and reflect bi-modal interactions between\nthe linguistic features of the query and the visual features of the video in\nmultiple levels. The proposed method effectively predicts the target time\ninterval by exploiting contextual information from local to global during\nbi-modal interactions. Through in-depth ablation studies, we find out that\nincorporating both local and global context in video and text interactions is\ncrucial to the accurate grounding. Our experiment shows that the proposed\nmethod outperforms the state of the arts on Charades-STA and ActivityNet\nCaptions datasets by large margins, 7.44\\% and 4.61\\% points at Recall@tIoU=0.5\nmetric, respectively. Code is available in\nhttps://github.com/JonghwanMun/LGI4temporalgrounding."}, {"title": "Set-Constrained Viterbi for Set-Supervised Action Segmentation", "authors": "Jun Li, Sinisa Todorovic", "link": "https://arxiv.org/abs/2002.11925", "summary": "This paper is about weakly supervised action segmentation, where the ground\ntruth specifies only a set of actions present in a training video, but not\ntheir true temporal ordering. Prior work typically uses a classifier that\nindependently labels video frames for generating the pseudo ground truth, and\nmultiple instance learning for training the classifier. We extend this\nframework by specifying an HMM, which accounts for co-occurrences of action\nclasses and their temporal lengths, and by explicitly training the HMM on a\nViterbi-based loss. Our first contribution is the formulation of a new\nset-constrained Viterbi algorithm (SCV). Given a video, the SCV generates the\nMAP action segmentation that satisfies the ground truth. This prediction is\nused as a framewise pseudo ground truth in our HMM training. Our second\ncontribution in training is a new regularization of feature affinities between\ntraining videos that share the same action classes. Evaluation on action\nsegmentation and alignment on the Breakfast, MPII Cooking2, Hollywood Extended\ndatasets demonstrates our significant performance improvement for the two tasks\nover prior work."}, {"title": "Probabilistic Video Prediction From Noisy Data With a Posterior Confidence", "authors": "Yunbo Wang, Jiajun Wu, Mingsheng Long, Joshua B. Tenenbaum"}, {"title": "Beyond Short-Term Snippet: Video Relation Detection With Spatio-Temporal Global Context", "authors": "Chenchen Liu, Yang Jin, Kehan Xu, Guoqiang Gong, Yadong Mu"}, {"title": "Visual Grounding in Video for Unsupervised Word Translation", "authors": "Gunnar A. Sigurdsson, Jean-Baptiste Alayrac, Aida Nematzadeh, Lucas Smaira, Mateusz Malinowski, Jo\u00e3o Carreira, Phil Blunsom, Andrew Zisserman", "link": "https://arxiv.org/abs/2003.05078", "summary": "There are thousands of actively spoken languages on Earth, but a single\nvisual world. Grounding in this visual world has the potential to bridge the\ngap between all these languages. Our goal is to use visual grounding to improve\nunsupervised word mapping between languages. The key idea is to establish a\ncommon visual representation between two languages by learning embeddings from\nunpaired instructional videos narrated in the native language. Given this\nshared embedding we demonstrate that (i) we can map words between the\nlanguages, particularly the 'visual' words; (ii) that the shared embedding\nprovides a good initialization for existing unsupervised text-based word\ntranslation techniques, forming the basis for our proposed hybrid visual-text\nmapping algorithm, MUVE; and (iii) our approach achieves superior performance\nby addressing the shortcomings of text-based methods -- it is more robust,\nhandles datasets with less commonality, and is applicable to low-resource\nlanguages. We apply these methods to translate words from English to French,\nKorean, and Japanese -- all without any parallel corpora and simply by watching\nmany videos of people speaking while doing things."}, {"title": "Two Causal Principles for Improving Visual Dialog", "authors": "Jiaxin Qi, Yulei Niu, Jianqiang Huang, Hanwang Zhang", "link": "https://arxiv.org/abs/1911.10496", "summary": "This paper unravels the design tricks adopted by us, the champion team\nMReaL-BDAI, for Visual Dialog Challenge 2019: two causal principles for\nimproving Visual Dialog (VisDial). By \"improving\", we mean that they can\npromote almost every existing VisDial model to the state-of-the-art performance\non the leader-board. Such a major improvement is only due to our careful\ninspection on the causality behind the model and data, finding that the\ncommunity has overlooked two causalities in VisDial. Intuitively, Principle 1\nsuggests: we should remove the direct input of the dialog history to the answer\nmodel, otherwise a harmful shortcut bias will be introduced; Principle 2 says:\nthere is an unobserved confounder for history, question, and answer, leading to\nspurious correlations from training data. In particular, to remove the\nconfounder suggested in Principle 2, we propose several causal intervention\nalgorithms, which make the training fundamentally different from the\ntraditional likelihood estimation. Note that the two principles are\nmodel-agnostic, so they are applicable in any VisDial model. The code is\navailable at https://github.com/simpleshinobu/visdial-principles."}, {"title": "Spatio-Temporal Graph for Video Captioning With Knowledge Distillation", "authors": "Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, Adrien Gaidon, Ehsan Adeli, Juan Carlos Niebles", "link": "https://arxiv.org/abs/2003.13942", "summary": "Video captioning is a challenging task that requires a deep understanding of\nvisual scenes. State-of-the-art methods generate captions using either\nscene-level or object-level information but without explicitly modeling object\ninteractions. Thus, they often fail to make visually grounded predictions, and\nare sensitive to spurious correlations. In this paper, we propose a novel\nspatio-temporal graph model for video captioning that exploits object\ninteractions in space and time. Our model builds interpretable links and is\nable to provide explicit visual grounding. To avoid unstable performance caused\nby the variable number of objects, we further propose an object-aware knowledge\ndistillation mechanism, in which local object information is used to regularize\nglobal scene features. We demonstrate the efficacy of our approach through\nextensive experiments on two benchmarks, showing our approach yields\ncompetitive performance with interpretable predictions."}, {"title": "A Real-Time Cross-Modality Correlation Filtering Method for Referring Expression Comprehension", "authors": "Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, Bo Li", "link": "http://arxiv.org/abs/1909.07072", "summary": "Referring expression comprehension aims to localize the object instance\ndescribed by a natural language expression. Current referring expression\nmethods have achieved good performance. However, none of them is able to\nachieve real-time inference without accuracy drop. The reason for the\nrelatively slow inference speed is that these methods artificially split the\nreferring expression comprehension into two sequential stages including\nproposal generation and proposal ranking. It does not exactly conform to the\nhabit of human cognition. To this end, we propose a novel Realtime\nCross-modality Correlation Filtering method (RCCF). RCCF reformulates the\nreferring expression comprehension as a correlation filtering process. The\nexpression is first mapped from the language domain to the visual domain and\nthen treated as a template (kernel) to perform correlation filtering on the\nimage feature map. The peak value in the correlation heatmap indicates the\ncenter points of the target box. In addition, RCCF also regresses a 2-D object\nsize and 2-D offset. The center point coordinates, object size and center point\noffset together to form the target bounding box. Our method runs at 40 FPS\nwhile achieving leading performance in RefClef, RefCOCO, RefCOCO+ and RefCOCOg\nbenchmarks. In the challenging RefClef dataset, our methods almost double the\nstate-of-the-art performance (34.70% increased to 63.79%). We hope this work\ncan arouse more attention and studies to the new cross-modality correlation\nfiltering framework as well as the one-stage framework for referring expression\ncomprehension."}, {"title": "Better Captioning With Sequence-Level Exploration", "authors": "Jia Chen, Qin Jin", "link": "https://arxiv.org/abs/2003.03749", "summary": "Sequence-level learning objective has been widely used in captioning tasks to\nachieve the state-of-the-art performance for many models. In this objective,\nthe model is trained by the reward on the quality of its generated captions\n(sequence-level). In this work, we show the limitation of the current\nsequence-level learning objective for captioning tasks from both theory and\nempirical result. In theory, we show that the current objective is equivalent\nto only optimizing the precision side of the caption set generated by the model\nand therefore overlooks the recall side. Empirical result shows that the model\ntrained by this objective tends to get lower score on the recall side. We\npropose to add a sequence-level exploration term to the current objective to\nboost recall. It guides the model to explore more plausible captions in the\ntraining. In this way, the proposed objective takes both the precision and\nrecall sides of generated captions into account. Experiments show the\neffectiveness of the proposed method on both video and image captioning\ndatasets."}, {"title": "Violin: A Large-Scale Dataset for Video-and-Language Inference", "authors": "Jingzhou Liu, Wenhu Chen, Yu Cheng, Zhe Gan, Licheng Yu, Yiming Yang, Jingjing Liu", "link": "https://arxiv.org/abs/2003.11618", "summary": "We introduce a new task, Video-and-Language Inference, for joint multimodal\nunderstanding of video and text. Given a video clip with aligned subtitles as\npremise, paired with a natural language hypothesis based on the video content,\na model needs to infer whether the hypothesis is entailed or contradicted by\nthe given video clip. A new large-scale dataset, named Violin\n(VIdeO-and-Language INference), is introduced for this task, which consists of\n95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours\nof video. These video clips contain rich content with diverse temporal\ndynamics, event shifts, and people interactions, collected from two sources:\n(i) popular TV shows, and (ii) movie clips from YouTube channels. In order to\naddress our new multimodal inference task, a model is required to possess\nsophisticated reasoning skills, from surface-level grounding (e.g., identifying\nobjects and characters in the video) to in-depth commonsense reasoning (e.g.,\ninferring causal relations of events in the video). We present a detailed\nanalysis of the dataset and an extensive evaluation over many strong baselines,\nproviding valuable insights on the challenges of this new task."}, {"title": "RiFeGAN: Rich Feature Generation for Text-to-Image Synthesis From Prior Knowledge", "authors": "Jun Cheng, Fuxiang Wu, Yanling Tian, Lei Wang, Dapeng Tao"}, {"title": "Graph Structured Network for Image-Text Matching", "authors": "Chunxiao Liu, Zhendong Mao, Tianzhu Zhang, Hongtao Xie, Bin Wang, Yongdong Zhang", "link": "https://arxiv.org/abs/2004.00277", "summary": "Image-text matching has received growing interest since it bridges vision and\nlanguage. The key challenge lies in how to learn correspondence between image\nand text. Existing works learn coarse correspondence based on object\nco-occurrence statistics, while failing to learn fine-grained phrase\ncorrespondence. In this paper, we present a novel Graph Structured Matching\nNetwork (GSMN) to learn fine-grained correspondence. The GSMN explicitly models\nobject, relation and attribute as a structured phrase, which not only allows to\nlearn correspondence of object, relation and attribute separately, but also\nbenefits to learn fine-grained correspondence of structured phrase. This is\nachieved by node-level matching and structure-level matching. The node-level\nmatching associates each node with its relevant nodes from another modality,\nwhere the node can be object, relation or attribute. The associated nodes then\njointly infer fine-grained correspondence by fusing neighborhood associations\nat structure-level matching. Comprehensive experiments show that GSMN\noutperforms state-of-the-art methods on benchmarks, with relative Recall@1\nimprovements of nearly 7% and 2% on Flickr30K and MSCOCO, respectively. Code\nwill be released at: https://github.com/CrossmodalGroup/GSMN."}, {"title": "Straight to the Point: Fast-Forwarding Videos via Reinforcement Learning Using Textual Data", "authors": "Washington Ramos, Michel Silva, Edson Araujo, Leandro Soriano Marcolino, Erickson Nascimento", "link": "https://arxiv.org/abs/2003.14229", "summary": "The rapid increase in the amount of published visual data and the limited\ntime of users bring the demand for processing untrimmed videos to produce\nshorter versions that convey the same information. Despite the remarkable\nprogress that has been made by summarization methods, most of them can only\nselect a few frames or skims, which creates visual gaps and breaks the video\ncontext. In this paper, we present a novel methodology based on a reinforcement\nlearning formulation to accelerate instructional videos. Our approach can\nadaptively select frames that are not relevant to convey the information\nwithout creating gaps in the final video. Our agent is textually and visually\noriented to select which frames to remove to shrink the input video.\nAdditionally, we propose a novel network, called Visually-guided Document\nAttention Network (VDAN), able to generate a highly discriminative embedding\nspace to represent both textual and visual data. Our experiments show that our\nmethod achieves the best performance in terms of F1 Score and coverage at the\nvideo segment level."}, {"title": "Multi-Modality Cross Attention Network for Image and Sentence Matching", "authors": "Xi Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, Feng Wu"}, {"title": "Generalized ODIN: Detecting Out-of-Distribution Image Without Learning From Out-of-Distribution Data", "authors": "Yen-Chang Hsu, Yilin Shen, Hongxia Jin, Zsolt Kira", "link": "https://arxiv.org/abs/2002.11297", "summary": "Deep neural networks have attained remarkable performance when applied to\ndata that comes from the same distribution as that of the training set, but can\nsignificantly degrade otherwise. Therefore, detecting whether an example is\nout-of-distribution (OoD) is crucial to enable a system that can reject such\nsamples or alert users. Recent works have made significant progress on OoD\nbenchmarks consisting of small image datasets. However, many recent methods\nbased on neural networks rely on training or tuning with both in-distribution\nand out-of-distribution data. The latter is generally hard to define a-priori,\nand its selection can easily bias the learning. We base our work on a popular\nmethod ODIN, proposing two strategies for freeing it from the needs of tuning\nwith OoD data, while improving its OoD detection performance. We specifically\npropose to decompose confidence scoring as well as a modified input\npre-processing method. We show that both of these significantly help in\ndetection performance. Our further analysis on a larger scale image dataset\nshows that the two types of distribution shifts, specifically semantic shift\nand non-semantic shift, present a significant difference in the difficulty of\nthe problem, providing an analysis of when ODIN-like strategies do or do not\nwork."}, {"title": "Learning Augmentation Network via Influence Functions", "authors": "Donghoon Lee, Hyunsin Park, Trung Pham, Chang D. Yoo"}, {"title": "X-Linear Attention Networks for Image Captioning", "authors": "Yingwei Pan, Ting Yao, Yehao Li, Tao Mei", "link": "https://arxiv.org/abs/2003.14080", "summary": "Recent progress on fine-grained visual recognition and visual question\nanswering has featured Bilinear Pooling, which effectively models the 2$^{nd}$\norder interactions across multi-modal inputs. Nevertheless, there has not been\nevidence in support of building such interactions concurrently with attention\nmechanism for image captioning. In this paper, we introduce a unified attention\nblock -- X-Linear attention block, that fully employs bilinear pooling to\nselectively capitalize on visual information or perform multi-modal reasoning.\nTechnically, X-Linear attention block simultaneously exploits both the spatial\nand channel-wise bilinear attention distributions to capture the 2$^{nd}$ order\ninteractions between the input single-modal or multi-modal features. Higher and\neven infinity order feature interactions are readily modeled through stacking\nmultiple X-Linear attention blocks and equipping the block with Exponential\nLinear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we\npresent X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates\nX-Linear attention block(s) into image encoder and sentence decoder of image\ncaptioning model to leverage higher order intra- and inter-modal interactions.\nThe experiments on COCO benchmark demonstrate that our X-LAN obtains to-date\nthe best published CIDEr performance of 132.0% on COCO Karpathy test split.\nWhen further endowing Transformer with X-Linear attention blocks, CIDEr is\nboosted up to 132.8%. Source code is available at\n\\url{https://github.com/Panda-Peter/image-captioning}."}, {"title": "Unsupervised Person Re-Identification via Multi-Label Classification", "authors": "Dongkai Wang, Shiliang Zhang", "link": "https://arxiv.org/abs/2004.09228", "summary": "The challenge of unsupervised person re-identification (ReID) lies in\nlearning discriminative features without true labels. This paper formulates\nunsupervised person ReID as a multi-label classification task to progressively\nseek true labels. Our method starts by assigning each person image with a\nsingle-class label, then evolves to multi-label classification by leveraging\nthe updated ReID model for label prediction. The label prediction comprises\nsimilarity computation and cycle consistency to ensure the quality of predicted\nlabels. To boost the ReID model training efficiency in multi-label\nclassification, we further propose the memory-based multi-label classification\nloss (MMCL). MMCL works with memory-based non-parametric classifier and\nintegrates multi-label classification and single-label classification in a\nunified framework. Our label prediction and MMCL work iteratively and\nsubstantially boost the ReID performance. Experiments on several large-scale\nperson ReID datasets demonstrate the superiority of our method in unsupervised\nperson ReID. Our method also allows to use labeled person images in other\ndomains. Under this transfer learning setting, our method also achieves\nstate-of-the-art performance."}, {"title": "Overcoming Classifier Imbalance for Long-Tail Object Detection With Balanced Group Softmax", "authors": "Yu Li, Tao Wang, Bingyi Kang, Sheng Tang, Chunfeng Wang, Jintao Li, Jiashi Feng"}, {"title": "What You See is What You Get: Exploiting Visibility for 3D Object Detection", "authors": "Peiyun Hu, Jason Ziglar, David Held, Deva Ramanan", "link": "https://arxiv.org/abs/1912.04986", "summary": "Recent advances in 3D sensing have created unique challenges for computer\nvision. One fundamental challenge is finding a good representation for 3D\nsensor data. Most popular representations (such as PointNet) are proposed in\nthe context of processing truly 3D data (e.g. points sampled from mesh models),\nignoring the fact that 3D sensored data such as a LiDAR sweep is in fact 2.5D.\nWe argue that representing 2.5D data as collections of (x, y, z) points\nfundamentally destroys hidden information about freespace. In this paper, we\ndemonstrate such knowledge can be efficiently recovered through 3D raycasting\nand readily incorporated into batch-based gradient learning. We describe a\nsimple approach to augmenting voxel-based networks with visibility: we add a\nvoxelized visibility map as an additional input stream. In addition, we show\nthat visibility can be combined with two crucial modifications common to\nstate-of-the-art 3D detectors: synthetic data augmentation of virtual objects\nand temporal aggregation of LiDAR sweeps over multiple time frames. On the\nNuScenes 3D detection benchmark, we show that, by adding an additional stream\nfor visibility input, we can significantly improve the overall detection\naccuracy of a state-of-the-art 3D detector."}, {"title": "Deep Structure-Revealed Network for Texture Recognition", "authors": "Wei Zhai, Yang Cao, Zheng-Jun Zha, HaiYong Xie, Feng Wu"}, {"title": "Online Knowledge Distillation via Collaborative Learning", "authors": "Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, Ping Luo", "link": "", "summary": ""}, {"title": "Dynamic Convolution: Attention Over Convolution Kernels", "authors": "Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, Zicheng Liu", "link": "https://arxiv.org/abs/1912.03458", "summary": "Light-weight convolutional neural networks (CNNs) suffer performance\ndegradation as their low computational budgets constrain both the depth (number\nof convolution layers) and the width (number of channels) of CNNs, resulting in\nlimited representation capability. To address this issue, we present Dynamic\nConvolution, a new design that increases model complexity without increasing\nthe network depth or width. Instead of using a single convolution kernel per\nlayer, dynamic convolution aggregates multiple parallel convolution kernels\ndynamically based upon their attentions, which are input dependent. Assembling\nmultiple kernels is not only computationally efficient due to the small kernel\nsize, but also has more representation power since these kernels are aggregated\nin a non-linear way via attention. By simply using dynamic convolution for the\nstate-of-the-art architecture MobileNetV3-Small, the top-1 accuracy of ImageNet\nclassification is boosted by 2.9% with only 4% additional FLOPs and 2.9 AP gain\nis achieved on COCO keypoint detection."}, {"title": "3DSSD: Point-Based 3D Single Stage Object Detector", "authors": "Zetong Yang, Yanan Sun, Shu Liu, Jiaya Jia", "link": "", "summary": ""}, {"title": "Deep Degradation Prior for Low-Quality Image Classification", "authors": "Yang Wang, Yang Cao, Zheng-Jun Zha, Jing Zhang, Zhiwei Xiong"}, {"title": "ViBE: Dressing for Diverse Body Shapes", "authors": "Wei-Lin Hsiao, Kristen Grauman", "link": "https://arxiv.org/abs/1912.06697", "summary": "Body shape plays an important role in determining what garments will best\nsuit a given person, yet today's clothing recommendation methods take a \"one\nshape fits all\" approach. These body-agnostic vision methods and datasets are a\nbarrier to inclusion, ill-equipped to provide good suggestions for diverse body\nshapes. We introduce ViBE, a VIsual Body-aware Embedding that captures\nclothing's affinity with different body shapes. Given an image of a person, the\nproposed embedding identifies garments that will flatter her specific body\nshape. We show how to learn the embedding from an online catalog displaying\nfashion models of various shapes and sizes wearing the products, and we devise\na method to explain the algorithm's suggestions for well-fitting garments. We\napply our approach to a dataset of diverse subjects, and demonstrate its strong\nadvantages over the status quo body-agnostic recommendation, both according to\nautomated metrics and human opinion."}, {"title": "Don\u2019t Judge an Object by Its Context: Learning to Overcome Contextual Bias", "authors": "Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, Deepti Ghadiyaram", "link": "https://arxiv.org/abs/2001.03152", "summary": "Existing models often leverage co-occurrences between objects and their\ncontext to improve recognition accuracy. However, strongly relying on context\nrisks a model's generalizability, especially when typical co-occurrence\npatterns are absent. This work focuses on addressing such contextual biases to\nimprove the robustness of the learnt feature representations. Our goal is to\naccurately recognize a category in the absence of its context, without\ncompromising on performance when it co-occurs with context. Our key idea is to\ndecorrelate feature representations of a category from its co-occurring\ncontext. We achieve this by learning a feature subspace that explicitly\nrepresents categories occurring in the absence of context along side a joint\nfeature subspace that represents both categories and context. Our very simple\nyet effective method is extensible to two multi-label tasks -- object and\nattribute classification. On 4 challenging datasets, we demonstrate the\neffectiveness of our method in reducing contextual bias."}, {"title": "SESS: Self-Ensembling Semi-Supervised 3D Object Detection", "authors": "Na Zhao, Tat-Seng Chua, Gim Hee Lee", "link": "https://arxiv.org/abs/1912.11803", "summary": "The performance of existing point cloud-based 3D object detection methods\nheavily relies on large-scale high-quality 3D annotations. However, such\nannotations are often tedious and expensive to collect. Semi-supervised\nlearning is a good alternative to mitigate the data annotation issue, but has\nremained largely unexplored in 3D object detection. Inspired by the recent\nsuccess of self-ensembling technique in semi-supervised image classification\ntask, we propose SESS, a self-ensembling semi-supervised 3D object detection\nframework. Specifically, we design a thorough perturbation scheme to enhance\ngeneralization of the network on unlabeled and new unseen data. Furthermore, we\npropose three consistency losses to enforce the consistency between two sets of\npredicted 3D object proposals, to facilitate the learning of structure and\nsemantic invariances of objects. Extensive experiments conducted on SUN RGB-D\nand ScanNet datasets demonstrate the effectiveness of SESS in both inductive\nand transductive semi-supervised 3D object detection. Our SESS achieves\ncompetitive performance compared to the state-of-the-art fully-supervised\nmethod by using only 50% labeled data. Our code is available at\nhttps://github.com/Na-Z/sess."}, {"title": "Combining Detection and Tracking for Human Pose Estimation in Videos", "authors": "Manchen Wang, Joseph Tighe, Davide Modolo", "link": "https://arxiv.org/abs/2003.13743", "summary": "We propose a novel top-down approach that tackles the problem of multi-person\nhuman pose estimation and tracking in videos. In contrast to existing top-down\napproaches, our method is not limited by the performance of its person detector\nand can predict the poses of person instances not localized. It achieves this\ncapability by propagating known person locations forward and backward in time\nand searching for poses in those regions. Our approach consists of three\ncomponents: (i) a Clip Tracking Network that performs body joint detection and\ntracking simultaneously on small video clips; (ii) a Video Tracking Pipeline\nthat merges the fixed-length tracklets produced by the Clip Tracking Network to\narbitrary length tracks; and (iii) a Spatial-Temporal Merging procedure that\nrefines the joint locations based on spatial and temporal smoothing terms.\nThanks to the precision of our Clip Tracking Network and our merging procedure,\nour approach produces very accurate joint predictions and can fix common\nmistakes on hard scenarios like heavily entangled people. Our approach achieves\nstate-of-the-art results on both joint detection and tracking, on both the\nPoseTrack 2017 and 2018 datasets, and against all top-down and bottom-down\napproaches."}, {"title": "SAPIEN: A SimulAted Part-Based Interactive ENvironment", "authors": "Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas, Hao Su", "link": "", "summary": ""}, {"title": "RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds", "authors": "Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, Andrew Markham", "link": "https://arxiv.org/abs/1911.11236", "summary": "We study the problem of efficient semantic segmentation for large-scale 3D\npoint clouds. By relying on expensive sampling techniques or computationally\nheavy pre/post-processing steps, most existing approaches are only able to be\ntrained and operate over small-scale point clouds. In this paper, we introduce\nRandLA-Net, an efficient and lightweight neural architecture to directly infer\nper-point semantics for large-scale point clouds. The key to our approach is to\nuse random point sampling instead of more complex point selection approaches.\nAlthough remarkably computation and memory efficient, random sampling can\ndiscard key features by chance. To overcome this, we introduce a novel local\nfeature aggregation module to progressively increase the receptive field for\neach 3D point, thereby effectively preserving geometric details. Extensive\nexperiments show that our RandLA-Net can process 1 million points in a single\npass with up to 200X faster than existing approaches. Moreover, our RandLA-Net\nclearly surpasses state-of-the-art approaches for semantic segmentation on two\nlarge-scale benchmarks Semantic3D and SemanticKITTI."}, {"title": "SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving", "authors": "Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru Erhan, Sean Rafferty, Henrik Kretzschmar", "link": "https://arxiv.org/abs/2005.03844", "summary": "Autonomous driving system development is critically dependent on the ability\nto replay complex and diverse traffic scenarios in simulation. In such\nscenarios, the ability to accurately simulate the vehicle sensors such as\ncameras, lidar or radar is essential. However, current sensor simulators\nleverage gaming engines such as Unreal or Unity, requiring manual creation of\nenvironments, objects and material properties. Such approaches have limited\nscalability and fail to produce realistic approximations of camera, lidar, and\nradar data without significant additional work.\n  In this paper, we present a simple yet effective approach to generate\nrealistic scenario sensor data, based only on a limited amount of lidar and\ncamera data collected by an autonomous vehicle. Our approach uses\ntexture-mapped surfels to efficiently reconstruct the scene from an initial\nvehicle pass or set of passes, preserving rich information about object 3D\ngeometry and appearance, as well as the scene conditions. We then leverage a\nSurfelGAN network to reconstruct realistic camera images for novel positions\nand orientations of the self-driving vehicle and moving objects in the scene.\nWe demonstrate our approach on the Waymo Open Dataset and show that it can\nsynthesize realistic camera data for simulated scenarios. We also create a\nnovel dataset that contains cases in which two self-driving vehicles observe\nthe same scene at the same time. We use this dataset to provide additional\nevaluation and demonstrate the usefulness of our SurfelGAN model."}, {"title": "A Programmatic and Semantic Approach to Explaining and Debugging Neural Network Based Object Detectors", "authors": "Edward Kim, Divya Gopinath, Corina P\u0103s\u0103reanu, Sanjit A. Seshia", "link": "https://arxiv.org/abs/1912.00289", "summary": "Even as deep neural networks have become very effective for tasks in vision\nand perception, it remains difficult to explain and debug their behavior. In\nthis paper, we present a programmatic and semantic approach to explaining,\nunderstanding, and debugging the correct and incorrect behaviors of a neural\nnetwork based perception system. Our approach is semantic in that it employs a\nhigh-level representation of the distribution of environment scenarios that the\ndetector is intended to work on. It is programmatic in that the representation\nis a program in a domain-specific probabilistic programming language using\nwhich synthetic data can be generated to train and test the neural network. We\npresent a framework that assesses the performance of the neural network to\nidentify correct and incorrect detections, extracts rules from those results\nthat semantically characterizes the correct and incorrect scenarios, and then\nspecializes the probabilistic program with those rules in order to more\nprecisely characterize the scenarios in which the neural network operates\ncorrectly or not, without human intervention to identify important features. We\ndemonstrate our results using the SCENIC probabilistic programming language and\na neural network-based object detector. Our experiments show that it is\npossible to automatically generate compact rules that significantly increase\nthe correct detection rate (or conversely the incorrect detection rate) of the\nnetwork and can thus help with debugging and understanding its behavior."}, {"title": "Predicting Semantic Map Representations From Images Using Pyramid Occupancy Networks", "authors": "Thomas Roddick, Roberto Cipolla", "link": "http://arxiv.org/abs/2003.13402", "summary": "Autonomous vehicles commonly rely on highly detailed birds-eye-view maps of\ntheir environment, which capture both static elements of the scene such as road\nlayout as well as dynamic elements such as other cars and pedestrians.\nGenerating these map representations on the fly is a complex multi-stage\nprocess which incorporates many important vision-based elements, including\nground plane estimation, road segmentation and 3D object detection. In this\nwork we present a simple, unified approach for estimating maps directly from\nmonocular images using a single end-to-end deep learning architecture. For the\nmaps themselves we adopt a semantic Bayesian occupancy grid framework, allowing\nus to trivially accumulate information over multiple cameras and timesteps. We\ndemonstrate the effectiveness of our approach by evaluating against several\nchallenging baselines on the NuScenes and Argoverse datasets, and show that we\nare able to achieve a relative improvement of 9.1% and 22.3% respectively\ncompared to the best-performing existing method."}, {"title": "Efficient Derivative Computation for Cumulative B-Splines on Lie Groups", "authors": "Christiane Sommer, Vladyslav Usenko, David Schubert, Nikolaus Demmel, Daniel Cremers", "link": "https://arxiv.org/abs/1911.08860", "summary": "Continuous-time trajectory representation has recently gained popularity for\ntasks where the fusion of high-frame-rate sensors and multiple unsynchronized\ndevices is required. Lie group cumulative B-splines are a popular way of\nrepresenting continuous trajectories without singularities. They have been used\nin near real-time SLAM and odometry systems with IMU, LiDAR, regular, RGB-D and\nevent cameras, as well as for offline calibration. These applications require\nefficient computation of time derivatives (velocity, acceleration), but all\nprior works rely on a computationally suboptimal formulation. In this work we\npresent an alternative derivation of time derivatives based on recurrence\nrelations that needs $\\mathcal{O}(k)$ instead of $\\mathcal{O}(k^2)$ matrix\noperations (for a spline of order $k$) and results in simple and elegant\nexpressions. While producing the same result, the proposed approach\nsignificantly speeds up the trajectory optimization and allows for computing\nsimple analytic derivatives with respect to spline knots. The results presented\nin this paper pave the way for incorporating continuous-time trajectory\nrepresentations into more applications where real-time performance is required."}, {"title": "RL-CycleGAN: Reinforcement Learning Aware Simulation-to-Real", "authors": "Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, Mohi Khansari", "link": "", "summary": ""}, {"title": "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World", "authors": "Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong, Wenyuan Zeng, Mikita Sazanovich, Shuhan Tan, Bin Yang, Wei-Chiu Ma, Raquel Urtasun"}, {"title": "Just Go With the Flow: Self-Supervised Scene Flow Estimation", "authors": "Himangi Mittal, Brian Okorn, David Held", "link": "https://arxiv.org/abs/1912.00497", "summary": "When interacting with highly dynamic environments, scene flow allows\nautonomous systems to reason about the non-rigid motion of multiple independent\nobjects. This is of particular interest in the field of autonomous driving, in\nwhich many cars, people, bicycles, and other objects need to be accurately\ntracked. Current state-of-the-art methods require annotated scene flow data\nfrom autonomous driving scenes to train scene flow networks with supervised\nlearning. As an alternative, we present a method of training scene flow that\nuses two self-supervised losses, based on nearest neighbors and cycle\nconsistency. These self-supervised losses allow us to train our method on large\nunlabeled autonomous driving datasets; the resulting method matches current\nstate-of-the-art supervised performance using no real world annotations and\nexceeds state-of-the-art performance when combining our self-supervised\napproach with supervised learning on a smaller labeled dataset."}, {"title": "TITAN: Future Forecast Using Action Priors", "authors": "Srikanth Malla, Behzad Dariush, Chiho Choi", "link": "", "summary": ""}, {"title": "Robust Learning Through Cross-Task Consistency", "authors": "Amir R. Zamir, Alexander Sax, Nikhil Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, Leonidas J. Guibas"}, {"title": "Dynamic Refinement Network for Oriented and Densely Packed Object Detection", "authors": "Xingjia Pan, Yuqiang Ren, Kekai Sheng, Weiming Dong, Haolei Yuan, Xiaowei Guo, Chongyang Ma, Changsheng Xu", "link": "http://arxiv.org/abs/2005.09973", "summary": "Object detection has achieved remarkable progress in the past decade.\nHowever, the detection of oriented and densely packed objects remains\nchallenging because of following inherent reasons: (1) receptive fields of\nneurons are all axis-aligned and of the same shape, whereas objects are usually\nof diverse shapes and align along various directions; (2) detection models are\ntypically trained with generic knowledge and may not generalize well to handle\nspecific objects at test time; (3) the limited dataset hinders the development\non this task. To resolve the first two issues, we present a dynamic refinement\nnetwork that consists of two novel components, i.e., a feature selection module\n(FSM) and a dynamic refinement head (DRH). Our FSM enables neurons to adjust\nreceptive fields in accordance with the shapes and orientations of target\nobjects, whereas the DRH empowers our model to refine the prediction\ndynamically in an object-aware manner. To address the limited availability of\nrelated benchmarks, we collect an extensive and fully annotated dataset,\nnamely, SKU110K-R, which is relabeled with oriented bounding boxes based on\nSKU110K. We perform quantitative evaluations on several publicly available\nbenchmarks including DOTA, HRSC2016, SKU110K, and our own SKU110K-R dataset.\nExperimental results show that our method achieves consistent and substantial\ngains compared with baseline approaches. The code and dataset are available at\nhttps://github.com/Anymake/DRN_CVPR2020."}, {"title": "AOWS: Adaptive and Optimal Network Width Search With Latency Constraints", "authors": "Maxim Berman, Leonid Pishchulin, Ning Xu, Matthew B. Blaschko, G\u00e9rard Medioni"}, {"title": "High-Dimensional Convolutional Networks for Geometric Pattern Recognition", "authors": "Christopher Choy, Junha Lee, Ren\u00e9 Ranftl, Jaesik Park, Vladlen Koltun", "link": "https://arxiv.org/abs/2005.08144", "summary": "Many problems in science and engineering can be formulated in terms of\ngeometric patterns in high-dimensional spaces. We present high-dimensional\nconvolutional networks (ConvNets) for pattern recognition problems that arise\nin the context of geometric registration. We first study the effectiveness of\nconvolutional networks in detecting linear subspaces in high-dimensional spaces\nwith up to 32 dimensions: much higher dimensionality than prior applications of\nConvNets. We then apply high-dimensional ConvNets to 3D registration under\nrigid motions and image correspondence estimation. Experiments indicate that\nour high-dimensional ConvNets outperform prior approaches that relied on deep\nnetworks based on global pooling operators."}, {"title": "Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks", "authors": "Saurabh Singh, Shankar Krishnan", "link": "https://arxiv.org/abs/1911.09737", "summary": "Batch Normalization (BN) uses mini-batch statistics to normalize the\nactivations during training, introducing dependence between mini-batch\nelements. This dependency can hurt the performance if the mini-batch size is\ntoo small, or if the elements are correlated. Several alternatives, such as\nBatch Renormalization and Group Normalization (GN), have been proposed to\naddress this issue. However, they either do not match the performance of BN for\nlarge batches, or still exhibit degradation in performance for smaller batches,\nor introduce artificial constraints on the model architecture. In this paper we\npropose the Filter Response Normalization (FRN) layer, a novel combination of a\nnormalization and an activation function, that can be used as a replacement for\nother normalizations and activations. Our method operates on each activation\nchannel of each batch element independently, eliminating the dependency on\nother batch elements. Our method outperforms BN and other alternatives in a\nvariety of settings for all batch sizes. FRN layer performs $\\approx 0.7-1.0\\%$\nbetter than BN on top-1 validation accuracy with large mini-batch sizes for\nImagenet classification using InceptionV3 and ResnetV2-50 architectures.\nFurther, it performs $>1\\%$ better than GN on the same problem in the small\nmini-batch size regime. For object detection problem on COCO dataset, FRN layer\noutperforms all other methods by at least $0.3-0.5\\%$ in all batch size\nregimes."}, {"title": "Deep Iterative Surface Normal Estimation", "authors": "Jan Eric Lenssen, Christian Osendorfer, Jonathan Masci", "link": "https://arxiv.org/abs/1904.07172", "summary": "This paper presents an end-to-end differentiable algorithm for robust and\ndetail-preserving surface normal estimation on unstructured point-clouds. We\nutilize graph neural networks to iteratively parameterize an adaptive\nanisotropic kernel that produces point weights for weighted least-squares plane\nfitting in local neighborhoods. The approach retains the interpretability and\nefficiency of traditional sequential plane fitting while benefiting from\nadaptation to data set statistics through deep learning. This results in a\nstate-of-the-art surface normal estimator that is robust to noise, outliers and\npoint density variation, preserves sharp features through anisotropic kernels\nand equivariance through a local quaternion-based spatial transformer. Contrary\nto previous deep learning methods, the proposed approach does not require any\nhand-crafted features or preprocessing. It improves on the state-of-the-art\nresults while being more than two orders of magnitude faster and more parameter\nefficient."}, {"title": "Dataless Model Selection With the Deep Frame Potential", "authors": "Calvin Murdock, Simon Lucey", "link": "https://arxiv.org/abs/2003.13866", "summary": "Choosing a deep neural network architecture is a fundamental problem in\napplications that require balancing performance and parameter efficiency.\nStandard approaches rely on ad-hoc engineering or computationally expensive\nvalidation on a specific dataset. We instead attempt to quantify networks by\ntheir intrinsic capacity for unique and robust representations, enabling\nefficient architecture comparisons without requiring any data. Building upon\ntheoretical connections between deep learning and sparse approximation, we\npropose the deep frame potential: a measure of coherence that is approximately\nrelated to representation stability but has minimizers that depend only on\nnetwork structure. This provides a framework for jointly quantifying the\ncontributions of architectural hyper-parameters such as depth, width, and skip\nconnections. We validate its use as a criterion for model selection and\ndemonstrate correlation with generalization error on a variety of common\nresidual and densely connected network architectures."}, {"title": "UNAS: Differentiable Architecture Search Meets Reinforcement Learning", "authors": "Arash Vahdat, Arun Mallya, Ming-Yu Liu, Jan Kautz", "link": "https://arxiv.org/abs/1912.07651", "summary": "Neural architecture search (NAS) aims to discover network architectures with\ndesired properties such as high accuracy or low latency. Recently,\ndifferentiable NAS (DNAS) has demonstrated promising results while maintaining\na search cost orders of magnitude lower than reinforcement learning (RL) based\nNAS. However, DNAS models can only optimize differentiable loss functions in\nsearch, and they require an accurate differentiable approximation of\nnon-differentiable criteria. In this work, we present UNAS, a unified framework\nfor NAS, that encapsulates recent DNAS and RL-based approaches under one\nframework. Our framework brings the best of both worlds, and it enables us to\nsearch for architectures with both differentiable and non-differentiable\ncriteria in one unified framework while maintaining a low search cost. Further,\nwe introduce a new objective function for search based on the generalization\ngap that prevents the selection of architectures prone to overfitting. We\npresent extensive experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets\nand we perform search in two fundamentally different search spaces. We show\nthat UNAS obtains the state-of-the-art average accuracy on all three datasets\nwhen compared to the architectures searched in the DARTS space. Moreover, we\nshow that UNAS can find an efficient and accurate architecture in the\nProxylessNAS search space, that outperforms existing MobileNetV2 based\narchitectures."}, {"title": "Local Context Normalization: Revisiting Local Normalization", "authors": "Anthony Ortiz, Caleb Robinson, Dan Morris, Olac Fuentes, Christopher Kiekintveld, Md Mahmudulla Hassan, Nebojsa Jojic", "link": "https://arxiv.org/abs/1912.05845", "summary": "Normalization layers have been shown to improve convergence in deep neural\nnetworks, and even add useful inductive biases. In many vision applications the\nlocal spatial context of the features is important, but most common\nnormalization schemes including Group Normalization (GN), Instance\nNormalization (IN), and Layer Normalization (LN) normalize over the entire\nspatial dimension of a feature. This can wash out important signals and degrade\nperformance. For example, in applications that use satellite imagery, input\nimages can be arbitrarily large; consequently, it is nonsensical to normalize\nover the entire area. Positional Normalization (PN), on the other hand, only\nnormalizes over a single spatial position at a time. A natural compromise is to\nnormalize features by local context, while also taking into account group level\ninformation. In this paper, we propose Local Context Normalization (LCN): a\nnormalization layer where every feature is normalized based on a window around\nit and the filters in its group. We propose an algorithmic solution to make LCN\nefficient for arbitrary window sizes, even if every point in the image has a\nunique window. LCN outperforms its Batch Normalization (BN), GN, IN, and LN\ncounterparts for object detection, semantic segmentation, and instance\nsegmentation applications in several benchmark datasets, while keeping\nperformance independent of the batch size and facilitating transfer learning."}, {"title": "ACNe: Attentive Context Normalization for Robust Permutation-Equivariant Learning", "authors": "Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, Kwang Moo Yi", "link": "https://arxiv.org/abs/1907.02545", "summary": "Many problems in computer vision require dealing with sparse, unordered data\nin the form of point clouds. Permutation-equivariant networks have become a\npopular solution-they operate on individual data points with simple perceptrons\nand extract contextual information with global pooling. This can be achieved\nwith a simple normalization of the feature maps, a global operation that is\nunaffected by the order. In this paper, we propose Attentive Context\nNormalization (ACN), a simple yet effective technique to build\npermutation-equivariant networks robust to outliers. Specifically, we show how\nto normalize the feature maps with weights that are estimated within the\nnetwork, excluding outliers from this normalization. We use this mechanism to\nleverage two types of attention: local and global-by combining them, our method\nis able to find the essential data points in high-dimensional space to solve a\ngiven task. We demonstrate through extensive experiments that our approach,\nwhich we call Attentive Context Networks (ACNe), provides a significant leap in\nperformance compared to the state-of-the-art on camera pose estimation, robust\nfitting, and point cloud classification under noise and outliers. Source code:\nhttps://github.com/vcg-uvic/acne."}, {"title": "Learning Situational Driving", "authors": "Eshed Ohn-Bar, Aditya Prakash, Aseem Behl, Kashyap Chitta, Andreas Geiger"}, {"title": "From Depth What Can You See? Depth Completion via Auxiliary Image Reconstruction", "authors": "Kaiyue Lu, Nick Barnes, Saeed Anwar, Liang Zheng"}, {"title": "Symmetry and Group in Attribute-Object Compositions", "authors": "Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu", "link": "https://arxiv.org/abs/2004.00587", "summary": "Attributes and objects can compose diverse compositions. To model the\ncompositional nature of these general concepts, it is a good choice to learn\nthem through transformations, such as coupling and decoupling. However, complex\ntransformations need to satisfy specific principles to guarantee the\nrationality. In this paper, we first propose a previously ignored principle of\nattribute-object transformation: Symmetry. For example, coupling peeled-apple\nwith attribute peeled should result in peeled-apple, and decoupling peeled from\napple should still output apple. Incorporating the symmetry principle, a\ntransformation framework inspired by group theory is built, i.e. SymNet. SymNet\nconsists of two modules, Coupling Network and Decoupling Network. With the\ngroup axioms and symmetry property as objectives, we adopt Deep Neural Networks\nto implement SymNet and train it in an end-to-end paradigm. Moreover, we\npropose a Relative Moving Distance (RMD) based recognition method to utilize\nthe attribute change instead of the attribute pattern itself to classify\nattributes. Our symmetry learning can be utilized for the Compositional\nZero-Shot Learning task and outperforms the state-of-the-art on widely-used\nbenchmarks. Code is available at https://github.com/DirtyHarryLYL/SymNet."}, {"title": "Noise-Aware Fully Webly Supervised Object Detection", "authors": "Yunhang Shen, Rongrong Ji, Zhiwei Chen, Xiaopeng Hong, Feng Zheng, Jianzhuang Liu, Mingliang Xu, Qi Tian"}, {"title": "3D Part Guided Image Editing for Fine-Grained Object Understanding", "authors": "Zongdai Liu, Feixiang Lu, Peng Wang, Hui Miao, Liangjun Zhang, Ruigang Yang, Bin Zhou"}, {"title": "STINet: Spatio-Temporal-Interactive Network for Pedestrian Detection and Trajectory Prediction", "authors": "Zhishuai Zhang, Jiyang Gao, Junhua Mao, Yukai Liu, Dragomir Anguelov, Congcong Li", "link": "https://arxiv.org/abs/2005.04255", "summary": "Detecting pedestrians and predicting future trajectories for them are\ncritical tasks for numerous applications, such as autonomous driving. Previous\nmethods either treat the detection and prediction as separate tasks or simply\nadd a trajectory regression head on top of a detector. In this work, we present\na novel end-to-end two-stage network: Spatio-Temporal-Interactive Network\n(STINet). In addition to 3D geometry modeling of pedestrians, we model the\ntemporal information for each of the pedestrians. To do so, our method predicts\nboth current and past locations in the first stage, so that each pedestrian can\nbe linked across frames and the comprehensive spatio-temporal information can\nbe captured in the second stage. Also, we model the interaction among objects\nwith an interaction graph, to gather the information among the neighboring\nobjects. Comprehensive experiments on the Lyft Dataset and the recently\nreleased large-scale Waymo Open Dataset for both object detection and future\ntrajectory prediction validate the effectiveness of the proposed method. For\nthe Waymo Open Dataset, we achieve a bird-eyes-view (BEV) detection AP of 80.73\nand trajectory prediction average displacement error (ADE) of 33.67cm for\npedestrians, which establish the state-of-the-art for both tasks."}, {"title": "Rethinking Performance Estimation in Neural Architecture Search", "authors": "Xiawu Zheng, Rongrong Ji, Qiang Wang, Qixiang Ye, Zhenguo Li, Yonghong Tian, Qi Tian", "link": "http://arxiv.org/abs/2005.09917", "summary": "Neural architecture search (NAS) remains a challenging problem, which is\nattributed to the indispensable and time-consuming component of performance\nestimation (PE). In this paper, we provide a novel yet systematic rethinking of\nPE in a resource constrained regime, termed budgeted PE (BPE), which precisely\nand effectively estimates the performance of an architecture sampled from an\narchitecture space. Since searching an optimal BPE is extremely time-consuming\nas it requires to train a large number of networks for evaluation, we propose a\nMinimum Importance Pruning (MIP) approach. Given a dataset and a BPE search\nspace, MIP estimates the importance of hyper-parameters using random forest and\nsubsequently prunes the minimum one from the next iteration. In this way, MIP\neffectively prunes less important hyper-parameters to allocate more\ncomputational resource on more important ones, thus achieving an effective\nexploration. By combining BPE with various search algorithms including\nreinforcement learning, evolution algorithm, random search, and differentiable\narchitecture search, we achieve 1, 000x of NAS speed up with a negligible\nperformance drop comparing to the SOTA"}, {"title": "Feature-Metric Registration: A Fast Semi-Supervised Approach for Robust Point Cloud Registration Without Correspondences", "authors": "Xiaoshui Huang, Guofeng Mei, Jian Zhang", "link": "https://arxiv.org/abs/2005.01014", "summary": "We present a fast feature-metric point cloud registration framework, which\nenforces the optimisation of registration by minimising a feature-metric\nprojection error without correspondences. The advantage of the feature-metric\nprojection error is robust to noise, outliers and density difference in\ncontrast to the geometric projection error. Besides, minimising the\nfeature-metric projection error does not need to search the correspondences so\nthat the optimisation speed is fast. The principle behind the proposed method\nis that the feature difference is smallest if point clouds are aligned very\nwell. We train the proposed method in a semi-supervised or unsupervised\napproach, which requires limited or no registration label data. Experiments\ndemonstrate our method obtains higher accuracy and robustness than the\nstate-of-the-art methods. Besides, experimental results show that the proposed\nmethod can handle significant noise and density difference, and solve both\nsame-source and cross-source point cloud registration."}, {"title": "Learning Multi-View Camera Relocalization With Graph Neural Networks", "authors": "Fei Xue, Xin Wu, Shaojun Cai, Junqiu Wang"}, {"title": "MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird\u2019s Eye View Maps", "authors": "Pengxiang Wu, Siheng Chen, Dimitris N. Metaxas", "link": "https://arxiv.org/abs/2003.06754", "summary": "The ability to reliably perceive the environmental states, particularly the\nexistence of objects and their motion behavior, is crucial for autonomous\ndriving. In this work, we propose an efficient deep model, called MotionNet, to\njointly perform perception and motion prediction from 3D point clouds.\nMotionNet takes a sequence of LiDAR sweeps as input and outputs a bird's eye\nview (BEV) map, which encodes the object category and motion information in\neach grid cell. The backbone of MotionNet is a novel spatio-temporal pyramid\nnetwork, which extracts deep spatial and temporal features in a hierarchical\nfashion. To enforce the smoothness of predictions over both space and time, the\ntraining of MotionNet is further regularized with novel spatial and temporal\nconsistency losses. Extensive experiments show that the proposed method overall\noutperforms the state-of-the-arts, including the latest scene-flow- and\n3D-object-detection-based methods. This indicates the potential value of the\nproposed method serving as a backup to the bounding-box-based system, and\nproviding complementary information to the motion planner in autonomous\ndriving. Code is available at https://github.com/pxiangwu/MotionNet."}, {"title": "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "authors": "Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang, Wanli Ouyang", "link": "https://arxiv.org/abs/2001.01233", "summary": "Neural Architecture Search (NAS) achieves significant progress in many\ncomputer vision tasks. While many methods have been proposed to improve the\nefficiency of NAS, the search progress is still laborious because training and\nevaluating plausible architectures over large search space is time-consuming.\nAssessing network candidates under a proxy (i.e., computationally reduced\nsetting) thus becomes inevitable. In this paper, we observe that most existing\nproxies exhibit different behaviors in maintaining the rank consistency among\nnetwork candidates. In particular, some proxies can be more reliable -- the\nrank of candidates does not differ much comparing their reduced setting\nperformance and final performance. In this paper, we systematically investigate\nsome widely adopted reduction factors and report our observations. Inspired by\nthese observations, we present a reliable proxy and further formulate a\nhierarchical proxy strategy. The strategy spends more computations on candidate\nnetworks that are potentially more accurate, while discards unpromising ones in\nearly stage with a fast proxy. This leads to an economical evolutionary-based\nNAS (EcoNAS), which achieves an impressive 400x search time reduction in\ncomparison to the evolutionary-based state of the art (8 vs. 3150 GPU days).\nSome new proxies led by our observations can also be applied to accelerate\nother NAS methods while still able to discover good candidate networks with\nperformance matching those found by previous proxy strategies."}, {"title": "Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection", "authors": "Jianyuan Guo, Kai Han, Yunhe Wang, Chao Zhang, Zhaohui Yang, Han Wu, Xinghao Chen, Chang Xu", "link": "https://arxiv.org/abs/2003.11818", "summary": "Neural Architecture Search (NAS) has achieved great success in image\nclassification task. Some recent works have managed to explore the automatic\ndesign of efficient backbone or feature fusion layer for object detection.\nHowever, these methods focus on searching only one certain component of object\ndetector while leaving others manually designed. We identify the inconsistency\nbetween searched component and manually designed ones would withhold the\ndetector of stronger performance. To this end, we propose a hierarchical\ntrinity search framework to simultaneously discover efficient architectures for\nall components (i.e. backbone, neck, and head) of object detector in an\nend-to-end manner. In addition, we empirically reveal that different parts of\nthe detector prefer different operators. Motivated by this, we employ a novel\nscheme to automatically screen different sub search spaces for different\ncomponents so as to perform the end-to-end search for each component on the\ncorresponding sub search space efficiently. Without bells and whistles, our\nsearched architecture, namely Hit-Detector, achieves 41.4\\% mAP on COCO minival\nset with 27M parameters. Our implementation is available at\nhttps://github.com/ggjy/HitDet.pytorch."}, {"title": "Geometrically Principled Connections in Graph Neural Networks", "authors": "Shunwang Gong, Mehdi Bahri, Michael M. Bronstein, Stefanos Zafeiriou", "link": "https://arxiv.org/abs/2004.02658", "summary": "Graph convolution operators bring the advantages of deep learning to a\nvariety of graph and mesh processing tasks previously deemed out of reach. With\ntheir continued success comes the desire to design more powerful architectures,\noften by adapting existing deep learning techniques to non-Euclidean data. In\nthis paper, we argue geometry should remain the primary driving force behind\ninnovation in the emerging field of geometric deep learning. We relate graph\nneural networks to widely successful computer graphics and data approximation\nmodels: radial basis functions (RBFs). We conjecture that, like RBFs, graph\nconvolution layers would benefit from the addition of simple functions to the\npowerful convolution kernels. We introduce affine skip connections, a novel\nbuilding block formed by combining a fully connected layer with any graph\nconvolution operator. We experimentally demonstrate the effectiveness of our\ntechnique and show the improved performance is the consequence of more than the\nincreased number of parameters. Operators equipped with the affine skip\nconnection markedly outperform their base performance on every task we\nevaluated, i.e., shape reconstruction, dense shape correspondence, and graph\nclassification. We hope our simple and effective approach will serve as a solid\nbaseline and help ease future research in graph neural networks."}, {"title": "On Vocabulary Reliance in Scene Text Recognition", "authors": "Zhaoyi Wan, Jielei Zhang, Liang Zhang, Jiebo Luo, Cong Yao", "link": "https://arxiv.org/abs/2005.03959", "summary": "The pursuit of high performance on public benchmarks has been the driving\nforce for research in scene text recognition, and notable progress has been\nachieved. However, a close investigation reveals a startling fact that the\nstate-of-the-art methods perform well on images with words within vocabulary\nbut generalize poorly to images with words outside vocabulary. We call this\nphenomenon \"vocabulary reliance\". In this paper, we establish an analytical\nframework to conduct an in-depth study on the problem of vocabulary reliance in\nscene text recognition. Key findings include: (1) Vocabulary reliance is\nubiquitous, i.e., all existing algorithms more or less exhibit such\ncharacteristic; (2) Attention-based decoders prove weak in generalizing to\nwords outside vocabulary and segmentation-based decoders perform well in\nutilizing visual features; (3) Context modeling is highly coupled with the\nprediction layers. These findings provide new insights and can benefit future\nresearch in scene text recognition. Furthermore, we propose a simple yet\neffective mutual learning strategy to allow models of two families\n(attention-based and segmentation-based) to learn collaboratively. This remedy\nalleviates the problem of vocabulary reliance and improves the overall scene\ntext recognition performance."}, {"title": "Generating Accurate Pseudo-Labels in Semi-Supervised Learning and Avoiding Overconfident Predictions via Hermite Polynomial Activations", "authors": "Vishnu Suresh Lokhande, Songwong Tasneeyapant, Abhay Venkatesh, Sathya N. Ravi, Vikas Singh", "link": "https://arxiv.org/abs/1909.05479", "summary": "Rectified Linear Units (ReLUs) are among the most widely used activation\nfunction in a broad variety of tasks in vision. Recent theoretical results\nsuggest that despite their excellent practical performance, in various cases, a\nsubstitution with basis expansions (e.g., polynomials) can yield significant\nbenefits from both the optimization and generalization perspective.\nUnfortunately, the existing results remain limited to networks with a couple of\nlayers, and the practical viability of these results is not yet known.\nMotivated by some of these results, we explore the use of Hermite polynomial\nexpansions as a substitute for ReLUs in deep networks. While our experiments\nwith supervised learning do not provide a clear verdict, we find that this\nstrategy offers considerable benefits in semi-supervised learning (SSL) /\ntransductive learning settings. We carefully develop this idea and show how the\nuse of Hermite polynomials based activations can yield improvements in\npseudo-label accuracies and sizable financial savings (due to concurrent\nruntime benefits). Further, we show via theoretical analysis, that the networks\n(with Hermite activations) offer robustness to noise and other attractive\nmathematical properties."}, {"title": "GraspNet-1Billion: A Large-Scale Benchmark for General Object Grasping", "authors": "Hao-Shu Fang, Chenxi Wang, Minghao Gou, Cewu Lu", "link": "", "summary": ""}, {"title": "PFRL: Pose-Free Reinforcement Learning for 6D Pose Estimation", "authors": "Jianzhun Shao, Yuhang Jiang, Gu Wang, Zhigang Li, Xiangyang Ji", "link": "", "summary": ""}, {"title": "Through Fog High-Resolution Imaging Using Millimeter Wave Radar", "authors": "Junfeng Guan, Sohrab Madani, Suraj Jog, Saurabh Gupta, Haitham Hassanieh"}, {"title": "Disentangling Physical Dynamics From Unknown Factors for Unsupervised Video Prediction", "authors": "Vincent Le Guen, Nicolas Thome", "link": "https://arxiv.org/abs/2003.01460", "summary": "Leveraging physical knowledge described by partial differential equations\n(PDEs) is an appealing way to improve unsupervised video prediction methods.\nSince physics is too restrictive for describing the full visual content of\ngeneric videos, we introduce PhyDNet, a two-branch deep architecture, which\nexplicitly disentangles PDE dynamics from unknown complementary information. A\nsecond contribution is to propose a new recurrent physical cell (PhyCell),\ninspired from data assimilation techniques, for performing PDE-constrained\nprediction in latent space. Extensive experiments conducted on four various\ndatasets show the ability of PhyDNet to outperform state-of-the-art methods.\nAblation studies also highlight the important gain brought out by both\ndisentanglement and PDE-constrained prediction. Finally, we show that PhyDNet\npresents interesting features for dealing with missing data and long-term\nforecasting."}, {"title": "D2Det: Towards High Quality Object Detection and Instance Segmentation", "authors": "Jiale Cao, Hisham Cholakkal, Rao Muhammad Anwer, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao", "link": "", "summary": ""}, {"title": "LiDAR-Based Online 3D Video Object Detection With Graph-Based Message Passing and Spatiotemporal Transformer Attention", "authors": "Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, Ruigang Yang", "link": "https://arxiv.org/abs/2004.01389", "summary": "Existing LiDAR-based 3D object detectors usually focus on the single-frame\ndetection, while ignoring the spatiotemporal information in consecutive point\ncloud frames. In this paper, we propose an end-to-end online 3D video object\ndetector that operates on point cloud sequences. The proposed model comprises a\nspatial feature encoding component and a spatiotemporal feature aggregation\ncomponent. In the former component, a novel Pillar Message Passing Network\n(PMPNet) is proposed to encode each discrete point cloud frame. It adaptively\ncollects information for a pillar node from its neighbors by iterative message\npassing, which effectively enlarges the receptive field of the pillar feature.\nIn the latter component, we propose an Attentive Spatiotemporal Transformer GRU\n(AST-GRU) to aggregate the spatiotemporal information, which enhances the\nconventional ConvGRU with an attentive memory gating mechanism. AST-GRU\ncontains a Spatial Transformer Attention (STA) module and a Temporal\nTransformer Attention (TTA) module, which can emphasize the foreground objects\nand align the dynamic objects, respectively. Experimental results demonstrate\nthat the proposed 3D video object detector achieves state-of-the-art\nperformance on the large-scale nuScenes benchmark."}, {"title": "Orthogonal Convolutional Neural Networks", "authors": "Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, Stella X. Yu", "link": "https://arxiv.org/abs/1911.12207", "summary": "Deep convolutional neural networks are hindered by training instability and\nfeature redundancy towards further performance improvement. A promising\nsolution is to impose orthogonality on convolutional filters.\n  We develop an efficient approach to impose filter orthogonality on a\nconvolutional layer based on the doubly block-Toeplitz matrix representation of\nthe convolutional kernel instead of using the common kernel orthogonality\napproach, which we show is only necessary but not sufficient for ensuring\northogonal convolutions.\n  Our proposed orthogonal convolution requires no additional parameters and\nlittle computational overhead. This method consistently outperforms the kernel\northogonality alternative on a wide range of tasks such as image classification\nand inpainting under supervised, semi-supervised and unsupervised settings.\nFurther, it learns more diverse and expressive features with better training\nstability, robustness, and generalization. Our code is publicly available at\nhttps://github.com/samaonline/Orthogonal-Convolutional-Neural-Networks."}, {"title": "Self-Robust 3D Point Recognition via Gather-Vector Guidance", "authors": "Xiaoyi Dong, Dongdong Chen, Hang Zhou, Gang Hua, Weiming Zhang, Nenghai Yu"}, {"title": "VectorNet: Encoding HD Maps and Agent Dynamics From Vectorized Representation", "authors": "Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, Cordelia Schmid", "link": "https://arxiv.org/abs/2005.04259", "summary": "Behavior prediction in dynamic, multi-agent systems is an important problem\nin the context of self-driving cars, due to the complex representations and\ninteractions of road components, including moving agents (e.g. pedestrians and\nvehicles) and road context information (e.g. lanes, traffic lights). This paper\nintroduces VectorNet, a hierarchical graph neural network that first exploits\nthe spatial locality of individual road components represented by vectors and\nthen models the high-order interactions among all components. In contrast to\nmost recent approaches, which render trajectories of moving agents and road\ncontext information as bird-eye images and encode them with convolutional\nneural networks (ConvNets), our approach operates on a vector representation.\nBy operating on the vectorized high definition (HD) maps and agent\ntrajectories, we avoid lossy rendering and computationally intensive ConvNet\nencoding steps. To further boost VectorNet's capability in learning context\nfeatures, we propose a novel auxiliary task to recover the randomly masked out\nmap entities and agent trajectories based on their context. We evaluate\nVectorNet on our in-house behavior prediction benchmark and the recently\nreleased Argoverse forecasting dataset. Our method achieves on par or better\nperformance than the competitive rendering approach on both benchmarks while\nsaving over 70% of the model parameters with an order of magnitude reduction in\nFLOPs. It also outperforms the state of the art on the Argoverse dataset."}, {"title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks", "authors": "Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, Qinghua Hu", "link": "https://arxiv.org/abs/1910.03151", "summary": "Recently, channel attention mechanism has demonstrated to offer great\npotential in improving the performance of deep convolutional neural networks\n(CNNs). However, most existing methods dedicate to developing more\nsophisticated attention modules for achieving better performance, which\ninevitably increase model complexity. To overcome the paradox of performance\nand complexity trade-off, this paper proposes an Efficient Channel Attention\n(ECA) module, which only involves a handful of parameters while bringing clear\nperformance gain. By dissecting the channel attention module in SENet, we\nempirically show avoiding dimensionality reduction is important for learning\nchannel attention, and appropriate cross-channel interaction can preserve\nperformance while significantly decreasing model complexity. Therefore, we\npropose a local cross-channel interaction strategy without dimensionality\nreduction, which can be efficiently implemented via $1D$ convolution.\nFurthermore, we develop a method to adaptively select kernel size of $1D$\nconvolution, determining coverage of local cross-channel interaction. The\nproposed ECA module is efficient yet effective, e.g., the parameters and\ncomputations of our modules against backbone of ResNet50 are 80 vs. 24.37M and\n4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more\nthan 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on\nimage classification, object detection and instance segmentation with backbones\nof ResNets and MobileNetV2. The experimental results show our module is more\nefficient while performing favorably against its counterparts."}, {"title": "MTL-NAS: Task-Agnostic Neural Architecture Search Towards General-Purpose Multi-Task Learning", "authors": "Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, Wei Liu", "link": "https://arxiv.org/abs/2003.14058", "summary": "We propose to incorporate neural architecture search (NAS) into\ngeneral-purpose multi-task learning (GP-MTL). Existing NAS methods typically\ndefine different search spaces according to different tasks. In order to adapt\nto different task combinations (i.e., task sets), we disentangle the GP-MTL\nnetworks into single-task backbones (optionally encode the task priors), and a\nhierarchical and layerwise features sharing/fusing scheme across them. This\nenables us to design a novel and general task-agnostic search space, which\ninserts cross-task edges (i.e., feature fusion connections) into fixed\nsingle-task network backbones. Moreover, we also propose a novel single-shot\ngradient-based search algorithm that closes the performance gap between the\nsearched architectures and the final evaluation architecture. This is realized\nwith a minimum entropy regularization on the architecture weights during the\nsearch phase, which makes the architecture weights converge to near-discrete\nvalues and therefore achieves a single model. As a result, our searched model\ncan be directly used for evaluation without (re-)training from scratch. We\nperform extensive experiments using different single-task backbones on various\ntask sets, demonstrating the promising performance obtained by exploiting the\nhierarchical and layerwise features, as well as the desirable generalizability\nto different i) task sets and ii) single-task backbones. The code of our paper\nis available at https://github.com/bhpfelix/MTLNAS."}, {"title": "PnPNet: End-to-End Perception and Prediction With Tracking in the Loop", "authors": "Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, Raquel Urtasun", "link": "https://arxiv.org/abs/2005.14711", "summary": "We tackle the problem of joint perception and motion forecasting in the\ncontext of self-driving vehicles. Towards this goal we propose PnPNet, an\nend-to-end model that takes as input sequential sensor data, and outputs at\neach time step object tracks and their future trajectories. The key component\nis a novel tracking module that generates object tracks online from detections\nand exploits trajectory level features for motion forecasting. Specifically,\nthe object tracks get updated at each time step by solving both the data\nassociation problem and the trajectory estimation problem. Importantly, the\nwhole model is end-to-end trainable and benefits from joint optimization of all\ntasks. We validate PnPNet on two large-scale driving datasets, and show\nsignificant improvements over the state-of-the-art with better occlusion\nrecovery and more accurate future prediction."}, {"title": "Revisiting the Sibling Head in Object Detector", "authors": "Guanglu Song, Yu Liu, Xiaogang Wang", "link": "https://arxiv.org/abs/2003.07540", "summary": "The ``shared head for classification and localization'' (sibling head),\nfirstly denominated in Fast RCNN~\\cite{girshick2015fast}, has been leading the\nfashion of the object detection community in the past five years. This paper\nprovides the observation that the spatial misalignment between the two object\nfunctions in the sibling head can considerably hurt the training process, but\nthis misalignment can be resolved by a very simple operator called task-aware\nspatial disentanglement (TSD). Considering the classification and regression,\nTSD decouples them from the spatial dimension by generating two disentangled\nproposals for them, which are estimated by the shared proposal. This is\ninspired by the natural insight that for one instance, the features in some\nsalient area may have rich information for classification while these around\nthe boundary may be good at bounding box regression. Surprisingly, this simple\ndesign can boost all backbones and models on both MS COCO and Google OpenImage\nconsistently by ~3% mAP. Further, we propose a progressive constraint to\nenlarge the performance margin between the disentangled and the shared\nproposals, and gain ~1% more mAP. We show the \\algname{} breaks through the\nupper bound of nowadays single-model detector by a large margin (mAP 49.4 with\nResNet-101, 51.2 with SENet154), and is the core model of our 1st place\nsolution on the Google OpenImage Challenge 2019."}, {"title": "Visual Reaction: Learning to Play Catch With Your Drone", "authors": "Kuo-Hao Zeng, Roozbeh Mottaghi, Luca Weihs, Ali Farhadi", "link": "https://arxiv.org/abs/1912.02155", "summary": "In this paper we address the problem of visual reaction: the task of\ninteracting with dynamic environments where the changes in the environment are\nnot necessarily caused by the agent itself. Visual reaction entails predicting\nthe future changes in a visual environment and planning accordingly. We study\nthe problem of visual reaction in the context of playing catch with a drone in\nvisually rich synthetic environments. This is a challenging problem since the\nagent is required to learn (1) how objects with different physical properties\nand shapes move, (2) what sequence of actions should be taken according to the\nprediction, (3) how to adjust the actions based on the visual feedback from the\ndynamic environment (e.g., when objects bouncing off a wall), and (4) how to\nreason and act with an unexpected state change in a timely manner. We propose a\nnew dataset for this task, which includes 30K throws of 20 types of objects in\ndifferent directions with different forces. Our results show that our model\nthat integrates a forecaster with a planner outperforms a set of strong\nbaselines that are based on tracking as well as pure model-based and model-free\nRL baselines. The code and dataset are available at\ngithub.com/KuoHaoZeng/Visual_Reaction."}, {"title": "Prime Sample Attention in Object Detection", "authors": "Yuhang Cao, Kai Chen, Chen Change Loy, Dahua Lin", "link": "https://arxiv.org/abs/1904.04821", "summary": "It is a common paradigm in object detection frameworks to treat all samples\nequally and target at maximizing the performance on average. In this work, we\nrevisit this paradigm through a careful study on how different samples\ncontribute to the overall performance measured in terms of mAP. Our study\nsuggests that the samples in each mini-batch are neither independent nor\nequally important, and therefore a better classifier on average does not\nnecessarily mean higher mAP. Motivated by this study, we propose the notion of\nPrime Samples, those that play a key role in driving the detection performance.\nWe further develop a simple yet effective sampling and learning strategy called\nPrIme Sample Attention (PISA) that directs the focus of the training process\ntowards such samples. Our experiments demonstrate that it is often more\neffective to focus on prime samples than hard samples when training a detector.\nParticularly, On the MSCOCO dataset, PISA outperforms the random sampling\nbaseline and hard mining schemes, e.g., OHEM and Focal Loss, consistently by\naround 2% on both single-stage and two-stage detectors, even with a strong\nbackbone ResNeXt-101."}, {"title": "SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization", "authors": "Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, Xiaodan Song", "link": "https://arxiv.org/abs/1912.05027", "summary": "Convolutional neural networks typically encode an input image into a series\nof intermediate features with decreasing resolutions. While this structure is\nsuited to classification tasks, it does not perform well for tasks requiring\nsimultaneous recognition and localization (e.g., object detection). The\nencoder-decoder architectures are proposed to resolve this by applying a\ndecoder network onto a backbone model designed for classification tasks. In\nthis paper, we argue encoder-decoder architecture is ineffective in generating\nstrong multi-scale features because of the scale-decreased backbone. We propose\nSpineNet, a backbone with scale-permuted intermediate features and cross-scale\nconnections that is learned on an object detection task by Neural Architecture\nSearch. Using similar building blocks, SpineNet models outperform ResNet-FPN\nmodels by 3%+ AP at various scales while using 10-20% fewer FLOPs. In\nparticular, SpineNet-190 achieves 52.1% AP on COCO for a single model without\ntest-time augmentation, significantly outperforms prior art of detectors.\nSpineNet can transfer to classification tasks, achieving 5% top-1 accuracy\nimprovement on a challenging iNaturalist fine-grained dataset. Code is at:\nhttps://github.com/tensorflow/tpu/tree/master/models/official/detection."}, {"title": "KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent Objects", "authors": "Xingyu Liu, Rico Jonschkowski, Anelia Angelova, Kurt Konolige", "link": "https://arxiv.org/abs/1912.02805", "summary": "Estimating the 3D pose of desktop objects is crucial for applications such as\nrobotic manipulation. Many existing approaches to this problem require a depth\nmap of the object for both training and prediction, which restricts them to\nopaque, lambertian objects that produce good returns in an RGBD sensor. In this\npaper we forgo using a depth sensor in favor of raw stereo input. We address\ntwo problems: first, we establish an easy method for capturing and labeling 3D\nkeypoints on desktop objects with an RGB camera; and second, we develop a deep\nneural network, called $KeyPose$, that learns to accurately predict object\nposes using 3D keypoints, from stereo input, and works even for transparent\nobjects. To evaluate the performance of our method, we create a dataset of 15\nclear objects in five classes, with 48K 3D-keypoint labeled images. We train\nboth instance and category models, and show generalization to new textures,\nposes, and objects. KeyPose surpasses state-of-the-art performance in 3D pose\nestimation on this dataset by factors of 1.5 to 3.5, even in cases where the\ncompeting method is provided with ground-truth depth. Stereo input is essential\nfor this performance as it improves results compared to using monocular input\nby a factor of 2. We will release a public version of the data capture and\nlabeling pipeline, the transparent object database, and the KeyPose models and\nevaluation code. Project website: https://sites.google.com/corp/view/keypose."}, {"title": "SegGCN: Efficient 3D Point Cloud Segmentation With Fuzzy Spherical Kernel", "authors": "Huan Lei, Naveed Akhtar, Ajmal Mian"}, {"title": "nuScenes: A Multimodal Dataset for Autonomous Driving", "authors": "Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom", "link": "https://arxiv.org/abs/1903.11027", "summary": "Robust detection and tracking of objects is crucial for the deployment of\nautonomous vehicle technology. Image based benchmark datasets have driven\ndevelopment in computer vision tasks such as object detection, tracking and\nsegmentation of agents in the environment. Most autonomous vehicles, however,\ncarry a combination of cameras and range sensors such as lidar and radar. As\nmachine learning based methods for detection and tracking become more\nprevalent, there is a need to train and evaluate such methods on datasets\ncontaining range sensor data along with images. In this work we present\nnuTonomy scenes (nuScenes), the first dataset to carry the full autonomous\nvehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree\nfield of view. nuScenes comprises 1000 scenes, each 20s long and fully\nannotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as\nmany annotations and 100x as many images as the pioneering KITTI dataset. We\ndefine novel 3D detection and tracking metrics. We also provide careful dataset\nanalysis as well as baselines for lidar and image based detection and tracking.\nData, development kit and more information are available online."}, {"title": "PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF Pose Estimation", "authors": "Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, Jian Sun", "link": "", "summary": ""}, {"title": "Probabilistic Pixel-Adaptive Refinement Networks", "authors": "Anne S. Wannenwetsch, Stefan Roth", "link": "https://arxiv.org/abs/2003.14407", "summary": "Encoder-decoder networks have found widespread use in various dense\nprediction tasks. However, the strong reduction of spatial resolution in the\nencoder leads to a loss of location information as well as boundary artifacts.\nTo address this, image-adaptive post-processing methods have shown beneficial\nby leveraging the high-resolution input image(s) as guidance data. We extend\nsuch approaches by considering an important orthogonal source of information:\nthe network's confidence in its own predictions. We introduce probabilistic\npixel-adaptive convolutions (PPACs), which not only depend on image guidance\ndata for filtering, but also respect the reliability of per-pixel predictions.\nAs such, PPACs allow for image-adaptive smoothing and simultaneously\npropagating pixels of high confidence into less reliable regions, while\nrespecting object boundaries. We demonstrate their utility in refinement\nnetworks for optical flow and semantic segmentation, where PPACs lead to a\nclear reduction in boundary artifacts. Moreover, our proposed refinement step\nis able to substantially improve the accuracy on various widely used\nbenchmarks."}, {"title": "Discovering Human Interactions With Novel Objects via Zero-Shot Learning", "authors": "Suchen Wang, Kim-Hui Yap, Junsong Yuan, Yap-Peng Tan"}, {"title": "Equalization Loss for Long-Tailed Object Recognition", "authors": "Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, Junjie Yan", "link": "https://arxiv.org/abs/2003.05176", "summary": "Object recognition techniques using convolutional neural networks (CNN) have\nachieved great success. However, state-of-the-art object detection methods\nstill perform poorly on large vocabulary and long-tailed datasets, e.g. LVIS.\nIn this work, we analyze this problem from a novel perspective: each positive\nsample of one category can be seen as a negative sample for other categories,\nmaking the tail categories receive more discouraging gradients. Based on it, we\npropose a simple but effective loss, named equalization loss, to tackle the\nproblem of long-tailed rare categories by simply ignoring those gradients for\nrare categories. The equalization loss protects the learning of rare categories\nfrom being at a disadvantage during the network parameter updating. Thus the\nmodel is capable of learning better discriminative features for objects of rare\nclasses. Without any bells and whistles, our method achieves AP gains of 4.1%\nand 4.8% for the rare and common categories on the challenging LVIS benchmark,\ncompared to the Mask R-CNN baseline. With the utilization of the effective\nequalization loss, we finally won the 1st place in the LVIS Challenge 2019.\nCode has been made available at: https: //github.com/tztztztztz/eql.detectron2"}, {"title": "Learning Depth-Guided Convolutions for Monocular 3D Object Detection", "authors": "Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, Ping Luo", "link": "https://arxiv.org/abs/1912.04799", "summary": "3D object detection from a single image without LiDAR is a challenging task\ndue to the lack of accurate depth information. Conventional 2D convolutions are\nunsuitable for this task because they fail to capture local object and its\nscale information, which are vital for 3D object detection. To better represent\n3D structure, prior arts typically transform depth maps estimated from 2D\nimages into a pseudo-LiDAR representation, and then apply existing 3D\npoint-cloud based object detectors. However, their results depend heavily on\nthe accuracy of the estimated depth maps, resulting in suboptimal performance.\nIn this work, instead of using pseudo-LiDAR representation, we improve the\nfundamental 2D fully convolutions by proposing a new local convolutional\nnetwork (LCN), termed Depth-guided Dynamic-Depthwise-Dilated LCN (D$^4$LCN),\nwhere the filters and their receptive fields can be automatically learned from\nimage-based depth maps, making different pixels of different images have\ndifferent filters. D$^4$LCN overcomes the limitation of conventional 2D\nconvolutions and narrows the gap between image representation and 3D point\ncloud representation. Extensive experiments show that D$^4$LCN outperforms\nexisting works by large margins. For example, the relative improvement of\nD$^4$LCN against the state-of-the-art on KITTI is 9.1\\% in the moderate\nsetting. The code is available at https://github.com/dingmyu/D4LCN."}, {"title": "Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather", "authors": "Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Kraus, Werner Ritter, Klaus Dietmayer, Felix Heide", "link": "https://arxiv.org/abs/1902.08913", "summary": "The fusion of multimodal sensor streams, such as camera, lidar, and radar\nmeasurements, plays a critical role in object detection for autonomous\nvehicles, which base their decision making on these inputs. While existing\nmethods exploit redundant information under good conditions, they fail to do\nthis in adverse weather where the sensory streams can be asymmetrically\ndistorted. These rare ``edge-case'' scenarios are not represented in available\ndatasets, and existing fusion architectures are not designed to handle them. To\naddress this data challenge we present a novel multimodal dataset acquired by\nover 10,000~km of driving in northern Europe. Although this dataset is the\nfirst large multimodal dataset in adverse weather, with 100k labels for lidar,\ncamera, radar and gated NIR sensors, it does not facilitate training as extreme\nweather is rare. To this end, we present a deep fusion network for robust\nfusion without a large corpus of labeled training data covering all asymmetric\ndistortions. Departing from proposal-level fusion, we propose a single-shot\nmodel that adaptively fuses features, driven by measurement entropy. We\nvalidate the proposed method, trained on clean data, on our extensive\nvalidation dataset. The dataset and all models will be published."}, {"title": "Don\u2019t Even Look Once: Synthesizing Features for Zero-Shot Detection", "authors": "Pengkai Zhu, Hanxiao Wang, Venkatesh Saligrama", "link": "https://arxiv.org/abs/1911.07933", "summary": "Zero-shot detection, namely, localizing both seen and unseen objects,\nincreasingly gains importance for large-scale applications, with large number\nof object classes, since, collecting sufficient annotated data with ground\ntruth bounding boxes is simply not scalable. While vanilla deep neural networks\ndeliver high performance for objects available during training, unseen object\ndetection degrades significantly. At a fundamental level, while vanilla\ndetectors are capable of proposing bounding boxes, which include unseen\nobjects, they are often incapable of assigning high-confidence to unseen\nobjects, due to the inherent precision/recall tradeoffs that requires rejecting\nbackground objects. We propose a novel detection algorithm Dont Even Look Once\n(DELO), that synthesizes visual features for unseen objects and augments\nexisting training algorithms to incorporate unseen object detection. Our\nproposed scheme is evaluated on Pascal VOC and MSCOCO, and we demonstrate\nsignificant improvements in test accuracy over vanilla and other state-of-art\nzero-shot detectors"}, {"title": "EPOS: Estimating 6D Pose of Objects With Symmetries", "authors": "Tom\u00e1\u0161 Hoda\u0148, D\u00e1niel Bar\u00e1th, Ji\u0159\u00ed Matas", "link": "https://arxiv.org/abs/2004.00605", "summary": "We present a new method for estimating the 6D pose of rigid objects with\navailable 3D models from a single RGB input image. The method is applicable to\na broad range of objects, including challenging ones with global or partial\nsymmetries. An object is represented by compact surface fragments which allow\nhandling symmetries in a systematic manner. Correspondences between densely\nsampled pixels and the fragments are predicted using an encoder-decoder\nnetwork. At each pixel, the network predicts: (i) the probability of each\nobject's presence, (ii) the probability of the fragments given the object's\npresence, and (iii) the precise 3D location on each fragment. A data-dependent\nnumber of corresponding 3D locations is selected per pixel, and poses of\npossibly multiple object instances are estimated using a robust and efficient\nvariant of the PnP-RANSAC algorithm. In the BOP Challenge 2019, the method\noutperforms all RGB and most RGB-D and D methods on the T-LESS and LM-O\ndatasets. On the YCB-V dataset, it is superior to all competitors, with a large\nmargin over the second-best RGB method. Source code is at:\ncmp.felk.cvut.cz/epos."}, {"title": "Train in Germany, Test in the USA: Making 3D Object Detectors Generalize", "authors": "Yan Wang, Xiangyu Chen, Yurong You, Li Erran Li, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger, Wei-Lun Chao", "link": "https://arxiv.org/abs/2005.08139", "summary": "In the domain of autonomous driving, deep learning has substantially improved\nthe 3D object detection accuracy for LiDAR and stereo camera data alike. While\ndeep networks are great at generalization, they are also notorious to over-fit\nto all kinds of spurious artifacts, such as brightness, car sizes and models,\nthat may appear consistently throughout the data. In fact, most datasets for\nautonomous driving are collected within a narrow subset of cities within one\ncountry, typically under similar weather conditions. In this paper we consider\nthe task of adapting 3D object detectors from one dataset to another. We\nobserve that naively, this appears to be a very challenging task, resulting in\ndrastic drops in accuracy levels. We provide extensive experiments to\ninvestigate the true adaptation challenges and arrive at a surprising\nconclusion: the primary adaptation hurdle to overcome are differences in car\nsizes across geographic areas. A simple correction based on the average car\nsize yields a strong correction of the adaptation gap. Our proposed method is\nsimple and easily incorporated into most 3D object detection frameworks. It\nprovides a first baseline for 3D object detection adaptation across countries,\nand gives hope that the underlying problem may be more within grasp than one\nmay have hoped to believe. Our code is available at\nhttps://github.com/cxy1997/3D_adapt_auto_driving."}, {"title": "Exploring Categorical Regularization for Domain Adaptive Object Detection", "authors": "Chang-Dong Xu, Xing-Ran Zhao, Xin Jin, Xiu-Shen Wei", "link": "https://arxiv.org/abs/2003.09152", "summary": "In this paper, we tackle the domain adaptive object detection problem, where\nthe main challenge lies in significant domain gaps between source and target\ndomains. Previous work seeks to plainly align image-level and instance-level\nshifts to eventually minimize the domain discrepancy. However, they still\noverlook to match crucial image regions and important instances across domains,\nwhich will strongly affect domain shift mitigation. In this work, we propose a\nsimple but effective categorical regularization framework for alleviating this\nissue. It can be applied as a plug-and-play component on a series of Domain\nAdaptive Faster R-CNN methods which are prominent for dealing with domain\nadaptive detection. Specifically, by integrating an image-level multi-label\nclassifier upon the detection backbone, we can obtain the sparse but crucial\nimage regions corresponding to categorical information, thanks to the weakly\nlocalization ability of the classification manner. Meanwhile, at the instance\nlevel, we leverage the categorical consistency between image-level predictions\n(by the classifier) and instance-level predictions (by the detection head) as a\nregularization factor to automatically hunt for the hard aligned instances of\ntarget domains. Extensive experiments of various domain shift scenarios show\nthat our method obtains a significant performance gain over original Domain\nAdaptive Faster R-CNN detectors. Furthermore, qualitative visualization and\nanalyses can demonstrate the ability of our method for attending on the key\nregions/instances targeting on domain adaptation. Our code is open-source and\navailable at \\url{https://github.com/Megvii-Nanjing/CR-DA-DET}."}, {"title": "Neural Implicit Embedding for Point Cloud Analysis", "authors": "Kent Fujiwara, Taiichi Hashimoto"}, {"title": "Pose-Guided Visible Part Matching for Occluded Person ReID", "authors": "Shang Gao, Jingya Wang, Huchuan Lu, Zimo Liu", "link": "https://arxiv.org/abs/2004.00230", "summary": "Occluded person re-identification is a challenging task as the appearance\nvaries substantially with various obstacles, especially in the crowd scenario.\nTo address this issue, we propose a Pose-guided Visible Part Matching (PVPM)\nmethod that jointly learns the discriminative features with pose-guided\nattention and self-mines the part visibility in an end-to-end framework.\nSpecifically, the proposed PVPM includes two key components: 1) pose-guided\nattention (PGA) method for part feature pooling that exploits more\ndiscriminative local features; 2) pose-guided visibility predictor (PVP) that\nestimates whether a part suffers the occlusion or not. As there are no ground\ntruth training annotations for the occluded part, we turn to utilize the\ncharacteristic of part correspondence in positive pairs and self-mining the\ncorrespondence scores via graph matching. The generated correspondence scores\nare then utilized as pseudo-labels for visibility predictor (PVP). Experimental\nresults on three reported occluded benchmarks show that the proposed method\nachieves competitive performance to state-of-the-art methods. The source codes\nare available at https://github.com/hh23333/PVPM"}, {"title": "ContourNet: Taking a Further Step Toward Accurate Arbitrary-Shaped Scene Text Detection", "authors": "Yuxin Wang, Hongtao Xie, Zheng-Jun Zha, Mengting Xing, Zilong Fu, Yongdong Zhang", "link": "https://arxiv.org/abs/2004.04940", "summary": "Scene text detection has witnessed rapid development in recent years.\nHowever, there still exists two main challenges: 1) many methods suffer from\nfalse positives in their text representations; 2) the large scale variance of\nscene texts makes it hard for network to learn samples. In this paper, we\npropose the ContourNet, which effectively handles these two problems taking a\nfurther step toward accurate arbitrary-shaped text detection. At first, a\nscale-insensitive Adaptive Region Proposal Network (Adaptive-RPN) is proposed\nto generate text proposals by only focusing on the Intersection over Union\n(IoU) values between predicted and ground-truth bounding boxes. Then a novel\nLocal Orthogonal Texture-aware Module (LOTM) models the local texture\ninformation of proposal features in two orthogonal directions and represents\ntext region with a set of contour points. Considering that the strong\nunidirectional or weakly orthogonal activation is usually caused by the\nmonotonous texture characteristic of false-positive patterns (e.g. streaks.),\nour method effectively suppresses these false positives by only outputting\npredictions with high response value in both orthogonal directions. This gives\nmore accurate description of text regions. Extensive experiments on three\nchallenging datasets (Total-Text, CTW1500 and ICDAR2015) verify that our method\nachieves the state-of-the-art performance. Code is available at\nhttps://github.com/wangyuxin87/ContourNet."}, {"title": "Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving", "authors": "Aditya Prakash, Aseem Behl, Eshed Ohn-Bar, Kashyap Chitta, Andreas Geiger"}, {"title": "Look-Into-Object: Self-Supervised Structure Modeling for Object Recognition", "authors": "Mohan Zhou, Yalong Bai, Wei Zhang, Tiejun Zhao, Tao Mei", "link": "https://arxiv.org/abs/2003.14142", "summary": "Most object recognition approaches predominantly focus on learning\ndiscriminative visual patterns while overlooking the holistic object structure.\nThough important, structure modeling usually requires significant manual\nannotations and therefore is labor-intensive. In this paper, we propose to\n\"look into object\" (explicitly yet intrinsically model the object structure)\nthrough incorporating self-supervisions into the traditional framework. We show\nthe recognition backbone can be substantially enhanced for more robust\nrepresentation learning, without any cost of extra annotation and inference\nspeed. Specifically, we first propose an object-extent learning module for\nlocalizing the object according to the visual patterns shared among the\ninstances in the same category. We then design a spatial context learning\nmodule for modeling the internal structures of the object, through predicting\nthe relative positions within the extent. These two modules can be easily\nplugged into any backbone networks during training and detached at inference\ntime. Extensive experiments show that our look-into-object approach (LIO)\nachieves large performance gain on a number of benchmarks, including generic\nobject recognition (ImageNet) and fine-grained object recognition tasks (CUB,\nCars, Aircraft). We also show that this learning paradigm is highly\ngeneralizable to other tasks such as object detection and segmentation (MS\nCOCO). Project page: https://github.com/JDAI-CV/LIO."}, {"title": "Recognizing Objects From Any View With Object and Viewer-Centered Representations", "authors": "Sainan Liu, Vincent Nguyen, Isaac Rehg, Zhuowen Tu"}, {"title": "Gated Channel Transformation for Visual Recognition", "authors": "Zongxin Yang, Linchao Zhu, Yu Wu, Yi Yang", "link": "https://arxiv.org/abs/1909.11519", "summary": "In this work, we propose a generally applicable transformation unit for\nvisual recognition with deep convolutional neural networks. This transformation\nexplicitly models channel relationships with explainable control variables.\nThese variables determine the neuron behaviors of competition or cooperation,\nand they are jointly optimized with the convolutional weight towards more\naccurate recognition. In Squeeze-and-Excitation (SE) Networks, the channel\nrelationships are implicitly learned by fully connected layers, and the SE\nblock is integrated at the block-level. We instead introduce a channel\nnormalization layer to reduce the number of parameters and computational\ncomplexity. This lightweight layer incorporates a simple l2 normalization,\nenabling our transformation unit applicable to operator-level without much\nincrease of additional parameters. Extensive experiments demonstrate the\neffectiveness of our unit with clear margins on many vision tasks, i.e., image\nclassification on ImageNet, object detection and instance segmentation on COCO,\nvideo classification on Kinetics."}, {"title": "Non-Local Neural Networks With Grouped Bilinear Attentional Transforms", "authors": "Lu Chi, Zehuan Yuan, Yadong Mu, Changhu Wang"}, {"title": "Generative-Discriminative Feature Representations for Open-Set Recognition", "authors": "Pramuditha Perera, Vlad I. Morariu, Rajiv Jain, Varun Manjunatha, Curtis Wigington, Vicente Ordonez, Vishal M. Patel"}, {"title": "RPM-Net: Robust Point Matching Using Learned Features", "authors": "Zi Jian Yew, Gim Hee Lee", "link": "https://arxiv.org/abs/2003.13479", "summary": "Iterative Closest Point (ICP) solves the rigid point cloud registration\nproblem iteratively in two steps: (1) make hard assignments of spatially\nclosest point correspondences, and then (2) find the least-squares rigid\ntransformation. The hard assignments of closest point correspondences based on\nspatial distances are sensitive to the initial rigid transformation and\nnoisy/outlier points, which often cause ICP to converge to wrong local minima.\nIn this paper, we propose the RPM-Net -- a less sensitive to initialization and\nmore robust deep learning-based approach for rigid point cloud registration. To\nthis end, our network uses the differentiable Sinkhorn layer and annealing to\nget soft assignments of point correspondences from hybrid features learned from\nboth spatial coordinates and local geometry. To further improve registration\nperformance, we introduce a secondary network to predict optimal annealing\nparameters. Unlike some existing methods, our RPM-Net handles missing\ncorrespondences and point clouds with partial visibility. Experimental results\nshow that our RPM-Net achieves state-of-the-art performance compared to\nexisting non-deep learning and recent deep learning methods. Our source code is\navailable at the project website https://github.com/yewzijian/RPMNet ."}, {"title": "Sideways: Depth-Parallel Training of Video Models", "authors": "Mateusz Malinowski, Grzegorz \u015awirszcz, Jo\u00e3o Carreira, Viorica P\u0103tr\u0103ucean", "link": "https://arxiv.org/abs/2001.06232", "summary": "We propose Sideways, an approximate backpropagation scheme for training video\nmodels. In standard backpropagation, the gradients and activations at every\ncomputation step through the model are temporally synchronized. The forward\nactivations need to be stored until the backward pass is executed, preventing\ninter-layer (depth) parallelization. However, can we leverage smooth, redundant\ninput streams such as videos to develop a more efficient training scheme? Here,\nwe explore an alternative to backpropagation; we overwrite network activations\nwhenever new ones, i.e., from new frames, become available. Such a more gradual\naccumulation of information from both passes breaks the precise correspondence\nbetween gradients and activations, leading to theoretically more noisy weight\nupdates. Counter-intuitively, we show that Sideways training of deep\nconvolutional video networks not only still converges, but can also potentially\nexhibit better generalization compared to standard synchronized\nbackpropagation."}, {"title": "Basis Prediction Networks for Effective Burst Denoising With Large Kernels", "authors": "Zhihao Xia, Federico Perazzi, Micha\u00ebl Gharbi, Kalyan Sunkavalli, Ayan Chakrabarti", "link": "http://arxiv.org/abs/1912.04421", "summary": "Bursts of images exhibit significant self-similarity across both time and\nspace. This motivates a representation of the kernels as linear combinations of\na small set of basis elements. To this end, we introduce a novel basis\nprediction network that, given an input burst, predicts a set of global basis\nkernels --- shared within the image --- and the corresponding mixing\ncoefficients --- which are specific to individual pixels. Compared to other\nstate-of-the-art deep learning techniques that output a large tensor of\nper-pixel spatiotemporal kernels, our formulation substantially reduces the\ndimensionality of the network output. This allows us to effectively exploit\nlarger denoising kernels and achieve significant quality improvements (over 1dB\nPSNR) at reduced run-times compared to state-of-the-art methods."}, {"title": "Private-kNN: Practical Differential Privacy for Computer Vision", "authors": "Yuqing Zhu, Xiang Yu, Manmohan Chandraker, Yu-Xiang Wang"}, {"title": "SP-NAS: Serial-to-Parallel Backbone Search for Object Detection", "authors": "Chenhan Jiang, Hang Xu, Wei Zhang, Xiaodan Liang, Zhenguo Li", "link": "", "summary": ""}, {"title": "Structure Aware Single-Stage 3D Object Detection From Point Cloud", "authors": "Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, Lei Zhang"}, {"title": "\u201cLooking at the Right Stuff\u201d \u2013 Guided Semantic-Gaze for Autonomous Driving", "authors": "Anwesan Pal, Sayan Mondal, Henrik I. Christensen", "link": "https://arxiv.org/abs/1911.10455", "summary": "In recent years, predicting driver's focus of attention has been a very\nactive area of research in the autonomous driving community. Unfortunately,\nexisting state-of-the-art techniques achieve this by relying only on human gaze\ninformation, thereby ignoring scene semantics. We propose a novel Semantics\nAugmented GazE (SAGE) detection approach that captures driving specific\ncontextual information, in addition to the raw gaze. Such a combined attention\nmechanism serves as a powerful tool to focus on the relevant regions in an\nimage frame in order to make driving both safe and efficient. Using this, we\ndesign a complete saliency prediction framework - SAGE-Net, which modifies the\ninitial prediction from SAGE by taking into account vital aspects such as\ndistance to objects (depth), ego vehicle speed, and pedestrian crossing intent.\nExhaustive experiments conducted through four popular saliency algorithms show\nthat on $\\mathbf{49/56\\text{ }(87.5\\%)}$ cases - considering both the overall\ndataset and crucial driving scenarios, SAGE outperforms existing techniques\nwithout any additional computational overhead during the training process. The\naugmented dataset along with the relevant code are available as part of the\nsupplementary material."}, {"title": "What\u2019s Hidden in a Randomly Weighted Neural Network?", "authors": "Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, Mohammad Rastegari", "link": "https://arxiv.org/abs/1911.13299", "summary": "Training a neural network is synonymous with learning the values of the\nweights. By contrast, we demonstrate that randomly weighted neural networks\ncontain subnetworks which achieve impressive performance without ever training\nthe weight values. Hidden in a randomly weighted Wide ResNet-50 we show that\nthere is a subnetwork (with random weights) that is smaller than, but matches\nthe performance of a ResNet-34 trained on ImageNet. Not only do these\n\"untrained subnetworks\" exist, but we provide an algorithm to effectively find\nthem. We empirically show that as randomly weighted neural networks with fixed\nweights grow wider and deeper, an \"untrained subnetwork\" approaches a network\nwith learned weights in accuracy. Our code and pretrained models are available\nat https://github.com/allenai/hidden-networks."}, {"title": "Structured Multi-Hashing for Model Compression", "authors": "Elad Eban, Yair Movshovitz-Attias, Hao Wu, Mark Sandler, Andrew Poon, Yerlan Idelbayev, Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "link": "https://arxiv.org/abs/1911.11177", "summary": "Despite the success of deep neural networks (DNNs), state-of-the-art models\nare too large to deploy on low-resource devices or common server configurations\nin which multiple models are held in memory. Model compression methods address\nthis limitation by reducing the memory footprint, latency, or energy\nconsumption of a model with minimal impact on accuracy. We focus on the task of\nreducing the number of learnable variables in the model. In this work we\ncombine ideas from weight hashing and dimensionality reductions resulting in a\nsimple and powerful structured multi-hashing method based on matrix products\nthat allows direct control of model size of any deep network and is trained\nend-to-end. We demonstrate the strength of our approach by compressing models\nfrom the ResNet, EfficientNet, and MobileNet architecture families. Our method\nallows us to drastically decrease the number of variables while maintaining\nhigh accuracy. For instance, by applying our approach to EfficentNet-B4 (16M\nparameters) we reduce it to to the size of B0 (5M parameters), while gaining\nover 3% in accuracy over B0 baseline. On the commonly used benchmark CIFAR10 we\nreduce the ResNet32 model by 75% with no loss in quality, and are able to do a\n10x compression while still achieving above 90% accuracy."}, {"title": "DOPS: Learning to Detect 3D Objects and Predict Their 3D Shapes", "authors": "Mahyar Najibi, Guangda Lai, Abhijit Kundu, Zhichao Lu, Vivek Rathod, Thomas Funkhouser, Caroline Pantofaru, David Ross, Larry S. Davis, Alireza Fathi", "link": "https://arxiv.org/abs/2004.01170", "summary": "We propose DOPS, a fast single-stage 3D object detection method for LIDAR\ndata. Previous methods often make domain-specific design decisions, for example\nprojecting points into a bird-eye view image in autonomous driving scenarios.\nIn contrast, we propose a general-purpose method that works on both indoor and\noutdoor scenes. The core novelty of our method is a fast, single-pass\narchitecture that both detects objects in 3D and estimates their shapes. 3D\nbounding box parameters are estimated in one pass for every point, aggregated\nthrough graph convolutions, and fed into a branch of the network that predicts\nlatent codes representing the shape of each detected object. The latent shape\nspace and shape decoder are learned on a synthetic dataset and then used as\nsupervision for the end-to-end training of the 3D object detection pipeline.\nThus our model is able to extract shapes without access to ground-truth shape\ninformation in the target dataset. During experiments, we find that our\nproposed method achieves state-of-the-art results by ~5% on object detection in\nScanNet scenes, and it gets top results by 3.4% in the Waymo Open Dataset,\nwhile reproducing the shapes of detected cars."}, {"title": "AutoTrack: Towards High-Performance Visual Tracking for UAV With Automatic Spatio-Temporal Regularization", "authors": "Yiming Li, Changhong Fu, Fangqiang Ding, Ziyuan Huang, Geng Lu", "link": "https://arxiv.org/abs/2003.12949", "summary": "Most existing trackers based on discriminative correlation filters (DCF) try\nto introduce predefined regularization term to improve the learning of target\nobjects, e.g., by suppressing background learning or by restricting change rate\nof correlation filters. However, predefined parameters introduce much effort in\ntuning them and they still fail to adapt to new situations that the designer\ndid not think of. In this work, a novel approach is proposed to online\nautomatically and adaptively learn spatio-temporal regularization term.\nSpatially local response map variation is introduced as spatial regularization\nto make DCF focus on the learning of trust-worthy parts of the object, and\nglobal response map variation determines the updating rate of the filter.\nExtensive experiments on four UAV benchmarks have proven the superiority of our\nmethod compared to the state-of-the-art CPU- and GPU-based trackers, with a\nspeed of ~60 frames per second running on a single CPU.\n  Our tracker is additionally proposed to be applied in UAV localization.\nConsiderable tests in the indoor practical scenarios have proven the\neffectiveness and versatility of our localization method. The code is available\nat https://github.com/vision4robotics/AutoTrack."}, {"title": "GP-NAS: Gaussian Process Based Neural Architecture Search", "authors": "Zhihang Li, Teng Xi, Jiankang Deng, Gang Zhang, Shengzhao Wen, Ran He"}, {"title": "NAS-FCOS: Fast Neural Architecture Search for Object Detection", "authors": "Ning Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, Chunhua Shen, Yanning Zhang", "link": "", "summary": ""}, {"title": "TCTS: A Task-Consistent Two-Stage Framework for Person Search", "authors": "Cheng Wang, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen"}, {"title": "SCATTER: Selective Context Attentional Scene Text Recognizer", "authors": "Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor, R. Manmatha", "link": "", "summary": ""}, {"title": "Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation", "authors": "Dengsheng Chen, Jun Li, Zheng Wang, Kai Xu", "link": "https://arxiv.org/abs/2001.09322", "summary": "We present a novel approach to category-level 6D object pose and size\nestimation. To tackle intra-class shape variations, we learn canonical shape\nspace (CASS), a unified representation for a large variety of instances of a\ncertain object category. In particular, CASS is modeled as the latent space of\na deep generative model of canonical 3D shapes with normalized pose. We train a\nvariational auto-encoder (VAE) for generating 3D point clouds in the canonical\nspace from an RGBD image. The VAE is trained in a cross-category fashion,\nexploiting the publicly available large 3D shape repositories. Since the 3D\npoint cloud is generated in normalized pose (with actual size), the encoder of\nthe VAE learns view-factorized RGBD embedding. It maps an RGBD image in\narbitrary view into a pose-independent 3D shape representation. Object pose is\nthen estimated via contrasting it with a pose-dependent feature of the input\nRGBD extracted with a separate deep neural networks. We integrate the learning\nof CASS and pose and size estimation into an end-to-end trainable network,\nachieving the state-of-the-art performance."}, {"title": "Hierarchical Scene Coordinate Classification and Regression for Visual Localization", "authors": "Xiaotian Li, Shuzhe Wang, Yi Zhao, Jakob Verbeek, Juho Kannala", "link": "https://arxiv.org/abs/1909.06216", "summary": "Visual localization is critical to many applications in computer vision and\nrobotics. To address single-image RGB localization, state-of-the-art\nfeature-based methods match local descriptors between a query image and a\npre-built 3D model. Recently, deep neural networks have been exploited to\nregress the mapping between raw pixels and 3D coordinates in the scene, and\nthus the matching is implicitly performed by the forward pass through the\nnetwork. However, in a large and ambiguous environment, learning such a\nregression task directly can be difficult for a single network. In this work,\nwe present a new hierarchical scene coordinate network to predict pixel scene\ncoordinates in a coarse-to-fine manner from a single RGB image. The network\nconsists of a series of output layers, each of them conditioned on the previous\nones. The final output layer predicts the 3D coordinates and the others produce\nprogressively finer discrete location labels. The proposed method outperforms\nthe baseline regression-only network and allows us to train compact models\nwhich scale robustly to large environments. It sets a new state-of-the-art for\nsingle-image RGB localization performance on the 7-Scenes, 12-Scenes, Cambridge\nLandmarks datasets, and three combined scenes. Moreover, for large-scale\noutdoor localization on the Aachen Day-Night dataset, we present a hybrid\napproach which outperforms existing scene coordinate regression methods, and\nreduces significantly the performance gap w.r.t. explicit feature matching\nmethods."}, {"title": "MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation", "authors": "Chaoyang He, Haishan Ye, Li Shen, Tong Zhang", "link": "https://arxiv.org/abs/2003.12238", "summary": "Many recently proposed methods for Neural Architecture Search (NAS) can be\nformulated as bilevel optimization. For efficient implementation, its solution\nrequires approximations of second-order methods. In this paper, we demonstrate\nthat gradient errors caused by such approximations lead to suboptimality, in\nthe sense that the optimization procedure fails to converge to a (locally)\noptimal solution. To remedy this, this paper proposes \\mldas, a mixed-level\nreformulation for NAS that can be optimized efficiently and reliably. It is\nshown that even when using a simple first-order method on the mixed-level\nformulation, \\mldas\\ can achieve a lower validation error for NAS problems.\nConsequently, architectures obtained by our method achieve consistently higher\naccuracies than those obtained from bilevel optimization. Moreover, \\mldas\\\nproposes a framework beyond DARTS. It is upgraded via model size-based search\nand early stopping strategies to complete the search process in around 5 hours.\nExtensive experiments within the convolutional architecture search space\nvalidate the effectiveness of our approach."}, {"title": "Scalable Uncertainty for Computer Vision With Functional Variational Inference", "authors": "Eduardo D. C. Carvalho, Ronald Clark, Andrea Nicastro, Paul H. J. Kelly", "link": "https://arxiv.org/abs/2003.03396", "summary": "As Deep Learning continues to yield successful applications in Computer\nVision, the ability to quantify all forms of uncertainty is a paramount\nrequirement for its safe and reliable deployment in the real-world. In this\nwork, we leverage the formulation of variational inference in function space,\nwhere we associate Gaussian Processes (GPs) to both Bayesian CNN priors and\nvariational family. Since GPs are fully determined by their mean and covariance\nfunctions, we are able to obtain predictive uncertainty estimates at the cost\nof a single forward pass through any chosen CNN architecture and for any\nsupervised learning task. By leveraging the structure of the induced covariance\nmatrices, we propose numerically efficient algorithms which enable fast\ntraining in the context of high-dimensional tasks such as depth estimation and\nsemantic segmentation. Additionally, we provide sufficient conditions for\nconstructing regression loss functions whose probabilistic counterparts are\ncompatible with aleatoric uncertainty quantification."}, {"title": "Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning to End", "authors": "Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist, Michael Persson"}, {"title": "Butterfly Transform: An Efficient FFT Based Neural Architecture Design", "authors": "Keivan Alizadeh vahid, Anish Prabhu, Ali Farhadi, Mohammad Rastegari", "link": "https://arxiv.org/abs/1906.02256", "summary": "In this paper, we show that extending the butterfly operations from the FFT\nalgorithm to a general Butterfly Transform (BFT) can be beneficial in building\nan efficient block structure for CNN designs. Pointwise convolutions, which we\nrefer to as channel fusions, are the main computational bottleneck in the\nstate-of-the-art efficient CNNs (e.g. MobileNets ). We introduce a set of\ncriteria for channel fusion and prove that BFT yields an asymptotically optimal\nFLOP count with respect to these criteria. By replacing pointwise convolutions\nwith BFT, we reduce the computational complexity of these layers from O(n^2) to\nO(n\\log n) with respect to the number of channels. Our experimental evaluations\nshow that our method results in significant accuracy gains across a wide range\nof network architectures, especially at low FLOP ranges. For example, BFT\nresults in up to a 6.75% absolute Top-1 improvement for MobileNetV1, 4.4 \\% for\nShuffleNet V2 and 5.4% for MobileNetV3 on ImageNet under a similar number of\nFLOPS. Notably, ShuffleNet-V2+BFT outperforms state-of-the-art architecture\nsearch methods MNasNet, FBNet and MobilenetV3 in the low FLOP regime."}, {"title": "A Certifiably Globally Optimal Solution to Generalized Essential Matrix Estimation", "authors": "Ji Zhao, Wanting Xu, Laurent Kneip"}, {"title": "MUXConv: Information Multiplexing in Convolutional Neural Networks", "authors": "Zhichao Lu, Kalyanmoy Deb, Vishnu Naresh Boddeti", "link": "https://arxiv.org/abs/2003.13880", "summary": "Convolutional neural networks have witnessed remarkable improvements in\ncomputational efficiency in recent years. A key driving force has been the idea\nof trading-off model expressivity and efficiency through a combination of\n$1\\times 1$ and depth-wise separable convolutions in lieu of a standard\nconvolutional layer. The price of the efficiency, however, is the sub-optimal\nflow of information across space and channels in the network. To overcome this\nlimitation, we present MUXConv, a layer that is designed to increase the flow\nof information by progressively multiplexing channel and spatial information in\nthe network, while mitigating computational complexity. Furthermore, to\ndemonstrate the effectiveness of MUXConv, we integrate it within an efficient\nmulti-objective evolutionary algorithm to search for the optimal model\nhyper-parameters while simultaneously optimizing accuracy, compactness, and\ncomputational efficiency. On ImageNet, the resulting models, dubbed MUXNets,\nmatch the performance (75.3% top-1 accuracy) and multiply-add operations (218M)\nof MobileNetV3 while being 1.6$\\times$ more compact, and outperform other\nmobile models in all the three criteria. MUXNet also performs well under\ntransfer learning and when adapted to object detection. On the ChestX-Ray 14\nbenchmark, its accuracy is comparable to the state-of-the-art while being\n$3.3\\times$ more compact and $14\\times$ more efficient. Similarly, detection on\nPASCAL VOC 2007 is 1.2% more accurate, 28% faster and 6% more compact compared\nto MobileNetV2. Code is available from\nhttps://github.com/human-analysis/MUXConv"}, {"title": "PointGMM: A Neural GMM Network for Point Clouds", "authors": "Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "link": "", "summary": ""}, {"title": "Noisier2Noise: Learning to Denoise From Unpaired Noisy Data", "authors": "Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady", "link": "https://arxiv.org/abs/1910.11908", "summary": "We present a method for training a neural network to perform image denoising\nwithout access to clean training examples or access to paired noisy training\nexamples. Our method requires only a single noisy realization of each training\nexample and a statistical model of the noise distribution, and is applicable to\na wide variety of noise models, including spatially structured noise. Our model\nproduces results which are competitive with other learned methods which require\nricher training data, and outperforms traditional non-learned denoising\nmethods. We present derivations of our method for arbitrary additive noise, an\nimprovement specific to Gaussian additive noise, and an extension to\nmultiplicative Bernoulli noise."}, {"title": "TRPLP \u2013 Trifocal Relative Pose From Lines at Points", "authors": "Ricardo Fabbri, Timothy Duff, Hongyi Fan, Margaret H. Regan, David da Costa de Pinho, Elias Tsigaridas, Charles W. Wampler, Jonathan D. Hauenstein, Peter J. Giblin, Benjamin Kimia, Anton Leykin, Tomas Pajdla", "link": "", "summary": ""}, {"title": "DSNAS: Direct Neural Architecture Search Without Parameter Retraining", "authors": "Shoukang Hu, Sirui Xie, Hehui Zheng, Chunxiao Liu, Jianping Shi, Xunying Liu, Dahua Lin", "link": "https://arxiv.org/abs/2002.09128", "summary": "If NAS methods are solutions, what is the problem? Most existing NAS methods\nrequire two-stage parameter optimization. However, performance of the same\narchitecture in the two stages correlates poorly. In this work, we propose a\nnew problem definition for NAS, task-specific end-to-end, based on this\nobservation. We argue that given a computer vision task for which a NAS method\nis expected, this definition can reduce the vaguely-defined NAS evaluation to\ni) accuracy of this task and ii) the total computation consumed to finally\nobtain a model with satisfying accuracy. Seeing that most existing methods do\nnot solve this problem directly, we propose DSNAS, an efficient differentiable\nNAS framework that simultaneously optimizes architecture and parameters with a\nlow-biased Monte Carlo estimate. Child networks derived from DSNAS can be\ndeployed directly without parameter retraining. Comparing with two-stage\nmethods, DSNAS successfully discovers networks with comparable accuracy (74.4%)\non ImageNet in 420 GPU hours, reducing the total time by more than 34%. Our\nimplementation is available at https://github.com/SNAS-Series/SNAS-Series."}, {"title": "MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships", "authors": "Yongjian Chen, Lei Tai, Kai Sun, Mingyang Li", "link": "https://arxiv.org/abs/2003.00504", "summary": "Monocular 3D object detection is an essential component in autonomous driving\nwhile challenging to solve, especially for those occluded samples which are\nonly partially visible. Most detectors consider each 3D object as an\nindependent training target, inevitably resulting in a lack of useful\ninformation for occluded samples. To this end, we propose a novel method to\nimprove the monocular 3D object detection by considering the relationship of\npaired samples. This allows us to encode spatial constraints for\npartially-occluded objects from their adjacent neighbors. Specifically, the\nproposed detector computes uncertainty-aware predictions for object locations\nand 3D distances for the adjacent object pairs, which are subsequently jointly\noptimized by nonlinear least squares. Finally, the one-stage uncertainty-aware\nprediction structure and the post-optimization module are dedicatedly\nintegrated for ensuring the run-time efficiency. Experiments demonstrate that\nour method yields the best performance on KITTI 3D detection benchmark, by\noutperforming state-of-the-art competitors by wide margins, especially for the\nhard samples."}, {"title": "Regularization on Spatio-Temporally Smoothed Feature for Action Recognition", "authors": "Jinhyung Kim, Seunghwan Cha, Dongyoon Wee, Soonmin Bae, Junmo Kim"}, {"title": "Towards Accurate Scene Text Recognition With Semantic Reasoning Networks", "authors": "Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han, Jingtuo Liu, Errui Ding", "link": "https://arxiv.org/abs/2003.12294", "summary": "Scene text image contains two levels of contents: visual texture and semantic\ninformation. Although the previous scene text recognition methods have made\ngreat progress over the past few years, the research on mining semantic\ninformation to assist text recognition attracts less attention, only RNN-like\nstructures are explored to implicitly model semantic information. However, we\nobserve that RNN based methods have some obvious shortcomings, such as\ntime-dependent decoding manner and one-way serial transmission of semantic\ncontext, which greatly limit the help of semantic information and the\ncomputation efficiency. To mitigate these limitations, we propose a novel\nend-to-end trainable framework named semantic reasoning network (SRN) for\naccurate scene text recognition, where a global semantic reasoning module\n(GSRM) is introduced to capture global semantic context through multi-way\nparallel transmission. The state-of-the-art results on 7 public benchmarks,\nincluding regular text, irregular text and non-Latin long text, verify the\neffectiveness and robustness of the proposed method. In addition, the speed of\nSRN has significant advantages over the RNN based methods, demonstrating its\nvalue in practical use."}, {"title": "Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation", "authors": "Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, William Yang Wang", "link": "https://arxiv.org/abs/1911.07450", "summary": "Visual navigation is a task of training an embodied agent by intelligently\nnavigating to a target object (e.g., television) using only visual\nobservations. A key challenge for current deep reinforcement learning models\nlies in the requirements for a large amount of training data. It is exceedingly\nexpensive to construct sufficient 3D synthetic environments annotated with the\ntarget object information. In this paper, we focus on visual navigation in the\nlow-resource setting, where we have only a few training environments annotated\nwith object information. We propose a novel unsupervised reinforcement learning\napproach to learn transferable meta-skills (e.g., bypass obstacles, go\nstraight) from unannotated environments without any supervisory signals. The\nagent can then fast adapt to visual navigation through learning a high-level\nmaster policy to combine these meta-skills, when the\nvisual-navigation-specified reward is provided. Evaluation in the AI2-THOR\nenvironments shows that our method significantly outperforms the baseline by\n53.34% relatively on SPL, and further qualitative analysis demonstrates that\nour method learns transferable motor primitives for visual navigation."}, {"title": "Inferring Attention Shift Ranks of Objects for Image Saliency", "authors": "Avishek Siris, Jianbo Jiao, Gary K.L. Tam, Xianghua Xie, Rynson W.H. Lau"}, {"title": "Camera On-Boarding for Person Re-Identification Using Hypothesis Transfer Learning", "authors": "Sk Miraj Ahmed, Aske R. Lejb\u00f8lle, Rameswar Panda, Amit K. Roy-Chowdhury"}, {"title": "Joint Graph-Based Depth Refinement and Normal Estimation", "authors": "Mattia Rossi, Mireille El Gheche, Andreas Kuhn, Pascal Frossard", "link": "https://arxiv.org/abs/1912.01306", "summary": "Depth estimation is an essential component in understanding the 3D geometry\nof a scene, with numerous applications in urban and indoor settings. These\nscenes are characterized by a prevalence of human made structures, which in\nmost of the cases, are either inherently piece-wise planar, or can be\napproximated as such. In these settings, we devise a novel depth refinement\nframework that aims at recovering the underlying piece-wise planarity of the\ninverse depth map. We formulate this task as an optimization problem involving\na data fidelity term that minimizes the distance to the input inverse depth\nmap, as well as a regularization that enforces a piece-wise planar solution. As\nfor the regularization term, we model the inverse depth map as a weighted graph\nbetween pixels. The proposed regularization is designed to estimate a plane\nautomatically at each pixel, without any need for an a priori estimation of the\nscene planes, and at the same time it encourages similar pixels to be assigned\nto the same plane. The resulting optimization problem is efficiently solved\nwith ADAM algorithm. Experiments show that our method leads to a significant\nimprovement in depth refinement, both visually and numerically, with respect to\nstate-of-the-art algorithms on Middlebury, KITTI and ETH3D multi-view stereo\ndatasets."}, {"title": "DR Loss: Improving Object Detection by Distributional Ranking", "authors": "Qi Qian, Lei Chen, Hao Li, Rong Jin", "link": "https://arxiv.org/abs/1907.10156", "summary": "Most of object detection algorithms can be categorized into two classes:\ntwo-stage detectors and one-stage detectors. Recently, many efforts have been\ndevoted to one-stage detectors for the simple yet effective architecture.\nDifferent from two-stage detectors, one-stage detectors aim to identify\nforeground objects from all candidates in a single stage. This architecture is\nefficient but can suffer from the imbalance issue with respect to two aspects:\nthe inter-class imbalance between the number of candidates from foreground and\nbackground classes and the intra-class imbalance in the hardness of background\ncandidates, where only a few candidates are hard to be identified. In this\nwork, we propose a novel distributional ranking (DR) loss to handle the\nchallenge. For each image, we convert the classification problem to a ranking\nproblem, which considers pairs of candidates within the image, to address the\ninter-class imbalance problem. Then, we push the distributions of confidence\nscores for foreground and background towards the decision boundary. After that,\nwe optimize the rank of the expectations of derived distributions in lieu of\noriginal pairs. Our method not only mitigates the intra-class imbalance issue\nin background candidates but also improves the efficiency for the ranking\nalgorithm. By merely replacing the focal loss in RetinaNet with the developed\nDR loss and applying ResNet-101 as the backbone, mAP of the single-scale test\non COCO can be improved from 39.1% to 41.7% without bells and whistles, which\ndemonstrates the effectiveness of the proposed loss function. Code is available\nat \\url{https://github.com/idstcv/DR_loss}."}, {"title": "Self-Trained Deep Ordinal Regression for End-to-End Video Anomaly Detection", "authors": "Guansong Pang, Cheng Yan, Chunhua Shen, Anton van den Hengel, Xiao Bai", "link": "https://arxiv.org/abs/2003.06780", "summary": "Video anomaly detection is of critical practical importance to a variety of\nreal applications because it allows human attention to be focused on events\nthat are likely to be of interest, in spite of an otherwise overwhelming volume\nof video. We show that applying self-trained deep ordinal regression to video\nanomaly detection overcomes two key limitations of existing methods, namely, 1)\nbeing highly dependent on manually labeled normal training data; and 2)\nsub-optimal feature learning. By formulating a surrogate two-class ordinal\nregression task we devise an end-to-end trainable video anomaly detection\napproach that enables joint representation learning and anomaly scoring without\nmanually labeled normal/abnormal data. Experiments on eight real-world video\nscenes show that our proposed method outperforms state-of-the-art methods that\nrequire no labeled training data by a substantial margin, and enables easy and\naccurate localization of the identified anomalies. Furthermore, we demonstrate\nthat our method offers effective human-in-the-loop anomaly detection which can\nbe critical in applications where anomalies are rare and the false-negative\ncost is high."}, {"title": "Few-Shot Class-Incremental Learning", "authors": "Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, Yihong Gong", "link": "https://arxiv.org/abs/2004.10956", "summary": "The ability to incrementally learn new classes is crucial to the development\nof real-world artificial intelligence systems. In this paper, we focus on a\nchallenging but practical few-shot class-incremental learning (FSCIL) problem.\nFSCIL requires CNN models to incrementally learn new classes from very few\nlabelled samples, without forgetting the previously learned ones. To address\nthis problem, we represent the knowledge using a neural gas (NG) network, which\ncan learn and preserve the topology of the feature manifold formed by different\nclasses. On this basis, we propose the TOpology-Preserving knowledge\nInCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old\nclasses by stabilizing NG's topology and improves the representation learning\nfor few-shot new classes by growing and adapting NG to new training samples.\nComprehensive experimental results demonstrate that our proposed method\nsignificantly outperforms other state-of-the-art class-incremental learning\nmethods on CIFAR100, miniImageNet, and CUB200 datasets."}, {"title": "PolarMask: Single Shot Instance Segmentation With Polar Representation", "authors": "Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, Ping Luo", "link": "https://arxiv.org/abs/1909.13226", "summary": "In this paper, we introduce an anchor-box free and single shot instance\nsegmentation method, which is conceptually simple, fully convolutional and can\nbe used as a mask prediction module for instance segmentation, by easily\nembedding it into most off-the-shelf detection methods. Our method, termed\nPolarMask, formulates the instance segmentation problem as instance center\nclassification and dense distance regression in a polar coordinate. Moreover,\nwe propose two effective approaches to deal with sampling high-quality center\nexamples and optimization for dense distance regression, respectively, which\ncan significantly improve the performance and simplify the training process.\nWithout any bells and whistles, PolarMask achieves 32.9% in mask mAP with\nsingle-model and single-scale training/testing on challenging COCO dataset. For\nthe first time, we demonstrate a much simpler and flexible instance\nsegmentation framework achieving competitive accuracy. We hope that the\nproposed PolarMask framework can serve as a fundamental and strong baseline for\nsingle shot instance segmentation tasks. Code is available at:\ngithub.com/xieenze/PolarMask."}, {"title": "DeepEMD: Few-Shot Image Classification With Differentiable Earth Mover\u2019s Distance and Structured Classifiers", "authors": "Chi Zhang, Yujun Cai, Guosheng Lin, Chunhua Shen", "link": "", "summary": ""}, {"title": "Detection in Crowded Scenes: One Proposal, Multiple Predictions", "authors": "Xuangeng Chu, Anlin Zheng, Xiangyu Zhang, Jian Sun", "link": "https://arxiv.org/abs/2003.09163", "summary": "We propose a simple yet effective proposal-based object detector, aiming at\ndetecting highly-overlapped instances in crowded scenes. The key of our\napproach is to let each proposal predict a set of correlated instances rather\nthan a single one in previous proposal-based frameworks. Equipped with new\ntechniques such as EMD Loss and Set NMS, our detector can effectively handle\nthe difficulty of detecting highly overlapped objects. On a FPN-Res50 baseline,\nour detector can obtain 4.9\\% AP gains on challenging CrowdHuman dataset and\n1.0\\% $\\text{MR}^{-2}$ improvements on CityPersons dataset, without bells and\nwhistles. Moreover, on less crowed datasets like COCO, our approach can still\nachieve moderate improvement, suggesting the proposed method is robust to\ncrowdedness. Code and pre-trained models will be released at\nhttps://github.com/megvii-model/CrowdDetection."}, {"title": "Autolabeling 3D Objects With Differentiable Rendering of SDF Shape Priors", "authors": "Sergey Zakharov, Wadim Kehl, Arjun Bhargava, Adrien Gaidon", "link": "https://arxiv.org/abs/1911.11288", "summary": "We present an automatic annotation pipeline to recover 9D cuboids and 3D\nshapes from pre-trained off-the-shelf 2D detectors and sparse LIDAR data. Our\nautolabeling method solves an ill-posed inverse problem by considering learned\nshape priors and optimizing geometric and physical parameters. To address this\nchallenging problem, we apply a novel differentiable shape renderer to signed\ndistance fields (SDF), leveraged together with normalized object coordinate\nspaces (NOCS). Initially trained on synthetic data to predict shape and\ncoordinates, our method uses these predictions for projective and geometric\nalignment over real samples. Moreover, we also propose a curriculum learning\nstrategy, iteratively retraining on samples of increasing difficulty in\nsubsequent self-improving annotation rounds. Our experiments on the KITTI3D\ndataset show that we can recover a substantial amount of accurate cuboids, and\nthat these autolabels can be used to train 3D vehicle detectors with\nstate-of-the-art results."}, {"title": "Interactive Object Segmentation With Inside-Outside Guidance", "authors": "Shiyin Zhang, Jun Hao Liew, Yunchao Wei, Shikui Wei, Yao Zhao"}, {"title": "Mnemonics Training: Multi-Class Incremental Learning Without Forgetting", "authors": "Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, Qianru Sun", "link": "https://arxiv.org/abs/2002.10211", "summary": "Multi-Class Incremental Learning (MCIL) aims to learn new concepts by\nincrementally updating a model trained on previous concepts. However, there is\nan inherent trade-off to effectively learning new concepts without catastrophic\nforgetting of previous ones. To alleviate this issue, it has been proposed to\nkeep around a few examples of the previous concepts but the effectiveness of\nthis approach heavily depends on the representativeness of these examples. This\npaper proposes a novel and automatic framework we call mnemonics, where we\nparameterize exemplars and make them optimizable in an end-to-end manner. We\ntrain the framework through bilevel optimizations, i.e., model-level and\nexemplar-level. We conduct extensive experiments on three MCIL benchmarks,\nCIFAR-100, ImageNet-Subset and ImageNet, and show that using mnemonics\nexemplars can surpass the state-of-the-art by a large margin. Interestingly and\nquite intriguingly, the mnemonics exemplars tend to be on the boundaries\nbetween different classes."}, {"title": "Learning to Segment 3D Point Clouds in 2D Image Space", "authors": "Yecheng Lyu, Xinming Huang, Ziming Zhang", "link": "https://arxiv.org/abs/2003.05593", "summary": "In contrast to the literature where local patterns in 3D point clouds are\ncaptured by customized convolutional operators, in this paper we study the\nproblem of how to effectively and efficiently project such point clouds into a\n2D image space so that traditional 2D convolutional neural networks (CNNs) such\nas U-Net can be applied for segmentation. To this end, we are motivated by\ngraph drawing and reformulate it as an integer programming problem to learn the\ntopology-preserving graph-to-grid mapping for each individual point cloud. To\naccelerate the computation in practice, we further propose a novel hierarchical\napproximate algorithm. With the help of the Delaunay triangulation for graph\nconstruction from point clouds and a multi-scale U-Net for segmentation, we\nmanage to demonstrate the state-of-the-art performance on ShapeNet and PartNet,\nrespectively, with significant improvement over the literature. Code is\navailable at https://github.com/Zhang-VISLab."}, {"title": "Smooth Shells: Multi-Scale Shape Registration With Functional Maps", "authors": "Marvin Eisenberger, Zorah L\u00e4hner, Daniel Cremers", "link": "https://arxiv.org/abs/1905.12512", "summary": "We propose a novel 3D shape correspondence method based on the iterative\nalignment of so-called smooth shells. Smooth shells define a series of\ncoarse-to-fine shape approximations designed to work well with multiscale\nalgorithms. The main idea is to first align rough approximations of the\ngeometry and then add more and more details to refine the correspondence. We\nfuse classical shape registration with Functional Maps by embedding the input\nshapes into an intrinsic-extrinsic product space. Moreover, we disambiguate\nintrinsic symmetries by applying a surrogate based Markov chain Monte Carlo\ninitialization. Our method naturally handles various types of noise that\ncommonly occur in real scans, like non-isometry or incompatible meshing.\nFinally, we demonstrate state-of-the-art quantitative results on several\ndatasets and show that our pipeline produces smoother, more realistic results\nthan other automatic matching methods in real world applications."}, {"title": "Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation", "authors": "Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen", "link": "https://arxiv.org/abs/2004.04581", "summary": "Image-level weakly supervised semantic segmentation is a challenging problem\nthat has been deeply studied in recent years. Most of advanced solutions\nexploit class activation map (CAM). However, CAMs can hardly serve as the\nobject mask due to the gap between full and weak supervisions. In this paper,\nwe propose a self-supervised equivariant attention mechanism (SEAM) to discover\nadditional supervision and narrow the gap. Our method is based on the\nobservation that equivariance is an implicit constraint in fully supervised\nsemantic segmentation, whose pixel-level labels take the same spatial\ntransformation as the input images during data augmentation. However, this\nconstraint is lost on the CAMs trained by image-level supervision. Therefore,\nwe propose consistency regularization on predicted CAMs from various\ntransformed images to provide self-supervision for network learning. Moreover,\nwe propose a pixel correlation module (PCM), which exploits context appearance\ninformation and refines the prediction of current pixel by its similar\nneighbors, leading to further improvement on CAMs consistency. Extensive\nexperiments on PASCAL VOC 2012 dataset demonstrate our method outperforms\nstate-of-the-art methods using the same level of supervision. The code is\nreleased online."}, {"title": "Efficient Neural Vision Systems Based on Convolutional Image Acquisition", "authors": "Pedram Pad, Simon Narduzzi, Cl\u00e9ment K\u00fcndig, Engin T\u00fcretken, Siavash A. Bigdeli, L. Andrea Dunbar"}, {"title": "Visual Chirality", "authors": "Zhiqiu Lin, Jin Sun, Abe Davis, Noah Snavely"}, {"title": "What Machines See Is Not What They Get: Fooling Scene Text Recognition Models With Adversarial Text Images", "authors": "Xing Xu, Jiefu Chen, Jinhui Xiao, Lianli Gao, Fumin Shen, Heng Tao Shen", "link": "", "summary": ""}, {"title": "Dynamic Traffic Modeling From Overhead Imagery", "authors": "Scott Workman, Nathan Jacobs"}, {"title": "Satellite Image Time Series Classification With Pixel-Set Encoders and Temporal Self-Attention", "authors": "Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Giordano, Nesrine Chehata", "link": "https://arxiv.org/abs/1911.07757", "summary": "Satellite image time series, bolstered by their growing availability, are at\nthe forefront of an extensive effort towards automated Earth monitoring by\ninternational institutions. In particular, large-scale control of agricultural\nparcels is an issue of major political and economic importance. In this regard,\nhybrid convolutional-recurrent neural architectures have shown promising\nresults for the automated classification of satellite image time series.We\npropose an alternative approach in which the convolutional layers are\nadvantageously replaced with encoders operating on unordered sets of pixels to\nexploit the typically coarse resolution of publicly available satellite images.\nWe also propose to extract temporal features using a bespoke neural\narchitecture based on self-attention instead of recurrent networks. We\ndemonstrate experimentally that our method not only outperforms previous\nstate-of-the-art approaches in terms of precision, but also significantly\ndecreases processing time and memory requirements. Lastly, we release a large\nopen-access annotated dataset as a benchmark for future work on satellite image\ntime series."}, {"title": "DAVD-Net: Deep Audio-Aided Video Decompression of Talking Heads", "authors": "Xi Zhang, Xiaolin Wu, Xinliang Zhai, Xianye Ben, Chengjie Tu"}, {"title": "Learning When and Where to Zoom With Deep Reinforcement Learning", "authors": "Burak Uzkent, Stefano Ermon", "link": "https://arxiv.org/abs/2003.00425", "summary": "While high resolution images contain semantically more useful information\nthan their lower resolution counterparts, processing them is computationally\nmore expensive, and in some applications, e.g. remote sensing, they can be much\nmore expensive to acquire. For these reasons, it is desirable to develop an\nautomatic method to selectively use high resolution data when necessary while\nmaintaining accuracy and reducing acquisition/run-time cost. In this direction,\nwe propose PatchDrop a reinforcement learning approach to dynamically identify\nwhen and where to use/acquire high resolution data conditioned on the paired,\ncheap, low resolution images. We conduct experiments on CIFAR10, CIFAR100,\nImageNet and fMoW datasets where we use significantly less high resolution data\nwhile maintaining similar accuracy to models which use full high resolution\nimages."}, {"title": "Cross-Domain Detection via Graph-Induced Prototype Alignment", "authors": "Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, Wenjun Zhang", "link": "https://arxiv.org/abs/2003.12849", "summary": "Applying the knowledge of an object detector trained on a specific domain\ndirectly onto a new domain is risky, as the gap between two domains can\nseverely degrade model's performance. Furthermore, since different instances\ncommonly embody distinct modal information in object detection scenario, the\nfeature alignment of source and target domain is hard to be realized. To\nmitigate these problems, we propose a Graph-induced Prototype Alignment (GPA)\nframework to seek for category-level domain alignment via elaborate prototype\nrepresentations. In the nutshell, more precise instance-level features are\nobtained through graph-based information propagation among region proposals,\nand, on such basis, the prototype representation of each class is derived for\ncategory-level domain alignment. In addition, in order to alleviate the\nnegative effect of class-imbalance on domain adaptation, we design a\nClass-reweighted Contrastive Loss to harmonize the adaptation training process.\nCombining with Faster R-CNN, the proposed framework conducts feature alignment\nin a two-stage manner. Comprehensive results on various cross-domain detection\ntasks demonstrate that our approach outperforms existing methods with a\nremarkable margin. Our code is available at\nhttps://github.com/ChrisAllenMing/GPA-detection."}, {"title": "Meta-Learning of Neural Architectures for Few-Shot Learning", "authors": "Thomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, Frank Hutter", "link": "https://arxiv.org/abs/1911.11090", "summary": "The recent progress in neural architecture search (NAS) has allowed scaling\nthe automated design of neural architectures to real-world domains, such as\nobject detection and semantic segmentation. However, one prerequisite for the\napplication of NAS are large amounts of labeled data and compute resources.\nThis renders its application challenging in few-shot learning scenarios, where\nmany related tasks need to be learned, each with limited amounts of data and\ncompute time. Thus, few-shot learning is typically done with a fixed neural\narchitecture. To improve upon this, we propose MetaNAS, the first method which\nfully integrates NAS with gradient-based meta-learning. MetaNAS optimizes a\nmeta-architecture along with the meta-weights during meta-training. During\nmeta-testing, architectures can be adapted to a novel task with a few steps of\nthe task optimizer, that is: task adaptation becomes computationally cheap and\nrequires only little data per task. Moreover, MetaNAS is agnostic in that it\ncan be used with arbitrary model-agnostic meta-learning algorithms and\narbitrary gradient-based NAS methods. %We present encouraging results for\nMetaNAS with a combination of DARTS and REPTILE on few-shot classification\nbenchmarks. Empirical results on standard few-shot classification benchmarks\nshow that MetaNAS with a combination of DARTS and REPTILE yields\nstate-of-the-art results."}, {"title": "Towards Inheritable Models for Open-Set Domain Adaptation", "authors": "Jogendra Nath Kundu, Naveen Venkat, Ambareesh Revanur, Rahul M V, R. Venkatesh Babu", "link": "https://arxiv.org/abs/2004.04388", "summary": "There has been a tremendous progress in Domain Adaptation (DA) for visual\nrecognition tasks. Particularly, open-set DA has gained considerable attention\nwherein the target domain contains additional unseen categories. Existing\nopen-set DA approaches demand access to a labeled source dataset along with\nunlabeled target instances. However, this reliance on co-existing source and\ntarget data is highly impractical in scenarios where data-sharing is restricted\ndue to its proprietary nature or privacy concerns. Addressing this, we\nintroduce a practical DA paradigm where a source-trained model is used to\nfacilitate adaptation in the absence of the source dataset in future. To this\nend, we formalize knowledge inheritability as a novel concept and propose a\nsimple yet effective solution to realize inheritable models suitable for the\nabove practical paradigm. Further, we present an objective way to quantify\ninheritability to enable the selection of the most suitable source model for a\ngiven target domain, even in the absence of the source data. We provide\ntheoretical insights followed by a thorough empirical evaluation demonstrating\nstate-of-the-art open-set domain adaptation performance."}, {"title": "Learning From Synthetic Animals", "authors": "Jiteng Mu, Weichao Qiu, Gregory D. Hager, Alan L. Yuille", "link": "https://arxiv.org/abs/1912.08265", "summary": "Despite great success in human parsing, progress for parsing other deformable\narticulated objects, like animals, is still limited by the lack of labeled\ndata. In this paper, we use synthetic images and ground truth generated from\nCAD animal models to address this challenge. To bridge the domain gap between\nreal and synthetic images, we propose a novel consistency-constrained\nsemi-supervised learning method (CC-SSL). Our method leverages both spatial and\ntemporal consistencies, to bootstrap weak models trained on synthetic data with\nunlabeled real images. We demonstrate the effectiveness of our method on highly\ndeformable animals, such as horses and tigers. Without using any real image\nlabel, our method allows for accurate keypoint prediction on real images.\nMoreover, we quantitatively show that models using synthetic data achieve\nbetter generalization performance than models trained on real images across\ndifferent domains in the Visual Domain Adaptation Challenge dataset. Our\nsynthetic dataset contains 10+ animals with diverse poses and rich ground\ntruth, which enables us to use the multi-task learning strategy to further\nboost models' performance."}, {"title": "Distilling Cross-Task Knowledge via Relationship Matching", "authors": "Han-Jia Ye, Su Lu, De-Chuan Zhan"}, {"title": "Open Compound Domain Adaptation", "authors": "Ziwei Liu, Zhongqi Miao, Xingang Pan, Xiaohang Zhan, Dahua Lin, Stella X. Yu, Boqing Gong", "link": "https://arxiv.org/abs/1909.03403", "summary": "A typical domain adaptation approach is to adapt models trained on the\nannotated data in a source domain (e.g., sunny weather) for achieving high\nperformance on the test data in a target domain (e.g., rainy weather). Whether\nthe target contains a single homogeneous domain or multiple heterogeneous\ndomains, existing works always assume that there exist clear distinctions\nbetween the domains, which is often not true in practice (e.g., changes in\nweather). We study an open compound domain adaptation (OCDA) problem, in which\nthe target is a compound of multiple homogeneous domains without domain labels,\nreflecting realistic data collection from mixed and novel situations. We\npropose a new approach based on two technical insights into OCDA: 1) a\ncurriculum domain adaptation strategy to bootstrap generalization across\ndomains in a data-driven self-organizing fashion and 2) a memory module to\nincrease the model's agility towards novel domains. Our experiments on digit\nclassification, facial expression recognition, semantic segmentation, and\nreinforcement learning demonstrate the effectiveness of our approach."}, {"title": "Context Prior for Scene Segmentation", "authors": "Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, Nong Sang", "link": "https://arxiv.org/abs/2004.01547", "summary": "Recent works have widely explored the contextual dependencies to achieve more\naccurate segmentation results. However, most approaches rarely distinguish\ndifferent types of contextual dependencies, which may pollute the scene\nunderstanding. In this work, we directly supervise the feature aggregation to\ndistinguish the intra-class and inter-class context clearly. Specifically, we\ndevelop a Context Prior with the supervision of the Affinity Loss. Given an\ninput image and corresponding ground truth, Affinity Loss constructs an ideal\naffinity map to supervise the learning of Context Prior. The learned Context\nPrior extracts the pixels belonging to the same category, while the reversed\nprior focuses on the pixels of different classes. Embedded into a conventional\ndeep CNN, the proposed Context Prior Layer can selectively capture the\nintra-class and inter-class contextual dependencies, leading to robust feature\nrepresentation. To validate the effectiveness, we design an effective Context\nPrior Network (CPNet). Extensive quantitative and qualitative evaluations\ndemonstrate that the proposed model performs favorably against state-of-the-art\nsemantic segmentation approaches. More specifically, our algorithm achieves\n46.3% mIoU on ADE20K, 53.9% mIoU on PASCAL-Context, and 81.3% mIoU on\nCityscapes. Code is available at https://git.io/ContextPrior."}, {"title": "Tangent Images for Mitigating Spherical Distortion", "authors": "Marc Eder, Mykhailo Shvets, John Lim, Jan-Michael Frahm", "link": "https://arxiv.org/abs/1912.09390", "summary": "In this work, we propose \"tangent images,\" a spherical image representation\nthat facilitates transferable and scalable $360^\\circ$ computer vision.\nInspired by techniques in cartography and computer graphics, we render a\nspherical image to a set of distortion-mitigated, locally-planar image grids\ntangent to a subdivided icosahedron. By varying the resolution of these grids\nindependently of the subdivision level, we can effectively represent high\nresolution spherical images while still benefiting from the low-distortion\nicosahedral spherical approximation. We show that training standard\nconvolutional neural networks on tangent images compares favorably to the many\nspecialized spherical convolutional kernels that have been developed, while\nalso scaling efficiently to handle significantly higher spherical resolutions.\nFurthermore, because our approach does not require specialized kernels, we show\nthat we can transfer networks trained on perspective images to spherical data\nwithout fine-tuning and with limited performance drop-off. Finally, we\ndemonstrate that tangent images can be used to improve the quality of sparse\nfeature detection on spherical images, illustrating its usefulness for\ntraditional computer vision tasks like structure-from-motion and SLAM."}, {"title": "Learning a Dynamic Map of Visual Appearance", "authors": "Tawfiq Salem, Scott Workman, Nathan Jacobs"}, {"title": "Webly Supervised Knowledge Embedding Model for Visual Reasoning", "authors": "Wenbo Zheng, Lan Yan, Chao Gou, Fei-Yue Wang"}, {"title": "Gradually Vanishing Bridge for Adversarial Domain Adaptation", "authors": "Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, Qi Tian", "link": "https://arxiv.org/abs/2003.13183", "summary": "In unsupervised domain adaptation, rich domain-specific characteristics bring\ngreat challenge to learn domain-invariant representations. However, domain\ndiscrepancy is considered to be directly minimized in existing solutions, which\nis difficult to achieve in practice. Some methods alleviate the difficulty by\nexplicitly modeling domain-invariant and domain-specific parts in the\nrepresentations, but the adverse influence of the explicit construction lies in\nthe residual domain-specific characteristics in the constructed\ndomain-invariant representations. In this paper, we equip adversarial domain\nadaptation with Gradually Vanishing Bridge (GVB) mechanism on both generator\nand discriminator. On the generator, GVB could not only reduce the overall\ntransfer difficulty, but also reduce the influence of the residual\ndomain-specific characteristics in domain-invariant representations. On the\ndiscriminator, GVB contributes to enhance the discriminating ability, and\nbalance the adversarial training process. Experiments on three challenging\ndatasets show that our GVB methods outperform strong competitors, and cooperate\nwell with other adversarial methods. The code is available at\nhttps://github.com/cuishuhao/GVB."}, {"title": "Active Speakers in Context", "authors": "Juan Le\u00f3n Alc\u00e1zar, Fabian Caba, Long Mai, Federico Perazzi, Joon-Young Lee, Pablo Arbel\u00e1ez, Bernard Ghanem", "link": "http://arxiv.org/abs/2005.09812", "summary": "Current methods for active speak er detection focus on modeling short-term\naudiovisual information from a single speaker. Although this strategy can be\nenough for addressing single-speaker scenarios, it prevents accurate detection\nwhen the task is to identify who of many candidate speakers are talking. This\npaper introduces the Active Speaker Context, a novel representation that models\nrelationships between multiple speakers over long time horizons. Our Active\nSpeaker Context is designed to learn pairwise and temporal relations from an\nstructured ensemble of audio-visual observations. Our experiments show that a\nstructured feature ensemble already benefits the active speaker detection\nperformance. Moreover, we find that the proposed Active Speaker Context\nimproves the state-of-the-art on the AVA-ActiveSpeaker dataset achieving a mAP\nof 87.1%. We present ablation studies that verify that this result is a direct\nconsequence of our long-term multi-speaker analysis."}, {"title": "Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation", "authors": "Bowen Cheng, Maxwell D. Collins, Yukun Zhu, Ting Liu, Thomas S. Huang, Hartwig Adam, Liang-Chieh Chen", "link": "https://arxiv.org/abs/1911.10194", "summary": "In this work, we introduce Panoptic-DeepLab, a simple, strong, and fast\nsystem for panoptic segmentation, aiming to establish a solid baseline for\nbottom-up methods that can achieve comparable performance of two-stage methods\nwhile yielding fast inference speed. In particular, Panoptic-DeepLab adopts the\ndual-ASPP and dual-decoder structures specific to semantic, and instance\nsegmentation, respectively. The semantic segmentation branch is the same as the\ntypical design of any semantic segmentation model (e.g., DeepLab), while the\ninstance segmentation branch is class-agnostic, involving a simple instance\ncenter regression. As a result, our single Panoptic-DeepLab simultaneously\nranks first at all three Cityscapes benchmarks, setting the new state-of-art of\n84.2% mIoU, 39.0% AP, and 65.5% PQ on test set. Additionally, equipped with\nMobileNetV3, Panoptic-DeepLab runs nearly in real-time with a single 1025x2049\nimage (15.8 frames per second), while achieving a competitive performance on\nCityscapes (54.1 PQ% on test set). On Mapillary Vistas test set, our ensemble\nof six models attains 42.7% PQ, outperforming the challenge winner in 2018 by a\nhealthy margin of 1.5%. Finally, our Panoptic-DeepLab also performs on par with\nseveral top-down approaches on the challenging COCO dataset. For the first\ntime, we demonstrate a bottom-up approach could deliver state-of-the-art\nresults on panoptic segmentation."}, {"title": "Inter-Region Affinity Distillation for Road Marking Segmentation", "authors": "Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, Chen Change Loy", "link": "http://arxiv.org/abs/2004.05304", "summary": "We study the problem of distilling knowledge from a large deep teacher\nnetwork to a much smaller student network for the task of road marking\nsegmentation. In this work, we explore a novel knowledge distillation (KD)\napproach that can transfer 'knowledge' on scene structure more effectively from\na teacher to a student model. Our method is known as Inter-Region Affinity KD\n(IntRA-KD). It decomposes a given road scene image into different regions and\nrepresents each region as a node in a graph. An inter-region affinity graph is\nthen formed by establishing pairwise relationships between nodes based on their\nsimilarity in feature distribution. To learn structural knowledge from the\nteacher network, the student is required to match the graph generated by the\nteacher. The proposed method shows promising results on three large-scale road\nmarking segmentation benchmarks, i.e., ApolloScape, CULane and LLAMAS, by\ntaking various lightweight models as students and ResNet-101 as the teacher.\nIntRA-KD consistently brings higher performance gains on all lightweight\nmodels, compared to previous distillation methods. Our code is available at\nhttps://github.com/cardwing/Codes-for-IntRA-KD."}, {"title": "Unified Dynamic Convolutional Network for Super-Resolution With Variational Degradations", "authors": "Yu-Syuan Xu, Shou-Yao Roy Tseng, Yu Tseng, Hsien-Kai Kuo, Yi-Min Tsai", "link": "https://arxiv.org/abs/2004.06965", "summary": "Deep Convolutional Neural Networks (CNNs) have achieved remarkable results on\nSingle Image Super-Resolution (SISR). Despite considering only a single\ndegradation, recent studies also include multiple degrading effects to better\nreflect real-world cases. However, most of the works assume a fixed combination\nof degrading effects, or even train an individual network for different\ncombinations. Instead, a more practical approach is to train a single network\nfor wide-ranging and variational degradations. To fulfill this requirement,\nthis paper proposes a unified network to accommodate the variations from\ninter-image (cross-image variations) and intra-image (spatial variations).\nDifferent from the existing works, we incorporate dynamic convolution which is\na far more flexible alternative to handle different variations. In SISR with\nnon-blind setting, our Unified Dynamic Convolutional Network for Variational\nDegradations (UDVD) is evaluated on both synthetic and real images with an\nextensive set of variations. The qualitative results demonstrate the\neffectiveness of UDVD over various existing works. Extensive experiments show\nthat our UDVD achieves favorable or comparable performance on both synthetic\nand real images."}, {"title": "Making Better Mistakes: Leveraging Class Hierarchies With Deep Networks", "authors": "Luca Bertinetto, Romain Mueller, Konstantinos Tertikas, Sina Samangooei, Nicholas A. Lord", "link": "https://arxiv.org/abs/1912.09393", "summary": "Deep neural networks have improved image classification dramatically over the\npast decade, but have done so by focusing on performance measures that treat\nall classes other than the ground truth as equally wrong. This has led to a\nsituation in which mistakes are less likely to be made than before, but are\nequally likely to be absurd or catastrophic when they do occur. Past works have\nrecognised and tried to address this issue of mistake severity, often by using\ngraph distances in class hierarchies, but this has largely been neglected since\nthe advent of the current deep learning era in computer vision. In this paper,\nwe aim to renew interest in this problem by reviewing past approaches and\nproposing two simple modifications of the cross-entropy loss which outperform\nthe prior art under several metrics on two large datasets with complex class\nhierarchies: tieredImageNet and iNaturalist19."}, {"title": "Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN", "authors": "Jingwen Ye, Yixin Ji, Xinchao Wang, Xin Gao, Mingli Song", "link": "https://arxiv.org/abs/2003.09088", "summary": "Recent advances in deep learning have provided procedures for learning one\nnetwork to amalgamate multiple streams of knowledge from the pre-trained\nConvolutional Neural Network (CNN) models, thus reduce the annotation cost.\nHowever, almost all existing methods demand massive training data, which may be\nunavailable due to privacy or transmission issues. In this paper, we propose a\ndata-free knowledge amalgamate strategy to craft a well-behaved multi-task\nstudent network from multiple single/multi-task teachers. The main idea is to\nconstruct the group-stack generative adversarial networks (GANs) which have two\ndual generators. First one generator is trained to collect the knowledge by\nreconstructing the images approximating the original dataset utilized for\npre-training the teachers. Then a dual generator is trained by taking the\noutput from the former generator as input. Finally we treat the dual part\ngenerator as the target network and regroup it. As demonstrated on several\nbenchmarks of multi-label classification, the proposed method without any\ntraining data achieves the surprisingly competitive results, even compared with\nsome full-supervised methods."}, {"title": "Screencast Tutorial Video Understanding", "authors": "Kunpeng Li, Chen Fang, Zhaowen Wang, Seokhwan Kim, Hailin Jin, Yun Fu"}, {"title": "DSGN: Deep Stereo Geometry Network for 3D Object Detection", "authors": "Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia", "link": "https://arxiv.org/abs/2001.03398", "summary": "Most state-of-the-art 3D object detectors heavily rely on LiDAR sensors\nbecause there is a large performance gap between image-based and LiDAR-based\nmethods. It is caused by the way to form representation for the prediction in\n3D scenarios. Our method, called Deep Stereo Geometry Network (DSGN),\nsignificantly reduces this gap by detecting 3D objects on a differentiable\nvolumetric representation -- 3D geometric volume, which effectively encodes 3D\ngeometric structure for 3D regular space. With this representation, we learn\ndepth information and semantic cues simultaneously. For the first time, we\nprovide a simple and effective one-stage stereo-based 3D detection pipeline\nthat jointly estimates the depth and detects 3D objects in an end-to-end\nlearning manner. Our approach outperforms previous stereo-based 3D detectors\n(about 10 higher in terms of AP) and even achieves comparable performance with\nseveral LiDAR-based methods on the KITTI 3D object detection leaderboard. Our\ncode is publicly available at https://github.com/chenyilun95/DSGN."}, {"title": "Weakly-Supervised Salient Object Detection via Scribble Annotations", "authors": "Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, Yuchao Dai", "link": "https://arxiv.org/abs/2003.07685", "summary": "Compared with laborious pixel-wise dense labeling, it is much easier to label\ndata by scribbles, which only costs 1$\\sim$2 seconds to label one image.\nHowever, using scribble labels to learn salient object detection has not been\nexplored. In this paper, we propose a weakly-supervised salient object\ndetection model to learn saliency from such annotations. In doing so, we first\nrelabel an existing large-scale salient object detection dataset with\nscribbles, namely S-DUTS dataset. Since object structure and detail information\nis not identified by scribbles, directly training with scribble labels will\nlead to saliency maps of poor boundary localization. To mitigate this problem,\nwe propose an auxiliary edge detection task to localize object edges\nexplicitly, and a gated structure-aware loss to place constraints on the scope\nof structure to be recovered. Moreover, we design a scribble boosting scheme to\niteratively consolidate our scribble annotations, which are then employed as\nsupervision to learn high-quality saliency maps. As existing saliency\nevaluation metrics neglect to measure structure alignment of the predictions,\nthe saliency map ranking metric may not comply with human perception. We\npresent a new metric, termed saliency structure measure, to measure the\nstructure alignment of the predicted saliency maps, which is more consistent\nwith human perception. Extensive experiments on six benchmark datasets\ndemonstrate that our method not only outperforms existing\nweakly-supervised/unsupervised methods, but also is on par with several\nfully-supervised state-of-the-art models. Our code and data is publicly\navailable at https://github.com/JingZhang617/Scribble_Saliency."}, {"title": "Learning to Learn Single Domain Generalization", "authors": "Fengchun Qiao, Long Zhao, Xi Peng", "link": "https://arxiv.org/abs/2003.13216", "summary": "We are concerned with a worst-case scenario in model generalization, in the\nsense that a model aims to perform well on many unseen domains while there is\nonly one single domain available for training. We propose a new method named\nadversarial domain augmentation to solve this Out-of-Distribution (OOD)\ngeneralization problem. The key idea is to leverage adversarial training to\ncreate \"fictitious\" yet \"challenging\" populations, from which a model can learn\nto generalize with theoretical guarantees. To facilitate fast and desirable\ndomain augmentation, we cast the model training in a meta-learning scheme and\nuse a Wasserstein Auto-Encoder (WAE) to relax the widely used worst-case\nconstraint. Detailed theoretical analysis is provided to testify our\nformulation, while extensive experiments on multiple benchmark datasets\nindicate its superior performance in tackling single domain generalization."}, {"title": "Severity-Aware Semantic Segmentation With Reinforced Wasserstein Training", "authors": "Xiaofeng Liu, Wenxuan Ji, Jane You, Georges El Fakhri, Jonghye Woo"}, {"title": "Boosting Few-Shot Learning With Adaptive Margin Loss", "authors": "Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, Liwei Wang", "link": "https://arxiv.org/abs/2005.13826", "summary": "Few-shot learning (FSL) has attracted increasing attention in recent years\nbut remains challenging, due to the intrinsic difficulty in learning to\ngeneralize from a few examples. This paper proposes an adaptive margin\nprinciple to improve the generalization ability of metric-based meta-learning\napproaches for few-shot learning problems. Specifically, we first develop a\nclass-relevant additive margin loss, where semantic similarity between each\npair of classes is considered to separate samples in the feature embedding\nspace from similar classes. Further, we incorporate the semantic context among\nall classes in a sampled training task and develop a task-relevant additive\nmargin loss to better distinguish samples from different classes. Our adaptive\nmargin method can be easily extended to a more realistic generalized FSL\nsetting. Extensive experiments demonstrate that the proposed method can boost\nthe performance of current metric-based meta-learning approaches, under both\nthe standard FSL and generalized FSL settings."}, {"title": "JA-POLS: A Moving-Camera Background Model via Joint Alignment and Partially-Overlapping Local Subspaces", "authors": "Irit Chelly, Vlad Winter, Dor Litvak, David Rosen, Oren Freifeld"}, {"title": "AugFPN: Improving Multi-Scale Feature Learning for Object Detection", "authors": "Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, Chunhong Pan", "link": "https://arxiv.org/abs/1912.05384", "summary": "Current state-of-the-art detectors typically exploit feature pyramid to\ndetect objects at different scales. Among them, FPN is one of the\nrepresentative works that build a feature pyramid by multi-scale features\nsummation. However, the design defects behind prevent the multi-scale features\nfrom being fully exploited. In this paper, we begin by first analyzing the\ndesign defects of feature pyramid in FPN, and then introduce a new feature\npyramid architecture named AugFPN to address these problems. Specifically,\nAugFPN consists of three components: Consistent Supervision, Residual Feature\nAugmentation, and Soft RoI Selection. AugFPN narrows the semantic gaps between\nfeatures of different scales before feature fusion through Consistent\nSupervision. In feature fusion, ratio-invariant context information is\nextracted by Residual Feature Augmentation to reduce the information loss of\nfeature map at the highest pyramid level. Finally, Soft RoI Selection is\nemployed to learn a better RoI feature adaptively after feature fusion. By\nreplacing FPN with AugFPN in Faster R-CNN, our models achieve 2.3 and 1.6\npoints higher Average Precision (AP) when using ResNet50 and MobileNet-v2 as\nbackbone respectively. Furthermore, AugFPN improves RetinaNet by 1.6 points AP\nand FCOS by 0.9 points AP when using ResNet50 as backbone. Codes will be made\navailable."}, {"title": "xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation", "authors": "Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, Emilie Wirbel, Patrick P\u00e9rez", "link": "https://arxiv.org/abs/1911.12676", "summary": "Unsupervised Domain Adaptation (UDA) is crucial to tackle the lack of\nannotations in a new domain. There are many multi-modal datasets, but most UDA\napproaches are uni-modal. In this work, we explore how to learn from\nmulti-modality and propose cross-modal UDA (xMUDA) where we assume the presence\nof 2D images and 3D point clouds for 3D semantic segmentation. This is\nchallenging as the two input spaces are heterogeneous and can be impacted\ndifferently by domain shift. In xMUDA, modalities learn from each other through\nmutual mimicking, disentangled from the segmentation objective, to prevent the\nstronger modality from adopting false predictions from the weaker one. We\nevaluate on new UDA scenarios including day-to-night, country-to-country and\ndataset-to-dataset, leveraging recent autonomous driving datasets. xMUDA brings\nlarge improvements over uni-modal UDA on all tested scenarios, and is\ncomplementary to state-of-the-art UDA techniques. Code is available at\nhttps://github.com/valeoai/xmuda."}, {"title": "Norm-Aware Embedding for Efficient Person Search", "authors": "Di Chen, Shanshan Zhang, Jian Yang, Bernt Schiele"}, {"title": "Intelligent Home 3D: Automatic 3D-House Design From Linguistic Descriptions Only", "authors": "Qi Chen, Qi Wu, Rui Tang, Yuhan Wang, Shuai Wang, Mingkui Tan", "link": "https://arxiv.org/abs/2003.00397", "summary": "Home design is a complex task that normally requires architects to finish\nwith their professional skills and tools. It will be fascinating that if one\ncan produce a house plan intuitively without knowing much knowledge about home\ndesign and experience of using complex designing tools, for example, via\nnatural language. In this paper, we formulate it as a language conditioned\nvisual content generation problem that is further divided into a floor plan\ngeneration and an interior texture (such as floor and wall) synthesis task. The\nonly control signal of the generation process is the linguistic expression\ngiven by users that describe the house details. To this end, we propose a House\nPlan Generative Model (HPGM) that first translates the language input to a\nstructural graph representation and then predicts the layout of rooms with a\nGraph Conditioned Layout Prediction Network (GC LPN) and generates the interior\ntexture with a Language Conditioned Texture GAN (LCT-GAN). With some\npost-processing, the final product of this task is a 3D house model. To train\nand evaluate our model, we build the first Text-to-3D House Model dataset."}, {"title": "Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation", "authors": "Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas S. Huang, Honghui Shi", "link": "https://arxiv.org/abs/2003.08040", "summary": "We consider the problem of unsupervised domain adaptation for semantic\nsegmentation by easing the domain shift between the source domain (synthetic\ndata) and the target domain (real data) in this work. State-of-the-art\napproaches prove that performing semantic-level alignment is helpful in\ntackling the domain shift issue. Based on the observation that stuff categories\nusually share similar appearances across images of different domains while\nthings (i.e. object instances) have much larger differences, we propose to\nimprove the semantic-level alignment with different strategies for stuff\nregions and for things: 1) for the stuff categories, we generate feature\nrepresentation for each class and conduct the alignment operation from the\ntarget domain to the source domain; 2) for the thing categories, we generate\nfeature representation for each individual instance and encourage the instance\nin the target domain to align with the most similar one in the source domain.\nIn this way, the individual differences within thing categories will also be\nconsidered to alleviate over-alignment. In addition to our proposed method, we\nfurther reveal the reason why the current adversarial loss is often unstable in\nminimizing the distribution discrepancy and show that our method can help ease\nthis issue by minimizing the most similar stuff and instance features between\nthe source and the target domains. We conduct extensive experiments in two\nunsupervised domain adaptation tasks, i.e. GTA5 to Cityscapes and SYNTHIA to\nCityscapes, and achieve the new state-of-the-art segmentation accuracy."}, {"title": "Robust Object Detection Under Occlusion With Context-Aware CompositionalNets", "authors": "Angtian Wang, Yihong Sun, Adam Kortylewski, Alan L. Yuille", "link": "https://arxiv.org/abs/2005.11643", "summary": "Detecting partially occluded objects is a difficult task. Our experimental\nresults show that deep learning approaches, such as Faster R-CNN, are not\nrobust at object detection under occlusion. Compositional convolutional neural\nnetworks (CompositionalNets) have been shown to be robust at classifying\noccluded objects by explicitly representing the object as a composition of\nparts. In this work, we propose to overcome two limitations of\nCompositionalNets which will enable them to detect partially occluded objects:\n1) CompositionalNets, as well as other DCNN architectures, do not explicitly\nseparate the representation of the context from the object itself. Under strong\nobject occlusion, the influence of the context is amplified which can have\nsevere negative effects for detection at test time. In order to overcome this,\nwe propose to segment the context during training via bounding box annotations.\nWe then use the segmentation to learn a context-aware CompositionalNet that\ndisentangles the representation of the context and the object. 2) We extend the\npart-based voting scheme in CompositionalNets to vote for the corners of the\nobject's bounding box, which enables the model to reliably estimate bounding\nboxes for partially occluded objects. Our extensive experiments show that our\nproposed model can detect objects robustly, increasing the detection\nperformance of strongly occluded vehicles from PASCAL3D+ and MS-COCO by 41% and\n35% respectively in absolute performance relative to Faster R-CNN."}, {"title": "IMRAM: Iterative Matching With Recurrent Attention Memory for Cross-Modal Image-Text Retrieval", "authors": "Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, Jungong Han", "link": "http://arxiv.org/abs/2003.03772", "summary": "Enabling bi-directional retrieval of images and texts is important for\nunderstanding the correspondence between vision and language. Existing methods\nleverage the attention mechanism to explore such correspondence in a\nfine-grained manner. However, most of them consider all semantics equally and\nthus align them uniformly, regardless of their diverse complexities. In fact,\nsemantics are diverse (i.e. involving different kinds of semantic concepts),\nand humans usually follow a latent structure to combine them into\nunderstandable languages. It may be difficult to optimally capture such\nsophisticated correspondences in existing methods. In this paper, to address\nsuch a deficiency, we propose an Iterative Matching with Recurrent Attention\nMemory (IMRAM) method, in which correspondences between images and texts are\ncaptured with multiple steps of alignments. Specifically, we introduce an\niterative matching scheme to explore such fine-grained correspondence\nprogressively. A memory distillation unit is used to refine alignment knowledge\nfrom early steps to later ones. Experiment results on three benchmark datasets,\ni.e. Flickr8K, Flickr30K, and MS COCO, show that our IMRAM achieves\nstate-of-the-art performance, well demonstrating its effectiveness. Experiments\non a practical business advertisement dataset, named \\Ads{}, further validates\nthe applicability of our method in practical scenarios."}, {"title": "Domain-Aware Visual Bias Eliminating for Generalized Zero-Shot Learning", "authors": "Shaobo Min, Hantao Yao, Hongtao Xie, Chaoqun Wang, Zheng-Jun Zha, Yongdong Zhang", "link": "http://arxiv.org/abs/2003.13261", "summary": "Recent methods focus on learning a unified semantic-aligned visual\nrepresentation to transfer knowledge between two domains, while ignoring the\neffect of semantic-free visual representation in alleviating the biased\nrecognition problem. In this paper, we propose a novel Domain-aware Visual Bias\nEliminating (DVBE) network that constructs two complementary visual\nrepresentations, i.e., semantic-free and semantic-aligned, to treat seen and\nunseen domains separately. Specifically, we explore cross-attentive\nsecond-order visual statistics to compact the semantic-free representation, and\ndesign an adaptive margin Softmax to maximize inter-class divergences. Thus,\nthe semantic-free representation becomes discriminative enough to not only\npredict seen class accurately but also filter out unseen images, i.e., domain\ndetection, based on the predicted class entropy. For unseen images, we\nautomatically search an optimal semantic-visual alignment architecture, rather\nthan manual designs, to predict unseen classes. With accurate domain detection,\nthe biased recognition problem towards the seen domain is significantly\nreduced. Experiments on five benchmarks for classification and segmentation\nshow that DVBE outperforms existing methods by averaged 5.7% improvement."}, {"title": "Semi-Supervised Semantic Segmentation With Cross-Consistency Training", "authors": "Yassine Ouali, C\u00e9line Hudelot, Myriam Tami", "link": "https://arxiv.org/abs/2003.09005", "summary": "In this paper, we present a novel cross-consistency based semi-supervised\napproach for semantic segmentation. Consistency training has proven to be a\npowerful semi-supervised learning framework for leveraging unlabeled data under\nthe cluster assumption, in which the decision boundary should lie in\nlow-density regions. In this work, we first observe that for semantic\nsegmentation, the low-density regions are more apparent within the hidden\nrepresentations than within the inputs. We thus propose cross-consistency\ntraining, where an invariance of the predictions is enforced over different\nperturbations applied to the outputs of the encoder. Concretely, a shared\nencoder and a main decoder are trained in a supervised manner using the\navailable labeled examples. To leverage the unlabeled examples, we enforce a\nconsistency between the main decoder predictions and those of the auxiliary\ndecoders, taking as inputs different perturbed versions of the encoder's\noutput, and consequently, improving the encoder's representations. The proposed\nmethod is simple and can easily be extended to use additional training signal,\nsuch as image-level labels or pixel-level labels across different domains. We\nperform an ablation study to tease apart the effectiveness of each component,\nand conduct extensive experiments to demonstrate that our method achieves\nstate-of-the-art results in several datasets."}, {"title": "Learning to Learn Cropping Models for Different Aspect Ratio Requirements", "authors": "Debang Li, Junge Zhang, Kaiqi Huang"}, {"title": "What Makes Training Multi-Modal Classification Networks Hard?", "authors": "Weiyao Wang, Du Tran, Matt Feiszli", "link": "http://arxiv.org/abs/1905.12681", "summary": "Consider end-to-end training of a multi-modal vs. a single-modal network on a\ntask with multiple input modalities: the multi-modal network receives more\ninformation, so it should match or outperform its single-modal counterpart. In\nour experiments, however, we observe the opposite: the best single-modal\nnetwork always outperforms the multi-modal network. This observation is\nconsistent across different combinations of modalities and on different tasks\nand benchmarks.\n  This paper identifies two main causes for this performance drop: first,\nmulti-modal networks are often prone to overfitting due to increased capacity.\nSecond, different modalities overfit and generalize at different rates, so\ntraining them jointly with a single optimization strategy is sub-optimal. We\naddress these two problems with a technique we call Gradient Blending, which\ncomputes an optimal blend of modalities based on their overfitting behavior. We\ndemonstrate that Gradient Blending outperforms widely-used baselines for\navoiding overfitting and achieves state-of-the-art accuracy on various tasks\nincluding human action recognition, ego-centric action recognition, and\nacoustic event detection."}, {"title": "Selective Transfer With Reinforced Transfer Network for Partial Domain Adaptation", "authors": "Zhihong Chen, Chao Chen, Zhaowei Cheng, Boyuan Jiang, Ke Fang, Xinyu Jin", "link": "https://arxiv.org/abs/1905.10756", "summary": "One crucial aspect of partial domain adaptation (PDA) is how to select the\nrelevant source samples in the shared classes for knowledge transfer. Previous\nPDA methods tackle this problem by re-weighting the source samples based on\ntheir high-level information (deep features). However, since the domain shift\nbetween source and target domains, only using the deep features for sample\nselection is defective. We argue that it is more reasonable to additionally\nexploit the pixel-level information for PDA problem, as the appearance\ndifference between outlier source classes and target classes is significantly\nlarge. In this paper, we propose a reinforced transfer network (RTNet), which\nutilizes both high-level and pixel-level information for PDA problem. Our RTNet\nis composed of a reinforced data selector (RDS) based on reinforcement learning\n(RL), which filters out the outlier source samples, and a domain adaptation\nmodel which minimizes the domain discrepancy in the shared label space.\nSpecifically, in the RDS, we design a novel reward based on the reconstruct\nerrors of selected source samples on the target generator, which introduces the\npixel-level information to guide the learning of RDS. Besides, we develope a\nstate containing high-level information, which used by the RDS for sample\nselection. The proposed RDS is a general module, which can be easily integrated\ninto existing DA models to make them fit the PDA situation. Extensive\nexperiments indicate that RTNet can achieve state-of-the-art performance for\nPDA tasks on several benchmark datasets."}, {"title": "Semi-Supervised Semantic Image Segmentation With Self-Correcting Networks", "authors": "Mostafa S. Ibrahim, Arash Vahdat, Mani Ranjbar, William G. Macready", "link": "https://arxiv.org/abs/1811.07073", "summary": "Building a large image dataset with high-quality object masks for semantic\nsegmentation is costly and time consuming. In this paper, we introduce a\nprincipled semi-supervised framework that only uses a small set of fully\nsupervised images (having semantic segmentation labels and box labels) and a\nset of images with only object bounding box labels (we call it the weak set).\nOur framework trains the primary segmentation model with the aid of an\nancillary model that generates initial segmentation labels for the weak set and\na self-correction module that improves the generated labels during training\nusing the increasingly accurate primary model. We introduce two variants of the\nself-correction module using either linear or convolutional functions.\nExperiments on the PASCAL VOC 2012 and Cityscape datasets show that our models\ntrained with a small fully supervised set perform similar to, or better than,\nmodels trained with a large fully supervised set while requiring ~7x less\nannotation effort."}, {"title": "Exemplar Normalization for Learning Deep Representation", "authors": "Ruimao Zhang, Zhanglin Peng, Lingyun Wu, Zhen Li, Ping Luo", "link": "https://arxiv.org/abs/2003.08761", "summary": "Normalization techniques are important in different advanced neural networks\nand different tasks. This work investigates a novel dynamic\nlearning-to-normalize (L2N) problem by proposing Exemplar Normalization (EN),\nwhich is able to learn different normalization methods for different\nconvolutional layers and image samples of a deep network. EN significantly\nimproves flexibility of the recently proposed switchable normalization (SN),\nwhich solves a static L2N problem by linearly combining several normalizers in\neach normalization layer (the combination is the same for all samples). Instead\nof directly employing a multi-layer perceptron (MLP) to learn data-dependent\nparameters as conditional batch normalization (cBN) did, the internal\narchitecture of EN is carefully designed to stabilize its optimization, leading\nto many appealing benefits. (1) EN enables different convolutional layers,\nimage samples, categories, benchmarks, and tasks to use different normalization\nmethods, shedding light on analyzing them in a holistic view. (2) EN is\neffective for various network architectures and tasks. (3) It could replace any\nnormalization layers in a deep network and still produce stable model training.\nExtensive experiments demonstrate the effectiveness of EN in a wide spectrum of\ntasks including image recognition, noisy label learning, and semantic\nsegmentation. For example, by replacing BN in the ordinary ResNet50,\nimprovement produced by EN is 300% more than that of SN on both ImageNet and\nthe noisy WebVision dataset."}, {"title": "Imitative Non-Autoregressive Modeling for Trajectory Forecasting and Imputation", "authors": "Mengshi Qi, Jie Qin, Yu Wu, Yi Yang"}, {"title": "Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text", "authors": "Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, Xilin Chen", "link": "http://arxiv.org/abs/2003.13962", "summary": "Answering questions that require reading texts in an image is challenging for\ncurrent models. One key difficulty of this task is that rare, polysemous, and\nambiguous words frequently appear in images, e.g., names of places, products,\nand sports teams. To overcome this difficulty, only resorting to pre-trained\nword embedding models is far from enough. A desired model should utilize the\nrich information in multiple modalities of the image to help understand the\nmeaning of scene texts, e.g., the prominent text on a bottle is most likely to\nbe the brand. Following this idea, we propose a novel VQA approach, Multi-Modal\nGraph Neural Network (MM-GNN). It first represents an image as a graph\nconsisting of three sub-graphs, depicting visual, semantic, and numeric\nmodalities respectively. Then, we introduce three aggregators which guide the\nmessage passing from one graph to another to utilize the contexts in various\nmodalities, so as to refine the features of nodes. The updated nodes have\nbetter features for the downstream question answering module. Experimental\nevaluations show that our MM-GNN represents the scene texts better and\nobviously facilitates the performances on two VQA tasks that require reading\nscene texts."}, {"title": "StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization of Domain Translation and Stereo Matching", "authors": "Rui Liu, Chengxi Yang, Wenxiu Sun, Xiaogang Wang, Hongsheng Li", "link": "https://arxiv.org/abs/2005.01927", "summary": "Large-scale synthetic datasets are beneficial to stereo matching but usually\nintroduce known domain bias. Although unsupervised image-to-image translation\nnetworks represented by CycleGAN show great potential in dealing with domain\ngap, it is non-trivial to generalize this method to stereo matching due to the\nproblem of pixel distortion and stereo mismatch after translation. In this\npaper, we propose an end-to-end training framework with domain translation and\nstereo matching networks to tackle this challenge. First, joint optimization\nbetween domain translation and stereo matching networks in our end-to-end\nframework makes the former facilitate the latter one to the maximum extent.\nSecond, this framework introduces two novel losses, i.e., bidirectional\nmulti-scale feature re-projection loss and correlation consistency loss, to\nhelp translate all synthetic stereo images into realistic ones as well as\nmaintain epipolar constraints. The effective combination of above two\ncontributions leads to impressive stereo-consistent translation and disparity\nestimation accuracy. In addition, a mode seeking regularization term is added\nto endow the synthetic-to-real translation results with higher fine-grained\ndiversity. Extensive experiments demonstrate the effectiveness of the proposed\nframework on bridging the synthetic-to-real domain gap on stereo matching."}, {"title": "Self-Supervised Domain-Aware Generative Network for Generalized Zero-Shot Learning", "authors": "Jiamin Wu, Tianzhu Zhang, Zheng-Jun Zha, Jiebo Luo, Yongdong Zhang, Feng Wu", "link": "", "summary": ""}, {"title": "Sparse Layered Graphs for Multi-Object Segmentation", "authors": "Niels Jeppesen, Anders N. Christensen, Vedrana A. Dahl, Anders B. Dahl"}, {"title": "Visual-Semantic Matching by Exploring High-Order Attention and Distraction", "authors": "Yongzhi Li, Duo Zhang, Yadong Mu"}, {"title": "End-to-End 3D Point Cloud Instance Segmentation Without Detection", "authors": "Haiyong Jiang, Feilong Yan, Jianfei Cai, Jianmin Zheng, Jun Xiao"}, {"title": "Deep Adversarial Decomposition: A Unified Framework for Separating Superimposed Images", "authors": "Zhengxia Zou, Sen Lei, Tianyang Shi, Zhenwei Shi, Jieping Ye"}, {"title": "Differentiable Adaptive Computation Time for Visual Reasoning", "authors": "Crist\u00f3bal Eyzaguirre, \u00c1lvaro Soto", "link": "https://arxiv.org/abs/2004.12770", "summary": "This paper presents a novel attention-based algorithm for achieving adaptive\ncomputation called DACT, which, unlike existing ones, is end-to-end\ndifferentiable. Our method can be used in conjunction with many networks; in\nparticular, we study its application to the widely known MAC architecture,\nobtaining a significant reduction in the number of recurrent steps needed to\nachieve similar accuracies, therefore improving its performance to computation\nratio. Furthermore, we show that by increasing the maximum number of steps\nused, we surpass the accuracy of even our best non-adaptive MAC in the CLEVR\ndataset, demonstrating that our approach is able to control the number of steps\nwithout significant loss of performance. Additional advantages provided by our\napproach include considerably improving interpretability by discarding useless\nsteps and providing more insights into the underlying reasoning process.\nFinally, we present adaptive computation as an equivalent to an ensemble of\nmodels, similar to a mixture of expert formulation. Both the code and the\nconfiguration files for our experiments are made available to support further\nresearch in this area."}, {"title": "DeepLPF: Deep Local Parametric Filters for Image Enhancement", "authors": "Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, Gregory Slabaugh", "link": "https://arxiv.org/abs/2003.13985", "summary": "Digital artists often improve the aesthetic quality of digital photographs\nthrough manual retouching. Beyond global adjustments, professional image\nediting programs provide local adjustment tools operating on specific parts of\nan image. Options include parametric (graduated, radial filters) and\nunconstrained brush tools. These highly expressive tools enable a diverse set\nof local image enhancements. However, their use can be time consuming, and\nrequires artistic capability. State-of-the-art automated image enhancement\napproaches typically focus on learning pixel-level or global enhancements. The\nformer can be noisy and lack interpretability, while the latter can fail to\ncapture fine-grained adjustments. In this paper, we introduce a novel approach\nto automatically enhance images using learned spatially local filters of three\ndifferent types (Elliptical Filter, Graduated Filter, Polynomial Filter). We\nintroduce a deep neural network, dubbed Deep Local Parametric Filters\n(DeepLPF), which regresses the parameters of these spatially localized filters\nthat are then automatically applied to enhance the image. DeepLPF provides a\nnatural form of model regularization and enables interpretable, intuitive\nadjustments that lead to visually pleasing results. We report on multiple\nbenchmarks and show that DeepLPF produces state-of-the-art performance on two\nvariants of the MIT-Adobe-5K dataset, often using a fraction of the parameters\nrequired for competing methods."}, {"title": "Instance Credibility Inference for Few-Shot Learning", "authors": "Yikai Wang, Chengming Xu, Chen Liu, Li Zhang, Yanwei Fu", "link": "http://arxiv.org/abs/2003.11853", "summary": "Few-shot learning (FSL) aims to recognize new objects with extremely limited\ntraining data for each category. Previous efforts are made by either leveraging\nmeta-learning paradigm or novel principles in data augmentation to alleviate\nthis extremely data-scarce problem. In contrast, this paper presents a simple\nstatistical approach, dubbed Instance Credibility Inference (ICI) to exploit\nthe distribution support of unlabeled instances for few-shot learning.\nSpecifically, we first train a linear classifier with the labeled few-shot\nexamples and use it to infer the pseudo-labels for the unlabeled data. To\nmeasure the credibility of each pseudo-labeled instance, we then propose to\nsolve another linear regression hypothesis by increasing the sparsity of the\nincidental parameters and rank the pseudo-labeled instances with their sparsity\ndegree. We select the most trustworthy pseudo-labeled instances alongside the\nlabeled examples to re-train the linear classifier. This process is iterated\nuntil all the unlabeled samples are included in the expanded training set, i.e.\nthe pseudo-label is converged for unlabeled data pool. Extensive experiments\nunder two few-shot settings show that our simple approach can establish new\nstate-of-the-arts on four widely used few-shot learning benchmark datasets\nincluding miniImageNet, tieredImageNet, CIFAR-FS, and CUB. Our code is\navailable at: https://github.com/Yikai-Wang/ICI-FSL"}, {"title": "Learning From Web Data With Self-Organizing Memory Module", "authors": "Yi Tu, Li Niu, Junjie Chen, Dawei Cheng, Liqing Zhang", "link": "https://arxiv.org/abs/1906.12028", "summary": "Learning from web data has attracted lots of research interest in recent\nyears. However, crawled web images usually have two types of noises, label\nnoise and background noise, which induce extra difficulties in utilizing them\neffectively. Most existing methods either rely on human supervision or ignore\nthe background noise. In this paper, we propose a novel method, which is\ncapable of handling these two types of noises together, without the supervision\nof clean images in the training stage. Particularly, we formulate our method\nunder the framework of multi-instance learning by grouping ROIs (i.e., images\nand their region proposals) from the same category into bags. ROIs in each bag\nare assigned with different weights based on the representative/discriminative\nscores of their nearest clusters, in which the clusters and their scores are\nobtained via our designed memory module. Our memory module could be naturally\nintegrated with the classification module, leading to an end-to-end trainable\nsystem. Extensive experiments on four benchmark datasets demonstrate the\neffectiveness of our method."}, {"title": "TransMatch: A Transfer-Learning Scheme for Semi-Supervised Few-Shot Learning", "authors": "Zhongjie Yu, Lin Chen, Zhongwei Cheng, Jiebo Luo", "link": "https://arxiv.org/abs/1912.09033", "summary": "The successful application of deep learning to many visual recognition tasks\nrelies heavily on the availability of a large amount of labeled data which is\nusually expensive to obtain. The few-shot learning problem has attracted\nincreasing attention from researchers for building a robust model upon only a\nfew labeled samples. Most existing works tackle this problem under the\nmeta-learning framework by mimicking the few-shot learning task with an\nepisodic training strategy. In this paper, we propose a new transfer-learning\nframework for semi-supervised few-shot learning to fully utilize the auxiliary\ninformation from labeled base-class data and unlabeled novel-class data. The\nframework consists of three components: 1) pre-training a feature extractor on\nbase-class data; 2) using the feature extractor to initialize the classifier\nweights for the novel classes; and 3) further updating the model with a\nsemi-supervised learning method. Under the proposed framework, we develop a\nnovel method for semi-supervised few-shot learning called TransMatch by\ninstantiating the three components with Imprinting and MixMatch. Extensive\nexperiments on two popular benchmark datasets for few-shot learning,\nCUB-200-2011 and miniImageNet, demonstrate that our proposed method can\neffectively utilize the auxiliary information from labeled base-class data and\nunlabeled novel-class data to significantly improve the accuracy of few-shot\nlearning task."}, {"title": "Learning the Redundancy-Free Features for Generalized Zero-Shot Object Recognition", "authors": "Zongyan Han, Zhenyong Fu, Jian Yang", "link": "", "summary": ""}, {"title": "Neural Topological SLAM for Visual Navigation", "authors": "Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, Saurabh Gupta"}, {"title": "WaveletStereo: Learning Wavelet Coefficients of Disparity Map in Stereo Matching", "authors": "Menglong Yang, Fangrui Wu, Wei Li"}, {"title": "Robust Superpixel-Guided Attentional Adversarial Attack", "authors": "Xiaoyi Dong, Jiangfan Han, Dongdong Chen, Jiayang Liu, Huanyu Bian, Zehua Ma, Hongsheng Li, Xiaogang Wang, Weiming Zhang, Nenghai Yu"}, {"title": "BEDSR-Net: A Deep Shadow Removal Network From a Single Document Image", "authors": "Yun-Hsuan Lin, Wen-Chin Chen, Yung-Yu Chuang"}, {"title": "Cross-Domain Document Object Detection: Benchmark Suite and Method", "authors": "Kai Li, Curtis Wigington, Chris Tensmeyer, Handong Zhao, Nikolaos Barmpalios, Vlad I. Morariu, Varun Manjunatha, Tong Sun, Yun Fu", "link": "http://arxiv.org/abs/2003.13197", "summary": "Decomposing images of document pages into high-level semantic regions (e.g.,\nfigures, tables, paragraphs), document object detection (DOD) is fundamental\nfor downstream tasks like intelligent document editing and understanding. DOD\nremains a challenging problem as document objects vary significantly in layout,\nsize, aspect ratio, texture, etc. An additional challenge arises in practice\nbecause large labeled training datasets are only available for domains that\ndiffer from the target domain. We investigate cross-domain DOD, where the goal\nis to learn a detector for the target domain using labeled data from the source\ndomain and only unlabeled data from the target domain. Documents from the two\ndomains may vary significantly in layout, language, and genre. We establish a\nbenchmark suite consisting of different types of PDF document datasets that can\nbe utilized for cross-domain DOD model training and evaluation. For each\ndataset, we provide the page images, bounding box annotations, PDF files, and\nthe rendering layers extracted from the PDF files. Moreover, we propose a novel\ncross-domain DOD model which builds upon the standard detection model and\naddresses domain shifts by incorporating three novel alignment modules: Feature\nPyramid Alignment (FPA) module, Region Alignment (RA) module and Rendering\nLayer alignment (RLA) module. Extensive experiments on the benchmark suite\nsubstantiate the efficacy of the three proposed modules and the proposed method\nsignificantly outperforms the baseline methods. The project page is at\n\\url{https://github.com/kailigo/cddod}."}, {"title": "Explaining Knowledge Distillation by Quantifying the Knowledge", "authors": "Xu Cheng, Zhefan Rao, Yilan Chen, Quanshi Zhang", "link": "https://arxiv.org/abs/2003.03622", "summary": "This paper presents a method to interpret the success of knowledge\ndistillation by quantifying and analyzing task-relevant and task-irrelevant\nvisual concepts that are encoded in intermediate layers of a deep neural\nnetwork (DNN). More specifically, three hypotheses are proposed as follows. 1.\nKnowledge distillation makes the DNN learn more visual concepts than learning\nfrom raw data. 2. Knowledge distillation ensures that the DNN is prone to\nlearning various visual concepts simultaneously. Whereas, in the scenario of\nlearning from raw data, the DNN learns visual concepts sequentially. 3.\nKnowledge distillation yields more stable optimization directions than learning\nfrom raw data. Accordingly, we design three types of mathematical metrics to\nevaluate feature representations of the DNN. In experiments, we diagnosed\nvarious DNNs, and above hypotheses were verified."}, {"title": "Exploring Bottom-Up and Top-Down Cues With Attentive Learning for Webly Supervised Object Detection", "authors": "Zhonghua Wu, Qingyi Tao, Guosheng Lin, Jianfei Cai", "link": "http://arxiv.org/abs/2003.09790", "summary": "Fully supervised object detection has achieved great success in recent years.\nHowever, abundant bounding boxes annotations are needed for training a detector\nfor novel classes. To reduce the human labeling effort, we propose a novel\nwebly supervised object detection (WebSOD) method for novel classes which only\nrequires the web images without further annotations. Our proposed method\ncombines bottom-up and top-down cues for novel class detection. Within our\napproach, we introduce a bottom-up mechanism based on the well-trained fully\nsupervised object detector (i.e. Faster RCNN) as an object region estimator for\nweb images by recognizing the common objectiveness shared by base and novel\nclasses. With the estimated regions on the web images, we then utilize the\ntop-down attention cues as the guidance for region classification. Furthermore,\nwe propose a residual feature refinement (RFR) block to tackle the domain\nmismatch between web domain and the target domain. We demonstrate our proposed\nmethod on PASCAL VOC dataset with three different novel/base splits. Without\nany target-domain novel-class images and annotations, our proposed webly\nsupervised object detection model is able to achieve promising performance for\nnovel classes. Moreover, we also conduct transfer learning experiments on large\nscale ILSVRC 2013 detection dataset and achieve state-of-the-art performance."}, {"title": "Enhancing Generic Segmentation With Learned Region Representations", "authors": "Or Isaacs, Oran Shayer, Michael Lindenbaum", "link": "https://arxiv.org/abs/1911.08564", "summary": "Current successful approaches for generic (non-semantic) segmentation rely\nmostly on edge detection and have leveraged the strengths of deep learning\nmainly by improving the edge detection stage in the algorithmic pipeline. This\nis in contrast to semantic and instance segmentation, where DNNs are applied\ndirectly to generate pixel-wise segment representations. We propose a new\nmethod for learning a pixel-wise representation that reflects segment\nrelatedness. This representation is combined with an edge map to yield a new\nsegmentation algorithm. We show that the representations themselves achieve\nstate-of-the-art segment similarity scores. Moreover, the proposed combined\nsegmentation algorithm provides results that are either state of the art or\nimprove upon it, for most quality measures."}, {"title": "Adaptive Hierarchical Down-Sampling for Point Cloud Classification", "authors": "Ehsan Nezhadarya, Ehsan Taghavi, Ryan Razani, Bingbing Liu, Jun Luo", "link": "https://arxiv.org/abs/1904.08506", "summary": "While several convolution-like operators have recently been proposed for\nextracting features out of point clouds, down-sampling an unordered point cloud\nin a deep neural network has not been rigorously studied. Existing methods\ndown-sample the points regardless of their importance for the output. As a\nresult, some important points in the point cloud may be removed, while less\nvaluable points may be passed to the next layers. In contrast, adaptive\ndown-sampling methods sample the points by taking into account the importance\nof each point, which varies based on the application, task and training data.\nIn this paper, we propose a permutation-invariant learning-based adaptive\ndown-sampling layer, called Critical Points Layer (CPL), which reduces the\nnumber of points in an unordered point cloud while retaining the important\npoints. Unlike most graph-based point cloud down-sampling methods that use\n$k$-NN search algorithm to find the neighbouring points, CPL is a global\ndown-sampling method, rendering it computationally very efficient. The proposed\nlayer can be used along with any graph-based point cloud convolution layer to\nform a convolutional neural network, dubbed CP-Net in this paper. We introduce\na CP-Net for $3$D object classification that achieves the best accuracy for the\nModelNet$40$ dataset among point cloud-based methods, which validates the\neffectiveness of the CPL."}, {"title": "FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions", "authors": "Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuandong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu, Kan Chen, Peter Vajda, Joseph E. Gonzalez", "link": "https://arxiv.org/abs/2004.05565", "summary": "Differentiable Neural Architecture Search (DNAS) has demonstrated great\nsuccess in designing state-of-the-art, efficient neural networks. However,\nDARTS-based DNAS's search space is small when compared to other search\nmethods', since all candidate network layers must be explicitly instantiated in\nmemory. To address this bottleneck, we propose a memory and computationally\nefficient DNAS variant: DMaskingNAS. This algorithm expands the search space by\nup to $10^{14}\\times$ over conventional DNAS, supporting searches over spatial\nand channel dimensions that are otherwise prohibitively expensive: input\nresolution and number of filters. We propose a masking mechanism for feature\nmap reuse, so that memory and computational costs stay nearly constant as the\nsearch space expands. Furthermore, we employ effective shape propagation to\nmaximize per-FLOP or per-parameter accuracy. The searched FBNetV2s yield\nstate-of-the-art performance when compared with all previous architectures.\nWith up to 421$\\times$ less search cost, DMaskingNAS finds models with 0.9%\nhigher accuracy, 15% fewer FLOPs than MobileNetV3-Small; and with similar\naccuracy but 20% fewer FLOPs than Efficient-B0. Furthermore, our FBNetV2\noutperforms MobileNetV3 by 2.6% in accuracy, with equivalent model size.\nFBNetV2 models are open-sourced at\nhttps://github.com/facebookresearch/mobile-vision."}, {"title": "Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation", "authors": "Myeongjin Kim, Hyeran Byun", "link": "https://arxiv.org/abs/2003.00867", "summary": "Since annotating pixel-level labels for semantic segmentation is laborious,\nleveraging synthetic data is an attractive solution. However, due to the domain\ngap between synthetic domain and real domain, it is challenging for a model\ntrained with synthetic data to generalize to real data. In this paper,\nconsidering the fundamental difference between the two domains as the texture,\nwe propose a method to adapt to the texture of the target domain. First, we\ndiversity the texture of synthetic images using a style transfer algorithm. The\nvarious textures of generated images prevent a segmentation model from\noverfitting to one specific (synthetic) texture. Then, we fine-tune the model\nwith self-training to get direct supervision of the target texture. Our results\nachieve state-of-the-art performance and we analyze the properties of the model\ntrained on the stylized dataset with extensive experiments."}, {"title": "Putting Visual Object Recognition in Context", "authors": "Mengmi Zhang, Claire Tseng, Gabriel Kreiman", "link": "https://arxiv.org/abs/1911.07349", "summary": "Context plays an important role in visual recognition. Recent studies have\nshown that visual recognition networks can be fooled by placing objects in\ninconsistent contexts (e.g., a cow in the ocean). To model the role of\ncontextual information in visual recognition, we systematically investigated\nten critical properties of where, when, and how context modulates recognition,\nincluding the amount of context, context and object resolution, geometrical\nstructure of context, context congruence, and temporal dynamics of contextual\nmodulation. The tasks involved recognizing a target object surrounded with\ncontext in a natural image. As an essential benchmark, we conducted a series of\npsychophysics experiments where we altered one aspect of context at a time, and\nquantified recognition accuracy. We propose a biologically-inspired\ncontext-aware object recognition model consisting of a two-stream architecture.\nThe model processes visual information at the fovea and periphery in parallel,\ndynamically incorporates object and contextual information, and sequentially\nreasons about the class label for the target object. Across a wide range of\nbehavioral tasks, the model approximates human level performance without\nretraining for each task, captures the dependence of context enhancement on\nimage properties, and provides initial steps towards integrating scene and\nobject information for visual recognition. All source code and data are\npublicly available: https://github.com/kreimanlab/Put-In-Context."}, {"title": "SLV: Spatial Likelihood Voting for Weakly Supervised Object Detection", "authors": "Ze Chen, Zhihang Fu, Rongxin Jiang, Yaowu Chen, Xian-Sheng Hua"}, {"title": "Universal Weighting Metric Learning for Cross-Modal Matching", "authors": "Jiwei Wei, Xing Xu, Yang Yang, Yanli Ji, Zheng Wang, Heng Tao Shen"}, {"title": "IDA-3D: Instance-Depth-Aware 3D Object Detection From Stereo Vision for Autonomous Driving", "authors": "Wanli Peng, Hao Pan, He Liu, Yi Sun"}, {"title": "Label Decoupling Framework for Salient Object Detection", "authors": "Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang, Qi Tian"}, {"title": "Transform and Tell: Entity-Aware News Image Captioning", "authors": "Alasdair Tran, Alexander Mathews, Lexing Xie", "link": "https://arxiv.org/abs/2004.08070", "summary": "We propose an end-to-end model which generates captions for images embedded\nin news articles. News images present two key challenges: they rely on\nreal-world knowledge, especially about named entities; and they typically have\nlinguistically rich captions that include uncommon words. We address the first\nchallenge by associating words in the caption with faces and objects in the\nimage, via a multi-modal, multi-head attention mechanism. We tackle the second\nchallenge with a state-of-the-art transformer language model that uses\nbyte-pair-encoding to generate captions as a sequence of word parts. On the\nGoodNews dataset, our model outperforms the previous state of the art by a\nfactor of four in CIDEr score (13 to 54). This performance gain comes from a\nunique combination of language models, word representation, image embeddings,\nface embeddings, object embeddings, and improvements in neural network design.\nWe also introduce the NYTimes800k dataset which is 70% larger than GoodNews,\nhas higher article quality, and includes the locations of images within\narticles as an additional contextual cue."}, {"title": "HAMBox: Delving Into Mining High-Quality Anchors on Face Detection", "authors": "Yang Liu, Xu Tang, Junyu Han, Jingtuo Liu, Dinger Rui, Xiang Wu", "link": "", "summary": ""}, {"title": "Hierarchical Feature Embedding for Attribute Recognition", "authors": "Jie Yang, Jiarou Fan, Yiru Wang, Yige Wang, Weihao Gan, Lin Liu, Wei Wu"}, {"title": "Squeeze-and-Attention Networks for Semantic Segmentation", "authors": "Zilong Zhong, Zhong Qiu Lin, Rene Bidart, Xiaodan Hu, Ibrahim Ben Daya, Zhifeng Li, Wei-Shi Zheng, Jonathan Li, Alexander Wong", "link": "https://arxiv.org/abs/1909.03402", "summary": "The recent integration of attention mechanisms into segmentation networks\nimproves their representational capabilities through a great emphasis on more\ninformative features. However, these attention mechanisms ignore an implicit\nsub-task of semantic segmentation and are constrained by the grid structure of\nconvolution kernels. In this paper, we propose a novel squeeze-and-attention\nnetwork (SANet) architecture that leverages an effective squeeze-and-attention\n(SA) module to account for two distinctive characteristics of segmentation: i)\npixel-group attention, and ii) pixel-wise prediction. Specifically, the\nproposed SA modules impose pixel-group attention on conventional convolution by\nintroducing an 'attention' convolutional channel, thus taking into account\nspatial-channel inter-dependencies in an efficient manner. The final\nsegmentation results are produced by merging outputs from four hierarchical\nstages of a SANet to integrate multi-scale contexts for obtaining an enhanced\npixel-wise prediction. Empirical experiments on two challenging public datasets\nvalidate the effectiveness of the proposed SANets, which achieves 83.2% mIoU\n(without COCO pre-training) on PASCAL VOC and a state-of-the-art mIoU of 54.4%\non PASCAL Context."}, {"title": "Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection", "authors": "Sara Beery, Guanhang Wu, Vivek Rathod, Ronny Votel, Jonathan Huang", "link": "https://arxiv.org/abs/1912.03538", "summary": "In static monitoring cameras, useful contextual information can stretch far\nbeyond the few seconds typical video understanding models might see: subjects\nmay exhibit similar behavior over multiple days, and background objects remain\nstatic. Due to power and storage constraints, sampling frequencies are low,\noften no faster than one frame per second, and sometimes are irregular due to\nthe use of a motion trigger. In order to perform well in this setting, models\nmust be robust to irregular sampling rates. In this paper we propose a method\nthat leverages temporal context from the unlabeled frames of a novel camera to\nimprove performance at that camera. Specifically, we propose an attention-based\napproach that allows our model, Context R-CNN, to index into a long term memory\nbank constructed on a per-camera basis and aggregate contextual features from\nother frames to boost object detection performance on the current frame.\n  We apply Context R-CNN to two settings: (1) species detection using camera\ntraps, and (2) vehicle detection in traffic cameras, showing in both settings\nthat Context R-CNN leads to performance gains over strong baselines. Moreover,\nwe show that increasing the contextual time horizon leads to improved results.\nWhen applied to camera trap data from the Snapshot Serengeti dataset, Context\nR-CNN with context from up to a month of images outperforms a single-frame\nbaseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by\n11.2% mAP."}, {"title": "Mixture Dense Regression for Object Detection and Human Pose Estimation", "authors": "Ali Varamesh, Tinne Tuytelaars", "link": "https://arxiv.org/abs/1912.00821", "summary": "Mixture models are well-established learning approaches that, in computer\nvision, have mostly been applied to inverse or ill-defined problems. However,\nthey are general-purpose divide-and-conquer techniques, splitting the input\nspace into relatively homogeneous subsets in a data-driven manner. Not only\nill-defined but also well-defined complex problems should benefit from them. To\nthis end, we devise a framework for spatial regression using mixture density\nnetworks. We realize the framework for object detection and human pose\nestimation. For both tasks, a mixture model yields higher accuracy and divides\nthe input space into interpretable modes. For object detection, mixture\ncomponents focus on object scale, with the distribution of components closely\nfollowing that of ground truth the object scale. This practically alleviates\nthe need for multi-scale testing, providing a superior speed-accuracy\ntrade-off. For human pose estimation, a mixture model divides the data based on\nviewpoint and uncertainty -- namely, front and back views, with back view\nimposing higher uncertainty. We conduct experiments on the MS COCO dataset and\ndo not face any mode collapse."}, {"title": "Syntax-Aware Action Targeting for Video Captioning", "authors": "Qi Zheng, Chaoyue Wang, Dacheng Tao"}, {"title": "Learning Visual Emotion Representations From Web Data", "authors": "Zijun Wei, Jianming Zhang, Zhe Lin, Joon-Young Lee, Niranjan Balasubramanian, Minh Hoai, Dimitris Samaras"}, {"title": "The Edge of Depth: Explicit Constraints Between Segmentation and Depth", "authors": "Shengjie Zhu, Garrick Brazil, Xiaoming Liu", "link": "https://arxiv.org/abs/2004.00171", "summary": "In this work we study the mutual benefits of two common computer vision\ntasks, self-supervised depth estimation and semantic segmentation from images.\nFor example, to help unsupervised monocular depth estimation, constraints from\nsemantic segmentation has been explored implicitly such as sharing and\ntransforming features. In contrast, we propose to explicitly measure the border\nconsistency between segmentation and depth and minimize it in a greedy manner\nby iteratively supervising the network towards a locally optimal solution.\nPartially this is motivated by our observation that semantic segmentation even\ntrained with limited ground truth (200 images of KITTI) can offer more accurate\nborder than that of any (monocular or stereo) image-based depth estimation.\nThrough extensive experiments, our proposed approach advances the state of the\nart on unsupervised monocular depth estimation in the KITTI."}, {"title": "A Context-Aware Loss Function for Action Spotting in Soccer Videos", "authors": "Anthony Cioppa, Adrien Deli\u00e8ge, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck, Rikke Gade, Thomas B. Moeslund", "link": "https://arxiv.org/abs/1912.01326", "summary": "In video understanding, action spotting consists in temporally localizing\nhuman-induced events annotated with single timestamps. In this paper, we\npropose a novel loss function that specifically considers the temporal context\nnaturally present around each action, rather than focusing on the single\nannotated frame to spot. We benchmark our loss on a large dataset of soccer\nvideos, SoccerNet, and achieve an improvement of 12.8% over the baseline. We\nshow the generalization capability of our loss for generic activity proposals\nand detection on ActivityNet, by spotting the beginning and the end of each\nactivity. Furthermore, we provide an extended ablation study and display\nchallenging cases for action spotting in soccer videos. Finally, we\nqualitatively illustrate how our loss induces a precise temporal understanding\nof actions and show how such semantic knowledge can be used for automatic\nhighlights generation."}, {"title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training", "authors": "Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao", "link": "https://arxiv.org/abs/2002.10638", "summary": "Learning to navigate in a visual environment following natural-language\ninstructions is a challenging task, because the multimodal inputs to the agent\nare highly variable, and the training data on a new task is often limited. In\nthis paper, we present the first pre-training and fine-tuning paradigm for\nvision-and-language navigation (VLN) tasks. By training on a large amount of\nimage-text-action triplets in a self-supervised learning manner, the\npre-trained model provides generic representations of visual environments and\nlanguage instructions. It can be easily used as a drop-in for existing VLN\nframeworks, leading to the proposed agent called Prevalent. It learns more\neffectively in new tasks and generalizes better in a previously unseen\nenvironment. The performance is validated on three VLN tasks. On the\nRoom-to-Room benchmark, our model improves the state-of-the-art from 47% to 51%\non success rate weighted by path length. Further, the learned representation is\ntransferable to other VLN tasks. On two recent tasks, vision-and-dialog\nnavigation and \"Help, Anna!\" the proposed Prevalent leads to significant\nimprovement over existing methods, achieving a new state of the art."}, {"title": "Video Instance Segmentation Tracking With a Modified VAE Architecture", "authors": "Chung-Ching Lin, Ying Hung, Rogerio Feris, Linglin He", "link": "", "summary": ""}, {"title": "Deformation-Aware Unpaired Image Translation for Pose Estimation on Laboratory Animals", "authors": "Siyuan Li, Semih G\u00fcnel, Mirela Ostrek, Pavan Ramdya, Pascal Fua, Helge Rhodin", "link": "https://arxiv.org/abs/2001.08601", "summary": "Our goal is to capture the pose of neuroscience model organisms, without\nusing any manual supervision, to be able to study how neural circuits\norchestrate behaviour. Human pose estimation attains remarkable accuracy when\ntrained on real or simulated datasets consisting of millions of frames.\nHowever, for many applications simulated models are unrealistic and real\ntraining datasets with comprehensive annotations do not exist. We address this\nproblem with a new sim2real domain transfer method. Our key contribution is the\nexplicit and independent modeling of appearance, shape and poses in an unpaired\nimage translation framework. Our model lets us train a pose estimator on the\ntarget domain by transferring readily available body keypoint locations from\nthe source domain to generated target images. We compare our approach with\nexisting domain transfer methods and demonstrate improved pose estimation\naccuracy on Drosophila melanogaster (fruit fly), Caenorhabditis elegans (worm)\nand Danio rerio (zebrafish), without requiring any manual annotation on the\ntarget domain and despite using simplistic off-the-shelf animal characters for\nsimulation, or simple geometric shapes as models. Our new datasets, code, and\ntrained models will be published to support future neuroscientific studies."}, {"title": "ZeroQ: A Novel Zero Shot Quantization Framework", "authors": "Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W. Mahoney, Kurt Keutzer", "link": "https://arxiv.org/abs/2001.00281", "summary": "Quantization is a promising approach for reducing the inference time and\nmemory footprint of neural networks. However, most existing quantization\nmethods require access to the original training dataset for retraining during\nquantization. This is often not possible for applications with sensitive or\nproprietary data, e.g., due to privacy and security concerns. Existing\nzero-shot quantization methods use different heuristics to address this, but\nthey result in poor performance, especially when quantizing to ultra-low\nprecision. Here, we propose ZeroQ , a novel zero-shot quantization framework to\naddress this. ZeroQ enables mixed-precision quantization without any access to\nthe training or validation data. This is achieved by optimizing for a Distilled\nDataset, which is engineered to match the statistics of batch normalization\nacross different layers of the network. ZeroQ supports both uniform and\nmixed-precision quantization. For the latter, we introduce a novel Pareto\nfrontier based method to automatically determine the mixed-precision bit\nsetting for all layers, with no manual search involved. We extensively test our\nproposed method on a diverse set of models, including ResNet18/50/152,\nMobileNetV2, ShuffleNet, SqueezeNext, and InceptionV3 on ImageNet, as well as\nRetinaNet-ResNet50 on the Microsoft COCO dataset. In particular, we show that\nZeroQ can achieve 1.71\\% higher accuracy on MobileNetV2, as compared to the\nrecently proposed DFQ method. Importantly, ZeroQ has a very low computational\noverhead, and it can finish the entire quantization process in less than 30s\n(0.5\\% of one epoch training time of ResNet50 on ImageNet). We have\nopen-sourced the ZeroQ\nframework\\footnote{https://github.com/amirgholami/ZeroQ}."}, {"title": "Disparity-Aware Domain Adaptation in Stereo Image Restoration", "authors": "Bo Yan, Chenxi Ma, Bahetiyaer Bare, Weimin Tan, Steven C. H. Hoi"}, {"title": "Offset Bin Classification Network for Accurate Object Detection", "authors": "Heqian Qiu, Hongliang Li, Qingbo Wu, Hengcan Shi"}, {"title": "TBT: Targeted Neural Network Attack With Bit Trojan", "authors": "Adnan Siraj Rakin, Zhezhi He, Deliang Fan", "link": "https://arxiv.org/abs/1909.05193", "summary": "Security of modern Deep Neural Networks (DNNs) is under severe scrutiny as\nthe deployment of these models become widespread in many intelligence-based\napplications. Most recently, DNNs are attacked through Trojan which can\neffectively infect the model during the training phase and get activated only\nthrough specific input patterns (i.e, trigger) during inference. In this work,\nfor the first time, we propose a novel Targeted Bit Trojan(TBT) method, which\ncan insert a targeted neural Trojan into a DNN through the bit-flip attack. Our\nalgorithm efficiently generates a trigger specifically designed to locate\ncertain vulnerable bits of DNN weights stored in main memory (i.e., DRAM). The\nobjective is that once the attacker flips these vulnerable bits, the network\nstill operates with normal inference accuracy with benign input. However, when\nthe attacker activates the trigger by embedding it with any input, the network\nis forced to classify all inputs to a certain target class. We demonstrate that\nflipping only several vulnerable bits identified by our method, using available\nbit-flip techniques (i.e, row-hammer), can transform a fully functional DNN\nmodel into a Trojan-infected model. We perform extensive experiments of\nCIFAR-10, SVHN and ImageNet datasets on both VGG-16 and Resnet-18\narchitectures. Our proposed TBT could classify 92 % of test images to a target\nclass with as little as 84 bit-flips out of 88 million weight bits on Resnet-18\nfor CIFAR10 dataset."}, {"title": "Maintaining Discrimination and Fairness in Class Incremental Learning", "authors": "Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, Shu-Tao Xia", "link": "https://arxiv.org/abs/1911.07053", "summary": "Deep neural networks (DNNs) have been applied in class incremental learning,\nwhich aims to solve common real-world problems of learning new classes\ncontinually. One drawback of standard DNNs is that they are prone to\ncatastrophic forgetting. Knowledge distillation (KD) is a commonly used\ntechnique to alleviate this problem. In this paper, we demonstrate it can\nindeed help the model to output more discriminative results within old classes.\nHowever, it cannot alleviate the problem that the model tends to classify\nobjects into new classes, causing the positive effect of KD to be hidden and\nlimited. We observed that an important factor causing catastrophic forgetting\nis that the weights in the last fully connected (FC) layer are highly biased in\nclass incremental learning. In this paper, we propose a simple and effective\nsolution motivated by the aforementioned observations to address catastrophic\nforgetting. Firstly, we utilize KD to maintain the discrimination within old\nclasses. Then, to further maintain the fairness between old classes and new\nclasses, we propose Weight Aligning (WA) that corrects the biased weights in\nthe FC layer after normal training process. Unlike previous work, WA does not\nrequire any extra parameters or a validation set in advance, as it utilizes the\ninformation provided by the biased weights themselves. The proposed method is\nevaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings.\nExperimental results show that the proposed method can effectively alleviate\ncatastrophic forgetting and significantly outperform state-of-the-art methods."}, {"title": "Background Data Resampling for Outlier-Aware Classification", "authors": "Yi Li, Nuno Vasconcelos"}, {"title": "STEFANN: Scene Text Editor Using Font Adaptive Neural Network", "authors": "Prasun Roy, Saumik Bhattacharya, Subhankar Ghosh, Umapada Pal", "link": "", "summary": ""}, {"title": "Geometry and Learning Co-Supported Normal Estimation for Unstructured Point Cloud", "authors": "Haoran Zhou, Honghua Chen, Yidan Feng, Qiong Wang, Jing Qin, Haoran Xie, Fu Lee Wang, Mingqiang Wei, Jun Wang"}, {"title": "Sequential Motif Profiles and Topological Plots for Offline Signature Verification", "authors": "Elias N. Zois, Evangelos Zervas, Dimitrios Tsourounis, George Economou"}, {"title": "Optical Flow in Dense Foggy Scenes Using Semi-Supervised Learning", "authors": "Wending Yan, Aashish Sharma, Robby T. Tan", "link": "https://arxiv.org/abs/2004.01905", "summary": "In dense foggy scenes, existing optical flow methods are erroneous. This is\ndue to the degradation caused by dense fog particles that break the optical\nflow basic assumptions such as brightness and gradient constancy. To address\nthe problem, we introduce a semi-supervised deep learning technique that\nemploys real fog images without optical flow ground-truths in the training\nprocess. Our network integrates the domain transformation and optical flow\nnetworks in one framework. Initially, given a pair of synthetic fog images, its\ncorresponding clean images and optical flow ground-truths, in one training\nbatch we train our network in a supervised manner. Subsequently, given a pair\nof real fog images and a pair of clean images that are not corresponding to\neach other (unpaired), in the next training batch, we train our network in an\nunsupervised manner. We then alternate the training of synthetic and real data\niteratively. We use real data without ground-truths, since to have\nground-truths in such conditions is intractable, and also to avoid the\noverfitting problem of synthetic data training, where the knowledge learned on\nsynthetic data cannot be generalized to real data testing. Together with the\nnetwork architecture design, we propose a new training strategy that combines\nsupervised synthetic-data training and unsupervised real-data training.\nExperimental results show that our method is effective and outperforms the\nstate-of-the-art methods in estimating optical flow in dense foggy scenes."}, {"title": "A Spatial RNN Codec for End-to-End Image Compression", "authors": "Chaoyi Lin, Jiabao Yao, Fangdong Chen, Li Wang"}, {"title": "Object Relational Graph With Teacher-Recommended Learning for Video Captioning", "authors": "Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, Zheng-Jun Zha", "link": "https://arxiv.org/abs/2002.11566", "summary": "Taking full advantage of the information from both vision and language is\ncritical for the video captioning task. Existing models lack adequate visual\nrepresentation due to the neglect of interaction between object, and sufficient\ntraining for content-related words due to long-tailed problems. In this paper,\nwe propose a complete video captioning system including both a novel model and\nan effective training strategy. Specifically, we propose an object relational\ngraph (ORG) based encoder, which captures more detailed interaction features to\nenrich visual representation. Meanwhile, we design a teacher-recommended\nlearning (TRL) method to make full use of the successful external language\nmodel (ELM) to integrate the abundant linguistic knowledge into the caption\nmodel. The ELM generates more semantically similar word proposals which extend\nthe ground-truth words used for training to deal with the long-tailed problem.\nExperimental evaluations on three benchmarks: MSVD, MSR-VTT and VATEX show the\nproposed ORG-TRL system achieves state-of-the-art performance. Extensive\nablation studies and visualizations illustrate the effectiveness of our system."}, {"title": "MMTM: Multimodal Transfer Module for CNN Fusion", "authors": "Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L. Iuzzolino, Kazuhito Koishida", "link": "", "summary": ""}, {"title": "Generalized Zero-Shot Learning via Over-Complete Distribution", "authors": "Rohit Keshari, Richa Singh, Mayank Vatsa", "link": "https://arxiv.org/abs/2004.00666", "summary": "A well trained and generalized deep neural network (DNN) should be robust to\nboth seen and unseen classes. However, the performance of most of the existing\nsupervised DNN algorithms degrade for classes which are unseen in the training\nset. To learn a discriminative classifier which yields good performance in\nZero-Shot Learning (ZSL) settings, we propose to generate an Over-Complete\nDistribution (OCD) using Conditional Variational Autoencoder (CVAE) of both\nseen and unseen classes. In order to enforce the separability between classes\nand reduce the class scatter, we propose the use of Online Batch Triplet Loss\n(OBTL) and Center Loss (CL) on the generated OCD. The effectiveness of the\nframework is evaluated using both Zero-Shot Learning and Generalized Zero-Shot\nLearning protocols on three publicly available benchmark databases, SUN, CUB\nand AWA2. The results show that generating over-complete distributions and\nenforcing the classifier to learn a transform function from overlapping to\nnon-overlapping distributions can improve the performance on both seen and\nunseen classes."}, {"title": "Gait Recognition via Semi-supervised Disentangled Representation Learning to Identity and Covariate Features", "authors": "Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi, Mingwu Ren", "link": "", "summary": ""}, {"title": "Unifying Training and Inference for Panoptic Segmentation", "authors": "Qizhu Li, Xiaojuan Qi, Philip H.S. Torr", "link": "https://arxiv.org/abs/2001.04982", "summary": "We present an end-to-end network to bridge the gap between training and\ninference pipeline for panoptic segmentation, a task that seeks to partition an\nimage into semantic regions for \"stuff\" and object instances for \"things\". In\ncontrast to recent works, our network exploits a parametrised, yet lightweight\npanoptic segmentation submodule, powered by an end-to-end learnt dense instance\naffinity, to capture the probability that any pair of pixels belong to the same\ninstance. This panoptic submodule gives rise to a novel propagation mechanism\nfor panoptic logits and enables the network to output a coherent panoptic\nsegmentation map for both \"stuff\" and \"thing\" classes, without any\npost-processing. Reaping the benefits of end-to-end training, our full system\nsets new records on the popular street scene dataset, Cityscapes, achieving\n61.4 PQ with a ResNet-50 backbone using only the fine annotations. On the\nchallenging COCO dataset, our ResNet-50-based network also delivers\nstate-of-the-art accuracy of 43.4 PQ. Moreover, our network flexibly works with\nand without object mask cues, performing competitively under both settings,\nwhich is of interest for applications with computation budgets."}, {"title": "Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection", "authors": "Liang Du, Xiaoqing Ye, Xiao Tan, Jianfeng Feng, Zhenbo Xu, Errui Ding, Shilei Wen"}, {"title": "Interactive Image Segmentation With First Click Attention", "authors": "Zheng Lin, Zhao Zhang, Lin-Zhuo Chen, Ming-Ming Cheng, Shao-Ping Lu"}, {"title": "NETNet: Neighbor Erasing and Transferring Network for Better Single Shot Object Detection", "authors": "Yazhao Li, Yanwei Pang, Jianbing Shen, Jiale Cao, Ling Shao", "link": "https://arxiv.org/abs/2001.06690", "summary": "Due to the advantages of real-time detection and improved performance,\nsingle-shot detectors have gained great attention recently. To solve the\ncomplex scale variations, single-shot detectors make scale-aware predictions\nbased on multiple pyramid layers. However, the features in the pyramid are not\nscale-aware enough, which limits the detection performance. Two common problems\nin single-shot detectors caused by object scale variations can be observed: (1)\nsmall objects are easily missed; (2) the salient part of a large object is\nsometimes detected as an object. With this observation, we propose a new\nNeighbor Erasing and Transferring (NET) mechanism to reconfigure the pyramid\nfeatures and explore scale-aware features. In NET, a Neighbor Erasing Module\n(NEM) is designed to erase the salient features of large objects and emphasize\nthe features of small objects in shallow layers. A Neighbor Transferring Module\n(NTM) is introduced to transfer the erased features and highlight large objects\nin deep layers. With this mechanism, a single-shot network called NETNet is\nconstructed for scale-aware object detection. In addition, we propose to\naggregate nearest neighboring pyramid features to enhance our NET. NETNet\nachieves 38.5% AP at a speed of 27 FPS and 32.0% AP at a speed of 55 FPS on MS\nCOCO dataset. As a result, NETNet achieves a better trade-off for real-time and\naccurate object detection."}, {"title": "Scale-Equalizing Pyramid Convolution for Object Detection", "authors": "Xinjiang Wang, Shilong Zhang, Zhuoran Yu, Litong Feng, Wayne Zhang", "link": "https://arxiv.org/abs/2005.03101", "summary": "Feature pyramid has been an efficient method to extract features at different\nscales. Development over this method mainly focuses on aggregating contextual\ninformation at different levels while seldom touching the inter-level\ncorrelation in the feature pyramid. Early computer vision methods extracted\nscale-invariant features by locating the feature extrema in both spatial and\nscale dimension. Inspired by this, a convolution across the pyramid level is\nproposed in this study, which is termed pyramid convolution and is a modified\n3-D convolution. Stacked pyramid convolutions directly extract 3-D (scale and\nspatial) features and outperforms other meticulously designed feature fusion\nmodules. Based on the viewpoint of 3-D convolution, an integrated batch\nnormalization that collects statistics from the whole feature pyramid is\nnaturally inserted after the pyramid convolution. Furthermore, we also show\nthat the naive pyramid convolution, together with the design of RetinaNet head,\nactually best applies for extracting features from a Gaussian pyramid, whose\nproperties can hardly be satisfied by a feature pyramid. In order to alleviate\nthis discrepancy, we build a scale-equalizing pyramid convolution (SEPC) that\naligns the shared pyramid convolution kernel only at high-level feature maps.\nBeing computationally efficient and compatible with the head design of most\nsingle-stage object detectors, the SEPC module brings significant performance\nimprovement ($>4$AP increase on MS-COCO2017 dataset) in state-of-the-art\none-stage object detectors, and a light version of SEPC also has $\\sim3.5$AP\ngain with only around 7% inference time increase. The pyramid convolution also\nfunctions well as a stand-alone module in two-stage object detectors and is\nable to improve the performance by $\\sim2$AP. The source code can be found at\nhttps://github.com/jshilong/SEPC."}, {"title": "Learning to Cluster Faces via Confidence and Connectivity Estimation", "authors": "Lei Yang, Dapeng Chen, Xiaohang Zhan, Rui Zhao, Chen Change Loy, Dahua Lin", "link": "https://arxiv.org/abs/2004.00445", "summary": "Face clustering is an essential tool for exploiting the unlabeled face data,\nand has a wide range of applications including face annotation and retrieval.\nRecent works show that supervised clustering can result in noticeable\nperformance gain. However, they usually involve heuristic steps and require\nnumerous overlapped subgraphs, severely restricting their accuracy and\nefficiency. In this paper, we propose a fully learnable clustering framework\nwithout requiring a large number of overlapped subgraphs. Instead, we transform\nthe clustering problem into two sub-problems. Specifically, two graph\nconvolutional networks, named GCN-V and GCN-E, are designed to estimate the\nconfidence of vertices and the connectivity of edges, respectively. With the\nvertex confidence and edge connectivity, we can naturally organize more\nrelevant vertices on the affinity graph and group them into clusters.\nExperiments on two large-scale benchmarks show that our method significantly\nimproves clustering accuracy and thus performance of the recognition models\ntrained on top, yet it is an order of magnitude more efficient than existing\nsupervised methods."}, {"title": "Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer", "authors": "Yan Lu, Yue Wu, Bin Liu, Tianzhu Zhang, Baopu Li, Qi Chu, Nenghai Yu", "link": "https://arxiv.org/abs/2002.12489", "summary": "Cross-modality person re-identification (cm-ReID) is a challenging but key\ntechnology for intelligent video analysis. Existing works mainly focus on\nlearning common representation by embedding different modalities into a same\nfeature space. However, only learning the common characteristics means great\ninformation loss, lowering the upper bound of feature distinctiveness. In this\npaper, we tackle the above limitation by proposing a novel cross-modality\nshared-specific feature transfer algorithm (termed cm-SSFT) to explore the\npotential of both the modality-shared information and the modality-specific\ncharacteristics to boost the re-identification performance. We model the\naffinities of different modality samples according to the shared features and\nthen transfer both shared and specific features among and across modalities. We\nalso propose a complementary feature learning strategy including modality\nadaption, project adversarial learning and reconstruction enhancement to learn\ndiscriminative and complementary shared and specific features of each modality,\nrespectively. The entire cm-SSFT algorithm can be trained in an end-to-end\nmanner. We conducted comprehensive experiments to validate the superiority of\nthe overall algorithm and the effectiveness of each component. The proposed\nalgorithm significantly outperforms state-of-the-arts by 22.5% and 19.3% mAP on\nthe two mainstream benchmark datasets SYSU-MM01 and RegDB, respectively."}, {"title": "DPGN: Distribution Propagation Graph Network for Few-Shot Learning", "authors": "Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, Yu Liu", "link": "https://arxiv.org/abs/2003.14247", "summary": "Most graph-network-based meta-learning approaches model instance-level\nrelation of examples. We extend this idea further to explicitly model the\ndistribution-level relation of one example to all other examples in a 1-vs-N\nmanner. We propose a novel approach named distribution propagation graph\nnetwork (DPGN) for few-shot learning. It conveys both the distribution-level\nrelations and instance-level relations in each few-shot learning task. To\ncombine the distribution-level relations and instance-level relations for all\nexamples, we construct a dual complete graph network which consists of a point\ngraph and a distribution graph with each node standing for an example. Equipped\nwith dual graph architecture, DPGN propagates label information from labeled\nexamples to unlabeled examples within several update generations. In extensive\nexperiments on few-shot learning benchmarks, DPGN outperforms state-of-the-art\nresults by a large margin in 5% $\\sim$ 12% under supervised setting and 7%\n$\\sim$ 13% under semi-supervised setting. Code will be released."}, {"title": "Density-Aware Graph for Deep Semi-Supervised Visual Recognition", "authors": "Suichan Li, Bin Liu, Dongdong Chen, Qi Chu, Lu Yuan, Nenghai Yu", "link": "https://arxiv.org/abs/2003.13194", "summary": "Semi-supervised learning (SSL) has been extensively studied to improve the\ngeneralization ability of deep neural networks for visual recognition. To\ninvolve the unlabelled data, most existing SSL methods are based on common\ndensity-based cluster assumption: samples lying in the same high-density region\nare likely to belong to the same class, including the methods performing\nconsistency regularization or generating pseudo-labels for the unlabelled\nimages. Despite their impressive performance, we argue three limitations exist:\n1) Though the density information is demonstrated to be an important clue, they\nall use it in an implicit way and have not exploited it in depth. 2) For\nfeature learning, they often learn the feature embedding based on the single\ndata sample and ignore the neighborhood information. 3) For label-propagation\nbased pseudo-label generation, it is often done offline and difficult to be\nend-to-end trained with feature learning. Motivated by these limitations, this\npaper proposes to solve the SSL problem by building a novel density-aware\ngraph, based on which the neighborhood information can be easily leveraged and\nthe feature learning and label propagation can also be trained in an end-to-end\nway. Specifically, we first propose a new Density-aware Neighborhood\nAggregation(DNA) module to learn more discriminative features by incorporating\nthe neighborhood information in a density-aware manner. Then a novel\nDensity-ascending Path based Label Propagation(DPLP) module is proposed to\ngenerate the pseudo-labels for unlabeled samples more efficiently according to\nthe feature distribution characterized by density. Finally, the DNA module and\nDPLP module evolve and improve each other end-to-end."}, {"title": "Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation", "authors": "Moab Arar, Yiftach Ginger, Dov Danon, Amit H. Bermano, Daniel Cohen-Or", "link": "https://arxiv.org/abs/2003.08073", "summary": "Many applications, such as autonomous driving, heavily rely on multi-modal\ndata where spatial alignment between the modalities is required. Most\nmulti-modal registration methods struggle computing the spatial correspondence\nbetween the images using prevalent cross-modality similarity measures. In this\nwork, we bypass the difficulties of developing cross-modality similarity\nmeasures, by training an image-to-image translation network on the two input\nmodalities. This learned translation allows training the registration network\nusing simple and reliable mono-modality metrics. We perform multi-modal\nregistration using two networks - a spatial transformation network and a\ntranslation network. We show that by encouraging our translation network to be\ngeometry preserving, we manage to train an accurate spatial transformation\nnetwork. Compared to state-of-the-art multi-modal methods our presented method\nis unsupervised, requiring no pairs of aligned modalities for training, and can\nbe adapted to any pair of modalities. We evaluate our method quantitatively and\nqualitatively on commercial datasets, showing that it performs well on several\nmodalities and achieves accurate alignment."}, {"title": "Binarizing MobileNet via Evolution-Based Searching", "authors": "Hai Phan, Zechun Liu, Dang Huynh, Marios Savvides, Kwang-Ting Cheng, Zhiqiang Shen", "link": "https://arxiv.org/abs/2005.06305", "summary": "Binary Neural Networks (BNNs), known to be one among the effectively compact\nnetwork architectures, have achieved great outcomes in the visual tasks.\nDesigning efficient binary architectures is not trivial due to the binary\nnature of the network. In this paper, we propose a use of evolutionary search\nto facilitate the construction and training scheme when binarizing MobileNet, a\ncompact network with separable depth-wise convolution. Inspired by one-shot\narchitecture search frameworks, we manipulate the idea of group convolution to\ndesign efficient 1-Bit Convolutional Neural Networks (CNNs), assuming an\napproximately optimal trade-off between computational cost and model accuracy.\nOur objective is to come up with a tiny yet efficient binary neural\narchitecture by exploring the best candidates of the group convolution while\noptimizing the model performance in terms of complexity and latency. The\napproach is threefold. First, we train strong baseline binary networks with a\nwide range of random group combinations at each convolutional layer. This\nset-up gives the binary neural networks a capability of preserving essential\ninformation through layers. Second, to find a good set of hyperparameters for\ngroup convolutions we make use of the evolutionary search which leverages the\nexploration of efficient 1-bit models. Lastly, these binary models are trained\nfrom scratch in a usual manner to achieve the final binary model. Various\nexperiments on ImageNet are conducted to show that following our construction\nguideline, the final model achieves 60.09% Top-1 accuracy and outperforms the\nstate-of-the-art CI-BCNN with the same computational cost."}, {"title": "Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians", "authors": "Jialian Wu, Chunluan Zhou, Ming Yang, Qian Zhang, Yuan Li, Junsong Yuan"}, {"title": "Orderless Recurrent Models for Multi-Label Classification", "authors": "Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa, Bart\u0142omiej Twardowski, Joost van de Weijer", "link": "https://arxiv.org/abs/1911.09996", "summary": "Recurrent neural networks (RNN) are popular for many computer vision tasks,\nincluding multi-label classification. Since RNNs produce sequential outputs,\nlabels need to be ordered for the multi-label classification task. Current\napproaches sort labels according to their frequency, typically ordering them in\neither rare-first or frequent-first. These imposed orderings do not take into\naccount that the natural order to generate the labels can change for each\nimage, e.g.\\ first the dominant object before summing up the smaller objects in\nthe image. Therefore, in this paper, we propose ways to dynamically order the\nground truth labels with the predicted label sequence. This allows for the\nfaster training of more optimal LSTM models for multi-label classification.\nAnalysis evidences that our method does not suffer from duplicate generation,\nsomething which is common for other models. Furthermore, it outperforms other\nCNN-RNN models, and we show that a standard architecture of an image encoder\nand language decoder trained with our proposed loss obtains the\nstate-of-the-art results on the challenging MS-COCO, WIDER Attribute and\nPA-100K and competitive results on NUS-WIDE."}, {"title": "Gold Seeker: Information Gain From Policy Distributions for Goal-Oriented Vision-and-Langauge Reasoning", "authors": "Ehsan Abbasnejad, Iman Abbasnejad, Qi Wu, Javen Shi, Anton van den Hengel", "link": "https://arxiv.org/abs/1812.06398", "summary": "As Computer Vision moves from a passive analysis of pixels to active analysis\nof semantics, the breadth of information algorithms need to reason over has\nexpanded significantly. One of the key challenges in this vein is the ability\nto identify the information required to make a decision, and select an action\nthat will recover it. We propose a reinforcement-learning approach that\nmaintains a distribution over its internal information, thus explicitly\nrepresenting the ambiguity in what it knows, and needs to know, towards\nachieving its goal. Potential actions are then generated according to this\ndistribution. For each potential action a distribution of the expected outcomes\nis calculated, and the value of the potential information gain assessed. The\naction taken is that which maximizes the potential information gain. We\ndemonstrate this approach applied to two vision-and-language problems that have\nattracted significant recent interest, visual dialog and visual query\ngeneration. In both cases, the method actively selects actions that will best\nreduce its internal uncertainty and outperforms its competitors in achieving\nthe goal of the challenge."}, {"title": "Rethinking the Route Towards Weakly Supervised Object Localization", "authors": "Chen-Lin Zhang, Yun-Hao Cao, Jianxin Wu", "link": "https://arxiv.org/abs/2002.11359", "summary": "Weakly supervised object localization (WSOL) aims to localize objects with\nonly image-level labels. Previous methods often try to utilize feature maps and\nclassification weights to localize objects using image level annotations\nindirectly. In this paper, we demonstrate that weakly supervised object\nlocalization should be divided into two parts: class-agnostic object\nlocalization and object classification. For class-agnostic object localization,\nwe should use class-agnostic methods to generate noisy pseudo annotations and\nthen perform bounding box regression on them without class labels. We propose\nthe pseudo supervised object localization (PSOL) method as a new way to solve\nWSOL. Our PSOL models have good transferability across different datasets\nwithout fine-tuning. With generated pseudo bounding boxes, we achieve 58.00%\nlocalization accuracy on ImageNet and 74.97% localization accuracy on CUB-200,\nwhich have a large edge over previous models."}, {"title": "Adversarial Feature Hallucination Networks for Few-Shot Learning", "authors": "Kai Li, Yulun Zhang, Kunpeng Li, Yun Fu", "link": "https://arxiv.org/abs/2003.13193", "summary": "The recent flourish of deep learning in various tasks is largely accredited\nto the rich and accessible labeled data. Nonetheless, massive supervision\nremains a luxury for many real applications, boosting great interest in\nlabel-scarce techniques such as few-shot learning (FSL), which aims to learn\nconcept of new classes with a few labeled samples. A natural approach to FSL is\ndata augmentation and many recent works have proved the feasibility by\nproposing various data synthesis models. However, these models fail to well\nsecure the discriminability and diversity of the synthesized data and thus\noften produce undesirable results. In this paper, we propose Adversarial\nFeature Hallucination Networks (AFHN) which is based on conditional Wasserstein\nGenerative Adversarial networks (cWGAN) and hallucinates diverse and\ndiscriminative features conditioned on the few labeled samples. Two novel\nregularizers, i.e., the classification regularizer and the anti-collapse\nregularizer, are incorporated into AFHN to encourage discriminability and\ndiversity of the synthesized features, respectively. Ablation study verifies\nthe effectiveness of the proposed cWGAN based feature hallucination framework\nand the proposed regularizers. Comparative results on three common benchmark\ndatasets substantiate the superiority of AFHN to existing data augmentation\nbased FSL approaches and other state-of-the-art ones."}, {"title": "Conditional Gaussian Distribution Learning for Open Set Recognition", "authors": "Xin Sun, Zhenning Yang, Chi Zhang, Keck-Voon Ling, Guohao Peng", "link": "https://arxiv.org/abs/2003.08823", "summary": "Deep neural networks have achieved state-of-the-art performance in a wide\nrange of recognition/classification tasks. However, when applying deep learning\nto real-world applications, there are still multiple challenges. A typical\nchallenge is that unknown samples may be fed into the system during the testing\nphase and traditional deep neural networks will wrongly recognize the unknown\nsample as one of the known classes. Open set recognition is a potential\nsolution to overcome this problem, where the open set classifier should have\nthe ability to reject unknown samples as well as maintain high classification\naccuracy on known classes. The variational auto-encoder (VAE) is a popular\nmodel to detect unknowns, but it cannot provide discriminative representations\nfor known classification. In this paper, we propose a novel method, Conditional\nGaussian Distribution Learning (CGDL), for open set recognition. In addition to\ndetecting unknown samples, this method can also classify known samples by\nforcing different latent features to approximate different Gaussian models.\nMeanwhile, to avoid information hidden in the input vanishing in the middle\nlayers, we also adopt the probabilistic ladder architecture to extract\nhigh-level abstract features. Experiments on several standard image datasets\nreveal that the proposed method significantly outperforms the baseline method\nand achieves new state-of-the-art results."}, {"title": "Connect-and-Slice: An Hybrid Approach for Reconstructing 3D Objects", "authors": "Hao Fang, Florent Lafarge"}, {"title": "Attentive Weights Generation for Few Shot Learning via Information Maximization", "authors": "Yiluan Guo, Ngai-Man Cheung"}, {"title": "Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting", "authors": "Bo Yan, Qing Lin, Weimin Tan, Shili Zhou"}, {"title": "PuppeteerGAN: Arbitrary Portrait Animation With Semantic-Aware Appearance Transformation", "authors": "Zhuo Chen, Chaoyue Wang, Bo Yuan, Dacheng Tao"}, {"title": "SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition", "authors": "Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, Weiping Wang", "link": "https://arxiv.org/abs/2005.10977", "summary": "Scene text recognition is a hot research topic in computer vision. Recently,\nmany recognition methods based on the encoder-decoder framework have been\nproposed, and they can handle scene texts of perspective distortion and curve\nshape. Nevertheless, they still face lots of challenges like image blur, uneven\nillumination, and incomplete characters. We argue that most encoder-decoder\nmethods are based on local visual features without explicit global semantic\ninformation. In this work, we propose a semantics enhanced encoder-decoder\nframework to robustly recognize low-quality scene texts. The semantic\ninformation is used both in the encoder module for supervision and in the\ndecoder module for initializing. In particular, the state-of-the art ASTER\nmethod is integrated into the proposed framework as an exemplar. Extensive\nexperiments demonstrate that the proposed framework is more robust for\nlow-quality text images, and achieves state-of-the-art results on several\nbenchmark datasets."}, {"title": "Texture and Shape Biased Two-Stream Networks for Clothing Classification and Attribute Recognition", "authors": "Yuwei Zhang, Peng Zhang, Chun Yuan, Zhi Wang"}, {"title": "Distortion Agnostic Deep Watermarking", "authors": "Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, Peyman Milanfar", "link": "https://arxiv.org/abs/2001.04580", "summary": "Watermarking is the process of embedding information into an image that can\nsurvive under distortions, while requiring the encoded image to have little or\nno perceptual difference from the original image. Recently, deep learning-based\nmethods achieved impressive results in both visual quality and message payload\nunder a wide variety of image distortions. However, these methods all require\ndifferentiable models for the image distortions at training time, and may\ngeneralize poorly to unknown distortions. This is undesirable since the types\nof distortions applied to watermarked images are usually unknown and\nnon-differentiable. In this paper, we propose a new framework for\ndistortion-agnostic watermarking, where the image distortion is not explicitly\nmodeled during training. Instead, the robustness of our system comes from two\nsources: adversarial training and channel coding. Compared to training on a\nfixed set of distortions and noise levels, our method achieves comparable or\nbetter results on distortions available during training, and better performance\non unknown distortions."}, {"title": "RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network", "authors": "Bing Han, Gopalakrishnan Srinivasan, Kaushik Roy", "link": "https://arxiv.org/abs/2003.01811", "summary": "Spiking Neural Networks (SNNs) have recently attracted significant research\ninterest as the third generation of artificial neural networks that can enable\nlow-power event-driven data analytics. The best performing SNNs for image\nrecognition tasks are obtained by converting a trained Analog Neural Network\n(ANN), consisting of Rectified Linear Units (ReLU), to SNN composed of\nintegrate-and-fire neurons with \"proper\" firing thresholds. The converted SNNs\ntypically incur loss in accuracy compared to that provided by the original ANN\nand require sizable number of inference time-steps to achieve the best\naccuracy. We find that performance degradation in the converted SNN stems from\nusing \"hard reset\" spiking neuron that is driven to fixed reset potential once\nits membrane potential exceeds the firing threshold, leading to information\nloss during SNN inference. We propose ANN-SNN conversion using \"soft reset\"\nspiking neuron model, referred to as Residual Membrane Potential (RMP) spiking\nneuron, which retains the \"residual\" membrane potential above threshold at the\nfiring instants. We demonstrate near loss-less ANN-SNN conversion using RMP\nneurons for VGG-16, ResNet-20, and ResNet-34 SNNs on challenging datasets\nincluding CIFAR-10 (93.63% top-1), CIFAR-100 (70.93% top-1), and ImageNet\n(73.09% top-1 accuracy). Our results also show that RMP-SNN surpasses the best\ninference accuracy provided by the converted SNN with \"hard reset\" spiking\nneurons using 2-8 times fewer inference time-steps across network architectures\nand datasets."}, {"title": "BFBox: Searching Face-Appropriate Backbone and Feature Pyramid Network for Face Detector", "authors": "Yang Liu, Xu Tang"}, {"title": "PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames", "authors": "Yuqi Yang, Shilin Liu, Hao Pan, Yang Liu, Xin Tong", "link": "https://arxiv.org/abs/1808.04952", "summary": "We extend Convolutional Neural Networks (CNNs) on flat and regular domains\n(e.g. 2D images) to curved surfaces embedded in 3D Euclidean space that are\ndiscretized as irregular meshes and widely used to represent geometric data in\nComputer Vision and Graphics. We define surface convolution on tangent spaces\nof a surface domain, where the convolution has two desirable properties: 1) the\ndistortion of surface domain signals is locally minimal when being projected to\nthe tangent space, and 2) the translation equi-variance property holds locally,\nby aligning tangent spaces with the canonical parallel transport that preserves\nmetric. For computation, we rely on a parallel N-direction frame field on the\nsurface that minimizes field variation and therefore is as compatible as\npossible to and approximates the parallel transport. On the tangent spaces\nequipped with parallel frames, the computation of surface convolution becomes\nstandard routine. The frames have rotational symmetry which we disambiguate by\nconstructing the covering space of surface induced by the parallel frames and\ngrouping the feature maps into N sets accordingly; convolution is computed on\nthe N branches of the cover space with respective feature maps while the kernel\nweights are shared. To handle irregular points of a discrete mesh while sharing\nkernel weights, we make the convolution semi-discrete, i.e. the convolution\nkernels are polynomial functions, and their convolution with discrete surface\npoints becomes sampling and weighted summation. Pooling and unpooling\noperations are computed along a mesh hierarchy built through simplification.\nThe presented surface CNNs allow effective deep learning on meshes. We show\nthat for tasks of classification, segmentation and non-rigid registration,\nsurface CNNs using only raw input signals achieve superior performances than\nprevious models using sophisticated input features."}, {"title": "iTAML: An Incremental Task-Agnostic Meta-learning Approach", "authors": "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah", "link": "http://arxiv.org/abs/2003.11652", "summary": "Humans can continuously learn new knowledge as their experience grows. In\ncontrast, previous learning in deep neural networks can quickly fade out when\nthey are trained on a new task. In this paper, we hypothesize this problem can\nbe avoided by learning a set of generalized parameters, that are neither\nspecific to old nor new tasks. In this pursuit, we introduce a novel\nmeta-learning approach that seeks to maintain an equilibrium between all the\nencountered tasks. This is ensured by a new meta-update rule which avoids\ncatastrophic forgetting. In comparison to previous meta-learning techniques,\nour approach is task-agnostic. When presented with a continuum of data, our\nmodel automatically identifies the task and quickly adapts to it with just a\nsingle update. We perform extensive experiments on five datasets in a\nclass-incremental setting, leading to significant improvements over the state\nof the art methods (e.g., a 21.3% boost on CIFAR100 with 10 incremental tasks).\nSpecifically, on large-scale datasets that generally prove difficult cases for\nincremental learning, our approach delivers absolute gains as high as 19.1% and\n7.4% on ImageNet and MS-Celeb datasets, respectively."}, {"title": "Optimal least-squares solution to the hand-eye calibration problem", "authors": "Amit Dekel, Linus H\u00e4renstam-Nielsen, Sergio Caccamo", "link": "https://arxiv.org/abs/2002.10838", "summary": "We propose a least-squares formulation to the noisy hand-eye calibration\nproblem using dual-quaternions, and introduce efficient algorithms to find the\nexact optimal solution, based on analytic properties of the problem, avoiding\nnon-linear optimization. We further present simple analytic approximate\nsolutions which provide remarkably good estimations compared to the exact\nsolution. In addition, we show how to generalize our solution to account for a\ngiven extrinsic prior in the cost function. To the best of our knowledge our\nalgorithm is the most efficient approach to optimally solve the hand-eye\ncalibration problem."}, {"title": "MnasFPN: Learning Latency-Aware Pyramid Architecture for Object Detection on Mobile Devices", "authors": "Bo Chen, Golnaz Ghiasi, Hanxiao Liu, Tsung-Yi Lin, Dmitry Kalenichenko, Hartwig Adam, Quoc V. Le", "link": "https://arxiv.org/abs/1912.01106", "summary": "Despite the blooming success of architecture search for vision tasks in\nresource-constrained environments, the design of on-device object detection\narchitectures have mostly been manual. The few automated search efforts are\neither centered around non-mobile-friendly search spaces or not guided by\non-device latency. We propose Mnasfpn, a mobile-friendly search space for the\ndetection head, and combine it with latency-aware architecture search to\nproduce efficient object detection models. The learned Mnasfpn head, when\npaired with MobileNetV2 body, outperforms MobileNetV3+SSDLite by 1.8 mAP at\nsimilar latency on Pixel. It is also both 1.0 mAP more accurate and 10% faster\nthan NAS-FPNLite. Ablation studies show that the majority of the performance\ngain comes from innovations in the search space. Further explorations reveal an\ninteresting coupling between the search space design and the search algorithm,\nand that the complexity of Mnasfpn search space may be at a local optimum."}, {"title": "VSGNet: Spatial Attention Network for Detecting Human Object Interactions Using Graph Convolutions", "authors": "Oytun Ulutan, A S M Iftekhar, B. S. Manjunath", "link": "https://arxiv.org/abs/2003.05541", "summary": "Comprehensive visual understanding requires detection frameworks that can\neffectively learn and utilize object interactions while analyzing objects\nindividually. This is the main objective in Human-Object Interaction (HOI)\ndetection task. In particular, relative spatial reasoning and structural\nconnections between objects are essential cues for analyzing interactions,\nwhich is addressed by the proposed Visual-Spatial-Graph Network (VSGNet)\narchitecture. VSGNet extracts visual features from the human-object pairs,\nrefines the features with spatial configurations of the pair, and utilizes the\nstructural connections between the pair via graph convolutions. The performance\nof VSGNet is thoroughly evaluated using the Verbs in COCO (V-COCO) and HICO-DET\ndatasets. Experimental results indicate that VSGNet outperforms\nstate-of-the-art solutions by 8% or 4 mAP in V-COCO and 16% or 3 mAP in\nHICO-DET."}, {"title": "End-to-End Camera Calibration for Broadcast Videos", "authors": "Long Sha, Jennifer Hobbs, Panna Felsen, Xinyu Wei, Patrick Lucey, Sujoy Ganguly"}, {"title": "Regularizing CNN Transfer Learning With Randomised Regression", "authors": "Yang Zhong, Atsuto Maki", "link": "https://arxiv.org/abs/1908.05997", "summary": "This paper is about regularizing deep convolutional networks (CNNs) based on\nan adaptive framework for transfer learning with limited training data in the\ntarget domain. Recent advances of CNN regularization in this context are\ncommonly due to the use of additional regularization objectives. They guide the\ntraining away from the target task using some forms of concrete tasks. Unlike\nthose related approaches, we suggest that an objective without a concrete goal\ncan still serve well as a regularized. In particular, we demonstrate\nPseudo-task Regularization (PtR) which dynamically regularizes a network by\nsimply attempting to regress image representations to pseudo-regression targets\nduring fine-tuning. That is, a CNN is efficiently regularized without\nadditional resources of data or prior domain expertise. In sum, the proposed\nPtR provides: a) an alternative for network regularization without dependence\non the design of concrete regularization objectives or extra annotations; b) a\ndynamically adjusted and maintained strength of regularization effect by\nbalancing the gradient norms between objectives on-line. Through numerous\nexperiments, surprisingly, the improvements on classification accuracy by PtR\nare shown greater or on a par to the recent state-of-the-art methods."}, {"title": "KeypointNet: A Large-Scale 3D Keypoint Dataset Aggregated From Numerous Human Annotations", "authors": "Yang You, Yujing Lou, Chengkun Li, Zhoujun Cheng, Liangwei Li, Lizhuang Ma, Cewu Lu, Weiming Wang", "link": "https://arxiv.org/abs/2002.12687", "summary": "Detecting 3D objects keypoints is of great interest to the areas of both\ngraphics and computer vision. There have been several 2D and 3D keypoint\ndatasets aiming to address this problem in a data-driven way. These datasets,\nhowever, either lack scalability or bring ambiguity to the definition of\nkeypoints. Therefore, we present KeypointNet: the first large-scale and diverse\n3D keypoint dataset that contains 83,231 keypoints and 8,329 3D models from 16\nobject categories, by leveraging numerous human annotations. To handle the\ninconsistency between annotations from different people, we propose a novel\nmethod to aggregate these keypoints automatically, through minimization of a\nfidelity loss. Finally, ten state-of-the-art methods are benchmarked on our\nproposed dataset. Our code and data are available on\nhttps://github.com/qq456cvb/KeypointNet."}, {"title": "Hierarchical Clustering With Hard-Batch Triplet Loss for Person Re-Identification", "authors": "Kaiwei Zeng, Munan Ning, Yaohua Wang, Yang Guo", "link": "https://arxiv.org/abs/1910.12278", "summary": "For most unsupervised person re-identification (re-ID), people often adopt\nunsupervised domain adaptation (UDA) method. UDA often train on the labeled\nsource dataset and evaluate on the target dataset, which often focuses on\nlearning differences between the source dataset and the target dataset to\nimprove the generalization of the model. Base on these, we explore how to make\nuse of the similarity of samples to conduct a fully unsupervised method which\njust trains on the unlabeled target dataset. Concretely, we propose a\nhierarchical clustering-guided re-ID (HCR) method. We use hierarchical\nclustering to generate pseudo labels and use these pseudo labels as monitors to\nconduct the training. In order to exclude hard examples and promote the\nconvergence of the model, We use PK sampling in each iteration, which randomly\nselects a fixed number of samples from each cluster for training. We evaluate\nour model on Market-1501, DukeMTMC-reID and MSMT17. Results show that HCR gets\nthe state-of-the-arts and achieves 55.3% mAP on Market-1501 and 46.8% mAP on\nDukeMTMC-reID. Our code will be released soon."}, {"title": "Joint Semantic Segmentation and Boundary Detection Using Iterative Pyramid Contexts", "authors": "Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, Long Quan", "link": "https://arxiv.org/abs/2004.07684", "summary": "In this paper, we present a joint multi-task learning framework for semantic\nsegmentation and boundary detection. The critical component in the framework is\nthe iterative pyramid context module (PCM), which couples two tasks and stores\nthe shared latent semantics to interact between the two tasks. For semantic\nboundary detection, we propose the novel spatial gradient fusion to suppress\nnonsemantic edges. As semantic boundary detection is the dual task of semantic\nsegmentation, we introduce a loss function with boundary consistency constraint\nto improve the boundary pixel accuracy for semantic segmentation. Our extensive\nexperiments demonstrate superior performance over state-of-the-art works, not\nonly in semantic segmentation but also in semantic boundary detection. In\nparticular, a mean IoU score of 81:8% on Cityscapes test set is achieved\nwithout using coarse data or any external data for semantic segmentation. For\nsemantic boundary detection, we improve over previous state-of-the-art works by\n9.9% in terms of AP and 6:8% in terms of MF(ODS)."}, {"title": "Attention-Guided Hierarchical Structure Aggregation for Image Matting", "authors": "Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, Xiaopeng Wei"}, {"title": "MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation", "authors": "Rongchang Xie, Chunyu Wang, Yizhou Wang", "link": "https://arxiv.org/abs/2003.13239", "summary": "Cross view feature fusion is the key to address the occlusion problem in\nhuman pose estimation. The current fusion methods need to train a separate\nmodel for every pair of cameras making them difficult to scale. In this work,\nwe introduce MetaFuse, a pre-trained fusion model learned from a large number\nof cameras in the Panoptic dataset. The model can be efficiently adapted or\nfinetuned for a new pair of cameras using a small number of labeled images. The\nstrong adaptation power of MetaFuse is due in large part to the proposed\nfactorization of the original fusion model into two parts (1) a generic fusion\nmodel shared by all cameras, and (2) lightweight camera-dependent\ntransformations. Furthermore, the generic model is learned from many cameras by\na meta-learning style algorithm to maximize its adaptation capability to\nvarious camera poses. We observe in experiments that MetaFuse finetuned on the\npublic datasets outperforms the state-of-the-arts by a large margin which\nvalidates its value in practice."}, {"title": "Prior Guided GAN Based Semantic Inpainting", "authors": "Avisek Lahiri, Arnav Kumar Jain, Sanskar Agrawal, Pabitra Mitra, Prabir Kumar Biswas", "link": "", "summary": ""}, {"title": "Weakly Supervised Semantic Point Cloud Segmentation: Towards 10\u00d7 Fewer Labels", "authors": "Xun Xu, Gim Hee Lee", "link": "https://arxiv.org/abs/2004.04091", "summary": "Point cloud analysis has received much attention recently; and segmentation\nis one of the most important tasks. The success of existing approaches is\nattributed to deep network design and large amount of labelled training data,\nwhere the latter is assumed to be always available. However, obtaining 3d point\ncloud segmentation labels is often very costly in practice. In this work, we\npropose a weakly supervised point cloud segmentation approach which requires\nonly a tiny fraction of points to be labelled in the training stage. This is\nmade possible by learning gradient approximation and exploitation of additional\nspatial and color smoothness constraints. Experiments are done on three public\ndatasets with different degrees of weak supervision. In particular, our\nproposed method can produce results that are close to and sometimes even better\nthan its fully supervised counterpart with 10$\\times$ fewer labels."}, {"title": "Physically Realizable Adversarial Examples for LiDAR Object Detection", "authors": "James Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Bin Yang, Richard Du, Frank Cheng, Raquel Urtasun", "link": "https://arxiv.org/abs/2004.00543", "summary": "Modern autonomous driving systems rely heavily on deep learning models to\nprocess point cloud sensory data; meanwhile, deep models have been shown to be\nsusceptible to adversarial attacks with visually imperceptible perturbations.\nDespite the fact that this poses a security concern for the self-driving\nindustry, there has been very little exploration in terms of 3D perception, as\nmost adversarial attacks have only been applied to 2D flat images. In this\npaper, we address this issue and present a method to generate universal 3D\nadversarial objects to fool LiDAR detectors. In particular, we demonstrate that\nplacing an adversarial object on the rooftop of any target vehicle to hide the\nvehicle entirely from LiDAR detectors with a success rate of 80%. We report\nattack results on a suite of detectors using various input representation of\npoint clouds. We also conduct a pilot study on adversarial defense using data\naugmentation. This is one step closer towards safer self-driving under unseen\nconditions from limited training data."}, {"title": "Combating Noisy Labels by Agreement: A Joint Training Method with Co-Regularization", "authors": "Hongxin Wei, Lei Feng, Xiangyu Chen, Bo An", "link": "https://arxiv.org/abs/2003.02752", "summary": "Deep Learning with noisy labels is a practically challenging problem in\nweakly supervised learning. The state-of-the-art approaches \"Decoupling\" and\n\"Co-teaching+\" claim that the \"disagreement\" strategy is crucial for\nalleviating the problem of learning with noisy labels. In this paper, we start\nfrom a different perspective and propose a robust learning paradigm called\nJoCoR, which aims to reduce the diversity of two networks during training.\nSpecifically, we first use two networks to make predictions on the same\nmini-batch data and calculate a joint loss with Co-Regularization for each\ntraining example. Then we select small-loss examples to update the parameters\nof both two networks simultaneously. Trained by the joint loss, these two\nnetworks would be more and more similar due to the effect of Co-Regularization.\nExtensive experimental results on corrupted data from benchmark datasets\nincluding MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is\nsuperior to many state-of-the-art approaches for learning with noisy labels."}, {"title": "Light-weight Calibrator: A Separable Component for Unsupervised Domain Adaptation", "authors": "Shaokai Ye, Kailu Wu, Mu Zhou, Yunfei Yang, Sia Huat Tan, Kaidi Xu, Jiebo Song, Chenglong Bao, Kaisheng Ma", "link": "http://arxiv.org/abs/1911.12796", "summary": "Existing domain adaptation methods aim at learning features that can be\ngeneralized among domains. These methods commonly require to update source\nclassifier to adapt to the target domain and do not properly handle the trade\noff between the source domain and the target domain. In this work, instead of\ntraining a classifier to adapt to the target domain, we use a separable\ncomponent called data calibrator to help the fixed source classifier recover\ndiscrimination power in the target domain, while preserving the source domain's\nperformance. When the difference between two domains is small, the source\nclassifier's representation is sufficient to perform well in the target domain\nand outperforms GAN-based methods in digits. Otherwise, the proposed method can\nleverage synthetic images generated by GANs to boost performance and achieve\nstate-of-the-art performance in digits datasets and driving scene semantic\nsegmentation. Our method empirically reveals that certain intriguing hints,\nwhich can be mitigated by adversarial attack to domain discriminators, are one\nof the sources for performance degradation under the domain shift."}, {"title": "Learn to Augment: Joint Data Augmentation and Network Optimization for Text Recognition", "authors": "Canjie Luo, Yuanzhi Zhu, Lianwen Jin, Yongpan Wang", "link": "https://arxiv.org/abs/2003.06606", "summary": "Handwritten text and scene text suffer from various shapes and distorted\npatterns. Thus training a robust recognition model requires a large amount of\ndata to cover diversity as much as possible. In contrast to data collection and\nannotation, data augmentation is a low cost way. In this paper, we propose a\nnew method for text image augmentation. Different from traditional augmentation\nmethods such as rotation, scaling and perspective transformation, our proposed\naugmentation method is designed to learn proper and efficient data augmentation\nwhich is more effective and specific for training a robust recognizer. By using\na set of custom fiducial points, the proposed augmentation method is flexible\nand controllable. Furthermore, we bridge the gap between the isolated processes\nof data augmentation and network optimization by joint learning. An agent\nnetwork learns from the output of the recognition network and controls the\nfiducial points to generate more proper training samples for the recognition\nnetwork. Extensive experiments on various benchmarks, including regular scene\ntext, irregular scene text and handwritten text, show that the proposed\naugmentation and the joint learning methods significantly boost the performance\nof the recognition networks. A general toolkit for geometric augmentation is\navailable."}, {"title": "Learning Selective Self-Mutual Attention for RGB-D Saliency Detection", "authors": "Nian Liu, Ni Zhang, Junwei Han"}, {"title": "Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation", "authors": "Yangtao Zheng, Di Huang, Songtao Liu, Yunhong Wang", "link": "http://arxiv.org/abs/2003.10275", "summary": "Recent years have witnessed great progress in deep learning based object\ndetection. However, due to the domain shift problem, applying off-the-shelf\ndetectors to an unseen domain leads to significant performance drop. To address\nsuch an issue, this paper proposes a novel coarse-to-fine feature adaptation\napproach to cross-domain object detection. At the coarse-grained stage,\ndifferent from the rough image-level or instance-level feature alignment used\nin the literature, foreground regions are extracted by adopting the attention\nmechanism, and aligned according to their marginal distributions via\nmulti-layer adversarial learning in the common feature space. At the\nfine-grained stage, we conduct conditional distribution alignment of\nforegrounds by minimizing the distance of global prototypes with the same\ncategory but from different domains. Thanks to this coarse-to-fine feature\nadaptation, domain knowledge in foreground regions can be effectively\ntransferred. Extensive experiments are carried out in various cross-domain\ndetection scenarios. The results are state-of-the-art, which demonstrate the\nbroad applicability and effectiveness of the proposed approach."}, {"title": "Estimating Low-Rank Region Likelihood Maps", "authors": "Gabriela Csurka, Zoltan Kato, Andor Juhasz, Martin Humenberger"}, {"title": "Neural Head Reenactment with Latent Pose Descriptors", "authors": "Egor Burkov, Igor Pasechnik, Artur Grigorev, Victor Lempitsky", "link": "https://arxiv.org/abs/2004.12000", "summary": "We propose a neural head reenactment system, which is driven by a latent pose\nrepresentation and is capable of predicting the foreground segmentation\nalongside the RGB image. The latent pose representation is learned as a part of\nthe entire reenactment system, and the learning process is based solely on\nimage reconstruction losses. We show that despite its simplicity, with a large\nand diverse enough training dataset, such learning successfully decomposes pose\nfrom identity. The resulting system can then reproduce mimics of the driving\nperson and, furthermore, can perform cross-person reenactment. Additionally, we\nshow that the learned descriptors are useful for other pose-related tasks, such\nas keypoint prediction and pose-based retrieval."}, {"title": "Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis", "authors": "K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, C.V. Jawahar"}, {"title": "Self-Supervised Learning of Video-Induced Visual Invariances", "authors": "Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain Gelly, Mario Lucic", "link": "https://arxiv.org/abs/1912.02783", "summary": "We propose a general framework for self-supervised learning of transferable\nvisual representations based on Video-Induced Visual Invariances (VIVI). We\nconsider the implicit hierarchy present in the videos and make use of (i)\nframe-level invariances (e.g. stability to color and contrast perturbations),\n(ii) shot/clip-level invariances (e.g. robustness to changes in object\norientation and lighting conditions), and (iii) video-level invariances\n(semantic relationships of scenes across shots/clips), to define a holistic\nself-supervised loss. Training models using different variants of the proposed\nframework on videos from the YouTube-8M (YT8M) data set, we obtain\nstate-of-the-art self-supervised transfer learning results on the 19 diverse\ndownstream tasks of the Visual Task Adaptation Benchmark (VTAB), using only\n1000 labels per task. We then show how to co-train our models jointly with\nlabeled images, outperforming an ImageNet-pretrained ResNet-50 by 0.8 points\nwith 10x fewer labeled images, as well as the previous best supervised model by\n3.7 points using the full ImageNet data set."}, {"title": "Two-Stage Peer-Regularized Feature Recombination for Arbitrary Image Style Transfer", "authors": "Jan Svoboda, Asha Anoosheh, Christian Osendorfer, Jonathan Masci", "link": "https://arxiv.org/abs/1906.02913", "summary": "This paper introduces a neural style transfer model to generate a stylized\nimage conditioning on a set of examples describing the desired style. The\nproposed solution produces high-quality images even in the zero-shot setting\nand allows for more freedom in changes to the content geometry. This is made\npossible by introducing a novel Two-Stage Peer-Regularization Layer that\nrecombines style and content in latent space by means of a custom graph\nconvolutional layer. Contrary to the vast majority of existing solutions, our\nmodel does not depend on any pre-trained networks for computing perceptual\nlosses and can be trained fully end-to-end thanks to a new set of cyclic losses\nthat operate directly in latent space and not on the RGB images. An extensive\nablation study confirms the usefulness of the proposed losses and of the\nTwo-Stage Peer-Regularization Layer, with qualitative results that are\ncompetitive with respect to the current state of the art using a single model\nfor all presented styles. This opens the door to more abstract and artistic\nneural image generation scenarios, along with simpler deployment of the model."}, {"title": "MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment", "authors": "Florian Bernard, Zeeshan Khan Suri, Christian Theobalt", "link": "https://arxiv.org/abs/2002.12623", "summary": "We present a convex mixed-integer programming formulation for non-rigid shape\nmatching. To this end, we propose a novel shape deformation model based on an\nefficient low-dimensional discrete model, so that finding a globally optimal\nsolution is tractable in (most) practical cases. Our approach combines several\nfavourable properties: it is independent of the initialisation, it is much more\nefficient to solve to global optimality compared to analogous quadratic\nassignment problem formulations, and it is highly flexible in terms of the\nvariants of matching problems it can handle. Experimentally we demonstrate that\nour approach outperforms existing methods for sparse shape matching, that it\ncan be used for initialising dense shape matching methods, and we showcase its\nflexibility on several examples."}, {"title": "Improving One-Shot NAS by Suppressing the Posterior Fading", "authors": "Xiang Li, Chen Lin, Chuming Li, Ming Sun, Wei Wu, Junjie Yan, Wanli Ouyang", "link": "https://arxiv.org/abs/1910.02543", "summary": "There is a growing interest in automated neural architecture search (NAS). To\nimprove the efficiency of NAS, previous approaches adopt weight sharing method\nto force all models share the same set of weights. However, it has been\nobserved that a model performing better with shared weights does not\nnecessarily perform better when trained alone. In this paper, we analyse\nexisting weight sharing one-shot NAS approaches from a Bayesian point of view\nand identify the posterior fading problem, which compromises the effectiveness\nof shared weights. To alleviate this problem, we present a practical approach\nto guide the parameter posterior towards its true distribution. Moreover, a\nhard latency constraint is introduced during the search so that the desired\nlatency can be achieved. The resulted method, namely Posterior Convergent NAS\n(PC-NAS), achieves state-of-the-art performance under standard GPU latency\nconstraint on ImageNet. In our small search space, our model PC-NAS-S attains\n76.8 % top-1 accuracy, 2.1% higher than MobileNetV2 (1.4x) with the same\nlatency. When adopted to the large search space, PC-NAS-L achieves 78.1 % top-1\naccuracy within 11ms. The discovered architecture also transfers well to other\ncomputer vision applications such as object detection and person\nre-identification."}, {"title": "Incremental Few-Shot Object Detection", "authors": "Juan-Manuel P\u00e9rez-R\u00faa, Xiatian Zhu, Timothy M. Hospedales, Tao Xiang", "link": "https://arxiv.org/abs/2003.04668", "summary": "Most existing object detection methods rely on the availability of abundant\nlabelled training samples per class and offline model training in a batch mode.\nThese requirements substantially limit their scalability to open-ended\naccommodation of novel classes with limited labelled training data. We present\na study aiming to go beyond these limitations by considering the Incremental\nFew-Shot Detection (iFSD) problem setting, where new classes must be registered\nincrementally (without revisiting base classes) and with few examples. To this\nend we propose OpeN-ended Centre nEt (ONCE), a detector designed for\nincrementally learning to detect novel class objects with few examples. This is\nachieved by an elegant adaptation of the CentreNet detector to the few-shot\nlearning scenario, and meta-learning a class-specific code generator model for\nregistering novel classes. ONCE fully respects the incremental learning\nparadigm, with novel class registration requiring only a single forward pass of\nfew-shot training samples, and no access to base classes -- thus making it\nsuitable for deployment on embedded devices. Extensive experiments conducted on\nboth the standard object detection and fashion landmark detection tasks show\nthe feasibility of iFSD for the first time, opening an interesting and very\nimportant line of research."}, {"title": "Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN Without Sharing Medical Image Data", "authors": "Qi Chang, Hui Qu, Yikai Zhang, Mert Sabuncu, Chao Chen, Tong Zhang, Dimitris N. Metaxas", "link": "https://arxiv.org/abs/2006.00080", "summary": "In this paper, we propose a data privacy-preserving and communication\nefficient distributed GAN learning framework named Distributed Asynchronized\nDiscriminator GAN (AsynDGAN). Our proposed framework aims to train a central\ngenerator learns from distributed discriminator, and use the generated\nsynthetic image solely to train the segmentation model.We validate the proposed\nframework on the application of health entities learning problem which is known\nto be privacy sensitive. Our experiments show that our approach: 1) could learn\nthe real image's distribution from multiple datasets without sharing the\npatient's raw data. 2) is more efficient and requires lower bandwidth than\nother distributed deep learning methods. 3) achieves higher performance\ncompared to the model trained by one real dataset, and almost the same\nperformance compared to the model trained by all real datasets. 4) has provable\nguarantees that the generator could learn the distributed distribution in an\nall important fashion thus is unbiased."}, {"title": "Exploring Category-Agnostic Clusters for Open-Set Domain Adaptation", "authors": "Yingwei Pan, Ting Yao, Yehao Li, Chong-Wah Ngo, Tao Mei"}, {"title": "Regularizing Class-Wise Predictions via Self-Knowledge Distillation", "authors": "Sukmin Yun, Jongjin Park, Kimin Lee, Jinwoo Shin", "link": "https://arxiv.org/abs/2003.13964", "summary": "Deep neural networks with millions of parameters may suffer from poor\ngeneralization due to overfitting. To mitigate the issue, we propose a new\nregularization method that penalizes the predictive distribution between\nsimilar samples. In particular, we distill the predictive distribution between\ndifferent samples of the same label during training. This results in\nregularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a\nsingle network (i.e., a self-knowledge distillation) by forcing it to produce\nmore meaningful and consistent predictions in a class-wise manner.\nConsequently, it mitigates overconfident predictions and reduces intra-class\nvariations. Our experimental results on various image classification tasks\ndemonstrate that the simple yet powerful method can significantly improve not\nonly the generalization ability but also the calibration performance of modern\nconvolutional neural networks."}, {"title": "Hierarchical Graph Attention Network for Visual Relationship Detection", "authors": "Li Mi, Zhenzhong Chen", "link": "", "summary": ""}, {"title": "M2m: Imbalanced Classification via Major-to-Minor Translation", "authors": "Jaehyung Kim, Jongheon Jeong, Jinwoo Shin", "link": "https://arxiv.org/abs/2004.00431", "summary": "In most real-world scenarios, labeled training datasets are highly\nclass-imbalanced, where deep neural networks suffer from generalizing to a\nbalanced testing criterion. In this paper, we explore a novel yet simple way to\nalleviate this issue by augmenting less-frequent classes via translating\nsamples (e.g., images) from more-frequent classes. This simple approach enables\na classifier to learn more generalizable features of minority classes, by\ntransferring and leveraging the diversity of the majority information. Our\nexperimental results on a variety of class-imbalanced datasets show that the\nproposed method improves the generalization on minority classes significantly\ncompared to other existing re-sampling or re-weighting methods. The performance\nof our method even surpasses those of previous state-of-the-art methods for the\nimbalanced classification."}, {"title": "CenterMask: Real-Time Anchor-Free Instance Segmentation", "authors": "Youngwan Lee, Jongyoul Park", "link": "https://arxiv.org/abs/1911.06667", "summary": "We propose a simple yet efficient anchor-free instance segmentation, called\nCenterMask, that adds a novel spatial attention-guided mask (SAG-Mask) branch\nto anchor-free one stage object detector (FCOS) in the same vein with Mask\nR-CNN. Plugged into the FCOS object detector, the SAG-Mask branch predicts a\nsegmentation mask on each box with the spatial attention map that helps to\nfocus on informative pixels and suppress noise. We also present an improved\nbackbone networks, VoVNetV2, with two effective strategies: (1) residual\nconnection for alleviating the optimization problem of larger VoVNet\n\\cite{lee2019energy} and (2) effective Squeeze-Excitation (eSE) dealing with\nthe channel information loss problem of original SE. With SAG-Mask and\nVoVNetV2, we deign CenterMask and CenterMask-Lite that are targeted to large\nand small models, respectively. Using the same ResNet-101-FPN backbone,\nCenterMask achieves 38.3%, surpassing all previous state-of-the-art methods\nwhile at a much faster speed. CenterMask-Lite also outperforms the\nstate-of-the-art by large margins at over 35fps on Titan Xp. We hope that\nCenterMask and VoVNetV2 can serve as a solid baseline of real-time instance\nsegmentation and backbone network for various vision tasks, respectively. The\nCode is available at https://github.com/youngwanLEE/CenterMask."}, {"title": "Multi-Path Learning for Object Pose Estimation Across Domains", "authors": "Martin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius, Kai O. Arras, Rudolph Triebel", "link": "https://arxiv.org/abs/1908.00151", "summary": "We introduce a scalable approach for object pose estimation trained on\nsimulated RGB views of multiple 3D models together. We learn an encoding of\nobject views that does not only describe an implicit orientation of all objects\nseen during training, but can also relate views of untrained objects. Our\nsingle-encoder-multi-decoder network is trained using a technique we denote\n\"multi-path learning\": While the encoder is shared by all objects, each decoder\nonly reconstructs views of a single object. Consequently, views of different\ninstances do not have to be separated in the latent space and can share common\nfeatures. The resulting encoder generalizes well from synthetic to real data\nand across various instances, categories, model types and datasets. We\nsystematically investigate the learned encodings, their generalization, and\niterative refinement strategies on the ModelNet40 and T-LESS dataset. Despite\ntraining jointly on multiple objects, our 6D Object Detection pipeline achieves\nstate-of-the-art results on T-LESS at much lower runtimes than competing\napproaches."}, {"title": "Incremental Learning in Online Scenario", "authors": "Jiangpeng He, Runyu Mao, Zeman Shao, Fengqing Zhu", "link": "https://arxiv.org/abs/2003.13191", "summary": "Modern deep learning approaches have achieved great success in many vision\napplications by training a model using all available task-specific data.\nHowever, there are two major obstacles making it challenging to implement for\nreal life applications: (1) Learning new classes makes the trained model\nquickly forget old classes knowledge, which is referred to as catastrophic\nforgetting. (2) As new observations of old classes come sequentially over time,\nthe distribution may change in unforeseen way, making the performance degrade\ndramatically on future data, which is referred to as concept drift. Current\nstate-of-the-art incremental learning methods require a long time to train the\nmodel whenever new classes are added and none of them takes into consideration\nthe new observations of old classes. In this paper, we propose an incremental\nlearning framework that can work in the challenging online learning scenario\nand handle both new classes data and new observations of old classes. We\naddress problem (1) in online mode by introducing a modified cross-distillation\nloss together with a two-step learning technique. Our method outperforms the\nresults obtained from current state-of-the-art offline incremental learning\nmethods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the\nsame experiment protocol but in online scenario. We also provide a simple yet\neffective method to mitigate problem (2) by updating exemplar set using the\nfeature of each new observation of old classes and demonstrate a real life\napplication of online food image classification based on our complete framework\nusing the Food-101 dataset."}, {"title": "Enhanced Transport Distance for Unsupervised Domain Adaptation", "authors": "Mengxue Li, Yi-Ming Zhai, You-Wei Luo, Peng-Fei Ge, Chuan-Xian Ren"}, {"title": "TESA: Tensor Element Self-Attention via Matricization", "authors": "Francesca Babiloni, Ioannis Marras, Gregory Slabaugh, Stefanos Zafeiriou"}, {"title": "Training a Steerable CNN for Guidewire Detection", "authors": "Donghang Li, Adrian Barbu"}, {"title": "Superpixel Segmentation With Fully Convolutional Networks", "authors": "Fengting Yang, Qian Sun, Hailin Jin, Zihan Zhou", "link": "https://arxiv.org/abs/2003.12929", "summary": "In computer vision, superpixels have been widely used as an effective way to\nreduce the number of image primitives for subsequent processing. But only a few\nattempts have been made to incorporate them into deep neural networks. One main\nreason is that the standard convolution operation is defined on regular grids\nand becomes inefficient when applied to superpixels. Inspired by an\ninitialization strategy commonly adopted by traditional superpixel algorithms,\nwe present a novel method that employs a simple fully convolutional network to\npredict superpixels on a regular image grid. Experimental results on benchmark\ndatasets show that our method achieves state-of-the-art superpixel segmentation\nperformance while running at about 50fps. Based on the predicted superpixels,\nwe further develop a downsampling/upsampling scheme for deep networks with the\ngoal of generating high-resolution outputs for dense prediction tasks.\nSpecifically, we modify a popular network architecture for stereo matching to\nsimultaneously predict superpixels and disparities. We show that improved\ndisparity estimation accuracy can be obtained on public datasets."}, {"title": "SharinGAN: Combining Synthetic and Real Data for Unsupervised Geometry Estimation", "authors": "Koutilya PNVR, Hao Zhou, David Jacobs"}, {"title": "Label Distribution Learning on Auxiliary Label Space Graphs for Facial Expression Recognition", "authors": "Shikai Chen, Jianfeng Wang, Yuedong Chen, Zhongchao Shi, Xin Geng, Yong Rui"}, {"title": "Deep Residual Flow for Out of Distribution Detection", "authors": "Ev Zisselman, Aviv Tamar", "link": "https://arxiv.org/abs/2001.05419", "summary": "The effective application of neural networks in the real-world relies on\nproficiently detecting out-of-distribution examples. Contemporary methods seek\nto model the distribution of feature activations in the training data for\nadequately distinguishing abnormalities, and the state-of-the-art method uses\nGaussian distribution models. In this work, we present a novel approach that\nimproves upon the state-of-the-art by leveraging an expressive density model\nbased on normalizing flows. We introduce the residual flow, a novel flow\narchitecture that learns the residual distribution from a base Gaussian\ndistribution. Our model is general, and can be applied to any data that is\napproximately Gaussian. For out of distribution detection in image datasets,\nour approach provides a principled improvement over the state-of-the-art.\nSpecifically, we demonstrate the effectiveness of our method in ResNet and\nDenseNet architectures trained on various image datasets. For example, on a\nResNet trained on CIFAR-100 and evaluated on detection of out-of-distribution\nsamples from the ImageNet dataset, holding the true positive rate (TPR) at\n$95\\%$, we improve the true negative rate (TNR) from $56.7\\%$ (current\nstate-of-the-art) to $77.5\\%$ (ours)."}, {"title": "FeatureFlow: Robust Video Interpolation via Structure-to-Texture Generation", "authors": "Shurui Gui, Chaoyue Wang, Qihua Chen, Dacheng Tao"}, {"title": "Learning Nanoscale Motion Patterns of Vesicles in Living Cells", "authors": "Arif Ahmed Sekh, Ida Sundvor Opstad, \u00c5sa Birna Birgisdottir, Truls Myrmel, Balpreet Singh Ahluwalia, Krishna Agarwal, Dilip K. Prasad"}, {"title": "Improving Action Segmentation via Graph-Based Temporal Reasoning", "authors": "Yifei Huang, Yusuke Sugano, Yoichi Sato"}, {"title": "Episode-Based Prototype Generating Network for Zero-Shot Learning", "authors": "Yunlong Yu, Zhong Ji, Jungong Han, Zhongfei Zhang", "link": "https://arxiv.org/abs/1909.03360", "summary": "We introduce a simple yet effective episode-based training framework for\nzero-shot learning (ZSL), where the learning system requires to recognize\nunseen classes given only the corresponding class semantics. During training,\nthe model is trained within a collection of episodes, each of which is designed\nto simulate a zero-shot classification task. Through training multiple\nepisodes, the model progressively accumulates ensemble experiences on\npredicting the mimetic unseen classes, which will generalize well on the real\nunseen classes. Based on this training framework, we propose a novel generative\nmodel that synthesizes visual prototypes conditioned on the class semantic\nprototypes. The proposed model aligns the visual-semantic interactions by\nformulating both the visual prototype generation and the class semantic\ninference into an adversarial framework paired with a parameter-economic\nMulti-modal Cross-Entropy Loss to capture the discriminative information.\nExtensive experiments on four datasets under both traditional ZSL and\ngeneralized ZSL tasks show that our model outperforms the state-of-the-art\napproaches by large margins."}, {"title": "Learning to Segment the Tail", "authors": "Xinting Hu, Yi Jiang, Kaihua Tang, Jingyuan Chen, Chunyan Miao, Hanwang Zhang", "link": "https://arxiv.org/abs/2004.00900", "summary": "Real-world visual recognition requires handling the extreme sample imbalance\nin large-scale long-tailed data. We propose a \"divide&conquer\" strategy for the\nchallenging LVIS task: divide the whole data into balanced parts and then apply\nincremental learning to conquer each one. This derives a novel learning\nparadigm: class-incremental few-shot learning, which is especially effective\nfor the challenge evolving over time: 1) the class imbalance among the\nold-class knowledge review and 2) the few-shot data in new-class learning. We\ncall our approach Learning to Segment the Tail (LST). In particular, we design\nan instance-level balanced replay scheme, which is a memory-efficient\napproximation to balance the instance-level samples from the old-class images.\nWe also propose to use a meta-module for new-class learning, where the module\nparameters are shared across incremental phases, gaining the learning-to-learn\nknowledge incrementally, from the data-rich head to the data-poor tail. We\nempirically show that: at the expense of a little sacrifice of head-class\nforgetting, we can gain a significant 8.3% AP improvement for the tail classes\nwith less than 10 instances, achieving an overall 2.0% AP boost for the whole\n1,230 classes."}, {"title": "Learning to Evaluate Perception Models Using Planner-Centric Metrics", "authors": "Jonah Philion, Amlan Kar, Sanja Fidler", "link": "https://arxiv.org/abs/2004.08745", "summary": "Variants of accuracy and precision are the gold-standard by which the\ncomputer vision community measures progress of perception algorithms. One\nreason for the ubiquity of these metrics is that they are largely\ntask-agnostic; we in general seek to detect zero false negatives or positives.\nThe downside of these metrics is that, at worst, they penalize all incorrect\ndetections equally without conditioning on the task or scene, and at best,\nheuristics need to be chosen to ensure that different mistakes count\ndifferently. In this paper, we propose a principled metric for 3D object\ndetection specifically for the task of self-driving. The core idea behind our\nmetric is to isolate the task of object detection and measure the impact the\nproduced detections would induce on the downstream task of driving. Without\nhand-designing it to, we find that our metric penalizes many of the mistakes\nthat other metrics penalize by design. In addition, our metric downweighs\ndetections based on additional factors such as distance from a detection to the\nego car and the speed of the detection in intuitive ways that other detection\nmetrics do not. For human evaluation, we generate scenes in which standard\nmetrics and our metric disagree and find that humans side with our metric 79%\nof the time. Our project page including an evaluation server can be found at\nhttps://nv-tlabs.github.io/detection-relevance."}, {"title": "Where, What, Whether: Multi-Modal Learning Meets Pedestrian Detection", "authors": "Yan Luo, Chongyang Zhang, Muming Zhao, Hao Zhou, Jun Sun"}, {"title": "CoverNet: Multimodal Behavior Prediction Using Trajectory Sets", "authors": "Tung Phan-Minh, Elena Corina Grigore, Freddy A. Boulton, Oscar Beijbom, Eric M. Wolff", "link": "https://arxiv.org/abs/1911.10298", "summary": "We present CoverNet, a new method for multimodal, probabilistic trajectory\nprediction for urban driving. Previous work has employed a variety of methods,\nincluding multimodal regression, occupancy maps, and 1-step stochastic\npolicies. We instead frame the trajectory prediction problem as classification\nover a diverse set of trajectories. The size of this set remains manageable due\nto the limited number of distinct actions that can be taken over a reasonable\nprediction horizon. We structure the trajectory set to a) ensure a desired\nlevel of coverage of the state space, and b) eliminate physically impossible\ntrajectories. By dynamically generating trajectory sets based on the agent's\ncurrent state, we can further improve our method's efficiency. We demonstrate\nour approach on public, real-world self-driving datasets, and show that it\noutperforms state-of-the-art methods."}, {"title": "Real-World Person Re-Identification via Degradation Invariance Learning", "authors": "Yukun Huang, Zheng-Jun Zha, Xueyang Fu, Richang Hong, Liang Li", "link": "https://arxiv.org/abs/2004.04933", "summary": "Person re-identification (Re-ID) in real-world scenarios usually suffers from\nvarious degradation factors, e.g., low-resolution, weak illumination, blurring\nand adverse weather. On the one hand, these degradations lead to severe\ndiscriminative information loss, which significantly obstructs identity\nrepresentation learning; on the other hand, the feature mismatch problem caused\nby low-level visual variations greatly reduces retrieval performance. An\nintuitive solution to this problem is to utilize low-level image restoration\nmethods to improve the image quality. However, existing restoration methods\ncannot directly serve to real-world Re-ID due to various limitations, e.g., the\nrequirements of reference samples, domain gap between synthesis and reality,\nand incompatibility between low-level and high-level methods. In this paper, to\nsolve the above problem, we propose a degradation invariance learning framework\nfor real-world person Re-ID. By introducing a self-supervised disentangled\nrepresentation learning strategy, our method is able to simultaneously extract\nidentity-related robust features and remove real-world degradations without\nextra supervision. We use low-resolution images as the main demonstration, and\nexperiments show that our approach is able to achieve state-of-the-art\nperformance on several Re-ID benchmarks. In addition, our framework can be\neasily extended to other real-world degradation factors, such as weak\nillumination, with only a few modifications."}, {"title": "Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack", "authors": "Zhezhi He, Adnan Siraj Rakin, Jingtao Li, Chaitali Chakrabarti, Deliang Fan"}, {"title": "Adversarial Latent Autoencoders", "authors": "Stanislav Pidhorskyi, Donald A. Adjeroh, Gianfranco Doretto", "link": "https://arxiv.org/abs/2004.04467", "summary": "Autoencoder networks are unsupervised approaches aiming at combining\ngenerative and representational properties by learning simultaneously an\nencoder-generator map. Although studied extensively, the issues of whether they\nhave the same generative power of GANs, or learn disentangled representations,\nhave not been fully addressed. We introduce an autoencoder that tackles these\nissues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a\ngeneral architecture that can leverage recent improvements on GAN training\nprocedures. We designed two autoencoders: one based on a MLP encoder, and\nanother based on a StyleGAN generator, which we call StyleALAE. We verify the\ndisentanglement properties of both architectures. We show that StyleALAE can\nnot only generate 1024x1024 face images with comparable quality of StyleGAN,\nbut at the same resolution can also produce face reconstructions and\nmanipulations based on real images. This makes ALAE the first autoencoder able\nto compare with, and go beyond the capabilities of a generator-only type of\narchitecture."}, {"title": "Adaptive Fractional Dilated Convolution Network for Image Aesthetics Assessment", "authors": "Qiuyu Chen, Wei Zhang, Ning Zhou, Peng Lei, Yi Xu, Yu Zheng, Jianping Fan", "link": "https://arxiv.org/abs/2004.03015", "summary": "To leverage deep learning for image aesthetics assessment, one critical but\nunsolved issue is how to seamlessly incorporate the information of image aspect\nratios to learn more robust models. In this paper, an adaptive fractional\ndilated convolution (AFDC), which is aspect-ratio-embedded,\ncomposition-preserving and parameter-free, is developed to tackle this issue\nnatively in convolutional kernel level. Specifically, the fractional dilated\nkernel is adaptively constructed according to the image aspect ratios, where\nthe interpolation of nearest two integers dilated kernels is used to cope with\nthe misalignment of fractional sampling. Moreover, we provide a concise\nformulation for mini-batch training and utilize a grouping strategy to reduce\ncomputational overhead. As a result, it can be easily implemented by common\ndeep learning libraries and plugged into popular CNN architectures in a\ncomputation-efficient manner. Our experimental results demonstrate that our\nproposed method achieves state-of-the-art performance on image aesthetics\nassessment over the AVA dataset."}, {"title": "Deep Generative Model for Robust Imbalance Classification", "authors": "Xinyue Wang, Yilin Lyu, Liping Jing"}, {"title": "Learning Deep Network for Detecting 3D Object Keypoints and 6D Poses", "authors": "Wanqing Zhao, Shaobo Zhang, Ziyu Guan, Wei Zhao, Jinye Peng, Jianping Fan"}, {"title": "MetaIQA: Deep Meta-Learning for No-Reference Image Quality Assessment", "authors": "Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, Guangming Shi", "link": "https://arxiv.org/abs/2004.05508", "summary": "Recently, increasing interest has been drawn in exploiting deep convolutional\nneural networks (DCNNs) for no-reference image quality assessment (NR-IQA).\nDespite of the notable success achieved, there is a broad consensus that\ntraining DCNNs heavily relies on massive annotated data. Unfortunately, IQA is\na typical small sample problem. Therefore, most of the existing DCNN-based IQA\nmetrics operate based on pre-trained networks. However, these pre-trained\nnetworks are not designed for IQA task, leading to generalization problem when\nevaluating different types of distortions. With this motivation, this paper\npresents a no-reference IQA metric based on deep meta-learning. The underlying\nidea is to learn the meta-knowledge shared by human when evaluating the quality\nof images with various distortions, which can then be adapted to unknown\ndistortions easily. Specifically, we first collect a number of NR-IQA tasks for\ndifferent distortions. Then meta-learning is adopted to learn the prior\nknowledge shared by diversified distortions. Finally, the quality prior model\nis fine-tuned on a target NR-IQA task for quickly obtaining the quality model.\nExtensive experiments demonstrate that the proposed metric outperforms the\nstate-of-the-arts by a large margin. Furthermore, the meta-model learned from\nsynthetic distortions can also be easily generalized to authentic distortions,\nwhich is highly desired in real-world applications of IQA metrics."}, {"title": "Sketchformer: Transformer-Based Representation for Sketched Structure", "authors": "Leo Sampaio Ferraz Ribeiro, Tu Bui, John Collomosse, Moacir Ponti", "link": "https://arxiv.org/abs/2002.10381", "summary": "Sketchformer is a novel transformer-based representation for encoding\nfree-hand sketches input in a vector form, i.e. as a sequence of strokes.\nSketchformer effectively addresses multiple tasks: sketch classification,\nsketch based image retrieval (SBIR), and the reconstruction and interpolation\nof sketches. We report several variants exploring continuous and tokenized\ninput representations, and contrast their performance. Our learned embedding,\ndriven by a dictionary learning tokenization scheme, yields state of the art\nperformance in classification and image retrieval tasks, when compared against\nbaseline representations driven by LSTM sequence to sequence architectures:\nSketchRNN and derivatives. We show that sketch reconstruction and interpolation\nare improved significantly by the Sketchformer embedding for complex sketches\nwith longer stroke sequences."}, {"title": "Cylindrical Convolutional Networks for Joint Object Detection and Viewpoint Estimation", "authors": "Sunghun Joung, Seungryong Kim, Hanjae Kim, Minsu Kim, Ig-Jae Kim, Junghyun Cho, Kwanghoon Sohn", "link": "https://arxiv.org/abs/2003.11303", "summary": "Existing techniques to encode spatial invariance within deep convolutional\nneural networks only model 2D transformation fields. This does not account for\nthe fact that objects in a 2D space are a projection of 3D ones, and thus they\nhave limited ability to severe object viewpoint changes. To overcome this\nlimitation, we introduce a learnable module, cylindrical convolutional networks\n(CCNs), that exploit cylindrical representation of a convolutional kernel\ndefined in the 3D space. CCNs extract a view-specific feature through a\nview-specific convolutional kernel to predict object category scores at each\nviewpoint. With the view-specific feature, we simultaneously determine\nobjective category and viewpoints using the proposed sinusoidal soft-argmax\nmodule. Our experiments demonstrate the effectiveness of the cylindrical\nconvolutional networks on joint object detection and viewpoint estimation."}, {"title": "Learning a Unified Sample Weighting Network for Object Detection", "authors": "Qi Cai, Yingwei Pan, Yu Wang, Jingen Liu, Ting Yao, Tao Mei"}, {"title": "Old Is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm", "authors": "Muhammad Zaigham Zaheer, Jin-Ha Lee, Marcella Astrid, Seung-Ik Lee", "link": "http://arxiv.org/abs/2004.07657", "summary": "A popular method for anomaly detection is to use the generator of an\nadversarial network to formulate anomaly scores over reconstruction loss of\ninput. Due to the rare occurrence of anomalies, optimizing such networks can be\na cumbersome task. Another possible approach is to use both generator and\ndiscriminator for anomaly detection. However, attributed to the involvement of\nadversarial training, this model is often unstable in a way that the\nperformance fluctuates drastically with each training step. In this study, we\npropose a framework that effectively generates stable results across a wide\nrange of training steps and allows us to use both the generator and the\ndiscriminator of an adversarial model for efficient and robust anomaly\ndetection. Our approach transforms the fundamental role of a discriminator from\nidentifying real and fake data to distinguishing between good and bad quality\nreconstructions. To this end, we prepare training examples for the good quality\nreconstruction by employing the current generator, whereas poor quality\nexamples are obtained by utilizing an old state of the same generator. This\nway, the discriminator learns to detect subtle distortions that often appear in\nreconstructions of the anomaly inputs. Extensive experiments performed on\nCaltech-256 and MNIST image datasets for novelty detection show superior\nresults. Furthermore, on UCSD Ped2 video dataset for anomaly detection, our\nmodel achieves a frame-level AUC of 98.1%, surpassing recent state-of-the-art\nmethods."}, {"title": "An Adaptive Neural Network for Unsupervised Mosaic Consistency Analysis in Image Forensics", "authors": "Quentin Bammey, Rafael Grompone von Gioi, Jean-Michel Morel"}, {"title": "McFlow: Monte Carlo Flow Models for Data Imputation", "authors": "Trevor W. Richardson, Wencheng Wu, Lei Lin, Beilei Xu, Edgar A. Bernal", "link": "https://arxiv.org/abs/2003.12628", "summary": "We consider the topic of data imputation, a foundational task in machine\nlearning that addresses issues with missing data. To that end, we propose\nMCFlow, a deep framework for imputation that leverages normalizing flow\ngenerative models and Monte Carlo sampling. We address the causality dilemma\nthat arises when training models with incomplete data by introducing an\niterative learning scheme which alternately updates the density estimate and\nthe values of the missing entries in the training data. We provide extensive\nempirical validation of the effectiveness of the proposed method on standard\nmultivariate and image datasets, and benchmark its performance against\nstate-of-the-art alternatives. We demonstrate that MCFlow is superior to\ncompeting methods in terms of the quality of the imputed data, as well as with\nregards to its ability to preserve the semantic structure of the data."}, {"title": "Learning to See Through Obstructions", "authors": "Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang", "link": "https://arxiv.org/abs/2004.01180", "summary": "We present a learning-based approach for removing unwanted obstructions, such\nas window reflections, fence occlusions or raindrops, from a short sequence of\nimages captured by a moving camera. Our method leverages the motion differences\nbetween the background and the obstructing elements to recover both layers.\nSpecifically, we alternate between estimating dense optical flow fields of the\ntwo layers and reconstructing each layer from the flow-warped images via a deep\nconvolutional neural network. The learning-based layer reconstruction allows us\nto accommodate potential errors in the flow estimation and brittle assumptions\nsuch as brightness consistency. We show that training on synthetically\ngenerated data transfers well to real images. Our results on numerous\nchallenging scenarios of reflection and fence removal demonstrate the\neffectiveness of the proposed method."}, {"title": "GaitPart: Temporal Part-Based Model for Gait Recognition", "authors": "Chao Fan, Yunjie Peng, Chunshui Cao, Xu Liu, Saihui Hou, Jiannan Chi, Yongzhen Huang, Qing Li, Zhiqiang He", "link": "", "summary": ""}, {"title": "EmotiCon: Context-Aware Multimodal Emotion Recognition Using Frege\u2019s Principle", "authors": "Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, Dinesh Manocha", "link": "https://arxiv.org/abs/2003.06692", "summary": "We present EmotiCon, a learning-based algorithm for context-aware perceived\nhuman emotion recognition from videos and images. Motivated by Frege's Context\nPrinciple from psychology, our approach combines three interpretations of\ncontext for emotion recognition. Our first interpretation is based on using\nmultiple modalities(e.g. faces and gaits) for emotion recognition. For the\nsecond interpretation, we gather semantic context from the input image and use\na self-attention-based CNN to encode this information. Finally, we use depth\nmaps to model the third interpretation related to socio-dynamic interactions\nand proximity among agents. We demonstrate the efficiency of our network\nthrough experiments on EMOTIC, a benchmark dataset. We report an Average\nPrecision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8\nover prior methods. We also introduce a new dataset, GroupWalk, which is a\ncollection of videos captured in multiple real-world settings of people\nwalking. We report an AP of 65.83 across 4 categories on GroupWalk, which is\nalso an improvement over prior methods."}, {"title": "Can Deep Learning Recognize Subtle Human Activities?", "authors": "Vincent Jacquot, Zhuofan Ying, Gabriel Kreiman", "link": "http://arxiv.org/abs/2003.13852", "summary": "Deep Learning has driven recent and exciting progress in computer vision,\ninstilling the belief that these algorithms could solve any visual task. Yet,\ndatasets commonly used to train and test computer vision algorithms have\npervasive confounding factors. Such biases make it difficult to truly estimate\nthe performance of those algorithms and how well computer vision models can\nextrapolate outside the distribution in which they were trained. In this work,\nwe propose a new action classification challenge that is performed well by\nhumans, but poorly by state-of-the-art Deep Learning models. As a\nproof-of-principle, we consider three exemplary tasks: drinking, reading, and\nsitting. The best accuracies reached using state-of-the-art computer vision\nmodels were 61.7%, 62.8%, and 76.8%, respectively, while human participants\nscored above 90% accuracy on the three tasks. We propose a rigorous method to\nreduce confounds when creating datasets, and when comparing human versus\ncomputer vision performance. Source code and datasets are publicly available."}, {"title": "PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving", "authors": "Zelun Kong, Junfeng Guo, Ang Li, Cong Liu", "link": "https://arxiv.org/abs/1907.04449", "summary": "Although Deep neural networks (DNNs) are being pervasively used in\nvision-based autonomous driving systems, they are found vulnerable to\nadversarial attacks where small-magnitude perturbations into the inputs during\ntest time cause dramatic changes to the outputs. While most of the recent\nattack methods target at digital-world adversarial scenarios, it is unclear how\nthey perform in the physical world, and more importantly, the generated\nperturbations under such methods would cover a whole driving scene including\nthose fixed background imagery such as the sky, making them inapplicable to\nphysical world implementation. We present PhysGAN, which generates\nphysical-world-resilient adversarial examples for mislead-ing autonomous\ndriving systems in a continuous manner. We show the effectiveness and\nrobustness of PhysGAN via extensive digital and real-world evaluations. Digital\nexperiments show that PhysGAN is effective for various steer-ing models and\nscenes, which misleads the average steer-ing angle by up to 23.06 degrees under\nvarious scenarios. The real-world studies further demonstrate that PhysGAN is\nsufficiently resilient in practice, which misleads the average steering angle\nby up to 19.17 degrees. We compare PhysGAN with a set of state-of-the-art\nbaseline methods including several of our self-designed ones, which further\ndemonstrate the robustness and efficacy of our approach. We also show that\nPhysGAN outperforms state-of-the-art baseline methods To the best of our\nknowledge, PhysGANis probably the first technique of generating realistic and\nphysical-world-resilient adversarial examples for attacking common autonomous\ndriving scenarios."}, {"title": "ILFO: Adversarial Attack on Adaptive Neural Networks", "authors": "Mirazul Haque, Anki Chauhan, Cong Liu, Wei Yang"}, {"title": "On Translation Invariance in CNNs: Convolutional Layers Can Exploit Absolute Spatial Location", "authors": "Osman Semih Kayhan, Jan C. van Gemert", "link": "https://arxiv.org/abs/2003.07064", "summary": "In this paper we challenge the common assumption that convolutional layers in\nmodern CNNs are translation invariant. We show that CNNs can and will exploit\nthe absolute spatial location by learning filters that respond exclusively to\nparticular absolute locations by exploiting image boundary effects. Because\nmodern CNNs filters have a huge receptive field, these boundary effects operate\neven far from the image boundary, allowing the network to exploit absolute\nspatial location all over the image. We give a simple solution to remove\nspatial location encoding which improves translation invariance and thus gives\na stronger visual inductive bias which particularly benefits small data sets.\nWe broadly demonstrate these benefits on several architectures and various\napplications such as image classification, patch matching, and two video\nclassification datasets."}, {"title": "Diverse Image Generation via Self-Conditioned GANs", "authors": "Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, Antonio Torralba"}, {"title": "Inducing Hierarchical Compositional Model by Sparsifying Generator Network", "authors": "Xianglei Xing, Tianfu Wu, Song-Chun Zhu, Ying Nian Wu"}, {"title": "CARP: Compression Through Adaptive Recursive Partitioning for Multi-Dimensional Images", "authors": "Rongjie Liu, Meng Li, Li Ma", "link": "https://arxiv.org/abs/1912.05622", "summary": "Fast and effective image compression for multi-dimensional images has become\nincreasingly important for efficient storage and transfer of massive amounts of\nhigh-resolution images and videos. Desirable properties in compression methods\ninclude (1) high reconstruction quality at a wide range of compression rates\nwhile preserving key local details, (2) computational scalability, (3)\napplicability to a variety of different image/video types and of different\ndimensions, and (4) ease of tuning. We present such a method for\nmulti-dimensional image compression called Compression via Adaptive Recursive\nPartitioning (CARP). CARP uses an optimal permutation of the image pixels\ninferred from a Bayesian probabilistic model on recursive partitions of the\nimage to reduce its effective dimensionality, leading to a parsimonious\nrepresentation that preserves information. CARP uses a multi-layer Bayesian\nhierarchical model to achieve self-tuning and regularization to avoid\noverfitting resulting in one single parameter to be specified by the user to\nattain the desired compression rate. Extensive numerical experiments using a\nvariety of datasets including 2D ImageNet, 3D medical image, and real-life\nYouTube and surveillance videos show that CARP dominates the state-of-the-art\ncompression approaches including JPEG, JPEG2000, MPEG4, and a neural\nnetwork-based method for all of these different image types and often on nearly\nall of the individual images."}, {"title": "GrappaNet: Combining Parallel Imaging With Deep Learning for Multi-Coil MRI Reconstruction", "authors": "Anuroop Sriram, Jure Zbontar, Tullie Murrell, C. Lawrence Zitnick, Aaron Defazio, Daniel K. Sodickson", "link": "https://arxiv.org/abs/1910.12325", "summary": "Magnetic Resonance Image (MRI) acquisition is an inherently slow process\nwhich has spurred the development of two different acceleration methods:\nacquiring multiple correlated samples simultaneously (parallel imaging) and\nacquiring fewer samples than necessary for traditional signal processing\nmethods (compressed sensing). Both methods provide complementary approaches to\naccelerating the speed of MRI acquisition. In this paper, we present a novel\nmethod to integrate traditional parallel imaging methods into deep neural\nnetworks that is able to generate high quality reconstructions even for high\nacceleration factors. The proposed method, called GrappaNet, performs\nprogressive reconstruction by first mapping the reconstruction problem to a\nsimpler one that can be solved by a traditional parallel imaging methods using\na neural network, followed by an application of a parallel imaging method, and\nfinally fine-tuning the output with another neural network. The entire network\ncan be trained end-to-end. We present experimental results on the recently\nreleased fastMRI dataset and show that GrappaNet can generate higher quality\nreconstructions than competing methods for both $4\\times$ and $8\\times$\nacceleration."}, {"title": "Can Weight Sharing Outperform Random Architecture Search? An Investigation With TuNAS", "authors": "Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans, Quoc V. Le"}, {"title": "Context Aware Graph Convolution for Skeleton-Based Action Recognition", "authors": "Xikun Zhang, Chang Xu, Dacheng Tao"}, {"title": "Fast(er) Reconstruction of Shredded Text Documents via Self-Supervised Deep Asymmetric Metric Learning", "authors": "Thiago M. Paix\u00e3o, Rodrigo F. Berriel, Maria C. S. Boeres, Alessandro L. Koerich, Claudine Badue, Alberto F. De Souza, Thiago Oliveira-Santos", "link": "https://arxiv.org/abs/2003.10063", "summary": "The reconstruction of shredded documents consists in arranging the pieces of\npaper (shreds) in order to reassemble the original aspect of such documents.\nThis task is particularly relevant for supporting forensic investigation as\ndocuments may contain criminal evidence. As an alternative to the laborious and\ntime-consuming manual process, several researchers have been investigating ways\nto perform automatic digital reconstruction. A central problem in automatic\nreconstruction of shredded documents is the pairwise compatibility evaluation\nof the shreds, notably for binary text documents. In this context, deep\nlearning has enabled great progress for accurate reconstructions in the domain\nof mechanically-shredded documents. A sensitive issue, however, is that current\ndeep model solutions require an inference whenever a pair of shreds has to be\nevaluated. This work proposes a scalable deep learning approach for measuring\npairwise compatibility in which the number of inferences scales linearly\n(rather than quadratically) with the number of shreds. Instead of predicting\ncompatibility directly, deep models are leveraged to asymmetrically project the\nraw shred content onto a common metric space in which distance is proportional\nto the compatibility. Experimental results show that our method has accuracy\ncomparable to the state-of-the-art with a speed-up of about 22 times for a test\ninstance with 505 shreds (20 mixed shredded-pages from different documents)."}, {"title": "Revisiting Pose-Normalization for Fine-Grained Few-Shot Recognition", "authors": "Luming Tang, Davis Wertheimer, Bharath Hariharan", "link": "https://arxiv.org/abs/2004.00705", "summary": "Few-shot, fine-grained classification requires a model to learn subtle,\nfine-grained distinctions between different classes (e.g., birds) based on a\nfew images alone. This requires a remarkable degree of invariance to pose,\narticulation and background. A solution is to use pose-normalized\nrepresentations: first localize semantic parts in each image, and then describe\nimages by characterizing the appearance of each part. While such\nrepresentations are out of favor for fully supervised classification, we show\nthat they are extremely effective for few-shot fine-grained classification.\nWith a minimal increase in model capacity, pose normalization improves accuracy\nbetween 10 and 20 percentage points for shallow and deep architectures,\ngeneralizes better to new domains, and is effective for multiple few-shot\nalgorithms and network backbones. Code is available at\nhttps://github.com/Tsingularity/PoseNorm_Fewshot"}, {"title": "RankMI: A Mutual Information Maximizing Ranking Loss", "authors": "Mete Kemertas, Leila Pishdad, Konstantinos G. Derpanis, Afsaneh Fazly"}, {"title": "Learning Memory-Guided Normality for Anomaly Detection", "authors": "Hyunjong Park, Jongyoun Noh, Bumsub Ham", "link": "http://arxiv.org/abs/2003.13228", "summary": "We address the problem of anomaly detection, that is, detecting anomalous\nevents in a video sequence. Anomaly detection methods based on convolutional\nneural networks (CNNs) typically leverage proxy tasks, such as reconstructing\ninput video frames, to learn models describing normality without seeing\nanomalous samples at training time, and quantify the extent of abnormalities\nusing the reconstruction error at test time. The main drawbacks of these\napproaches are that they do not consider the diversity of normal patterns\nexplicitly, and the powerful representation capacity of CNNs allows to\nreconstruct abnormal video frames. To address this problem, we present an\nunsupervised learning approach to anomaly detection that considers the\ndiversity of normal patterns explicitly, while lessening the representation\ncapacity of CNNs. To this end, we propose to use a memory module with a new\nupdate scheme where items in the memory record prototypical patterns of normal\ndata. We also present novel feature compactness and separateness losses to\ntrain the memory, boosting the discriminative power of both memory items and\ndeeply learned features from normal data. Experimental results on standard\nbenchmarks demonstrate the effectiveness and efficiency of our approach, which\noutperforms the state of the art."}, {"title": "Appearance Shock Grammar for Fast Medial Axis Extraction From Real Images", "authors": "Charles-Olivier Dufresne Camaro, Morteza Rezanejad, Stavros Tsogkas, Kaleem Siddiqi, Sven Dickinson", "link": "https://arxiv.org/abs/2004.02677", "summary": "We combine ideas from shock graph theory with more recent appearance-based\nmethods for medial axis extraction from complex natural scenes, improving upon\nthe present best unsupervised method, in terms of efficiency and performance.\nWe make the following specific contributions: i) we extend the shock graph\nrepresentation to the domain of real images, by generalizing the shock type\ndefinitions using local, appearance-based criteria; ii) we then use the rules\nof a Shock Grammar to guide our search for medial points, drastically reducing\nrun time when compared to other methods, which exhaustively consider all points\nin the input image;iii) we remove the need for typical post-processing steps\nincluding thinning, non-maximum suppression, and grouping, by adhering to the\nShock Grammar rules while deriving the medial axis solution; iv) finally, we\nraise some fundamental concerns with the evaluation scheme used in previous\nwork and propose a more appropriate alternative for assessing the performance\nof medial axis extraction from scenes. Our experiments on the BMAX500 and\nSK-LARGE datasets demonstrate the effectiveness of our approach. We outperform\nthe present state-of-the-art, excelling particularly in the high-precision\nregime, while running an order of magnitude faster and requiring no\npost-processing."}, {"title": "Generalizing Hand Segmentation in Egocentric Videos With Uncertainty-Guided Model Adaptation", "authors": "Minjie Cai, Feng Lu, Yoichi Sato"}, {"title": "DeFeat-Net: General Monocular Depth via Simultaneous Unsupervised Representation Learning", "authors": "Jaime Spencer, Richard Bowden, Simon Hadfield", "link": "https://arxiv.org/abs/2003.13446", "summary": "In the current monocular depth research, the dominant approach is to employ\nunsupervised training on large datasets, driven by warped photometric\nconsistency. Such approaches lack robustness and are unable to generalize to\nchallenging domains such as nighttime scenes or adverse weather conditions\nwhere assumptions about photometric consistency break down.\n  We propose DeFeat-Net (Depth & Feature network), an approach to\nsimultaneously learn a cross-domain dense feature representation, alongside a\nrobust depth-estimation framework based on warped feature consistency. The\nresulting feature representation is learned in an unsupervised manner with no\nexplicit ground-truth correspondences required.\n  We show that within a single domain, our technique is comparable to both the\ncurrent state of the art in monocular depth estimation and supervised feature\nrepresentation learning. However, by simultaneously learning features, depth\nand motion, our technique is able to generalize to challenging domains,\nallowing DeFeat-Net to outperform the current state-of-the-art with around 10%\nreduction in all error measures on more challenging sequences such as nighttime\ndriving."}, {"title": "Learning Visual Motion Segmentation Using Event Surfaces", "authors": "Anton Mitrokhin, Zhiyuan Hua, Cornelia Ferm\u00fcller, Yiannis Aloimonos"}, {"title": "Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction", "authors": "Abduallah Mohamed, Kun Qian, Mohamed Elhoseiny, Christian Claudel", "link": "https://arxiv.org/abs/2002.11927", "summary": "Better machine understanding of pedestrian behaviors enables faster progress\nin modeling interactions between agents such as autonomous vehicles and humans.\nPedestrian trajectories are not only influenced by the pedestrian itself but\nalso by interaction with surrounding objects. Previous methods modeled these\ninteractions by using a variety of aggregation methods that integrate different\nlearned pedestrians states. We propose the Social Spatio-Temporal Graph\nConvolutional Neural Network (Social-STGCNN), which substitutes the need of\naggregation methods by modeling the interactions as a graph. Our results show\nan improvement over the state of art by 20% on the Final Displacement Error\n(FDE) and an improvement on the Average Displacement Error (ADE) with 8.5 times\nless parameters and up to 48 times faster inference speed than previously\nreported methods. In addition, our model is data efficient, and exceeds\nprevious state of the art on the ADE metric with only 20% of the training data.\nWe propose a kernel function to embed the social interactions between\npedestrians within the adjacency matrix. Through qualitative analysis, we show\nthat our model inherited social behaviors that can be expected between\npedestrians trajectories. Code is available at\nhttps://github.com/abduallahmohamed/Social-STGCNN."}, {"title": "Discriminative Multi-Modality Speech Recognition", "authors": "Bo Xu, Cheng Lu, Yandong Guo, Jacob Wang", "link": "https://arxiv.org/abs/2005.05592", "summary": "Vision is often used as a complementary modality for audio speech recognition\n(ASR), especially in the noisy environment where performance of solo audio\nmodality significantly deteriorates. After combining visual modality, ASR is\nupgraded to the multi-modality speech recognition (MSR). In this paper, we\npropose a two-stage speech recognition model. In the first stage, the target\nvoice is separated from background noises with help from the corresponding\nvisual information of lip movements, making the model 'listen' clearly. At the\nsecond stage, the audio modality combines visual modality again to better\nunderstand the speech by a MSR sub-network, further improving the recognition\nrate. There are some other key contributions: we introduce a pseudo-3D residual\nconvolution (P3D)-based visual front-end to extract more discriminative\nfeatures; we upgrade the temporal convolution block from 1D ResNet with the\ntemporal convolutional network (TCN), which is more suitable for the temporal\ntasks; the MSR sub-network is built on the top of Element-wise-Attention Gated\nRecurrent Unit (EleAtt-GRU), which is more effective than Transformer in long\nsequences. We conducted extensive experiments on the LRS3-TED and the LRW\ndatasets. Our two-stage model (audio enhanced multi-modality speech\nrecognition, AE-MSR) consistently achieves the state-of-the-art performance by\na significant margin, which demonstrates the necessity and effectiveness of\nAE-MSR."}, {"title": "Clean-Label Backdoor Attacks on Video Recognition Models", "authors": "Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, Yu-Gang Jiang", "link": "http://arxiv.org/abs/2003.03030", "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks which can hide\nbackdoor triggers in DNNs by poisoning training data. A backdoored model\nbehaves normally on clean test images, yet consistently predicts a particular\ntarget class for any test examples that contain the trigger pattern. As such,\nbackdoor attacks are hard to detect, and have raised severe security concerns\nin real-world applications. Thus far, backdoor research has mostly been\nconducted in the image domain with image classification models. In this paper,\nwe show that existing image backdoor attacks are far less effective on videos,\nand outline 4 strict conditions where existing attacks are likely to fail: 1)\nscenarios with more input dimensions (eg. videos), 2) scenarios with high\nresolution, 3) scenarios with a large number of classes and few examples per\nclass (a \"sparse dataset\"), and 4) attacks with access to correct labels (eg.\nclean-label attacks). We propose the use of a universal adversarial trigger as\nthe backdoor trigger to attack video recognition models, a situation where\nbackdoor attacks are likely to be challenged by the above 4 strict conditions.\nWe show on benchmark video datasets that our proposed backdoor attack can\nmanipulate state-of-the-art video models with high success rates by poisoning\nonly a small proportion of training data (without changing the labels). We also\nshow that our proposed backdoor attack is resistant to state-of-the-art\nbackdoor defense/detection methods, and can even be applied to improve image\nbackdoor attacks. Our proposed video backdoor attack not only serves as a\nstrong baseline for improving the robustness of video models, but also provides\na new perspective for more understanding more powerful backdoor attacks."}, {"title": "Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors", "authors": "Gilad Cohen, Guillermo Sapiro, Raja Giryes", "link": "https://arxiv.org/abs/1909.06872", "summary": "Deep neural networks (DNNs) are notorious for their vulnerability to\nadversarial attacks, which are small perturbations added to their input images\nto mislead their prediction. Detection of adversarial examples is, therefore, a\nfundamental requirement for robust classification frameworks. In this work, we\npresent a method for detecting such adversarial attacks, which is suitable for\nany pre-trained neural network classifier. We use influence functions to\nmeasure the impact of every training sample on the validation set data. From\nthe influence scores, we find the most supportive training samples for any\ngiven validation example. A k-nearest neighbor (k-NN) model fitted on the DNN's\nactivation layers is employed to search for the ranking of these supporting\ntraining samples. We observe that these samples are highly correlated with the\nnearest neighbors of the normal inputs, while this correlation is much weaker\nfor adversarial inputs. We train an adversarial detector using the k-NN ranks\nand distances and show that it successfully distinguishes adversarial examples,\ngetting state-of-the-art results on six attack methods with three datasets.\nCode is available at https://github.com/giladcohen/NNIF_adv_defense."}, {"title": "Unsupervised Model Personalization While Preserving Privacy and Scalability: An Open Problem", "authors": "Matthias De Lange, Xu Jia, Sarah Parisot, Ale\u0161 Leonardis, Gregory Slabaugh, Tinne Tuytelaars", "link": "https://arxiv.org/abs/2003.13296", "summary": "This work investigates the task of unsupervised model personalization,\nadapted to continually evolving, unlabeled local user images. We consider the\npractical scenario where a high capacity server interacts with a myriad of\nresource-limited edge devices, imposing strong requirements on scalability and\nlocal data privacy. We aim to address this challenge within the continual\nlearning paradigm and provide a novel Dual User-Adaptation framework (DUA) to\nexplore the problem. This framework flexibly disentangles user-adaptation into\nmodel personalization on the server and local data regularization on the user\ndevice, with desirable properties regarding scalability and privacy\nconstraints. First, on the server, we introduce incremental learning of\ntask-specific expert models, subsequently aggregated using a concealed\nunsupervised user prior. Aggregation avoids retraining, whereas the user prior\nconceals sensitive raw user data, and grants unsupervised adaptation. Second,\nlocal user-adaptation incorporates a domain adaptation point of view, adapting\nregularizing batch normalization parameters to the user data. We explore\nvarious empirical user configurations with different priors in categories and a\ntenfold of transforms for MIT Indoor Scene recognition, and classify numbers in\na combined MNIST and SVHN setup. Extensive experiments yield promising results\nfor data-driven local adaptation and elicit user priors for server adaptation\nto depend on the model rather than user data. Hence, although user-adaptation\nremains a challenging open problem, the DUA framework formalizes a principled\nfoundation for personalizing both on server and user device, while maintaining\nprivacy and scalability."}, {"title": "GIFnets: Differentiable GIF Encoding Framework", "authors": "Innfarn Yoo, Xiyang Luo, Yilin Wang, Feng Yang, Peyman Milanfar"}, {"title": "Learning Invariant Representation for Unsupervised Image Restoration", "authors": "Wenchao Du, Hu Chen, Hongyu Yang", "link": "http://arxiv.org/abs/2003.12769", "summary": "Recently, cross domain transfer has been applied for unsupervised image\nrestoration tasks. However, directly applying existing frameworks would lead to\ndomain-shift problems in translated images due to lack of effective\nsupervision. Instead, we propose an unsupervised learning method that\nexplicitly learns invariant presentation from noisy data and reconstructs clear\nobservations. To do so, we introduce discrete disentangling representation and\nadversarial domain adaption into general domain transfer framework, aided by\nextra self-supervised modules including background and semantic consistency\nconstraints, learning robust representation under dual domain constraints, such\nas feature and image domains. Experiments on synthetic and real noise removal\ntasks show the proposed method achieves comparable performance with other\nstate-of-the-art supervised and unsupervised methods, while having faster and\nstable convergence than other domain adaption methods."}, {"title": "Improved Few-Shot Visual Classification", "authors": "Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, Leonid Sigal", "link": "https://arxiv.org/abs/1912.03432", "summary": "Few-shot learning is a fundamental task in computer vision that carries the\npromise of alleviating the need for exhaustively labeled data. Most few-shot\nlearning approaches to date have focused on progressively more complex neural\nfeature extractors and classifier adaptation strategies, as well as the\nrefinement of the task definition itself. In this paper, we explore the\nhypothesis that a simple class-covariance-based distance metric, namely the\nMahalanobis distance, adopted into a state of the art few-shot learning\napproach (CNAPS) can, in and of itself, lead to a significant performance\nimprovement. We also discover that it is possible to learn adaptive feature\nextractors that allow useful estimation of the high dimensional feature\ncovariances required by this metric from surprisingly few samples. The result\nof our work is a new \"Simple CNAPS\" architecture which has up to 9.2% fewer\ntrainable parameters than CNAPS and performs up to 6.1% better than state of\nthe art on the standard few-shot image classification benchmark dataset."}, {"title": "Learning Weighted Submanifolds With Variational Autoencoders and Riemannian Variational Autoencoders", "authors": "Nina Miolane, Susan Holmes", "link": "https://arxiv.org/abs/1911.08147", "summary": "Manifold-valued data naturally arises in medical imaging. In cognitive\nneuroscience, for instance, brain connectomes base the analysis of coactivation\npatterns between different brain regions on the analysis of the correlations of\ntheir functional Magnetic Resonance Imaging (fMRI) time series - an object thus\nconstrained by construction to belong to the manifold of symmetric positive\ndefinite matrices. One of the challenges that naturally arises consists of\nfinding a lower-dimensional subspace for representing such manifold-valued\ndata. Traditional techniques, like principal component analysis, are\nill-adapted to tackle non-Euclidean spaces and may fail to achieve a\nlower-dimensional representation of the data - thus potentially pointing to the\nabsence of lower-dimensional representation of the data. However, these\ntechniques are restricted in that: (i) they do not leverage the assumption that\nthe connectomes belong on a pre-specified manifold, therefore discarding\ninformation; (ii) they can only fit a linear subspace to the data. In this\npaper, we are interested in variants to learn potentially highly curved\nsubmanifolds of manifold-valued data. Motivated by the brain connectomes\nexample, we investigate a latent variable generative model, which has the added\nbenefit of providing us with uncertainty estimates - a crucial quantity in the\nmedical applications we are considering. While latent variable models have been\nproposed to learn linear and nonlinear spaces for Euclidean data, or geodesic\nsubspaces for manifold data, no intrinsic latent variable model exists to learn\nnongeodesic subspaces for manifold data. This paper fills this gap and\nformulates a Riemannian variational autoencoder with an intrinsic generative\nmodel of manifold-valued data. We evaluate its performances on synthetic and\nreal datasets by introducing the formalism of weighted Riemannian submanifolds."}, {"title": "Learning Geocentric Object Pose in Oblique Monocular Images", "authors": "Gordon Christie, Rodrigo Rene Rai Munoz Abujder, Kevin Foster, Shea Hagstrom, Gregory D. Hager, Myron Z. Brown"}, {"title": "Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations", "authors": "Chaoning Zhang, Philipp Benz, Tooba Imtiaz, In So Kweon"}, {"title": "Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models", "authors": "Giannis Daras, Augustus Odena, Han Zhang, Alexandros G. Dimakis", "link": "https://arxiv.org/abs/1911.12287", "summary": "We introduce a new local sparse attention layer that preserves\ntwo-dimensional geometry and locality. We show that by just replacing the dense\nattention layer of SAGAN with our construction, we obtain very significant FID,\nInception score and pure visual improvements. FID score is improved from\n$18.65$ to $15.94$ on ImageNet, keeping all other parameters the same. The\nsparse attention patterns that we propose for our new layer are designed using\na novel information theoretic criterion that uses information flow graphs. We\nalso present a novel way to invert Generative Adversarial Networks with\nattention. Our method extracts from the attention layer of the discriminator a\nsaliency map, which we use to construct a new loss function for the inversion.\nThis allows us to visualize the newly introduced attention heads and show that\nthey indeed capture interesting aspects of two-dimensional geometry of real\nimages."}, {"title": "MoreFusion: Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion", "authors": "Kentaro Wada, Edgar Sucar, Stephen James, Daniel Lenton, Andrew J. Davison", "link": "https://arxiv.org/abs/2004.04336", "summary": "Robots and other smart devices need efficient object-based scene\nrepresentations from their on-board vision systems to reason about contact,\nphysics and occlusion. Recognized precise object models will play an important\nrole alongside non-parametric reconstructions of unrecognized structures. We\npresent a system which can estimate the accurate poses of multiple known\nobjects in contact and occlusion from real-time, embodied multi-view vision.\nOur approach makes 3D object pose proposals from single RGB-D views,\naccumulates pose estimates and non-parametric occupancy information from\nmultiple views as the camera moves, and performs joint optimization to estimate\nconsistent, non-intersecting poses for multiple objects in contact.\n  We verify the accuracy and robustness of our approach experimentally on 2\nobject datasets: YCB-Video, and our own challenging Cluttered YCB-Video. We\ndemonstrate a real-time robotics application where a robot arm precisely and\norderly disassembles complicated piles of objects, using only on-board RGB-D\nvision."}, {"title": "HCNAF: Hyper-Conditioned Neural Autoregressive Flow and its Application for Probabilistic Occupancy Map Forecasting", "authors": "Geunseob Oh, Jean-S\u00e9bastien Valois", "link": "https://arxiv.org/abs/1912.08111", "summary": "We introduce Hyper-Conditioned Neural Autoregressive Flow (HCNAF); a powerful\nuniversal distribution approximator designed to model arbitrarily complex\nconditional probability density functions. HCNAF consists of a neural-net based\nconditional autoregressive flow (AF) and a hyper-network that can take large\nconditions in non-autoregressive fashion and outputs the network parameters of\nthe AF. Like other flow models, HCNAF performs exact likelihood inference. We\nconduct a number of density estimation tasks on toy experiments and MNIST to\ndemonstrate the effectiveness and attributes of HCNAF, including its\ngeneralization capability over unseen conditions and expressivity. Finally, we\nshow that HCNAF scales up to complex high-dimensional prediction problems of\nthe magnitude of self-driving and that HCNAF yields a state-of-the-art\nperformance in a public self-driving dataset."}, {"title": "Detail-recovery Image Deraining via Context Aggregation Networks", "authors": "Sen Deng, Mingqiang Wei, Jun Wang, Yidan Feng, Luming Liang, Haoran Xie, Fu Lee Wang, Meng Wang", "link": "https://arxiv.org/abs/1908.10267", "summary": "Image deraining is a fundamental, yet not well-solved problem in computer\nvision and graphics. The traditional image deraining approaches commonly behave\nineffectively in medium and heavy rain removal, while the learning-based ones\nlead to image degradations such as the loss of image details, halo artifacts\nand/or color distortion. Unlike existing image deraining approaches that lack\nthe detail-recovery mechanism, we propose an end-to-end detail-recovery image\nderaining network (termed a DRD-Net) for single images. We for the first time\nintroduce two sub-networks with a comprehensive loss function which synergize\nto derain and recover the lost details caused by deraining. We have three key\ncontributions. First, we present a rain residual network to remove rain streaks\nfrom the rainy images, which combines the squeeze-and-excitation (SE) operation\nwith residual blocks to make full advantage of spatial contextual information.\nSecond, we design a new connection style block, named structure detail context\naggregation block (SDCAB), which aggregates context feature information and has\na large reception field. Third, benefiting from the SDCAB, we construct a\ndetail repair network to encourage the lost details to return for eliminating\nimage degradations. We have validated our approach on four recognized datasets\n(three synthetic and one real-world). Both quantitative and qualitative\ncomparisons show that our approach outperforms the state-of-the-art deraining\nmethods in terms of the deraining robustness and detail accuracy. The source\ncode has been available for public evaluation and use on GitHub."}, {"title": "MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model", "authors": "Han Fu, Rui Wu, Chenghao Liu, Jianling Sun", "link": "https://arxiv.org/abs/2004.01095", "summary": "Nowadays, driven by the increasing concern on diet and health, food computing\nhas attracted enormous attention from both industry and research community. One\nof the most popular research topics in this domain is Food Retrieval, due to\nits profound influence on health-oriented applications. In this paper, we focus\non the task of cross-modal retrieval between food images and cooking recipes.\nWe present Modality-Consistent Embedding Network (MCEN) that learns\nmodality-invariant representations by projecting images and texts to the same\nembedding space. To capture the latent alignments between modalities, we\nincorporate stochastic latent variables to explicitly exploit the interactions\nbetween textual and visual features. Importantly, our method learns the\ncross-modal alignments during training but computes embeddings of different\nmodalities independently at inference time for the sake of efficiency.\nExtensive experimental results clearly demonstrate that the proposed MCEN\noutperforms all existing approaches on the benchmark Recipe1M dataset and\nrequires less computational cost."}, {"title": "Hypergraph Attention Networks for Multimodal Learning", "authors": "Eun-Sol Kim, Woo Young Kang, Kyoung-Woon On, Yu-Jung Heo, Byoung-Tak Zhang"}, {"title": "Moving in the Right Direction: A Regularization for Deep Metric Learning", "authors": "Deen Dayal Mohan, Nishant Sankaran, Dennis Fedorishin, Srirangaraj Setlur, Venu Govindaraju"}, {"title": "Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets", "authors": "Daniel Haase, Manuel Amthor", "link": "https://arxiv.org/abs/2003.13549", "summary": "We introduce blueprint separable convolutions (BSConv) as highly efficient\nbuilding blocks for CNNs. They are motivated by quantitative analyses of kernel\nproperties from trained models, which show the dominance of correlations along\nthe depth axis. Based on our findings, we formulate a theoretical foundation\nfrom which we derive efficient implementations using only standard layers.\nMoreover, our approach provides a thorough theoretical derivation,\ninterpretation, and justification for the application of depthwise separable\nconvolutions (DSCs) in general, which have become the basis of many modern\nnetwork architectures. Ultimately, we reveal that DSC-based architectures such\nas MobileNets implicitly rely on cross-kernel correlations, while our BSConv\nformulation is based on intra-kernel correlations and thus allows for a more\nefficient separation of regular convolutions. Extensive experiments on\nlarge-scale and fine-grained classification datasets show that BSConvs clearly\nand consistently improve MobileNets and other DSC-based architectures without\nintroducing any further complexity. For fine-grained datasets, we achieve an\nimprovement of up to 13.7 percentage points. In addition, if used as drop-in\nreplacement for standard architectures such as ResNets, BSConv variants also\noutperform their vanilla counterparts by up to 9.5 percentage points on\nImageNet."}, {"title": "Seeing without Looking: Contextual Rescoring of Object Detections for AP Maximization", "authors": "Louren\u00e7o V. Pato, Renato Negrinho, Pedro M. Q. Aguiar", "link": "https://arxiv.org/abs/1912.12290", "summary": "The majority of current object detectors lack context: class predictions are\nmade independently from other detections. We propose to incorporate context in\nobject detection by post-processing the output of an arbitrary detector to\nrescore the confidences of its detections. Rescoring is done by conditioning on\ncontextual information from the entire set of detections: their confidences,\npredicted classes, and positions. We show that AP can be improved by simply\nreassigning the detection confidence values such that true positives that\nsurvive longer (i.e., those with the correct class and large IoU) are scored\nhigher than false positives or detections with small IoU. In this setting, we\nuse a bidirectional RNN with attention for contextual rescoring and introduce a\ntraining target that uses the IoU with ground truth to maximize AP for the\ngiven set of detections. The fact that our approach does not require access to\nvisual features makes it computationally inexpensive and agnostic to the\ndetection architecture. In spite of this simplicity, our model consistently\nimproves AP over strong pre-trained baselines (Cascade R-CNN and Faster R-CNN\nwith several backbones), particularly by reducing the confidence of duplicate\ndetections (a learned form of non-maximum suppression) and removing\nout-of-context objects by conditioning on the confidences, classes, positions,\nand sizes of the co-occurrent detections. Code is available at\nhttps://github.com/LourencoVazPato/seeing-without-looking/"}, {"title": "End-to-End Adversarial-Attention Network for Multi-Modal Clustering", "authors": "Runwu Zhou, Yi-Dong Shen"}, {"title": "Fast Sparse ConvNets", "authors": "Erich Elsen, Marat Dukhan, Trevor Gale, Karen Simonyan", "link": "", "summary": ""}, {"title": "Few Sample Knowledge Distillation for Efficient Network Compression", "authors": "Tianhong Li, Jianguo Li, Zhuang Liu, Changshui Zhang", "link": "https://arxiv.org/abs/1812.01839", "summary": "Deep neural network compression techniques such as pruning and weight tensor\ndecomposition usually require fine-tuning to recover the prediction accuracy\nwhen the compression ratio is high. However, conventional fine-tuning suffers\nfrom the requirement of a large training set and the time-consuming training\nprocedure. This paper proposes a novel solution for knowledge distillation from\nlabel-free few samples to realize both data efficiency and training/processing\nefficiency. We treat the original network as \"teacher-net\" and the compressed\nnetwork as \"student-net\". A 1x1 convolution layer is added at the end of each\nlayer block of the student-net, and we fit the block-level outputs of the\nstudent-net to the teacher-net by estimating the parameters of the added\nlayers. We prove that the added layer can be merged without adding extra\nparameters and computation cost during inference. Experiments on multiple\ndatasets and network architectures verify the method's effectiveness on\nstudent-nets obtained by various network pruning and weight decomposition\nmethods. Our method can recover student-net's accuracy to the same level as\nconventional fine-tuning methods in minutes while using only 1% label-free data\nof the full training data."}, {"title": "Predicting Sharp and Accurate Occlusion Boundaries in Monocular Depth Estimation Using Displacement Fields", "authors": "Micha\u00ebl Ramamonjisoa, Yuming Du, Vincent Lepetit", "link": "http://arxiv.org/abs/2002.12730", "summary": "Current methods for depth map prediction from monocular images tend to\npredict smooth, poorly localized contours for the occlusion boundaries in the\ninput image. This is unfortunate as occlusion boundaries are important cues to\nrecognize objects, and as we show, may lead to a way to discover new objects\nfrom scene reconstruction. To improve predicted depth maps, recent methods rely\non various forms of filtering or predict an additive residual depth map to\nrefine a first estimate. We instead learn to predict, given a depth map\npredicted by some reconstruction method, a 2D displacement field able to\nre-sample pixels around the occlusion boundaries into sharper reconstructions.\nOur method can be applied to the output of any depth estimation method, in an\nend-to-end trainable fashion. For evaluation, we manually annotated the\nocclusion boundaries in all the images in the test split of popular NYUv2-Depth\ndataset. We show that our approach improves the localization of occlusion\nboundaries for all state-of-the-art monocular depth estimation methods that we\ncould evaluate, without degrading the depth accuracy for the rest of the\nimages."}, {"title": "Shape correspondence using anisotropic Chebyshev spectral CNNs", "authors": "Qinsong Li, Shengjun Liu, Ling Hu, Xinru Liu"}, {"title": "RetinaTrack: Online Single Stage Joint Detection and Tracking", "authors": "Zhichao Lu, Vivek Rathod, Ronny Votel, Jonathan Huang", "link": "https://arxiv.org/abs/2003.13870", "summary": "Traditionally multi-object tracking and object detection are performed using\nseparate systems with most prior works focusing exclusively on one of these\naspects over the other. Tracking systems clearly benefit from having access to\naccurate detections, however and there is ample evidence in literature that\ndetectors can benefit from tracking which, for example, can help to smooth\npredictions over time. In this paper we focus on the tracking-by-detection\nparadigm for autonomous driving where both tasks are mission critical. We\npropose a conceptually simple and efficient joint model of detection and\ntracking, called RetinaTrack, which modifies the popular single stage RetinaNet\napproach such that it is amenable to instance-level embedding training. We\nshow, via evaluations on the Waymo Open Dataset, that we outperform a recent\nstate of the art tracking algorithm while requiring significantly less\ncomputation. We believe that our simple yet effective approach can serve as a\nstrong baseline for future work in this area."}, {"title": "Multimodal Categorization of Crisis Events in Social Media", "authors": "Mahdi Abavisani, Liwei Wu, Shengli Hu, Joel Tetreault, Alejandro Jaimes", "link": "https://arxiv.org/abs/2004.04917", "summary": "Recent developments in image classification and natural language processing,\ncoupled with the rapid growth in social media usage, have enabled fundamental\nadvances in detecting breaking events around the world in real-time. Emergency\nresponse is one such area that stands to gain from these advances. By\nprocessing billions of texts and images a minute, events can be automatically\ndetected to enable emergency response workers to better assess rapidly evolving\nsituations and deploy resources accordingly. To date, most event detection\ntechniques in this area have focused on image-only or text-only approaches,\nlimiting detection performance and impacting the quality of information\ndelivered to crisis response teams. In this paper, we present a new multimodal\nfusion method that leverages both images and texts as input. In particular, we\nintroduce a cross-attention module that can filter uninformative and misleading\ncomponents from weak modalities on a sample by sample basis. In addition, we\nemploy a multimodal graph-based approach to stochastically transition between\nembeddings of different multimodal pairs during training to better regularize\nthe learning process as well as dealing with limited training data by\nconstructing new matched pairs from different samples. We show that our method\noutperforms the unimodal approaches and strong multimodal baselines by a large\nmargin on three crisis-related tasks."}, {"title": "SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings", "authors": "Wenyu Han, Siyuan Xiang, Chenhui Liu, Ruoyu Wang, Chen Feng", "link": "", "summary": ""}, {"title": "SwapText: Image Based Texts Transfer in Scenes", "authors": "Qiangpeng Yang, Jun Huang, Wei Lin", "link": "https://arxiv.org/abs/2003.08152", "summary": "Swapping text in scene images while preserving original fonts, colors, sizes\nand background textures is a challenging task due to the complex interplay\nbetween different factors. In this work, we present SwapText, a three-stage\nframework to transfer texts across scene images. First, a novel text swapping\nnetwork is proposed to replace text labels only in the foreground image.\nSecond, a background completion network is learned to reconstruct background\nimages. Finally, the generated foreground image and background image are used\nto generate the word image by the fusion network. Using the proposing\nframework, we can manipulate the texts of the input images even with severe\ngeometric distortion. Qualitative and quantitative results are presented on\nseveral scene text datasets, including regular and irregular text datasets. We\nconducted extensive experiments to prove the usefulness of our method such as\nimage based text translation, text image synthesis, etc."}, {"title": "OrigamiNet: Weakly-Supervised, Segmentation-Free, One-Step, Full Page Text Recognition by learning to unfold", "authors": "Mohamed Yousef, Tom E. Bishop"}, {"title": "FroDO: From Detections to 3D Objects", "authors": "Martin R\u00fcnz, Kejie Li, Meng Tang, Lingni Ma, Chen Kong, Tanner Schmidt, Ian Reid, Lourdes Agapito, Julian Straub, Steven Lovegrove, Richard Newcombe", "link": "https://arxiv.org/abs/2005.05125", "summary": "Object-oriented maps are important for scene understanding since they jointly\ncapture geometry and semantics, allow individual instantiation and meaningful\nreasoning about objects. We introduce FroDO, a method for accurate 3D\nreconstruction of object instances from RGB video that infers object location,\npose and shape in a coarse-to-fine manner. Key to FroDO is to embed object\nshapes in a novel learnt space that allows seamless switching between sparse\npoint cloud and dense DeepSDF decoding. Given an input sequence of localized\nRGB frames, FroDO first aggregates 2D detections to instantiate a\ncategory-aware 3D bounding box per object. A shape code is regressed using an\nencoder network before optimizing shape and pose further under the learnt shape\npriors using sparse and dense shape representations. The optimization uses\nmulti-view geometric, photometric and silhouette losses. We evaluate on\nreal-world datasets, including Pix3D, Redwood-OS, and ScanNet, for single-view,\nmulti-view, and multi-object reconstruction."}]