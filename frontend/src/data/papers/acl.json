[{"title": "2kenize: Tying Subword Sequences for Chinese Script Conversion", "authors": "<b>2kenize: Tying Subword Sequences for Chinese Script Conversion</b>", "link": "https://arxiv.org/abs/2005.03375", "summary": "Simplified Chinese to Traditional Chinese character conversion is a common\npreprocessing step in Chinese NLP. Despite this, current approaches have poor\nperformance because they do not take into account that a simplified Chinese\ncharacter can correspond to multiple traditional characters. Here, we propose a\nmodel that can disambiguate between mappings and convert between the two\nscripts. The model is based on subword segmentation, two language models, as\nwell as a method for mapping between subword sequences. We further construct\nbenchmark datasets for topic classification and script conversion. Our proposed\nmethod outperforms previous Chinese Character conversion approaches by 6 points\nin accuracy. These results are further confirmed in a downstream application,\nwhere 2kenize is used to convert pretraining dataset for topic classification.\nAn error analysis reveals that our method's particular strengths are in dealing\nwith code-mixing and named entities."}, {"title": "A Batch Normalized Inference Network Keeps the KL Vanishing Away", "authors": "Qile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li, Dapeng Wu", "link": "https://arxiv.org/abs/2004.12585", "summary": "Variational Autoencoder (VAE) is widely used as a generative model to\napproximate a model's posterior on latent variables by combining the amortized\nvariational inference and deep neural networks. However, when paired with\nstrong autoregressive decoders, VAE often converges to a degenerated local\noptimum known as \"posterior collapse\". Previous approaches consider the\nKullback Leibler divergence (KL) individual for each datapoint. We propose to\nlet the KL follow a distribution across the whole dataset, and analyze that it\nis sufficient to prevent posterior collapse by keeping the expectation of the\nKL's distribution positive. Then we propose Batch Normalized-VAE (BN-VAE), a\nsimple but effective approach to set a lower bound of the expectation by\nregularizing the distribution of the approximate posterior's parameters.\nWithout introducing any new model component or modifying the objective, our\napproach can avoid the posterior collapse effectively and efficiently. We\nfurther show that the proposed BN-VAE can be extended to conditional VAE\n(CVAE). Empirically, our approach surpasses strong autoregressive baselines on\nlanguage modeling, text classification and dialogue generation, and rivals more\ncomplex approaches while keeping almost the same training time as VAE."}, {"title": "A Call for More Rigor in Unsupervised Cross-lingual Learning", "authors": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, Eneko Agirre", "link": "https://arxiv.org/abs/2004.14958", "summary": "We review motivations, definition, approaches, and methodology for\nunsupervised cross-lingual learning and call for a more rigorous position in\neach of them. An existing rationale for such research is based on the lack of\nparallel data for many of the world's languages. However, we argue that a\nscenario without any parallel data and abundant monolingual data is unrealistic\nin practice. We also discuss different training signals that have been used in\nprevious work, which depart from the pure unsupervised setting. We then\ndescribe common methodological issues in tuning and evaluation of unsupervised\ncross-lingual models and present best practices. Finally, we provide a unified\noutlook for different types of research in this area (i.e., cross-lingual word\nembeddings, deep multilingual pretraining, and unsupervised machine\ntranslation) and argue for comparable evaluation of these models."}, {"title": "A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks", "authors": "Nastaran Babanejad, Ameeta Agrawal, Aijun An, Manos Papagelis"}, {"title": "A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking", "authors": "Yong Shan, Zekang Li, Jinchao Zhang, Fandong Meng, Yang Feng, Cheng Niu, Jie Zhou", "link": "https://arxiv.org/abs/2006.01554", "summary": "Recent studies in dialogue state tracking (DST) leverage historical\ninformation to determine states which are generally represented as slot-value\npairs. However, most of them have limitations to efficiently exploit relevant\ncontext due to the lack of a powerful mechanism for modeling interactions\nbetween the slot and the dialogue history. Besides, existing methods usually\nignore the slot imbalance problem and treat all slots indiscriminately, which\nlimits the learning of hard slots and eventually hurts overall performance. In\nthis paper, we propose to enhance the DST through employing a contextual\nhierarchical attention network to not only discern relevant information at both\nword level and turn level but also learn contextual representations. We further\npropose an adaptive objective to alleviate the slot imbalance problem by\ndynamically adjust weights of different slots during training. Experimental\nresults show that our approach reaches 52.68% and 58.55% joint accuracy on\nMultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new\nstate-of-the-art performance with considerable improvements (+1.24% and\n+5.98%)."}, {"title": "A Corpus for Large-Scale Phonetic Typology", "authors": "Elizabeth Salesky, Eleanor Chodroff, Tiago Pimentel, Matthew Wiesner, Ryan Cotterell, Alan W Black, Jason Eisner", "link": "https://arxiv.org/abs/2005.13962", "summary": "A major hurdle in data-driven research on typology is having sufficient data\nin many languages to draw meaningful conclusions. We present VoxClamantis v1.0,\nthe first large-scale corpus for phonetic typology, with aligned segments and\nestimated phoneme-level labels in 690 readings spanning 635 languages, along\nwith acoustic-phonetic measures of vowels and sibilants. Access to such data\ncan greatly facilitate investigation of phonetic typology at a large scale and\nacross many languages. However, it is non-trivial and computationally intensive\nto obtain such alignments for hundreds of languages, many of which have few to\nno resources presently available. We describe the methodology to create our\ncorpus, discuss caveats with current methods and their impact on the utility of\nthis data, and illustrate possible research directions through a series of case\nstudies on the 48 highest-quality readings. Our corpus and scripts are publicly\navailable for non-commercial use at https://voxclamantisproject.github.io."}, {"title": "A Formal Hierarchy of RNN Architectures", "authors": "William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, Eran Yahav", "link": "https://arxiv.org/abs/2004.08500", "summary": "We develop a formal hierarchy of the expressive capacity of RNN\narchitectures. The hierarchy is based on two formal properties: space\ncomplexity, which measures the RNN's memory, and rational recurrence, defined\nas whether the recurrent update can be described by a weighted finite-state\nmachine. We place several RNN variants within this hierarchy. For example, we\nprove the LSTM is not rational, which formally separates it from the related\nQRNN (Bradbury et al., 2016). We also show how these models' expressive\ncapacity is expanded by stacking multiple layers or composing them with\ndifferent pooling functions. Our results build on the theory of \"saturated\"\nRNNs (Merrill, 2019). While formally extending these findings to unsaturated\nRNNs is left to future work, we hypothesize that the practical learnable\ncapacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings\nfrom training unsaturated networks on formal languages support this conjecture."}, {"title": "A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization", "authors": "Dongfang Xu, Zeyu Zhang, Steven Bethard"}, {"title": "A Generative Model for Joint Natural Language Understanding and Generation", "authors": "Bo-Hsiang Tseng, Jianpeng Cheng, Yimai Fang, David Vandyke", "link": "", "summary": ""}, {"title": "A Girl Has A Name: Detecting Authorship Obfuscation", "authors": "Asad Mahmood, Zubair Shafiq, Padmini Srinivasan", "link": "https://arxiv.org/abs/2005.00702", "summary": "Authorship attribution aims to identify the author of a text based on the\nstylometric analysis. Authorship obfuscation, on the other hand, aims to\nprotect against authorship attribution by modifying a text's style. In this\npaper, we evaluate the stealthiness of state-of-the-art authorship obfuscation\nmethods under an adversarial threat model. An obfuscator is stealthy to the\nextent an adversary finds it challenging to detect whether or not a text\nmodified by the obfuscator is obfuscated - a decision that is key to the\nadversary interested in authorship attribution. We show that the existing\nauthorship obfuscation methods are not stealthy as their obfuscated texts can\nbe identified with an average F1 score of 0.87. The reason for the lack of\nstealthiness is that these obfuscators degrade text smoothness, as ascertained\nby neural language models, in a detectable manner. Our results highlight the\nneed to develop stealthy authorship obfuscation methods that can better protect\nthe identity of an author seeking anonymity."}, {"title": "A Graph Auto-encoder Model of Derivational Morphology", "authors": "Valentin Hofmann, Hinrich Sch\u00fctze, Janet Pierrehumbert"}, {"title": "A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction", "authors": "Shuo Ren, Shujie Liu, Ming Zhou, Shuai Ma"}, {"title": "A Joint Model for Document Segmentation and Segment Labeling", "authors": "Joe Barrow, Rajiv Jain, Vlad Morariu, Varun Manjunatha, Douglas Oard, Philip Resnik"}, {"title": "A Joint Neural Model for Information Extraction with Global Features", "authors": "Ying Lin, Heng Ji, Fei Huang, Lingfei Wu"}, {"title": "A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation", "authors": "Jan Deriu, Katsiaryna Mlynchyk, Philippe Schl\u00e4pfer, Alvaro Rodrigo, Dirk von Gr\u00fcnigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, Mark Cieliebak", "link": "https://arxiv.org/abs/2004.07633", "summary": "In this paper, we introduce a novel methodology to efficiently construct a\ncorpus for question answering over structured data. For this, we introduce an\nintermediate representation that is based on the logical query plan in a\ndatabase called Operation Trees (OT). This representation allows us to invert\nthe annotation process without losing flexibility in the types of queries that\nwe generate. Furthermore, it allows for fine-grained alignment of query tokens\nto OT operations. In our method, we randomly generate OTs from a context-free\ngrammar. Afterwards, annotators have to write the appropriate natural language\nquestion that is represented by the OT. Finally, the annotators assign the\ntokens to the OT operations. We apply the method to create a new corpus OTTA\n(Operation Trees and Token Assignment), a large semantic parsing corpus for\nevaluating natural language interfaces to databases. We compare OTTA to Spider\nand LC-QuaD 2.0 and show that our methodology more than triples the annotation\nspeed while maintaining the complexity of the queries. Finally, we train a\nstate-of-the-art semantic parsing model on our data and show that our corpus is\na challenging dataset and that the token alignment can be leveraged to increase\nthe performance significantly."}, {"title": "A Mixture of h \u2212 1 Heads is Better than h Heads", "authors": "Hao Peng, Roy Schwartz, Dianqi Li, Noah A. Smith", "link": "", "summary": ""}, {"title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages", "authors": "Pedro Javier Ortiz Su\u00e1rez, Laurent Romary, Beno\u00eet Sagot"}, {"title": "A Multitask Learning Approach for Diacritic Restoration", "authors": "Sawsan Alqahtani, Ajay Mishra, Mona Diab"}, {"title": "A Novel Cascade Binary Tagging Framework for Relational Triple Extraction", "authors": "Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, Yi Chang", "link": "https://arxiv.org/abs/1909.03227", "summary": "Extracting relational triples from unstructured text is crucial for\nlarge-scale knowledge graph construction. However, few existing works excel in\nsolving the overlapping triple problem where multiple relational triples in the\nsame sentence share the same entities. In this work, we introduce a fresh\nperspective to revisit the relational triple extraction task and propose a\nnovel cascade binary tagging framework (CasRel) derived from a principled\nproblem formulation. Instead of treating relations as discrete labels as in\nprevious works, our new framework models relations as functions that map\nsubjects to objects in a sentence, which naturally handles the overlapping\nproblem. Experiments show that the CasRel framework already outperforms\nstate-of-the-art methods even when its encoder module uses a randomly\ninitialized BERT encoder, showing the power of the new tagging framework. It\nenjoys further performance boost when employing a pre-trained BERT encoder,\noutperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score\non two public datasets NYT and WebNLG, respectively. In-depth analysis on\ndifferent scenarios of overlapping triples shows that the method delivers\nconsistent performance gain across all these scenarios. The source code and\ndata are released online."}, {"title": "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation", "authors": "Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo"}, {"title": "A Prioritization Model for Suicidality Risk Assessment", "authors": "Han-Chin Shing, Philip Resnik, Douglas Oard"}, {"title": "A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks", "authors": "Angela Lin, Sudha Rao, Asli Celikyilmaz, Elnaz Nouri, Chris Brockett, Debadeepta Dey, Bill Dolan", "link": "https://arxiv.org/abs/2005.09606", "summary": "Many high-level procedural tasks can be decomposed into sequences of\ninstructions that vary in their order and choice of tools. In the cooking\ndomain, the web offers many partially-overlapping text and video recipes (i.e.\nprocedures) that describe how to make the same dish (i.e. high-level task).\nAligning instructions for the same dish across different sources can yield\ndescriptive visual explanations that are far richer semantically than\nconventional textual instructions, providing commonsense insight into how\nreal-world procedures are structured. Learning to align these different\ninstruction sets is challenging because: a) different recipes vary in their\norder of instructions and use of ingredients; and b) video instructions can be\nnoisy and tend to contain far more information than text instructions. To\naddress these challenges, we first use an unsupervised alignment algorithm that\nlearns pairwise alignments between instructions of different recipes for the\nsame dish. We then use a graph algorithm to derive a joint alignment between\nmultiple text and multiple video recipes for the same dish. We release the\nMicrosoft Research Multimodal Aligned Recipe Corpus containing 150K pairwise\nalignments between recipes across 4,262 dishes with rich commonsense\ninformation."}, {"title": "A Reinforced Generation of Adversarial Examples for Neural Machine Translation", "authors": "Wei Zou, Shujian Huang, Jun Xie, Xinyu Dai, Jiajun Chen", "link": "https://arxiv.org/abs/1911.03677", "summary": "Neural machine translation systems tend to fail on less decent inputs despite\nits significant efficacy, which may significantly harm the credibility of this\nsystems-fathoming how and when neural-based systems fail in such cases is\ncritical for industrial maintenance. Instead of collecting and analyzing bad\ncases using limited handcrafted error features, here we investigate this issue\nby generating adversarial examples via a new paradigm based on reinforcement\nlearning. Our paradigm could expose pitfalls for a given performance metric,\ne.g., BLEU, and could target any given neural machine translation architecture.\nWe conduct experiments of adversarial attacks on two mainstream neural machine\ntranslation architectures, RNN-search, and Transformer. The results show that\nour method efficiently produces stable attacks with meaning-preserving\nadversarial examples. We also present a qualitative and quantitative analysis\nfor the preference pattern of the attack, demonstrating its capability of\npitfall exposure."}, {"title": "A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction", "authors": "Yilin Niu, Fangkai Jiao, Mantong Zhou, Ting Yao, Jingfang Xu, Minlie Huang", "link": "https://arxiv.org/abs/2005.05189", "summary": "Neural models have achieved great success on machine reading comprehension\n(MRC), many of which typically consist of two components: an evidence extractor\nand an answer predictor. The former seeks the most relevant information from a\nreference text, while the latter is to locate or generate answers from the\nextracted evidence. Despite the importance of evidence labels for training the\nevidence extractor, they are not cheaply accessible, particularly in many\nnon-extractive MRC tasks such as YES/NO question answering and multi-choice\nMRC.\n  To address this problem, we present a Self-Training method (STM), which\nsupervises the evidence extractor with auto-generated evidence labels in an\niterative process. At each iteration, a base MRC model is trained with golden\nanswers and noisy evidence labels. The trained model will predict pseudo\nevidence labels as extra supervision in the next iteration. We evaluate STM on\nseven datasets over three MRC tasks. Experimental results demonstrate the\nimprovement on existing MRC models, and we also analyze how and why such a\nself-training method works in MRC."}, {"title": "A Span-based Linearization for Constituent Trees", "authors": "Yang Wei, Yuanbin Wu, Man Lan", "link": "https://arxiv.org/abs/2004.14704", "summary": "We propose a novel linearization of a constituent tree, together with a new\nlocally normalized model. For each split point in a sentence, our model\ncomputes the normalizer on all spans ending with that split point, and then\npredicts a tree span from them. Compared with global models, our model is fast\nand parallelizable. Different from previous local models, our linearization\nmethod is tied on the spans directly and considers more local features when\nperforming span prediction, which is more interpretable and effective.\nExperiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model\nsignificantly outperforms existing local models and efficiently achieves\ncompetitive results with global models."}, {"title": "A Study of Non-autoregressive Model for Sequence Generation", "authors": "Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, Tie-Yan Liu", "link": "https://arxiv.org/abs/2004.10454", "summary": "Non-autoregressive (NAR) models generate all the tokens of a sequence in\nparallel, resulting in faster generation speed compared to their autoregressive\n(AR) counterparts but at the cost of lower accuracy. Different techniques\nincluding knowledge distillation and source-target alignment have been proposed\nto bridge the gap between AR and NAR models in various tasks such as neural\nmachine translation (NMT), automatic speech recognition (ASR), and text to\nspeech (TTS). With the help of those techniques, NAR models can catch up with\nthe accuracy of AR models in some tasks but not in some others. In this work,\nwe conduct a study to understand the difficulty of NAR sequence generation and\ntry to answer: (1) Why NAR models can catch up with AR models in some tasks but\nnot all? (2) Why techniques like knowledge distillation and source-target\nalignment can help NAR models. Since the main difference between AR and NAR\nmodels is that NAR models do not use dependency among target tokens while AR\nmodels do, intuitively the difficulty of NAR sequence generation heavily\ndepends on the strongness of dependency among target tokens. To quantify such\ndependency, we propose an analysis model called CoMMA to characterize the\ndifficulty of different NAR sequence generation tasks. We have several\ninteresting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most\ntarget-token dependency while TTS has the least. 2) Knowledge distillation\nreduces the target-token dependency in target sequence and thus improves the\naccuracy of NAR models. 3) Source-target alignment constraint encourages\ndependency of a target token on source tokens and thus eases the training of\nNAR models."}, {"title": "A Systematic Assessment of Syntactic Generalization in Neural Language Models", "authors": "Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, Roger Levy", "link": "https://arxiv.org/abs/2005.03692", "summary": "While state-of-the-art neural network models continue to achieve lower\nperplexity scores on language modeling benchmarks, it remains unknown whether\noptimizing for broad-coverage predictive performance leads to human-like\nsyntactic knowledge. Furthermore, existing work has not provided a clear\npicture about the model properties required to produce proper syntactic\ngeneralizations. We present a systematic evaluation of the syntactic knowledge\nof neural language models, testing 20 combinations of model types and data\nsizes on a set of 34 English-language syntactic test suites. We find\nsubstantial differences in syntactic generalization performance by model\narchitecture, with sequential models underperforming other architectures.\nFactorially manipulating model architecture and training dataset size (1M--40M\nwords), we find that variability in syntactic generalization performance is\nsubstantially greater by architecture than by dataset size for the corpora\ntested in our experiments. Our results also reveal a dissociation between\nperplexity and syntactic generalization performance."}, {"title": "A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer\u2019s Type", "authors": "Trevor Cohen, Serguei Pakhomov", "link": "https://arxiv.org/abs/2005.03593", "summary": "In recent years there has been a burgeoning interest in the use of\ncomputational methods to distinguish between elicited speech samples produced\nby patients with dementia, and those from healthy controls. The difference\nbetween perplexity estimates from two neural language models (LMs) - one\ntrained on transcripts of speech produced by healthy participants and the other\ntrained on transcripts from patients with dementia - as a single feature for\ndiagnostic classification of unseen transcripts has been shown to produce\nstate-of-the-art performance. However, little is known about why this approach\nis effective, and on account of the lack of case/control matching in the most\nwidely-used evaluation set of transcripts (DementiaBank), it is unclear if\nthese approaches are truly diagnostic, or are sensitive to other variables. In\nthis paper, we interrogate neural LMs trained on participants with and without\ndementia using synthetic narratives previously developed to simulate\nprogressive semantic dementia by manipulating lexical frequency. We find that\nperplexity of neural LMs is strongly and differentially associated with lexical\nfrequency, and that a mixture model resulting from interpolating control and\ndementia LMs improves upon the current state-of-the-art for models trained on\ntranscript text exclusively."}, {"title": "A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure", "authors": "Longyin Zhang, Yuqing Xing, Fang Kong, Peifeng Li, Guodong Zhou", "link": "http://arxiv.org/abs/2005.02680", "summary": "Due to its great importance in deep natural language understanding and\nvarious down-stream applications, text-level parsing of discourse rhetorical\nstructure (DRS) has been drawing more and more attention in recent years.\nHowever, all the previous studies on text-level discourse parsing adopt\nbottom-up approaches, which much limit the DRS determination on local\ninformation and fail to well benefit from global information of the overall\ndiscourse. In this paper, we justify from both computational and perceptive\npoints-of-view that the top-down architecture is more suitable for text-level\nDRS parsing. On the basis, we propose a top-down neural architecture toward\ntext-level DRS parsing. In particular, we cast discourse parsing as a recursive\nsplit point ranking task, where a split point is classified to different levels\naccording to its rank and the elementary discourse units (EDUs) associated with\nit are arranged accordingly. In this way, we can determine the complete DRS as\na hierarchical tree structure via an encoder-decoder with an internal stack.\nExperimentation on both the English RST-DT corpus and the Chinese CDTB corpus\nshows the great effectiveness of our proposed top-down approach towards\ntext-level DRS parsing."}, {"title": "A Unified MRC Framework for Named Entity Recognition", "authors": "Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, Jiwei Li", "link": "https://arxiv.org/abs/1910.11476", "summary": "The task of named entity recognition (NER) is normally divided into nested\nNER and flat NER depending on whether named entities are nested or not. Models\nare usually separately developed for the two tasks, since sequence labeling\nmodels, the most widely used backbone for flat NER, are only able to assign a\nsingle label to a particular token, which is unsuitable for nested NER where a\ntoken may be assigned several labels.\n  In this paper, we propose a unified framework that is capable of handling\nboth flat and nested NER tasks. Instead of treating the task of NER as a\nsequence labeling problem, we propose to formulate it as a machine reading\ncomprehension (MRC) task. For example, extracting entities with the\n\\textsc{per} label is formalized as extracting answer spans to the question\n\"{\\it which person is mentioned in the text?}\". This formulation naturally\ntackles the entity overlapping issue in nested NER: the extraction of two\noverlapping entities for different categories requires answering two\nindependent questions. Additionally, since the query encodes informative prior\nknowledge, this strategy facilitates the process of entity extraction, leading\nto better performances for not only nested NER, but flat NER.\n  We conduct experiments on both {\\em nested} and {\\em flat} NER datasets.\nExperimental results demonstrate the effectiveness of the proposed formulation.\nWe are able to achieve vast amount of performance boost over current SOTA\nmodels on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively\non ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets,\ni.e.,+0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English\nOntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0."}, {"title": "Adaptive Compression of Word Embeddings", "authors": "Yeachan Kim, Kang-Min Kim, SangKeun Lee", "link": "", "summary": ""}, {"title": "Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation", "authors": "Arya D. McCarthy, Xian Li, Jiatao Gu, Ning Dong", "link": "", "summary": ""}, {"title": "AdvAug: Robust Adversarial Augmentation for Neural Machine Translation", "authors": "Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein"}, {"title": "Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis", "authors": "Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao"}, {"title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "authors": "Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela", "link": "https://arxiv.org/abs/1910.14599", "summary": "We introduce a new large-scale NLI benchmark dataset, collected via an\niterative, adversarial human-and-model-in-the-loop procedure. We show that\ntraining models on this new dataset leads to state-of-the-art performance on a\nvariety of popular NLI benchmarks, while posing a more difficult challenge with\nits new test set. Our analysis sheds light on the shortcomings of current\nstate-of-the-art models, and shows that non-expert annotators are successful at\nfinding their weaknesses. The data collection method can be applied in a\nnever-ending learning scenario, becoming a moving target for NLU, rather than a\nstatic benchmark that will quickly saturate."}, {"title": "Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity", "authors": "Joseph Sirrianni, Xiaoqing Liu, Douglas Adams"}, {"title": "Aligned Dual Channel Graph Convolutional Network for Visual Question Answering", "authors": "Qingbao Huang, Jielong Wei, Yi Cai, Changmeng Zheng, Junying Chen, Ho-fung Leung, Qing Li"}, {"title": "Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification", "authors": "Pratik Dutta, Sriparna Saha"}, {"title": "AMR Parsing via Graph-Sequence Iterative Inference", "authors": "Deng Cai, Wai Lam", "link": "https://arxiv.org/abs/2004.05572", "summary": "We propose a new end-to-end model that treats AMR parsing as a series of dual\ndecisions on the input sequence and the incrementally constructed graph. At\neach time step, our model performs multiple rounds of attention, reasoning, and\ncomposition that aim to answer two critical questions: (1) which part of the\ninput \\textit{sequence} to abstract; and (2) where in the output \\textit{graph}\nto construct the new concept. We show that the answers to these two questions\nare mutually causalities. We design a model based on iterative inference that\nhelps achieve better answers in both perspectives, leading to greatly improved\nparsing accuracy. Our experimental results significantly outperform all\npreviously reported \\textsc{Smatch} scores by large margins. Remarkably,\nwithout the help of any large-scale pre-trained language model (e.g., BERT),\nour model already surpasses previous state-of-the-art using BERT. With the help\nof BERT, we can push the state-of-the-art results to 80.2\\% on LDC2017T10 (AMR\n2.0) and 75.4\\% on LDC2014T12 (AMR 1.0)."}, {"title": "AMR Parsing with Latent Structural Information", "authors": "Qiji Zhou, Yue Zhang, Donghong Ji, Hao Tang"}, {"title": "An analysis of the utility of explicit negative examples to improve the syntactic abilities of neural language models", "authors": "Hiroshi Noji, Hiroya Takamura", "link": "https://arxiv.org/abs/2004.02451", "summary": "We explore the utilities of explicit negative examples in training neural\nlanguage models. Negative examples here are incorrect words in a sentence, such\nas \"barks\" in \"*The dogs barks\". Neural language models are commonly trained\nonly on positive examples, a set of sentences in the training data, but recent\nstudies suggest that the models trained in this way are not capable of robustly\nhandling complex syntactic constructions, such as long-distance agreement. In\nthis paper, using English data, we first demonstrate that appropriately using\nnegative examples about particular constructions (e.g., subject-verb agreement)\nwill boost the model's robustness on them, with a negligible loss of\nperplexity. The key to our success is an additional margin loss between the\nlog-likelihoods of a correct word and an incorrect word. We then provide a\ndetailed analysis of the trained models. One of our findings is the difficulty\nof object-relative clauses for RNNs. We find that even with our direct learning\nsignals the models still suffer from resolving agreement across an\nobject-relative clause. Augmentation of training sentences involving the\nconstructions somewhat helps, but the accuracy still does not reach the level\nof subject-relative clauses. Although not directly cognitively appealing, our\nmethod can be a tool to analyze the true architectural limitation of neural\nmodels on challenging linguistic constructions."}, {"title": "An Effective Transition-based Model for Discontinuous NER", "authors": "Xiang Dai, Sarvnaz Karimi, Ben Hachey, Cecile Paris", "link": "https://arxiv.org/abs/2004.13454", "summary": "Unlike widely used Named Entity Recognition (NER) data sets in generic\ndomains, biomedical NER data sets often contain mentions consisting of\ndiscontinuous spans. Conventional sequence tagging techniques encode Markov\nassumptions that are efficient but preclude recovery of these mentions. We\npropose a simple, effective transition-based model with generic neural encoding\nfor discontinuous NER. Through extensive experiments on three biomedical data\nsets, we show that our model can effectively recognize discontinuous mentions\nwithout sacrificing the accuracy on continuous mentions."}, {"title": "An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results", "authors": "Enrique Amigo, Julio Gonzalo, Stefano Mizzaro, Jorge Carrillo-de-Albornoz", "link": "https://arxiv.org/abs/2006.01245", "summary": "In Ordinal Classification tasks, items have to be assigned to classes that\nhave a relative ordering, such as positive, neutral, negative in sentiment\nanalysis. Remarkably, the most popular evaluation metrics for ordinal\nclassification tasks either ignore relevant information (for instance,\nprecision/recall on each of the classes ignores their relative ordering) or\nassume additional information (for instance, Mean Average Error assumes\nabsolute distances between classes). In this paper we propose a new metric for\nOrdinal Classification, Closeness Evaluation Measure, that is rooted on\nMeasurement Theory and Information Theory. Our theoretical analysis and\nexperimental results over both synthetic data and data from NLP shared tasks\nindicate that the proposed metric captures quality aspects from different\ntraditional tasks simultaneously. In addition, it generalizes some popular\nclassification (nominal scale) and error minimization (interval scale) metrics,\ndepending on the measurement scale in which it is instantiated."}, {"title": "An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering", "authors": "Jay Kumar, Junming Shao, Salah Uddin, Wazir Ali"}, {"title": "Analysing Lexical Semantic Change with Contextualised Word Representations", "authors": "Mario Giulianelli, Marco Del Tredici, Raquel Fern\u00e1ndez"}, {"title": "Analyzing analytical methods: The case of phonology in neural models of spoken language", "authors": "Grzegorz Chrupa\u0142a, Bertrand Higy, Afra Alishahi", "link": "https://arxiv.org/abs/2004.07070", "summary": "Given the fast development of analysis techniques for NLP and speech\nprocessing systems, few systematic studies have been conducted to compare the\nstrengths and weaknesses of each method. As a step in this direction we study\nthe case of representations of phonology in neural network models of spoken\nlanguage. We use two commonly applied analytical techniques, diagnostic\nclassifiers and representational similarity analysis, to quantify to what\nextent neural activation patterns encode phonemes and phoneme sequences. We\nmanipulate two factors that can affect the outcome of analysis. First, we\ninvestigate the role of learning by comparing neural activations extracted from\ntrained versus randomly-initialized models. Second, we examine the temporal\nscope of the activations by probing both local activations corresponding to a\nfew milliseconds of the speech signal, and global activations pooled over the\nwhole utterance. We conclude that reporting analysis results with randomly\ninitialized models is crucial, and that global-scope methods tend to yield more\nconsistent results and we recommend their use as a complement to local-scope\ndiagnostic methods."}, {"title": "Analyzing Political Parody in Social Media", "authors": "Antonios Maronikolakis, Danae S\u00e1nchez Villegas, Daniel Preotiuc-Pietro, Nikolaos Aletras", "link": "https://arxiv.org/abs/2004.13878", "summary": "Parody is a figurative device used to imitate an entity for comedic or\ncritical purposes and represents a widespread phenomenon in social media\nthrough many popular parody accounts. In this paper, we present the first\ncomputational study of parody. We introduce a new publicly available data set\nof tweets from real politicians and their corresponding parody accounts. We run\na battery of supervised machine learning models for automatically detecting\nparody tweets with an emphasis on robustness by testing on tweets from accounts\nunseen in training, across different genders and across countries. Our results\nshow that political parody tweets can be predicted with an accuracy up to 90%.\nFinally, we identify the markers of parody through a linguistic analysis.\nBeyond research in linguistics and political communication, accurately and\nautomatically detecting parody is important to improving fact checking for\njournalists and analytics such as sentiment analysis through filtering out\nparodical utterances."}, {"title": "Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition", "authors": "Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, Adina Williams", "link": "", "summary": ""}, {"title": "Asking and Answering Questions to Evaluate the Factual Consistency of Summaries", "authors": "Alex Wang, Kyunghyun Cho, Mike Lewis", "link": "https://arxiv.org/abs/2004.04228", "summary": "Practical applications of abstractive summarization models are limited by\nfrequent factual inconsistencies with respect to their input. Existing\nautomatic evaluation metrics for summarization are largely insensitive to such\nerrors. We propose an automatic evaluation protocol called QAGS (pronounced\n\"kags\") that is designed to identify factual inconsistencies in a generated\nsummary. QAGS is based on the intuition that if we ask questions about a\nsummary and its source, we will receive similar answers if the summary is\nfactually consistent with the source. To evaluate QAGS, we collect human\njudgments of factual consistency on model-generated summaries for the\nCNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018)\nsummarization datasets. QAGS has substantially higher correlations with these\njudgments than other automatic evaluation metrics. Also, QAGS offers a natural\nform of interpretability: The answers and questions generated while computing\nQAGS indicate which tokens of a summary are inconsistent and why. We believe\nQAGS is a promising tool in automatically generating usable and factually\nconsistent text."}, {"title": "Aspect Sentiment Classification with Document-level Sentiment Preference Modeling", "authors": "Xiao Chen, Changlong Sun, Jingjing Wang, Shoushan Li, Luo Si, Min Zhang, Guodong Zhou"}, {"title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations", "authors": "Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno\u00eet Sagot, Lucia Specia", "link": "https://arxiv.org/abs/2005.00481", "summary": "In order to simplify a sentence, human editors perform multiple rewriting\ntransformations: they split it into several shorter sentences, paraphrase words\n(i.e. replacing complex words or phrases by simpler synonyms), reorder\ncomponents, and/or delete information deemed unnecessary. Despite these varied\nrange of possible text alterations, current models for automatic sentence\nsimplification are evaluated using datasets that are focused on a single\ntransformation, such as lexical paraphrasing or splitting. This makes it\nimpossible to understand the ability of simplification models in more realistic\nsettings. To alleviate this limitation, this paper introduces ASSET, a new\ndataset for assessing sentence simplification in English. ASSET is a\ncrowdsourced multi-reference corpus where each simplification was produced by\nexecuting several rewriting transformations. Through quantitative and\nqualitative experiments, we show that simplifications in ASSET are better at\ncapturing characteristics of simplicity when compared to other standard\nevaluation datasets for the task. Furthermore, we motivate the need for\ndeveloping better methods for automatic evaluation using ASSET, since we show\nthat current popular metrics may not be suitable when multiple simplification\ntransformations are performed."}, {"title": "Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization", "authors": "Junnan Zhu, Yu Zhou, Jiajun Zhang, Chengqing Zong"}, {"title": "Attentive Pooling with Learnable Norms for Text Representation", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui, Yongfeng Huang", "link": "", "summary": ""}, {"title": "Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics", "authors": "Guy Emerson", "link": "https://arxiv.org/abs/2005.02991", "summary": "Functional Distributional Semantics provides a linguistically interpretable\nframework for distributional semantics, by representing the meaning of a word\nas a function (a binary classifier), instead of a vector. However, the large\nnumber of latent variables means that inference is computationally expensive,\nand training a model is therefore slow to converge. In this paper, I introduce\nthe Pixie Autoencoder, which augments the generative model of Functional\nDistributional Semantics with a graph-convolutional neural network to perform\namortised variational inference. This allows the model to be trained more\neffectively, achieving better results on two tasks (semantic similarity in\ncontext and semantic composition), and outperforming BERT, a large pre-trained\nlanguage model."}, {"title": "Automated Evaluation of Writing \u2013 50 Years and Counting", "authors": "Beata Beigman Klebanov, Nitin Madnani"}, {"title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "authors": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck", "link": "https://arxiv.org/abs/1911.00650", "summary": "Recent advancements in neural language modelling make it possible to rapidly\ngenerate vast amounts of human-sounding text. The capabilities of humans and\nautomatic discriminators to detect machine-generated text have been a large\nsource of research interest, but humans and machines rely on different cues to\nmake their decisions. Here, we perform careful benchmarking and analysis of\nthree popular sampling-based decoding strategies---top-$k$, nucleus sampling,\nand untruncated random sampling---and show that improvements in decoding\nmethods have primarily optimized for fooling humans. This comes at the expense\nof introducing statistical abnormalities that make detection easy for automatic\nsystems. We also show that though both human and automatic detector performance\nimprove with longer excerpt length, even multi-sentence excerpts can fool\nexpert human raters over 30% of the time. Our findings reveal the importance of\nusing both human and automatic detectors to assess the humanness of text\ngeneration systems."}, {"title": "Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study", "authors": "Xinyu Xing, Xiaosheng Fan, Xiaojun Wan"}, {"title": "Automatic Poetry Generation from Prosaic Text", "authors": "Tim Van de Cruys"}, {"title": "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps", "authors": "Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, Fei Sha", "link": "https://arxiv.org/abs/2005.04625", "summary": "Learning to follow instructions is of fundamental importance to autonomous\nagents for vision-and-language navigation (VLN). In this paper, we study how an\nagent can navigate long paths when learning from a corpus that consists of\nshorter ones. We show that existing state-of-the-art agents do not generalize\nwell. To this end, we propose BabyWalk, a new VLN agent that is learned to\nnavigate by decomposing long instructions into shorter ones (BabySteps) and\ncompleting them sequentially. A special design memory buffer is used by the\nagent to turn its past experiences into contexts for future steps. The learning\nprocess is composed of two phases. In the first phase, the agent uses imitation\nlearning from demonstration to accomplish BabySteps. In the second phase, the\nagent uses curriculum-based reinforcement learning to maximize rewards on\nnavigation tasks with increasingly longer instructions. We create two new\nbenchmark datasets (of long navigation tasks) and use them in conjunction with\nexisting ones to examine BabyWalk's generalization ability. Empirical results\nshow that BabyWalk achieves state-of-the-art results on several metrics, in\nparticular, is able to follow long instructions better. The codes and the\ndatasets are released on our project page https://github.com/Sha-Lab/babywalk."}, {"title": "Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards", "authors": "Justine Zhang, Cristian Danescu-Niculescu-Mizil", "link": "http://arxiv.org/abs/2005.04245", "summary": "Throughout a conversation, participants make choices that can orient the flow\nof the interaction. Such choices are particularly salient in the consequential\ndomain of crisis counseling, where a difficulty for counselors is balancing\nbetween two key objectives: advancing the conversation towards a resolution,\nand empathetically addressing the crisis situation.\n  In this work, we develop an unsupervised methodology to quantify how\ncounselors manage this balance. Our main intuition is that if an utterance can\nonly receive a narrow range of appropriate replies, then its likely aim is to\nadvance the conversation forwards, towards a target within that range.\nLikewise, an utterance that can only appropriately follow a narrow range of\npossible utterances is likely aimed backwards at addressing a specific\nsituation within that range. By applying this intuition, we can map each\nutterance to a continuous orientation axis that captures the degree to which it\nis intended to direct the flow of the conversation forwards or backwards.\n  This unsupervised method allows us to characterize counselor behaviors in a\nlarge dataset of crisis counseling conversations, where we show that known\ncounseling strategies intuitively align with this axis. We also illustrate how\nour measure can be indicative of a conversation's progress, as well as its\neffectiveness."}, {"title": "Balancing Training for Multilingual Neural Machine Translation", "authors": "Xinyi Wang, Yulia Tsvetkov, Graham Neubig", "link": "https://arxiv.org/abs/2004.06748", "summary": "When training multilingual machine translation (MT) models that can translate\nto/from multiple languages, we are faced with imbalanced training sets: some\nlanguages have much more training data than others. Standard practice is to\nup-sample less resourced languages to increase representation, and the degree\nof up-sampling has a large effect on the overall performance. In this paper, we\npropose a method that instead automatically learns how to weight training data\nthrough a data scorer that is optimized to maximize performance on all test\nlanguages. Experiments on two sets of languages under both one-to-many and\nmany-to-one MT settings show our method not only consistently outperforms\nheuristic baselines in terms of average performance, but also offers flexible\ncontrol over the performance of which languages are optimized."}, {"title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer", "link": "https://arxiv.org/abs/1910.13461", "summary": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance."}, {"title": "Benchmarking Multimodal Regex Synthesis with Complex Structures", "authors": "Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett", "link": "https://arxiv.org/abs/2005.00663", "summary": "Existing datasets for regular expression (regex) generation from natural\nlanguage are limited in complexity; compared to regex tasks that users post on\nStackOverflow, the regexes in these datasets are simple, and the language used\nto describe them is not diverse. We introduce StructuredRegex, a new regex\nsynthesis dataset differing from prior ones in three aspects. First, to obtain\nstructurally complex and realistic regexes, we generate the regexes using a\nprobabilistic grammar with pre-defined macros observed from real-world\nStackOverflow posts. Second, to obtain linguistically diverse natural language\ndescriptions, we show crowdworkers abstract depictions of the underlying regex\nand ask them to describe the pattern they see, rather than having them\nparaphrase synthetic language. Third, we augment each regex example with a\ncollection of strings that are and are not matched by the ground truth regex,\nsimilar to how real users give examples. Our quantitative and qualitative\nanalysis demonstrates the advantages of StructuredRegex over prior datasets.\nFurther experimental results using various multimodal synthesis techniques\nhighlight the challenge presented by our dataset, including non-local\nconstraints and multi-modal inputs."}, {"title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance", "authors": "Timo Schick, Hinrich Sch\u00fctze", "link": "https://arxiv.org/abs/1910.07181", "summary": "Pretraining deep language models has led to large performance gains in NLP.\nDespite this success, Schick and Sch\\\"utze (2020) recently showed that these\nmodels struggle to understand rare words. For static word embeddings, this\nproblem has been addressed by separately learning representations for rare\nwords. In this work, we transfer this idea to pretrained language models: We\nintroduce BERTRAM, a powerful architecture based on BERT that is capable of\ninferring high-quality embeddings for rare words that are suitable as input\nrepresentations for deep language models. This is achieved by enabling the\nsurface form and contexts of a word to interact with each other in a deep\narchitecture. Integrating BERTRAM into BERT leads to large performance\nincreases due to improved representations of rare and medium frequency words on\nboth a rare word probing task and three downstream tasks."}, {"title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList", "authors": "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh", "link": "http://arxiv.org/abs/2005.04118", "summary": "Although measuring held-out accuracy has been the primary approach to\nevaluate generalization, it often overestimates the performance of NLP models,\nwhile alternative approaches for evaluating models either focus on individual\ntasks or on specific behaviors. Inspired by principles of behavioral testing in\nsoftware engineering, we introduce CheckList, a task-agnostic methodology for\ntesting NLP models. CheckList includes a matrix of general linguistic\ncapabilities and test types that facilitate comprehensive test ideation, as\nwell as a software tool to generate a large and diverse number of test cases\nquickly. We illustrate the utility of CheckList with tests for three tasks,\nidentifying critical failures in both commercial and state-of-art models. In a\nuser study, a team responsible for a commercial sentiment analysis model found\nnew and actionable bugs in an extensively tested model. In another user study,\nNLP practitioners with CheckList created twice as many tests, and found almost\nthree times as many bugs as users without it."}, {"title": "Beyond Possession Existence: Duration and Co-Possession", "authors": "Dhivya Chinnappa, Srikala Murugan, Eduardo Blanco"}, {"title": "Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation", "authors": "Weixin Liang, James Zou, Zhou Yu", "link": "https://arxiv.org/abs/2005.10716", "summary": "Open Domain dialog system evaluation is one of the most important challenges\nin dialog research. Existing automatic evaluation metrics, such as BLEU are\nmostly reference-based. They calculate the difference between the generated\nresponse and a limited number of available references. Likert-score based\nself-reported user rating is widely adopted by social conversational systems,\nsuch as Amazon Alexa Prize chatbots. However, self-reported user rating suffers\nfrom bias and variance among different users. To alleviate this problem, we\nformulate dialog evaluation as a comparison task. We also propose an automatic\nevaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that\nautomatically cleans self-reported user ratings as it trains on them.\nSpecifically, we first use a self-supervised method to learn better dialog\nfeature representation, and then use KNN and Shapley to remove confusing\nsamples. Our experiments show that CMADE achieves 89.2% accuracy in the dialog\ncomparison task."}, {"title": "Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences", "authors": "Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min Zhang, Boxing Chen, Weihua Luo, Yue Zhang", "link": "", "summary": ""}, {"title": "Biomedical Entity Representations with Synonym Marginalization", "authors": "Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, Jaewoo Kang", "link": "https://arxiv.org/abs/2005.00239", "summary": "Biomedical named entities often play important roles in many biomedical text\nmining tools. However, due to the incompleteness of provided synonyms and\nnumerous variations in their surface forms, normalization of biomedical\nentities is very challenging. In this paper, we focus on learning\nrepresentations of biomedical entities solely based on the synonyms of\nentities. To learn from the incomplete synonyms, we use a model-based candidate\nselection and maximize the marginal likelihood of the synonyms present in top\ncandidates. Our model-based candidates are iteratively updated to contain more\ndifficult negative samples as our model evolves. In this way, we avoid the\nexplicit pre-selection of negative samples from more than 400K candidates. On\nfour biomedical entity normalization datasets having three different entity\ntypes (disease, chemical, adverse reaction), our model BioSyn consistently\noutperforms previous state-of-the-art models almost reaching the upper bound on\neach dataset."}, {"title": "Bipartite Flat-Graph Network for Nested Named Entity Recognition", "authors": "Ying Luo, Hai Zhao", "link": "https://arxiv.org/abs/2005.00436", "summary": "In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for\nnested named entity recognition (NER), which contains two subgraph modules: a\nflat NER module for outermost entities and a graph module for all the entities\nlocated in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional\nnetwork (GCN) are adopted to jointly learn flat entities and their inner\ndependencies. Different from previous models, which only consider the\nunidirectional delivery of information from innermost layers to outer ones (or\noutside-to-inside), our model effectively captures the bidirectional\ninteraction between them. We first use the entities recognized by the flat NER\nmodule to construct an entity graph, which is fed to the next graph module. The\nricher representation learned from graph module carries the dependencies of\ninner entities and can be exploited to improve outermost entity predictions.\nExperimental results on three standard nested NER datasets demonstrate that our\nBiFlaG outperforms previous state-of-the-art models."}, {"title": "BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection", "authors": "Chengyu Wang, Xiaofeng He"}, {"title": "BLEURT: Learning Robust Metrics for Text Generation", "authors": "Thibault Sellam, Dipanjan Das, Ankur Parikh", "link": "", "summary": ""}, {"title": "Boosting Neural Machine Translation with Similar Translations", "authors": "Jitao XU, Josep Crego, Jean Senellart", "link": "", "summary": ""}, {"title": "Bootstrapping Techniques for Polysynthetic Morphological Analysis", "authors": "William Lane, Steven Bird", "link": "https://arxiv.org/abs/2005.00956", "summary": "Polysynthetic languages have exceptionally large and sparse vocabularies,\nthanks to the number of morpheme slots and combinations in a word. This\ncomplexity, together with a general scarcity of written data, poses a challenge\nto the development of natural language technologies. To address this challenge,\nwe offer linguistically-informed approaches for bootstrapping a neural\nmorphological analyzer, and demonstrate its application to Kunwinjku, a\npolysynthetic Australian language. We generate data from a finite state\ntransducer to train an encoder-decoder model. We improve the model by\n\"hallucinating\" missing linguistic structure into the training data, and by\nresampling from a Zipf distribution to simulate a more natural distribution of\nmorphemes. The best model accounts for all instances of reduplication in the\ntest set and achieves an accuracy of 94.7% overall, a 10 percentage point\nimprovement over the FST baseline. This process demonstrates the feasibility of\nbootstrapping a neural morph analyzer from minimal resources."}, {"title": "BPE-Dropout: Simple and Effective Subword Regularization", "authors": "Ivan Provilkov, Dmitrii Emelianenko, Elena Voita", "link": "https://arxiv.org/abs/1910.13267", "summary": "Subword segmentation is widely used to address the open vocabulary problem in\nmachine translation. The dominant approach to subword segmentation is Byte Pair\nEncoding (BPE), which keeps the most frequent words intact while splitting the\nrare ones into multiple tokens. While multiple segmentations are possible even\nwith the same vocabulary, BPE splits words into unique sequences; this may\nprevent a model from better learning the compositionality of words and being\nrobust to segmentation errors. So far, the only way to overcome this BPE\nimperfection, its deterministic nature, was to create another subword\nsegmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself\nincorporates the ability to produce multiple segmentations of the same word. We\nintroduce BPE-dropout - simple and effective subword regularization method\nbased on and compatible with conventional BPE. It stochastically corrupts the\nsegmentation procedure of BPE, which leads to producing multiple segmentations\nwithin the same fixed BPE framework. Using BPE-dropout during training and the\nstandard BPE during inference improves translation quality up to 3 BLEU\ncompared to BPE and up to 0.9 BLEU compared to the previous subword\nregularization."}, {"title": "Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information", "authors": "Michele Bevilacqua, Roberto Navigli"}, {"title": "Bridging Anaphora Resolution as Question Answering", "authors": "Yufang Hou", "link": "https://arxiv.org/abs/2004.07898", "summary": "Most previous studies on bridging anaphora resolution (Poesio et al., 2004;\nHou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and\nassume that the gold mention information is given. In this paper, we cast\nbridging anaphora resolution as question answering based on context. This\nallows us to find the antecedent for a given anaphor without knowing any gold\nmention information (except the anaphor itself). We present a question\nanswering framework (BARQA) for this task, which leverages the power of\ntransfer learning. Furthermore, we propose a novel method to generate a large\namount of \"quasi-bridging\" training data. We show that our model pre-trained on\nthis dataset and fine-tuned on a small amount of in-domain dataset achieves new\nstate-of-the-art results for bridging anaphora resolution on two bridging\ncorpora (ISNotes (Markert et al., 2012) and BASHI (Roesiger, 2018))."}, {"title": "Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation", "authors": "Chao Zhao, Marilyn Walker, Snigdha Chaturvedi"}, {"title": "Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell", "authors": "Djam\u00e9 Seddah, Farah Essaidi, Amal Fethi, Matthieu Futeral, Benjamin Muller, Pedro Javier Ortiz Su\u00e1rez, Beno\u00eet Sagot, Abhishek Srivastava"}, {"title": "Calibrating Structured Output Predictors for Natural Language Processing", "authors": "Abhyuday Jagannatha, Hong Yu", "link": "https://arxiv.org/abs/2004.04361", "summary": "We address the problem of calibrating prediction confidence for output\nentities of interest in natural language processing (NLP) applications. It is\nimportant that NLP applications such as named entity recognition and question\nanswering produce calibrated confidence scores for their predictions,\nespecially if the system is to be deployed in a safety-critical domain such as\nhealthcare. However, the output space of such structured prediction models is\noften too large to adapt binary or multi-class calibration methods directly. In\nthis study, we propose a general calibration scheme for output entities of\ninterest in neural-network based structured prediction models. Our proposed\nmethod can be used with any binary class calibration scheme and a neural\nnetwork model. Additionally, we show that our calibration method can also be\nused as an uncertainty-aware, entity-specific decoding step to improve the\nperformance of the underlying model at no additional training cost or data\nrequirements. We show that our method outperforms current calibration\ntechniques for named-entity-recognition, part-of-speech and question answering.\nWe also improve our model's performance from our decoding step across several\ntasks and benchmark datasets. Our method improves the calibration and model\nperformance on out-of-domain test scenarios as well."}, {"title": "CamemBERT: a Tasty French Language Model", "authors": "Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\u00e1rez, Yoann Dupont, Laurent Romary, \u00c9ric de la Clergerie, Djam\u00e9 Seddah, Beno\u00eet Sagot", "link": "", "summary": ""}, {"title": "Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction", "authors": "Samuel Broscheit, Kiril Gashteovski, Yanjie Wang, Rainer Gemulla"}, {"title": "Can You Put it All Together: Evaluating Conversational Agents\u2019 Ability to Blend Skills", "authors": "Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-Lan Boureau", "link": "https://arxiv.org/abs/2004.08449", "summary": "Being engaging, knowledgeable, and empathetic are all desirable general\nqualities in a conversational agent. Previous work has introduced tasks and\ndatasets that aim to help agents to learn those qualities in isolation and\ngauge how well they can express them. But rather than being specialized in one\nsingle quality, a good open-domain conversational agent should be able to\nseamlessly blend them all into one cohesive conversational flow. In this work,\nwe investigate several ways to combine models trained towards isolated\ncapabilities, ranging from simple model aggregation schemes that require\nminimal additional training, to various forms of multi-task training that\nencompass several skills at all training stages. We further propose a new\ndataset, BlendedSkillTalk, to analyze how these capabilities would mesh\ntogether in a natural conversation, and compare the performance of different\narchitectures and training schemes. Our experiments show that multi-tasking\nover several tasks that focus on particular capabilities results in better\nblended conversation performance compared to models trained on a single skill,\nand that both unified or two-stage approaches perform well if they are\nconstructed to avoid unwanted bias in skill selection or are fine-tuned on our\nnew task."}, {"title": "CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation", "authors": "Lei Shen, Yang Feng", "link": "http://arxiv.org/abs/2005.00329", "summary": "Emotion-controllable response generation is an attractive and valuable task\nthat aims to make open-domain conversations more empathetic and engaging.\nExisting methods mainly enhance the emotion expression by adding regularization\nterms to standard cross-entropy loss and thus influence the training process.\nHowever, due to the lack of further consideration of content consistency, the\ncommon problem of response generation tasks, safe response, is intensified.\nBesides, query emotions that can help model the relationship between query and\nresponse are simply ignored in previous models, which would further hurt the\ncoherence. To alleviate these problems, we propose a novel framework named\nCurriculum Dual Learning (CDL) which extends the emotion-controllable response\ngeneration to a dual task to generate emotional responses and emotional queries\nalternatively. CDL utilizes two rewards focusing on emotion and content to\nimprove the duality. Additionally, it applies curriculum learning to gradually\ngenerate high-quality responses based on the difficulties of expressing various\nemotions. Experimental results show that CDL significantly outperforms the\nbaselines in terms of coherence, diversity, and relation to emotion factors."}, {"title": "ChartDialogs: Plotting from Natural Language Instructions", "authors": "Yutong Shao, Ndapa Nakashole"}, {"title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality", "authors": "Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, Kaicheng Yang"}, {"title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data", "authors": "Emily M. Bender, Alexander Koller"}, {"title": "Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset", "authors": "Xiang Yue, Bernal Jimenez Gutierrez, Huan Sun", "link": "https://arxiv.org/abs/2005.00574", "summary": "Machine reading comprehension has made great progress in recent years owing\nto large-scale annotated datasets. In the clinical domain, however, creating\nsuch datasets is quite difficult due to the domain expertise required for\nannotation. Recently, Pampari et al. (EMNLP'18) tackled this issue by using\nexpert-annotated question templates and existing i2b2 annotations to create\nemrQA, the first large-scale dataset for question answering (QA) based on\nclinical notes. In this paper, we provide an in-depth analysis of this dataset\nand the clinical reading comprehension (CliniRC) task. From our qualitative\nanalysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA\nquestions are often answerable without using domain knowledge. From our\nquantitative experiments, surprising results include that (iii) using a small\nsampled subset (5%-20%), we can obtain roughly equal performance compared to\nthe model trained on the entire dataset, (iv) this performance is close to\nhuman expert's performance, and (v) BERT models do not beat the best performing\nbase model. Following our analysis of the emrQA, we further explore two desired\naspects of CliniRC systems: the ability to utilize clinical domain knowledge\nand to generalize to unseen questions and contexts. We argue that both should\nbe considered when creating future datasets."}, {"title": "CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages", "authors": "Tommaso Pasini, Federico Scozzafava, Bianca Scarlini"}, {"title": "CluHTM - Semantic Hierarchical Topic Modeling based on CluWords", "authors": "Felipe Viegas, Washington Cunha, Christian Gomes, Ant\u00f4nio Pereira, Leonardo Rocha, Marcos Goncalves"}, {"title": "Code and Named Entity Recognition in StackOverflow", "authors": "Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter", "link": "https://arxiv.org/abs/2005.01634", "summary": "There is an increasing interest in studying natural language and computer\ncode together, as large corpora of programming texts become readily available\non the Internet. For example, StackOverflow currently has over 15 million\nprogramming related questions written by 8.5 million users. Meanwhile, there is\nstill a lack of fundamental NLP techniques for identifying code tokens or\nsoftware-related named entities that appear within natural language sentences.\nIn this paper, we introduce a new named entity recognition (NER) corpus for the\ncomputer programming domain, consisting of 15,372 sentences annotated with 20\nfine-grained entity types. We also present the SoftNER model that combines\ncontextual information with domain specific knowledge using an attention\nnetwork. The code token recognizer combined with an entity segmentation model\nwe proposed, consistently improves the performance of the named entity tagger.\nOur proposed SoftNER tagger outperforms the BiLSTM-CRF model with an absolute\nincrease of +9.73 F-1 score on StackOverflow data."}, {"title": "CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning", "authors": "Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, Oliver Lemon", "link": "https://arxiv.org/abs/2006.02174", "summary": "Approaches to Grounded Language Learning typically focus on a single\ntask-based final performance measure that may not depend on desirable\nproperties of the learned hidden representations, such as their ability to\npredict salient attributes or to generalise to unseen situations. To remedy\nthis, we present GROLLA, an evaluation framework for Grounded Language Learning\nwith Attributes with three sub-tasks: 1) Goal-oriented evaluation; 2) Object\nattribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a\nnew dataset CompGuessWhat?! as an instance of this framework for evaluating the\nquality of learned neural representations, in particular concerning attribute\ngrounding. To this end, we extend the original GuessWhat?! dataset by including\na semantic layer on top of the perceptual one. Specifically, we enrich the\nVisualGenome scene graphs associated with the GuessWhat?! images with abstract\nand situated attributes. By using diagnostic classifiers, we show that current\nmodels learn representations that are not expressive enough to encode object\nattributes (average F1 of 44.27). In addition, they do not learn strategies nor\nrepresentations that are robust enough to perform well when novel scenes or\nobjects are involved in gameplay (zero-shot best accuracy 50.06%)."}, {"title": "Compositionality and Generalization In Emergent Languages", "authors": "Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, Marco Baroni", "link": "https://arxiv.org/abs/2004.09124", "summary": "Natural language allows us to refer to novel composite concepts by combining\nexpressions denoting their parts according to systematic rules, a property\nknown as \\emph{compositionality}. In this paper, we study whether the language\nemerging in deep multi-agent simulations possesses a similar ability to refer\nto novel primitive combinations, and whether it accomplishes this feat by\nstrategies akin to human-language compositionality. Equipped with new ways to\nmeasure compositionality in emergent languages inspired by disentanglement in\nrepresentation learning, we establish three main results. First, given\nsufficiently large input spaces, the emergent language will naturally develop\nthe ability to refer to novel composite concepts. Second, there is no\ncorrelation between the degree of compositionality of an emergent language and\nits ability to generalize. Third, while compositionality is not necessary for\ngeneralization, it provides an advantage in terms of language transmission: The\nmore compositional a language is, the more easily it will be picked up by new\nlearners, even when the latter differ in architecture from the original agents.\nWe conclude that compositionality does not arise from simple generalization\npressure, but if an emergent language does chance upon it, it will be more\nlikely to survive and thrive."}, {"title": "Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation", "authors": "Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling, Yan Song", "link": "https://arxiv.org/abs/2004.14769", "summary": "Aspect term extraction aims to extract aspect terms from review texts as\nopinion targets for sentiment analysis. One of the big challenges with this\ntask is the lack of sufficient annotated data. While data augmentation is\npotentially an effective technique to address the above issue, it is\nuncontrollable as it may change aspect words and aspect labels unexpectedly. In\nthis paper, we formulate the data augmentation as a conditional generation\ntask: generating a new sentence while preserving the original opinion targets\nand labels. We propose a masked sequence-to-sequence method for conditional\naugmentation of aspect term extraction. Unlike existing augmentation\napproaches, ours is controllable and allows us to generate more diversified\nsentences. Experimental results confirm that our method alleviates the data\nscarcity problem significantly. It also effectively boosts the performances of\nseveral current models for aspect term extraction."}, {"title": "Connecting Embeddings for Knowledge Graph Entity Typing", "authors": "Yu Zhao, anxiang zhang, Ruobing Xie, Kang Liu, Xiaojie WANG"}, {"title": "Contextualized Weak Supervision for Text Classification", "authors": "Dheeraj Mekala, Jingbo Shang", "link": "", "summary": ""}, {"title": "Continual Relation Learning via Episodic Memory Activation and Reconsolidation", "authors": "Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou"}, {"title": "Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation", "authors": "Jun Xu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu"}, {"title": "CorefQA: Coreference Resolution as Query-based Span Prediction", "authors": "Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, Jiwei Li", "link": "https://arxiv.org/abs/1911.01746", "summary": "In this paper, we present an accurate and extensible approach for the\ncoreference resolution task. We formulate the problem as a span prediction\ntask, like in machine reading comprehension (MRC): A query is generated for\neach candidate mention using its surrounding context, and a span prediction\nmodule is employed to extract the text spans of the coreferences within the\ndocument using the generated query. This formulation comes with the following\nkey advantages: (1) The span prediction strategy provides the flexibility of\nretrieving mentions left out at the mention proposal stage; (2) In the MRC\nframework, encoding the mention and its context explicitly in a query makes it\npossible to have a deep and thorough examination of cues embedded in the\ncontext of coreferent mentions; and (3) A plethora of existing MRC datasets can\nbe used for data augmentation to improve the model's generalization capability.\nExperiments demonstrate significant performance boost over previous models,\nwith 87.5 (+2.5) F1 score on the GAP benchmark and 83.1 (+3.5) F1 score on the\nCoNLL-2012 benchmark."}, {"title": "Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation", "authors": "Ning Ding, Dingkun Long, Guangwei Xu, Muhua Zhu, Pengjun Xie, Xiaobin Wang, Haitao Zheng"}, {"title": "CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant", "authors": "Kavya Srinet, Yacine Jernite, Jonathan Gray, Arthur Szlam", "link": "https://arxiv.org/abs/1905.01978", "summary": "We propose a large scale semantic parsing dataset focused on\ninstruction-driven communication with an agent in Minecraft. We describe the\ndata collection process which yields additional 35K human generated\ninstructions with their semantic annotations. We report the performance of\nthree baseline models and find that while a dataset of this size helps us train\na usable instruction parser, it still poses interesting generalization\nchallenges which we hope will help develop better and more robust models."}, {"title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus", "authors": "Hao Fei, Meishan Zhang, Donghong Ji", "link": "https://arxiv.org/abs/2004.06295", "summary": "Many efforts of research are devoted to semantic role labeling (SRL) which is\ncrucial for natural language understanding. Supervised approaches have achieved\nimpressing performances when large-scale corpora are available for\nresource-rich languages such as English. While for the low-resource languages\nwith no annotated SRL dataset, it is still challenging to obtain competitive\nperformances. Cross-lingual SRL is one promising way to address the problem,\nwhich has achieved great advances with the help of model transferring and\nannotation projection. In this paper, we propose a novel alternative based on\ncorpus translation, constructing high-quality training datasets for the target\nlanguages from the source gold-standard SRL annotations. Experimental results\non Universal Proposition Bank show that the translation-based method is highly\neffective, and the automatic pseudo datasets can improve the target-language\nSRL performances significantly."}, {"title": "Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning", "authors": "Hongliang Fei, Ping Li", "link": "", "summary": ""}, {"title": "Cross-Linguistic Syntactic Evaluation of Word Prediction Models", "authors": "Aaron Mueller, Garrett Nicolai, Panayiota Petrou-Zeniou, Natalia Talmina, Tal Linzen", "link": "http://arxiv.org/abs/2005.00187", "summary": "A range of studies have concluded that neural word prediction models can\ndistinguish grammatical from ungrammatical sentences with high accuracy.\nHowever, these studies are based primarily on monolingual evidence from\nEnglish. To investigate how these models' ability to learn syntax varies by\nlanguage, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax),\na syntactic evaluation suite for monolingual and multilingual models. CLAMS\nincludes subject-verb agreement challenge sets for English, French, German,\nHebrew and Russian, generated from grammars we develop. We use CLAMS to\nevaluate LSTM language models as well as monolingual and multilingual BERT.\nAcross languages, monolingual LSTMs achieved high accuracy on dependencies\nwithout attractors, and generally poor accuracy on agreement across object\nrelative clauses. On other constructions, agreement accuracy was generally\nhigher in languages with richer morphology. Multilingual models generally\nunderperformed monolingual models. Multilingual BERT showed high syntactic\naccuracy on English, but noticeable deficiencies in other languages."}, {"title": "Cross-media Structured Common Space for Multimedia Event Extraction", "authors": "Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, Shih-Fu Chang", "link": "http://arxiv.org/abs/2005.02472", "summary": "We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to\nextract events and their arguments from multimedia documents. We develop the\nfirst benchmark and collect a dataset of 245 multimedia news articles with\nextensively annotated events and arguments. We propose a novel method, Weakly\nAligned Structured Embedding (WASE), that encodes structured representations of\nsemantic information from textual and visual data into a common embedding\nspace. The structures are aligned across modalities by employing a weakly\nsupervised training strategy, which enables exploiting available resources\nwithout explicit cross-media annotation. Compared to uni-modal state-of-the-art\nmethods, our approach achieves 4.0% and 9.8% absolute F-score gains on text\nevent argument role labeling and visual event extraction. Compared to\nstate-of-the-art multimedia unstructured representations, we achieve 8.3% and\n5.0% absolute F-score gains on multimedia event extraction and argument role\nlabeling, respectively. By utilizing images, we extract 21.4% more event\nmentions than traditional text-only methods."}, {"title": "Cross-modal Coherence Modeling for Caption Generation", "authors": "Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut, Matthew Stone", "link": "http://arxiv.org/abs/2005.00908", "summary": "We use coherence relations inspired by computational models of discourse to\nstudy the information needs and goals of image captioning. Using an annotation\nprotocol specifically devised for capturing image--caption coherence relations,\nwe annotate 10,000 instances from publicly-available image--caption pairs. We\nintroduce a new task for learning inferences in imagery and text, coherence\nrelation prediction, and show that these coherence annotations can be exploited\nto learn relation classifiers as an intermediary step, and also train\ncoherence-aware, controllable image captioning models. The results show a\ndramatic improvement in the consistency and quality of the generated captions\nwith respect to information needs specified via coherence relations."}, {"title": "Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage", "authors": "Ashish V. Thapliyal, Radu Soricut", "link": "https://arxiv.org/abs/2005.00246", "summary": "Cross-modal language generation tasks such as image captioning are directly\nhurt in their ability to support non-English languages by the trend of\ndata-hungry models combined with the lack of non-English annotations. We\ninvestigate potential solutions for combining existing language-generation\nannotations in English with translation capabilities in order to create\nsolutions at web-scale in both domain and language coverage. We describe an\napproach called Pivot-Language Generation Stabilization (PLuGS), which\nleverages directly at training time both existing English annotations (gold\ndata) as well as their machine-translated versions (silver data); at run-time,\nit generates first an English caption and then a corresponding target-language\ncaption. We show that PLuGS models outperform other candidate solutions in\nevaluations performed over 5 different target languages, under a large-domain\ntestset using images from the Open Images dataset. Furthermore, we find an\ninteresting effect where the English captions generated by the PLuGS models are\nbetter than the captions generated by the original, monolingual English model."}, {"title": "Cross-Modality Relevance for Reasoning on Language and Vision", "authors": "Chen Zheng, Quan Guo, Parisa Kordjamshidi", "link": "https://arxiv.org/abs/2005.06035", "summary": "This work deals with the challenge of learning and reasoning over language\nand vision data for the related downstream tasks such as visual question\nanswering (VQA) and natural language for visual reasoning (NLVR). We design a\nnovel cross-modality relevance module that is used in an end-to-end framework\nto learn the relevance representation between components of various input\nmodalities under the supervision of a target task, which is more generalizable\nto unobserved data compared to merely reshaping the original representation\nspace. In addition to modeling the relevance between the textual entities and\nvisual entities, we model the higher-order relevance between entity relations\nin the text and object relations in the image. Our proposed approach shows\ncompetitive performance on two different language and vision tasks using public\nbenchmarks and improves the state-of-the-art published results. The learned\nalignments of input spaces and their relevance representations by NLVR task\nboost the training efficiency of VQA task."}, {"title": "Curriculum Learning for Natural Language Understanding", "authors": "Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, Yongdong Zhang", "link": "", "summary": ""}, {"title": "Curriculum Pre-training for End-to-End Speech Translation", "authors": "Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, Zhenglu Yang", "link": "https://arxiv.org/abs/2004.10093", "summary": "End-to-end speech translation poses a heavy burden on the encoder, because it\nhas to transcribe, understand, and learn cross-lingual semantics\nsimultaneously. To obtain a powerful encoder, traditional methods pre-train it\non ASR data to capture speech features. However, we argue that pre-training the\nencoder only through simple speech recognition is not enough and high-level\nlinguistic knowledge should be considered. Inspired by this, we propose a\ncurriculum pre-training method that includes an elementary course for\ntranscription learning and two advanced courses for understanding the utterance\nand mapping words in two languages. The difficulty of these courses is\ngradually increasing. Experiments show that our curriculum pre-training method\nleads to significant improvements on En-De and En-Fr speech translation\nbenchmarks."}, {"title": "Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight", "authors": "Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao, Dawei Yin", "link": "https://arxiv.org/abs/2004.02594", "summary": "Current state-of-the-art neural dialogue models learn from human\nconversations following the data-driven paradigm. As such, a reliable training\ncorpus is the crux of building a robust and well-behaved dialogue model.\nHowever, due to the open-ended nature of human conversations, the quality of\nuser-generated training data varies greatly, and effective training samples are\ntypically insufficient while noisy samples frequently appear. This impedes the\nlearning of those data-driven neural dialogue models. Therefore, effective\ndialogue learning requires not only more reliable learning samples, but also\nfewer noisy samples. In this paper, we propose a data manipulation framework to\nproactively reshape the data distribution towards reliable samples by\naugmenting and highlighting effective learning samples as well as reducing the\neffect of inefficient samples simultaneously. In particular, the data\nmanipulation model selectively augments the training samples and assigns an\nimportance weight to each instance to reform the training data. Note that, the\nproposed data manipulation framework is fully data-driven and learnable. It not\nonly manipulates training samples to optimize the dialogue generation model,\nbut also learns to increase its manipulation skills through gradient descent\nwith validation samples. Extensive experiments show that our framework can\nimprove the dialogue generation performance with respect to various automatic\nevaluation metrics and human judgments."}, {"title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering", "authors": "Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian", "link": "https://arxiv.org/abs/2005.00697", "summary": "Transformer-based QA models use input-wide self-attention -- i.e. across both\nthe question and the input passage -- at all layers, causing them to be slow\nand memory-intensive. It turns out that we can get by without input-wide\nself-attention at all layers, especially in the lower layers. We introduce\nDeFormer, a decomposed transformer, which substitutes the full self-attention\nwith question-wide and passage-wide self-attentions in the lower layers. This\nallows for question-independent processing of the input text representations,\nwhich in turn enables pre-computing passage representations reducing runtime\ncompute drastically. Furthermore, because DeFormer is largely similar to the\noriginal model, we can initialize DeFormer with the pre-training weights of a\nstandard transformer, and directly fine-tune on the target QA dataset. We show\nDeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and\nwith simple distillation-based losses they incur only a 1% drop in accuracy. We\nopen source the code at https://github.com/StonyBrookNLP/deformer."}, {"title": "Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting", "authors": "Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Conghui Zhu, Tiejun Zhao", "link": "http://arxiv.org/abs/2004.14088", "summary": "With the recent proliferation of the use of text classifications, researchers\nhave found that there are certain unintended biases in text classification\ndatasets. For example, texts containing some demographic identity-terms (e.g.,\n\"gay\", \"black\") are more likely to be abusive in existing abusive language\ndetection datasets. As a result, models trained with these datasets may\nconsider sentences like \"She makes me happy to be gay\" as abusive simply\nbecause of the word \"gay.\" In this paper, we formalize the unintended biases in\ntext classification datasets as a kind of selection bias from the\nnon-discrimination distribution to the discrimination distribution. Based on\nthis formalization, we further propose a model-agnostic debiasing training\nframework by recovering the non-discrimination distribution using instance\nweighting, which does not require any extra resources or annotations apart from\na pre-defined set of demographic identity-terms. Experiments demonstrate that\nour method can effectively alleviate the impacts of the unintended biases\nwithout significantly hurting models' generalization ability."}, {"title": "Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA", "authors": "Hyounghun Kim, Zineng Tang, Mohit Bansal", "link": "https://arxiv.org/abs/2005.06409", "summary": "Videos convey rich information. Dynamic spatio-temporal relationships between\npeople/objects, and diverse multimodal events are present in a video clip.\nHence, it is important to develop automated models that can accurately extract\nsuch information from videos. Answering questions on videos is one of the tasks\nwhich can evaluate such AI abilities. In this paper, we propose a video\nquestion answering model which effectively integrates multi-modal input sources\nand finds the temporally relevant information to answer questions.\nSpecifically, we first employ dense image captions to help identify objects and\ntheir detailed salient regions and actions, and hence give the model useful\nextra information (in explicit textual format to allow easier matching) for\nanswering questions. Moreover, our model is also comprised of dual-level\nattention (word/object and frame level), multi-head self/cross-integration for\ndifferent sources (video and dense captions), and gates which pass more\nrelevant information to the classifier. Finally, we also cast the frame\nselection problem as a multi-label classification task and introduce two loss\nfunctions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary\nCross-Entropy (BBCE), to better supervise the model with human importance\nannotations. We evaluate our model on the challenging TVQA dataset, where each\nof our model components provides significant gains, and our overall model\noutperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We\nalso present several word, object, and frame level visualization studies. Our\ncode is publicly available at:\nhttps://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020"}, {"title": "Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification", "authors": "Hao Tang, Donghong Ji, Chenliang Li, Qiji Zhou"}, {"title": "DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking", "authors": "Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona Diab, Smaranda Muresan", "link": "https://arxiv.org/abs/2004.12864", "summary": "The increased focus on misinformation has spurred development of data and\nsystems for detecting the veracity of a claim as well as retrieving\nauthoritative evidence. The Fact Extraction and VERification (FEVER) dataset\nprovides such a resource for evaluating end-to-end fact-checking, requiring\nretrieval of evidence from Wikipedia to validate a veracity prediction. We show\nthat current systems for FEVER are vulnerable to three categories of realistic\nchallenges for fact-checking -- multiple propositions, temporal reasoning, and\nambiguity and lexical variation -- and introduce a resource with these types of\nclaims. Then we present a system designed to be resilient to these \"attacks\"\nusing multiple pointer networks for document selection and jointly modeling a\nsequence of evidence sentences and veracity relation predictions. We find that\nin handling these attacks we obtain state-of-the-art results on FEVER, largely\ndue to improved evidence retrieval."}, {"title": "Detecting Perceived Emotions in Hurricane Disasters", "authors": "Shrey Desai, Cornelia Caragea, Junyi Jessy Li", "link": "http://arxiv.org/abs/2004.14299", "summary": "Natural disasters (e.g., hurricanes) affect millions of people each year,\ncausing widespread destruction in their wake. People have recently taken to\nsocial media websites (e.g., Twitter) to share their sentiments and feelings\nwith the larger community. Consequently, these platforms have become\ninstrumental in understanding and perceiving emotions at scale. In this paper,\nwe introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning\nthree hurricanes: Harvey, Irma, and Maria. We present a comprehensive study of\nfine-grained emotions and propose classification tasks to discriminate between\ncoarse-grained emotion groups. Our best BERT model, even after task-guided\npre-training which leverages unlabeled Twitter data, achieves only 68% accuracy\n(averaged across all groups). HurricaneEmo serves not only as a challenging\nbenchmark for models but also as a valuable resource for analyzing emotions in\ndisaster-centric domains."}, {"title": "Dialogue Coherence Assessment Without Explicit Dialogue Act Labels", "authors": "Mohsen Mesgar, Sebastian B\u00fccker, Iryna Gurevych", "link": "http://arxiv.org/abs/1908.08486", "summary": "Recent dialogue coherence models use the coherence features designed for\nmonologue texts, e.g. nominal entities, to represent utterances and then\nexplicitly augment them with dialogue-relevant features, e.g., dialogue act\nlabels. It indicates two drawbacks, (a) semantics of utterances is limited to\nentity mentions, and (b) the performance of coherence models strongly relies on\nthe quality of the input dialogue act labels. We address these issues by\nintroducing a novel approach to dialogue coherence assessment. We use dialogue\nact prediction as an auxiliary task in a multi-task learning scenario to obtain\ninformative utterance representations for coherence assessment. Our approach\nalleviates the need for explicit dialogue act labels during evaluation. The\nresults of our experiments show that our model substantially (more than 20\naccuracy points) outperforms its strong competitors on the DailyDialogue\ncorpus, and performs on par with them on the SwitchBoard corpus for ranking\ndialogues concerning their coherence."}, {"title": "Dialogue-Based Relation Extraction", "authors": "Dian Yu, Kai Sun, Claire Cardie, Dong Yu", "link": "https://arxiv.org/abs/2004.08056", "summary": "We present the first human-annotated dialogue-based relation extraction (RE)\ndataset DialogRE, aiming to support the prediction of relation(s) between two\narguments that appear in a dialogue. We further offer DialogRE as a platform\nfor studying cross-sentence RE as most facts span multiple sentences. We argue\nthat speaker-related information plays a critical role in the proposed task,\nbased on an analysis of similarities and differences between dialogue-based and\ntraditional RE tasks. Considering the timeliness of communication in a\ndialogue, we design a new metric to evaluate the performance of RE methods in a\nconversational setting and investigate the performance of several\nrepresentative RE methods on DialogRE. Experimental results demonstrate that a\nspeaker-aware extension on the best-performing model leads to gains in both the\nstandard and conversational evaluation settings. DialogRE is available at\nhttps://dataset.org/dialogre/."}, {"title": "Dice Loss for Data-imbalanced NLP Tasks", "authors": "Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li", "link": "https://arxiv.org/abs/1911.02855", "summary": "Many NLP tasks such as tagging and machine reading comprehension are faced\nwith the severe data imbalance issue: negative examples significantly outnumber\npositive examples, and the huge number of background examples (or easy-negative\nexamples) overwhelms the training. The most commonly used cross entropy (CE)\ncriteria is actually an accuracy-oriented objective, and thus creates a\ndiscrepancy between training and test: at training time, each training instance\ncontributes equally to the objective function, while at test time F1 score\nconcerns more about positive examples. In this paper, we propose to use dice\nloss in replacement of the standard cross-entropy objective for data-imbalanced\nNLP tasks. Dice loss is based on the Sorensen-Dice coefficient or Tversky\nindex, which attaches similar importance to false positives and false\nnegatives, and is more immune to the data-imbalance issue. To further alleviate\nthe dominating influence from easy-negative examples in training, we propose to\nassociate training examples with dynamically adjusted weights to deemphasize\neasy-negative examples.Theoretical analysis shows that this strategy narrows\ndown the gap between the F1 score in evaluation and the dice loss in training.\nWith the proposed training objective, we observe significant performance boost\non a wide range of data imbalanced NLP tasks. Notably, we are able to achieve\nSOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA\nresults on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity\nrecognition task; along with competitive results on the tasks of machine\nreading comprehension and paraphrase identification."}, {"title": "Differentiable Window for Dynamic Local Attention", "authors": "Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, Xiaoli Li"}, {"title": "Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event", "authors": "Prafulla Kumar Choubey, Aaron Lee, Ruihong Huang, Lu Wang"}, {"title": "Discourse-Aware Neural Extractive Text Summarization", "authors": "Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu", "link": "https://arxiv.org/abs/1910.14142", "summary": "Recently BERT has been adopted for document encoding in state-of-the-art text\nsummarization models. However, sentence-based extractive models often result in\nredundant or uninformative phrases in the extracted summaries. Also, long-range\ndependencies throughout a document are not well captured by BERT, which is\npre-trained on sentence pairs instead of documents. To address these issues, we\npresent a discourse-aware neural summarization model - DiscoBert. DiscoBert\nextracts sub-sentential discourse units (instead of sentences) as candidates\nfor extractive selection on a finer granularity. To capture the long-range\ndependencies among discourse units, structural discourse graphs are constructed\nbased on RST trees and coreference mentions, encoded with Graph Convolutional\nNetworks. Experiments show that the proposed model outperforms state-of-the-art\nmethods by a significant margin on popular summarization benchmarks compared to\nother BERT-base models."}, {"title": "Discrete Latent Variable Representations for Low-Resource Text Classification", "authors": "Shuning Jin, Sam Wiseman, Karl Stratos, Karen Livescu"}, {"title": "Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction", "authors": "Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, Katja Markert", "link": "https://arxiv.org/abs/2005.01791", "summary": "Automatic sentence summarization produces a shorter version of a sentence,\nwhile preserving its most important information. A good summary is\ncharacterized by language fluency and high information overlap with the source\nsentence. We model these two aspects in an unsupervised objective function,\nconsisting of language modeling and semantic similarity metrics. We search for\na high-scoring summary by discrete optimization. Our proposed method achieves a\nnew state-of-the art for unsupervised sentence summarization according to ROUGE\nscores. Additionally, we demonstrate that the commonly reported ROUGE F1 metric\nis sensitive to summary length. Since this is unwillingly exploited in recent\nwork, we emphasize that future evaluation should explicitly group summarization\nsystems by output length brackets."}, {"title": "Distilling Annotations via Active Imitation Learning", "authors": "Kiant\u00e9 Brantley, Hal Daum\u00e9 III, Amr Sharaf", "link": "", "summary": ""}, {"title": "Distilling Knowledge Learned in BERT for Text Generation", "authors": "Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu", "link": "https://arxiv.org/abs/1911.03829", "summary": "Large-scale pre-trained language model such as BERT has achieved great\nsuccess in language understanding tasks. However, it remains an open question\nhow to utilize BERT for language generation. In this paper, we present a novel\napproach, Conditional Masked Language Modeling (C-MLM), to enable the\nfinetuning of BERT on target generation tasks. The finetuned BERT (teacher) is\nexploited as extra supervision to improve conventional Seq2Seq models (student)\nfor better text generation performance. By leveraging BERT's idiosyncratic\nbidirectional nature, distilling knowledge learned in BERT can encourage\nauto-regressive Seq2Seq models to plan ahead, imposing global sequence-level\nsupervision for coherent text generation. Experiments show that the proposed\napproach significantly outperforms strong Transformer baselines on multiple\nlanguage generation tasks such as machine translation and text summarization.\nOur proposed model also achieves new state of the art on IWSLT German-English\nand English-Vietnamese MT datasets."}, {"title": "Distinguish Confusing Law Articles for Legal Judgment Prediction", "authors": "Nuo Xu, Pinghui Wang, Long Chen, Li Pan, Xiaoyan Wang, Junzhou Zhao", "link": "https://arxiv.org/abs/2004.02557", "summary": "Legal Judgment Prediction (LJP) is the task of automatically predicting a law\ncase's judgment results given a text describing its facts, which has excellent\nprospects in judicial assistance systems and convenient services for the\npublic. In practice, confusing charges are frequent, because law cases\napplicable to similar law articles are easily misjudged. For addressing this\nissue, the existing method relies heavily on domain experts, which hinders its\napplication in different law systems. In this paper, we present an end-to-end\nmodel, LADAN, to solve the task of LJP. To distinguish confusing charges, we\npropose a novel graph neural network to automatically learn subtle differences\nbetween confusing law articles and design a novel attention mechanism that\nfully exploits the learned differences to extract compelling discriminative\nfeatures from fact descriptions attentively. Experiments conducted on\nreal-world datasets demonstrate the superiority of our LADAN."}, {"title": "Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness", "authors": "Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou, Zhonghai Wu"}, {"title": "Diversifying Dialogue Generation with Non-Conversational Text", "authors": "Hui Su, Xiaoyu Shen, Sanqiang Zhao, Zhou Xiao, Pengwei Hu, Randy Zhong, Cheng Niu, Jie Zhou", "link": "https://arxiv.org/abs/2005.04346", "summary": "Neural network-based sequence-to-sequence (seq2seq) models strongly suffer\nfrom the low-diversity problem when it comes to open-domain dialogue\ngeneration. As bland and generic utterances usually dominate the frequency\ndistribution in our daily chitchat, avoiding them to generate more interesting\nresponses requires complex data filtering, sampling techniques or modifying the\ntraining objective. In this paper, we propose a new perspective to diversify\ndialogue generation by leveraging non-conversational text. Compared with\nbilateral conversations, non-conversational text are easier to obtain, more\ndiverse and cover a much broader range of topics. We collect a large-scale\nnon-conversational corpus from multi sources including forum comments, idioms\nand book snippets. We further present a training paradigm to effectively\nincorporate these text via iterative back translation. The resulting model is\ntested on two conversational datasets and is shown to produce significantly\nmore diverse responses without sacrificing the relevance with context."}, {"title": "Do Neural Language Models Show Preferences for Syntactic Formalisms?", "authors": "Artur Kulmizev, Vinit Ravishankar, Mostafa Abdou, Joakim Nivre", "link": "https://arxiv.org/abs/2004.14096", "summary": "Recent work on the interpretability of deep neural language models has\nconcluded that many properties of natural language syntax are encoded in their\nrepresentational spaces. However, such studies often suffer from limited scope\nby focusing on a single language and a single linguistic formalism. In this\nstudy, we aim to investigate the extent to which the semblance of syntactic\nstructure captured by language models adheres to a surface-syntactic or deep\nsyntactic style of analysis, and whether the patterns are consistent across\ndifferent languages. We apply a probe for extracting directed dependency trees\nto BERT and ELMo models trained on 13 different languages, probing for two\ndifferent syntactic annotation styles: Universal Dependencies (UD),\nprioritizing deep syntactic relations, and Surface-Syntactic Universal\nDependencies (SUD), focusing on surface structure. We find that both models\nexhibit a preference for UD over SUD - with interesting variations across\nlanguages and layers - and that the strength of this preference is correlated\nwith differences in tree shape."}, {"title": "Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?", "authors": "Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui", "link": "http://arxiv.org/abs/2004.14839", "summary": "Despite the success of language models using neural networks, it remains\nunclear to what extent neural models have the generalization ability to perform\ninferences. In this paper, we introduce a method for evaluating whether neural\nmodels can learn systematicity of monotonicity inference in natural language,\nnamely, the regularity for performing arbitrary inferences with generalization\non composition. We consider four aspects of monotonicity inferences and test\nwhether the models can systematically interpret lexical and logical phenomena\non different training/test splits. A series of experiments show that three\nneural models systematically draw inferences on unseen combinations of lexical\nand logical phenomena when the syntactic structures of the sentences are\nsimilar between the training and test sets. However, the performance of the\nmodels significantly decreases when the structures are slightly changed in the\ntest set while retaining all vocabularies and constituents already appearing in\nthe training set. This indicates that the generalization ability of neural\nmodels is limited to cases where the syntactic structures are nearly the same\nas those in the training set."}, {"title": "Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension", "authors": "Bo Zheng, Haoyang Wen, Yaobo Liang, Nan Duan, Wanxiang Che, Daxin Jiang, Ming Zhou, Ting Liu", "link": "https://arxiv.org/abs/2005.05806", "summary": "Natural Questions is a new challenging machine reading comprehension\nbenchmark with two-grained answers, which are a long answer (typically a\nparagraph) and a short answer (one or more entities inside the long answer).\nDespite the effectiveness of existing methods on this benchmark, they treat\nthese two sub-tasks individually during training while ignoring their\ndependencies. To address this issue, we present a novel multi-grained machine\nreading comprehension framework that focuses on modeling documents at their\nhierarchical nature, which are different levels of granularity: documents,\nparagraphs, sentences, and tokens. We utilize graph attention networks to\nobtain different levels of representations so that they can be learned\nsimultaneously. The long and short answers can be extracted from\nparagraph-level representation and token-level representation, respectively. In\nthis way, we can model the dependencies between the two-grained answers to\nprovide evidence for each other. We jointly train the two sub-tasks, and our\nexperiments show that our approach significantly outperforms previous systems\nat both long and short answer criteria."}, {"title": "Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain", "authors": "Shadi Saleh, Pavel Pecina"}, {"title": "Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding", "authors": "Xinya Du, Claire Cardie", "link": "https://arxiv.org/abs/2005.06579", "summary": "Few works in the literature of event extraction have gone beyond individual\nsentences to make extraction decisions. This is problematic when the\ninformation needed to recognize an event argument is spread across multiple\nsentences. We argue that document-level event extraction is a difficult task\nsince it requires a view of a larger context to determine which spans of text\ncorrespond to event role fillers. We first investigate how end-to-end neural\nsequence models (with pre-trained language model representations) perform on\ndocument-level role filler extraction, as well as how the length of context\ncaptured affects the models' performance. To dynamically aggregate information\ncaptured by neural representations learned at different levels of granularity\n(e.g., the sentence- and paragraph-level), we propose a novel multi-granularity\nreader. We evaluate our models on the MUC-4 event extraction dataset, and show\nthat our best system performs substantially better than prior work. We also\nreport findings on the relationship between context length and neural model\nperformance on the task."}, {"title": "Don\u2019t Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training", "authors": "Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, Jason Weston", "link": "https://arxiv.org/abs/1911.03860", "summary": "Generative dialogue models currently suffer from a number of problems which\nstandard maximum likelihood training does not address. They tend to produce\ngenerations that (i) rely too much on copying from the context, (ii) contain\nrepetitions within utterances, (iii) overuse frequent words, and (iv) at a\ndeeper level, contain logical flaws. In this work we show how all of these\nproblems can be addressed by extending the recently introduced unlikelihood\nloss (Welleck et al., 2019) to these cases. We show that appropriate loss\nfunctions which regularize generated outputs to match human distributions are\neffective for the first three issues. For the last important general issue, we\nshow applying unlikelihood to collected data of what a model should not do is\neffective for improving logical consistency, potentially paving the way to\ngenerative models with greater reasoning ability. We demonstrate the efficacy\nof our approach across several dialogue tasks."}, {"title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks", "authors": "Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith", "link": "https://arxiv.org/abs/2004.10964", "summary": "Language models pretrained on text from a wide variety of sources form the\nfoundation of today's NLP. In light of the success of these broad-coverage\nmodels, we investigate whether it is still helpful to tailor a pretrained model\nto the domain of a target task. We present a study across four domains\n(biomedical and computer science publications, news, and reviews) and eight\nclassification tasks, showing that a second phase of pretraining in-domain\n(domain-adaptive pretraining) leads to performance gains, under both high- and\nlow-resource settings. Moreover, adapting to the task's unlabeled data\n(task-adaptive pretraining) improves performance even after domain-adaptive\npretraining. Finally, we show that adapting to a task corpus augmented using\nsimple data selection strategies is an effective alternative, especially when\nresources for domain-adaptive pretraining might be unavailable. Overall, we\nconsistently find that multi-phase adaptive pretraining offers large gains in\ntask performance."}, {"title": "DoQA - Accessing Domain-Specific FAQs via Conversational QA", "authors": "Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, Eneko Agirre", "link": "", "summary": ""}, {"title": "Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation", "authors": "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Rajani, Bryan McCann, Vicente Ordonez, Caiming Xiong", "link": "https://arxiv.org/abs/2005.00965", "summary": "Word embeddings derived from human-generated corpora inherit strong gender\nbias which can be further amplified by downstream models. Some commonly adopted\ndebiasing approaches, including the seminal Hard Debias algorithm, apply\npost-processing procedures that project pre-trained word embeddings into a\nsubspace orthogonal to an inferred gender subspace. We discover that\nsemantic-agnostic corpus regularities such as word frequency captured by the\nword embeddings negatively impact the performance of these algorithms. We\npropose a simple but effective technique, Double Hard Debias, which purifies\nthe word embeddings against such corpus regularities prior to inferring and\nremoving the gender subspace. Experiments on three bias mitigation benchmarks\nshow that our approach preserves the distributional semantics of the\npre-trained word embeddings while reducing gender bias to a significantly\nlarger degree than prior approaches."}, {"title": "DRTS Parsing with Structure-Aware Encoding and Decoding", "authors": "Qiankun Fu, Yue Zhang, Jiangming Liu, Meishan Zhang", "link": "https://arxiv.org/abs/2005.06901", "summary": "Discourse representation tree structure (DRTS) parsing is a novel semantic\nparsing task which has been concerned most recently. State-of-the-art\nperformance can be achieved by a neural sequence-to-sequence model, treating\nthe tree construction as an incremental sequence generation problem. Structural\ninformation such as input syntax and the intermediate skeleton of the partial\noutput has been ignored in the model, which could be potentially useful for the\nDRTS parsing. In this work, we propose a structural-aware model at both the\nencoder and decoder phase to integrate the structural information, where graph\nattention network (GAT) is exploited for effectively modeling. Experimental\nresults on a benchmark dataset show that our proposed model is effective and\ncan obtain the best performance in the literature."}, {"title": "DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification", "authors": "Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir", "link": "https://arxiv.org/abs/2004.13455", "summary": "Recently, many methods discover effective evidence from reliable sources by\nappropriate neural networks for explainable claim verification, which has been\nwidely recognized. However, in these methods, the discovery process of evidence\nis nontransparent and unexplained. Simultaneously, the discovered evidence only\nroughly aims at the interpretability of the whole sequence of claims but\ninsufficient to focus on the false parts of claims. In this paper, we propose a\nDecision Tree-based Co-Attention model (DTCA) to discover evidence for\nexplainable claim verification. Specifically, we first construct Decision\nTree-based Evidence model (DTE) to select comments with high credibility as\nevidence in a transparent and interpretable way. Then we design Co-attention\nSelf-attention networks (CaSa) to make the selected evidence interact with\nclaims, which is for 1) training DTE to determine the optimal decision\nthresholds and obtain more powerful evidence; and 2) utilizing the evidence to\nfind the false parts in the claim. Experiments on two public datasets,\nRumourEval and PHEME, demonstrate that DTCA not only provides explanations for\nthe results of claim verification but also achieves the state-of-the-art\nperformance, boosting the F1-score by 3.11%, 2.41%, respectively."}, {"title": "Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog", "authors": "Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, Ting Liu", "link": "http://arxiv.org/abs/2004.11019", "summary": "Recent studies have shown remarkable success in end-to-end task-oriented\ndialog system. However, most neural models rely on large training data, which\nare only available for a certain number of task domains, such as navigation and\nscheduling.\n  This makes it difficult to scalable for a new domain with limited labeled\ndata. However, there has been relatively little research on how to effectively\nuse data from all domains to improve the performance of each domain and also\nunseen domains. To this end, we investigate methods that can make explicit use\nof domain knowledge and introduce a shared-private network to learn shared and\nspecific knowledge. In addition, we propose a novel Dynamic Fusion Network\n(DF-Net) which automatically exploit the relevance between the target domain\nand each domain. Results show that our model outperforms existing methods on\nmulti-domain dialogue, giving the state-of-the-art in the literature. Besides,\nwith little training data, we show its transferability by outperforming prior\nbest model by 13.9\\% on average."}, {"title": "Dynamic Online Conversation Recommendation", "authors": "Xingshan Zeng, Jing Li, Lu Wang, Zhiming Mao, Kam-Fai Wong"}, {"title": "Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation", "authors": "Xuanli He, Gholamreza Haffari, Mohammad Norouzi", "link": "https://arxiv.org/abs/2005.06606", "summary": "This paper introduces Dynamic Programming Encoding (DPE), a new segmentation\nalgorithm for tokenizing sentences into subword units. We view the subword\nsegmentation of output sentences as a latent variable that should be\nmarginalized out for learning and inference. A mixed character-subword\ntransformer is proposed, which enables exact log marginal likelihood estimation\nand exact MAP inference to find target segmentations with maximum posterior\nprobability. DPE uses a lightweight mixed character-subword transformer as a\nmeans of pre-processing parallel data to segment output sentences using dynamic\nprogramming. Empirical results on machine translation suggest that DPE is\neffective for segmenting output sentences and can be combined with BPE dropout\nfor stochastic segmentation of source sentences. DPE achieves an average\nimprovement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average\nimprovement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several\nWMT datasets including English <=> (German, Romanian, Estonian, Finnish,\nHungarian)."}, {"title": "ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction", "authors": "Zixiang Ding, Rui Xia, Jianfei Yu", "link": "", "summary": ""}, {"title": "Effective Estimation of Deep Generative Language Models", "authors": "Tom Pelsmaeker, Wilker Aziz", "link": "https://arxiv.org/abs/1904.08194", "summary": "Advances in variational inference enable parameterisation of probabilistic\nmodels by deep neural networks. This combines the statistical transparency of\nthe probabilistic modelling framework with the representational power of deep\nlearning. Yet, due to a problem known as posterior collapse, it is difficult to\nestimate such models in the context of language modelling effectively. We\nconcentrate on one such model, the variational auto-encoder, which we argue is\nan important building block in hierarchical probabilistic models of language.\nThis paper contributes a sober view of the problem, a survey of techniques to\naddress it, novel techniques, and extensions to the model. To establish a\nranking of techniques, we perform a systematic comparison using Bayesian\noptimisation and find that many techniques perform reasonably similar, given\nenough resources. Still, a favourite can be named based on convenience. We also\nmake several empirical observations and recommendations of best practices that\nshould help researchers interested in this exciting field."}, {"title": "Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction", "authors": "Penghui Wei, Jiahao Zhao, Wenji Mao", "link": "", "summary": ""}, {"title": "Efficient Constituency Parsing by Pointing", "authors": "Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, Xiaoli Li"}, {"title": "Efficient Dialogue State Tracking by Selectively Overwriting Memory", "authors": "Sungdong Kim, Sohee Yang, Gyuwan Kim, Sang-Woo Lee", "link": "https://arxiv.org/abs/1911.03906", "summary": "Recent works in dialogue state tracking (DST) focus on an open\nvocabulary-based setting to resolve scalability and generalization issues of\nthe predefined ontology-based approaches. However, they are inefficient in that\nthey predict the dialogue state at every turn from scratch. Here, we consider\ndialogue state as an explicit fixed-sized memory and propose a selectively\noverwriting mechanism for more efficient DST. This mechanism consists of two\nsteps: (1) predicting state operation on each of the memory slots, and (2)\noverwriting the memory with new values, of which only a few are generated\naccording to the predicted state operations. Our method decomposes DST into two\nsub-tasks and guides the decoder to focus only on one of the tasks, thus\nreducing the burden of the decoder. This enhances the effectiveness of training\nand DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue\nState Tracking) model achieves state-of-the-art joint goal accuracy with 51.72%\nin MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST\nsetting. In addition, we analyze the accuracy gaps between the current and the\nground truth-given situations and suggest that it is a promising direction to\nimprove state operation prediction to boost the DST performance."}, {"title": "Efficient Pairwise Annotation of Argument Quality", "authors": "Lukas Gienapp, Benno Stein, Matthias Hagen, Martin Potthast"}, {"title": "Efficient Second-Order TreeCRF for Neural Dependency Parsing", "authors": "Yu Zhang, Zhenghua Li, Min Zhang", "link": "https://arxiv.org/abs/2005.00975", "summary": "In the deep learning (DL) era, parsing models are extremely simplified with\nlittle hurt on performance, thanks to the remarkable capability of multi-layer\nBiLSTMs in context representation. As the most popular graph-based dependency\nparser due to its high efficiency and performance, the biaffine parser directly\nscores single dependencies under the arc-factorization assumption, and adopts a\nvery simple local token-wise cross-entropy training loss. This paper for the\nfirst time presents a second-order TreeCRF extension to the biaffine parser.\nFor a long time, the complexity and inefficiency of the inside-outside\nalgorithm hinder the popularity of TreeCRF. To address this issue, we propose\nan effective way to batchify the inside and Viterbi algorithms for direct large\nmatrix operation on GPUs, and to avoid the complex outside algorithm via\nefficient back-propagation. Experiments and analysis on 27 datasets from 13\nlanguages clearly show that techniques developed before the DL era, such as\nstructural learning (global TreeCRF loss) and high-order modeling are still\nuseful, and can further boost parsing performance over the state-of-the-art\nbiaffine parser, especially for partially annotated training data. We release\nour code at https://github.com/yzhangcs/crfpar."}, {"title": "Emergence of Syntax Needs Minimal Supervision", "authors": "Rapha\u00ebl Bailly, Kata G\u00e1bor", "link": "https://arxiv.org/abs/2005.01119", "summary": "This paper is a theoretical contribution to the debate on the learnability of\nsyntax from a corpus without explicit syntax-specific guidance. Our approach\noriginates in the observable structure of a corpus, which we use to define and\nisolate grammaticality (syntactic information) and meaning/pragmatics\ninformation. We describe the formal characteristics of an autonomous syntax and\nshow that it becomes possible to search for syntax-based lexical categories\nwith a simple optimization process, without any prior hypothesis on the form of\nthe model."}, {"title": "Emerging Cross-lingual Structure in Pretrained Language Models", "authors": "Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov", "link": "https://arxiv.org/abs/1911.01464", "summary": "We study the problem of multilingual masked language modeling, i.e. the\ntraining of a single model on concatenated text from multiple languages, and\npresent a detailed study of several factors that influence why these models are\nso effective for cross-lingual transfer. We show, contrary to what was\npreviously hypothesized, that transfer is possible even when there is no shared\nvocabulary across the monolingual corpora and also when the text comes from\nvery different domains. The only requirement is that there are some shared\nparameters in the top layers of the multi-lingual encoder. To better understand\nthis result, we also show that representations from independently trained\nmodels in different languages can be aligned post-hoc quite effectively,\nstrongly suggesting that, much like for non-contextual word embeddings, there\nare universal latent symmetries in the learned embedding spaces. For\nmultilingual masked language modeling, these symmetries seem to be\nautomatically discovered and aligned during the joint training process."}, {"title": "Empower Entity Set Expansion via Language Model Probing", "authors": "Yunyi Zhang, Jiaming Shen, Jingbo Shang, Jiawei Han", "link": "https://arxiv.org/abs/2004.13897", "summary": "Entity set expansion, aiming at expanding a small seed entity set with new\nentities belonging to the same semantic class, is a critical task that benefits\nmany downstream NLP and IR applications, such as question answering, query\nunderstanding, and taxonomy construction. Existing set expansion methods\nbootstrap the seed entity set by adaptively selecting context features and\nextracting new entities. A key challenge for entity set expansion is to avoid\nselecting ambiguous context features which will shift the class semantics and\nlead to accumulative errors in later iterations. In this study, we propose a\nnovel iterative set expansion framework that leverages automatically generated\nclass names to address the semantic drift issue. In each iteration, we select\none positive and several negative class names by probing a pre-trained language\nmodel, and further score each candidate entity based on selected class names.\nExperiments on two datasets show that our framework generates high-quality\nclass names and outperforms previous state-of-the-art methods significantly."}, {"title": "Empowering Active Learning to Jointly Optimize System and User Demands", "authors": "Ji-Ung Lee, Christian M. Meyer, Iryna Gurevych", "link": "https://arxiv.org/abs/2005.04470", "summary": "Existing approaches to active learning maximize the system performance by\nsampling unlabeled instances for annotation that yield the most efficient\ntraining. However, when active learning is integrated with an end-user\napplication, this can lead to frustration for participating users, as they\nspend time labeling instances that they would not otherwise be interested in\nreading. In this paper, we propose a new active learning approach that jointly\noptimizes the seemingly counteracting objectives of the active learning system\n(training efficiently) and the user (receiving useful instances). We study our\napproach in an educational application, which particularly benefits from this\ntechnique as the system needs to rapidly learn to predict the appropriateness\nof an exercise to a particular user, while the users should receive only\nexercises that match their skills. We evaluate multiple learning strategies and\nuser types with data from real users and find that our joint approach better\nsatisfies both objectives when alternative methods lead to many unsuitable\nexercises for end users."}, {"title": "End-to-End Bias Mitigation by Modelling Biases in Corpora", "authors": "Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson", "link": "https://arxiv.org/abs/1909.06321", "summary": "Several recent studies have shown that strong natural language understanding\n(NLU) models are prone to relying on unwanted dataset biases without learning\nthe underlying task, resulting in models that fail to generalize to\nout-of-domain datasets and are likely to perform poorly in real-world\nscenarios. We propose two learning strategies to train neural models, which are\nmore robust to such biases and transfer better to out-of-domain datasets. The\nbiases are specified in terms of one or more bias-only models, which learn to\nleverage the dataset biases. During training, the bias-only models' predictions\nare used to adjust the loss of the base model to reduce its reliance on biases\nby down-weighting the biased examples and focusing the training on the hard\nexamples. We experiment on large-scale natural language inference and fact\nverification benchmarks, evaluating on out-of-domain datasets that are\nspecifically designed to assess the robustness of models against known biases\nin the training data. Results show that our debiasing methods greatly improve\nrobustness in all settings and better transfer to other textual entailment\ndatasets. Our code and data are publicly available in\n\\url{https://github.com/rabeehk/robust-nli}."}, {"title": "End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2", "authors": "Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, Kee-Eung Kim"}, {"title": "End-to-End Neural Word Alignment Outperforms GIZA++", "authors": "Thomas Zenkel, Joern Wuebker, John DeNero", "link": "https://arxiv.org/abs/2004.14675", "summary": "Word alignment was once a core unsupervised learning task in natural language\nprocessing because of its essential role in training statistical machine\ntranslation (MT) models. Although unnecessary for training neural MT models,\nword alignment still plays an important role in interactive applications of\nneural machine translation, such as annotation transfer and lexicon injection.\nWhile statistical MT methods have been replaced by neural approaches with\nsuperior performance, the twenty-year-old GIZA++ toolkit remains a key\ncomponent of state-of-the-art word alignment systems. Prior work on neural word\nalignment has only been able to outperform GIZA++ by using its output during\ntraining. We present the first end-to-end neural word alignment method that\nconsistently outperforms GIZA++ on three data sets. Our approach repurposes a\nTransformer model trained for supervised translation to also serve as an\nunsupervised word alignment model in a manner that is tightly integrated and\ndoes not affect translation quality."}, {"title": "Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension", "authors": "Fei Yuan, Linjun Shou, Xuanyu Bai, Ming Gong, Yaobo Liang, Nan Duan, Yan Fu, Daxin Jiang", "link": "https://arxiv.org/abs/2004.14069", "summary": "Multilingual pre-trained models could leverage the training data from a rich\nsource language (such as English) to improve performance on low resource\nlanguages. However, the transfer quality for multilingual Machine Reading\nComprehension (MRC) is significantly worse than sentence classification tasks\nmainly due to the requirement of MRC to detect the word level answer boundary.\nIn this paper, we propose two auxiliary tasks in the fine-tuning stage to\ncreate additional phrase boundary supervision: (1) A mixed MRC task, which\ntranslates the question or passage to other languages and builds cross-lingual\nquestion-passage pairs; (2) A language-agnostic knowledge masking task by\nleveraging knowledge phrases mined from web. Besides, extensive experiments on\ntwo cross-lingual MRC datasets show the effectiveness of our proposed approach."}, {"title": "Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge", "authors": "Bowen Zhang, Min Yang, Xutao Li, Yunming Ye, Xiaofei Xu, Kuai Dai"}, {"title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models", "authors": "Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C. Wallace", "link": "", "summary": ""}, {"title": "ESPRIT: Explaining Solutions to Physical Reasoning Tasks", "authors": "Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas, Abhijit Gupta, Caiming Xiong, Richard Socher, Dragomir Radev", "link": "http://arxiv.org/abs/2005.00730", "summary": "Neural networks lack the ability to reason about qualitative physics and so\ncannot generalize to scenarios and tasks unseen during training. We propose\nESPRIT, a framework for commonsense reasoning about qualitative physics in\nnatural language that generates interpretable descriptions of physical events.\nWe use a two-step approach of first identifying the pivotal physical events in\nan environment and then generating natural language descriptions of those\nevents using a data-to-text approach. Our framework learns to generate\nexplanations of how the physical simulation will causally evolve so that an\nagent or a human can easily reason about a solution using those interpretable\ndescriptions. Human evaluations indicate that ESPRIT produces crucial\nfine-grained details and has high coverage of physical concepts compared to\neven human annotations. Dataset, code and documentation are available at\nhttps://github.com/salesforce/esprit."}, {"title": "Estimating predictive uncertainty for rumour verification models", "authors": "Elena Kochkina, Maria Liakata", "link": "https://arxiv.org/abs/2005.07174", "summary": "The inability to correctly resolve rumours circulating online can have\nharmful real-world consequences. We present a method for incorporating model\nand data uncertainty estimates into natural language processing models for\nautomatic rumour verification. We show that these estimates can be used to\nfilter out model predictions likely to be erroneous, so that these difficult\ninstances can be prioritised by a human fact-checker. We propose two methods\nfor uncertainty-based instance rejection, supervised and unsupervised. We also\nshow how uncertainty estimates can be used to interpret model performance as a\nrumour unfolds."}, {"title": "Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks", "authors": "Fynn Schr\u00f6der, Chris Biemann"}, {"title": "Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples", "authors": "Xiaoqing Zheng, Jiehang Zeng, Yi Zhou, Cho-Jui Hsieh, Minhao Cheng, Xuanjing Huang"}, {"title": "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?", "authors": "Peter Hase, Mohit Bansal", "link": "https://arxiv.org/abs/2005.01831", "summary": "Algorithmic approaches to interpreting machine learning models have\nproliferated in recent years. We carry out human subject tests that are the\nfirst of their kind to isolate the effect of algorithmic explanations on a key\naspect of model interpretability, simulatability, while avoiding important\nconfounding experimental factors. A model is simulatable when a person can\npredict its behavior on new inputs. Through two kinds of simulation tests\ninvolving text and tabular data, we evaluate five explanations methods: (1)\nLIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a\nComposite approach that combines explanations from each method. Clear evidence\nof method effectiveness is found in very few cases: LIME improves\nsimulatability in tabular classification, and our Prototype method is effective\nin counterfactual simulation tests. We also collect subjective ratings of\nexplanations, but we do not find that ratings are predictive of how helpful\nexplanations are. Our results provide the first reliable and comprehensive\nestimates of how explanations influence simulatability across a variety of\nexplanation methods and data domains. We show that (1) we need to be careful\nabout the metrics we use to evaluate explanation methods, and (2) there is\nsignificant room for improvement in current methods. All our supporting code,\ndata, and models are publicly available at:\nhttps://github.com/peterbhase/InterpretableNLP-ACL2020"}, {"title": "Evaluating Explanation Methods for Neural Machine Translation", "authors": "Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming Shi", "link": "https://arxiv.org/abs/2005.01672", "summary": "Recently many efforts have been devoted to interpreting the black-box NMT\nmodels, but little progress has been made on metrics to evaluate explanation\nmethods. Word Alignment Error Rate can be used as such a metric that matches\nhuman understanding, however, it can not measure explanation methods on those\ntarget words that are not aligned to any source word. This paper thereby makes\nan initial attempt to evaluate explanation methods from an alternative\nviewpoint. To this end, it proposes a principled metric based on fidelity in\nregard to the predictive behavior of the NMT model. As the exact computation\nfor this metric is intractable, we employ an efficient approach as its\napproximation. On six standard translation tasks, we quantitatively evaluate\nseveral explanation methods in terms of the proposed metric and we reveal some\nvaluable findings for these explanation methods in our experiments."}, {"title": "Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder", "authors": "Daya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang, Ming Zhou"}, {"title": "Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis", "authors": "Yajie Ye, Weiwei Sun"}, {"title": "Examining Citations of Natural Language Processing Literature", "authors": "Saif M. Mohammad", "link": "https://arxiv.org/abs/2005.00912", "summary": "We extracted information from the ACL Anthology (AA) and Google Scholar (GS)\nto examine trends in citations of NLP papers. We explore questions such as: how\nwell cited are papers of different types (journal articles, conference papers,\ndemo papers, etc.)? how well cited are papers from different areas of within\nNLP? etc. Notably, we show that only about 56\\% of the papers in AA are cited\nten or more times. CL Journal has the most cited papers, but its citation\ndominance has lessened in recent years. On average, long papers get almost\nthree times as many citations as short papers; and papers on sentiment\nclassification, anaphora resolution, and entity recognition have the highest\nmedian citations. The analyses presented here, and the associated dataset of\nNLP papers mapped to citations, have a number of uses including: understanding\nhow the field is growing and quantifying the impact of different types of\npapers."}, {"title": "Examining the State-of-the-Art in News Timeline Summarization", "authors": "Demian Gholipour Ghalandari, Georgiana Ifrim"}, {"title": "Exclusive Hierarchical Decoding for Deep Keyphrase Generation", "authors": "Wang Chen, Hou Pong Chan, Piji Li, Irwin King", "link": "https://arxiv.org/abs/2004.08511", "summary": "Keyphrase generation (KG) aims to summarize the main ideas of a document into\na set of keyphrases. A new setting is recently introduced into this problem, in\nwhich, given a document, the model needs to predict a set of keyphrases and\nsimultaneously determine the appropriate number of keyphrases to produce.\nPrevious work in this setting employs a sequential decoding process to generate\nkeyphrases. However, such a decoding method ignores the intrinsic hierarchical\ncompositionality existing in the keyphrase set of a document. Moreover,\nprevious work tends to generate duplicated keyphrases, which wastes time and\ncomputing resources. To overcome these limitations, we propose an exclusive\nhierarchical decoding framework that includes a hierarchical decoding process\nand either a soft or a hard exclusion mechanism. The hierarchical decoding\nprocess is to explicitly model the hierarchical compositionality of a keyphrase\nset. Both the soft and the hard exclusion mechanisms keep track of\npreviously-predicted keyphrases within a window size to enhance the diversity\nof the generated keyphrases. Extensive experiments on multiple KG benchmark\ndatasets demonstrate the effectiveness of our method to generate less\nduplicated and more accurate keyphrases."}, {"title": "Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen", "authors": "Yixin Cao, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan Liu, Tat-Seng Chua", "link": "http://arxiv.org/abs/2005.00701", "summary": "The curse of knowledge can impede communication between experts and laymen.\nWe propose a new task of expertise style transfer and contribute a manually\nannotated dataset with the goal of alleviating such cognitive biases. Solving\nthis task not only simplifies the professional language, but also improves the\naccuracy and expertise level of laymen descriptions using simple words. This is\na challenging task, unaddressed in previous work, as it requires the models to\nhave expert intelligence in order to modify text with a deep understanding of\ndomain knowledge and structures. We establish the benchmark performance of five\nstate-of-the-art models for style transfer and text simplification. The results\ndemonstrate a significant gap between machine and human performance. We also\ndiscuss the challenges of automatic evaluation, to provide insights into future\nresearch directions. The dataset is publicly available at\nhttps://srhthu.github.io/expertise-style-transfer."}, {"title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions", "authors": "Xiaochuang Han, Byron C. Wallace, Yulia Tsvetkov", "link": "https://arxiv.org/abs/2005.06676", "summary": "Modern deep learning models for NLP are notoriously opaque. This has\nmotivated the development of methods for interpreting such models, e.g., via\ngradient-based saliency maps or the visualization of attention weights. Such\napproaches aim to provide explanations for a particular model prediction by\nhighlighting important words in the corresponding input text. While this might\nbe useful for tasks where decisions are explicitly influenced by individual\ntokens in the input, we suspect that such highlighting is not suitable for\ntasks where model decisions should be driven by more complex reasoning. In this\nwork, we investigate the use of influence functions for NLP, providing an\nalternative approach to interpreting neural text classifiers. Influence\nfunctions explain the decisions of a model by identifying influential training\nexamples. Despite the promise of this approach, influence functions have not\nyet been extensively evaluated in the context of NLP, a gap addressed by this\nwork. We conduct a comparison between influence functions and common\nword-saliency methods on representative tasks. As suspected, we find that\ninfluence functions are particularly useful for natural language inference, a\ntask in which 'saliency maps' may not have clear interpretation. Furthermore,\nwe develop a new quantitative measure based on influence functions that can\nreveal artifacts in training data."}, {"title": "Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading", "authors": "Yifan Gao, Chien-Sheng Wu, Shafiq Joty, Caiming Xiong, Richard Socher, Irwin King, Michael Lyu, Steven C.H. Hoi", "link": "https://arxiv.org/abs/2005.12484", "summary": "The goal of conversational machine reading is to answer user questions given\na knowledge base text which may require asking clarification questions.\nExisting approaches are limited in their decision making due to struggles in\nextracting question-related rules and reasoning about them. In this paper, we\npresent a new framework of conversational machine reading that comprises a\nnovel Explicit Memory Tracker (EMT) to track whether conditions listed in the\nrule text have already been satisfied to make a decision. Moreover, our\nframework generates clarification questions by adopting a coarse-to-fine\nreasoning strategy, utilizing sentence-level entailment scores to weight\ntoken-level distributions. On the ShARC benchmark (blind, held-out) testset,\nEMT achieves new state-of-the-art results of 74.6% micro-averaged decision\naccuracy and 49.5 BLEU4. We also show that EMT is more interpretable by\nvisualizing the entailment-oriented reasoning process as the conversation\nflows. Code and models are released at\n\\url{https://github.com/Yifan-Gao/explicit_memory_tracker}."}, {"title": "Explicit Semantic Decomposition for Definition Generation", "authors": "Jiahuan Li, Yu Bao, Shujian Huang, Xinyu Dai, Jiajun Chen", "link": "", "summary": ""}, {"title": "Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach", "authors": "Wenyu DU, Zhouhan Lin, Yikang Shen, Timothy J. O\u2019Donnell, Yoshua Bengio, Yue Zhang", "link": "https://arxiv.org/abs/2005.05864", "summary": "It is commonly believed that knowledge of syntactic structure should improve\nlanguage modeling. However, effectively and computationally efficiently\nincorporating syntactic structure into neural language models has been a\nchallenging topic. In this paper, we make use of a multi-task objective, i.e.,\nthe models simultaneously predict words as well as ground truth parse trees in\na form called \"syntactic distances\", where information between these two\nseparate objectives shares the same intermediate representation. Experimental\nresults on the Penn Treebank and Chinese Treebank datasets show that when\nground truth parse trees are provided as additional training signals, the model\nis able to achieve lower perplexity and induce trees with better quality."}, {"title": "Exploiting the Syntax-Model Consistency for Neural Relation Extraction", "authors": "Amir Pouran Ben Veyseh, Franck Dernoncourt, Dejing Dou, Thien Huu Nguyen"}, {"title": "Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer", "authors": "Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng Guo, Hua Wu", "link": "https://arxiv.org/abs/2005.02049", "summary": "Unsupervised style transfer aims to change the style of an input sentence\nwhile preserving its original content without using parallel training data. In\ncurrent dominant approaches, owing to the lack of fine-grained control on the\ninfluence from the target style,they are unable to yield desirable output\nsentences. In this paper, we propose a novel attentional sequence-to-sequence\n(Seq2seq) model that dynamically exploits the relevance of each output word to\nthe target style for unsupervised style transfer. Specifically, we first\npretrain a style classifier, where the relevance of each input word to the\noriginal style can be quantified via layer-wise relevance propagation. In a\ndenoising auto-encoding manner, we train an attentional Seq2seq model to\nreconstruct input sentences and repredict word-level previously-quantified\nstyle relevance simultaneously. In this way, this model is endowed with the\nability to automatically predict the style relevance of each output word. Then,\nwe equip the decoder of this model with a neural style component to exploit the\npredicted wordlevel style relevance for better style transfer. Particularly, we\nfine-tune this model using a carefully-designed objective function involving\nstyle transfer, style relevance consistency, content preservation and fluency\nmodeling loss terms. Experimental results show that our proposed model achieves\nstate-of-the-art performance in terms of both transfer accuracy and content\npreservation."}, {"title": "Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing", "authors": "Alane Suhr, Ming-Wei Chang, Peter Shaw, Kenton Lee"}, {"title": "Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches", "authors": "Tianze Shi, Lillian Lee", "link": "https://arxiv.org/abs/2005.03035", "summary": "An interesting and frequent type of multi-word expression (MWE) is the\nheadless MWE, for which there are no true internal syntactic dominance\nrelations; examples include many named entities (\"Wells Fargo\") and dates\n(\"July 5, 2020\") as well as certain productive constructions (\"blow for blow\",\n\"day after day\"). Despite their special status and prevalence, current\ndependency-annotation schemes require treating such flat structures as if they\nhad internal syntactic heads, and most current parsers handle them in the same\nfashion as headed constructions. Meanwhile, outside the context of parsing,\ntaggers are typically used for identifying MWEs, but taggers might benefit from\nstructural information. We empirically compare these two common\nstrategies--parsing and tagging--for predicting flat MWEs. Additionally, we\npropose an efficient joint decoding algorithm that combines scores from both\nstrategies. Experimental results on the MWE-Aware English Dependency Corpus and\non six non-English dependency treebanks with frequent flat structures show\nthat: (1) tagging is more accurate than parsing for identifying flat-structure\nMWEs, (2) our joint decoder reconciles the two different views and, for\nnon-BERT features, leads to higher accuracies, and (3) most of the gains result\nfrom feature sharing between the parsers and taggers."}, {"title": "Extractive Summarization as Text Matching", "authors": "Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang", "link": "https://arxiv.org/abs/2004.08795", "summary": "This paper creates a paradigm shift with regard to the way we build neural\nextractive summarization systems. Instead of following the commonly used\nframework of extracting sentences individually and modeling the relationship\nbetween sentences, we formulate the extractive summarization task as a semantic\ntext matching problem, in which a source document and candidate summaries will\nbe (extracted from the original text) matched in a semantic space. Notably,\nthis paradigm shift to semantic matching framework is well-grounded in our\ncomprehensive analysis of the inherent gap between sentence-level and\nsummary-level extractors based on the property of the dataset.\n  Besides, even instantiating the framework with a simple form of a matching\nmodel, we have driven the state-of-the-art extractive result on CNN/DailyMail\nto a new level (44.41 in ROUGE-1). Experiments on the other five datasets also\nshow the effectiveness of the matching framework. We believe the power of this\nmatching-based summarization framework has not been fully exploited. To\nencourage more instantiations in the future, we have released our codes,\nprocessed dataset, as well as generated summaries in\nhttps://github.com/maszhongming/MatchSum."}, {"title": "Facet-Aware Evaluation for Extractive Summarization", "authors": "Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren, Jiawei Han", "link": "https://arxiv.org/abs/1908.10383", "summary": "Commonly adopted metrics for extractive summarization focus on lexical\noverlap at the token level. In this paper, we present a facet-aware evaluation\nsetup for better assessment of the information coverage in extracted summaries.\nSpecifically, we treat each sentence in the reference summary as a\n\\textit{facet}, identify the sentences in the document that express the\nsemantics of each facet as \\textit{support sentences} of the facet, and\nautomatically evaluate extractive summarization methods by comparing the\nindices of extracted sentences and support sentences of all the facets in the\nreference summary. To facilitate this new evaluation setup, we construct an\nextractive version of the CNN/Daily Mail dataset and perform a thorough\nquantitative investigation, through which we demonstrate that facet-aware\nevaluation manifests better correlation with human judgment than ROUGE, enables\nfine-grained evaluation as well as comparative analysis, and reveals valuable\ninsights of state-of-the-art summarization methods. Data can be found at\nhttps://github.com/morningmoni/FAR."}, {"title": "Fact-based Text Editing", "authors": "Hayate Iso, Chao Qiao, Hang Li"}, {"title": "Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning", "authors": "Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, Kyomin Jung", "link": "https://arxiv.org/abs/2004.08097", "summary": "Even though BERT achieves successful performance improvements in various\nsupervised learning tasks, applying BERT for unsupervised tasks still holds a\nlimitation that it requires repetitive inference for computing contextual\nlanguage representations. To resolve the limitation, we propose a novel deep\nbidirectional language model called Transformer-based Text Autoencoder (T-TA).\nThe T-TA computes contextual language representations without repetition and\nhas benefits of the deep bidirectional architecture like BERT. In run-time\nexperiments on CPU environments, the proposed T-TA performs over six times\nfaster than the BERT-based model in the reranking task and twelve times faster\nin the semantic similarity task. Furthermore, the T-TA shows competitive or\neven better accuracies than those of BERT on the above tasks."}, {"title": "Fast and Accurate Non-Projective Dependency Tree Linearization", "authors": "Xiang Yu, Simon Tannert, Ngoc Thang Vu, Jonas Kuhn"}, {"title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time", "authors": "Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, QI JU", "link": "", "summary": ""}, {"title": "Feature Projection for Improved Text Classification", "authors": "Qi Qin, Wenpeng Hu, Bing Liu"}, {"title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization", "authors": "Esin Durmus, He He, Mona Diab", "link": "https://arxiv.org/abs/2005.03754", "summary": "Neural abstractive summarization models are prone to generate content\ninconsistent with the source document, i.e. unfaithful. Existing automatic\nmetrics do not capture such mistakes effectively. We tackle the problem of\nevaluating faithfulness of a generated summary given its source document. We\nfirst collected human annotations of faithfulness for outputs from numerous\nmodels on two datasets. We find that current models exhibit a trade-off between\nabstractiveness and faithfulness: outputs with less word overlap with the\nsource document are more likely to be unfaithful. Next, we propose an automatic\nquestion answering (QA) based metric for faithfulness, FEQA, which leverages\nrecent advances in reading comprehension. Given question-answer pairs generated\nfrom the summary, a QA model extracts answers from the document; non-matched\nanswers indicate unfaithful information in the summary. Among metrics based on\nword overlap, embedding similarity, and learned language understanding models,\nour QA-based metric has significantly higher correlation with human\nfaithfulness scores, especially on highly abstractive summaries."}, {"title": "Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network", "authors": "Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou, Yijia Liu, Han Liu, Ting Liu"}, {"title": "Finding Universal Grammatical Relations in Multilingual BERT", "authors": "Ethan A. Chi, John Hewitt, Christopher D. Manning", "link": "https://arxiv.org/abs/2005.04511", "summary": "Recent work has found evidence that Multilingual BERT (mBERT), a\ntransformer-based multilingual masked language model, is capable of zero-shot\ncross-lingual transfer, suggesting that some aspects of its representations are\nshared cross-lingually. To better understand this overlap, we extend recent\nwork on finding syntactic trees in neural networks' internal representations to\nthe multilingual setting. We show that subspaces of mBERT representations\nrecover syntactic tree distances in languages other than English, and that\nthese subspaces are approximately shared across languages. Motivated by these\nresults, we present an unsupervised analysis method that provides evidence\nmBERT learns representations of syntactic dependency labels, in the form of\nclusters which largely agree with the Universal Dependencies taxonomy. This\nevidence suggests that even without explicit supervision, multilingual masked\nlanguage models learn certain linguistic universals."}, {"title": "Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences", "authors": "Dmitry Nikolaev, Ofir Arviv, Taelin Karidi, Neta Kenneth, Veronika Mitnik, Lilja Maria Saeboe, Omri Abend", "link": "https://arxiv.org/abs/2005.03436", "summary": "The patterns in which the syntax of different languages converges and\ndiverges are often used to inform work on cross-lingual transfer. Nevertheless,\nlittle empirical work has been done on quantifying the prevalence of different\nsyntactic divergences across language pairs. We propose a framework for\nextracting divergence patterns for any language pair from a parallel corpus,\nbuilding on Universal Dependencies. We show that our framework provides a\ndetailed picture of cross-language divergences, generalizes previous\napproaches, and lends itself to full automation. We further present a novel\ndataset, a manually word-aligned subset of the Parallel UD corpus in five\nlanguages, and use it to perform a detailed corpus study. We demonstrate the\nusefulness of the resulting analysis by showing that it can help account for\nperformance patterns of a cross-lingual parser."}, {"title": "Fine-grained Fact Verification with Kernel Graph Attention Network", "authors": "Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu", "link": "https://arxiv.org/abs/1910.09796", "summary": "Fact Verification requires fine-grained natural language inference capability\nthat finds subtle clues to identify the syntactical and semantically correct\nbut not well-supported claims. This paper presents Kernel Graph Attention\nNetwork (KGAT), which conducts more fine-grained fact verification with\nkernel-based attentions. Given a claim and a set of potential evidence\nsentences that form an evidence graph, KGAT introduces node kernels, which\nbetter measure the importance of the evidence node, and edge kernels, which\nconduct fine-grained evidence propagation in the graph, into Graph Attention\nNetworks for more accurate fact verification. KGAT achieves a 70.38% FEVER\nscore and significantly outperforms existing fact verification models on FEVER,\na large-scale benchmark for fact verification. Our analyses illustrate that,\ncompared to dot-product attentions, the kernel-based attention concentrates\nmore on relevant evidence sentences and meaningful clues in the evidence graph,\nwhich is the main source of KGAT's effectiveness."}, {"title": "Fine-grained Interest Matching for Neural News Recommendation", "authors": "Heyuan Wang, Fangzhao Wu, Zheng Liu, Xing Xie"}, {"title": "Fluent Response Generation for Conversational Question Answering", "authors": "Ashutosh Baheti, Alan Ritter, Kevin Small", "link": "https://arxiv.org/abs/2005.10464", "summary": "Question answering (QA) is an important aspect of open-domain conversational\nagents, garnering specific research focus in the conversational QA (ConvQA)\nsubtask. One notable limitation of recent ConvQA efforts is the response being\nanswer span extraction from the target corpus, thus ignoring the natural\nlanguage generation (NLG) aspect of high-quality conversational agents. In this\nwork, we propose a method for situating QA responses within a SEQ2SEQ NLG\napproach to generate fluent grammatical answer responses while maintaining\ncorrectness. From a technical perspective, we use data augmentation to generate\ntraining data for an end-to-end system. Specifically, we develop Syntactic\nTransformations (STs) to produce question-specific candidate answer responses\nand rank them using a BERT-based classifier (Devlin et al., 2019). Human\nevaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the\nproposed model outperforms baseline CoQA and QuAC models in generating\nconversational responses. We further show our model's scalability by conducting\ntests on the CoQA dataset. The code and data are available at\nhttps://github.com/abaheti95/QADialogSystem."}, {"title": "From Arguments to Key Points: Towards Automatic Argument Summarization", "authors": "Roy Bar-Haim, Lilach Eden, Roni Friedman, Yoav Kantor, Dan Lahav, Noam Slonim", "link": "https://arxiv.org/abs/2005.01619", "summary": "Generating a concise summary from a large collection of arguments on a given\ntopic is an intriguing yet understudied problem. We propose to represent such\nsummaries as a small set of talking points, termed \"key points\", each scored\naccording to its salience. We show, by analyzing a large dataset of\ncrowd-contributed arguments, that a small number of key points per topic is\ntypically sufficient for covering the vast majority of the arguments.\nFurthermore, we found that a domain expert can often predict these key points\nin advance. We study the task of argument-to-key point mapping, and introduce a\nnovel large-scale dataset for this task. We report empirical results for an\nextensive set of experiments with this dataset, showing promising performance."}, {"title": "From English to Code-Switching: Transfer Learning with Strong Morphological Clues", "authors": "Gustavo Aguilar, Thamar Solorio", "link": "https://arxiv.org/abs/1909.05158", "summary": "Linguistic Code-switching (CS) is still an understudied phenomenon in natural\nlanguage processing. The NLP community has mostly focused on monolingual and\nmulti-lingual scenarios, but little attention has been given to CS in\nparticular. This is partly because of the lack of resources and annotated data,\ndespite its increasing occurrence in social media platforms. In this paper, we\naim at adapting monolingual models to code-switched text in various tasks.\nSpecifically, we transfer English knowledge from a pre-trained ELMo model to\ndifferent code-switched language pairs (i.e., Nepali-English, Spanish-English,\nand Hindi-English) using the task of language identification. Our method,\nCS-ELMo, is an extension of ELMo with a simple yet effective position-aware\nattention mechanism inside its character convolutions. We show the\neffectiveness of this transfer learning step by outperforming multilingual BERT\nand homologous CS-unaware ELMo models and establishing a new state of the art\nin CS tasks, such as NER and POS tagging. Our technique can be expanded to more\nEnglish-paired code-switched languages, providing more resources to the CS\ncommunity."}, {"title": "From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?", "authors": "Reut Tsarfaty, Dan Bareket, Stav Klein, Amit Seker", "link": "", "summary": ""}, {"title": "From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains", "authors": "Jan-Christoph Klie, Richard Eckart de Castilho, Iryna Gurevych"}, {"title": "Frugal Paradigm Completion", "authors": "Alexander Erdmann, Tom Kenter, Markus Becker, Christian Schallhart"}, {"title": "Gated Convolutional Bidirectional Attention-based Model for\u00a0Off-topic Spoken Response Detection", "authors": "Yefei Zha, Ruobing Li, Hui Lin", "link": "https://arxiv.org/abs/2004.09036", "summary": "Off-topic spoken response detection, the task aiming at predicting whether a\nresponse is off-topic for the corresponding prompt, is important for an\nautomated speaking assessment system. In many real-world educational\napplications, off-topic spoken response detectors are required to achieve high\nrecall for off-topic responses not only on seen prompts but also on prompts\nthat are unseen during training. In this paper, we propose a novel approach for\noff-topic spoken response detection with high off-topic recall on both seen and\nunseen prompts. We introduce a new model, Gated Convolutional Bidirectional\nAttention-based Model (GCBiA), which applies bi-attention mechanism and\nconvolutions to extract topic words of prompts and key-phrases of responses,\nand introduces gated unit and residual connections between major layers to\nbetter represent the relevance of responses and prompts. Moreover, a new\nnegative sampling method is proposed to augment training data. Experiment\nresults demonstrate that our novel approach can achieve significant\nimprovements in detecting off-topic responses with extremely high on-topic\nrecall, for both seen and unseen prompts."}, {"title": "GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media", "authors": "Yi-Ju Lu, Cheng-Te Li", "link": "https://arxiv.org/abs/2004.11648", "summary": "This paper solves the fake news detection problem under a more realistic\nscenario on social media. Given the source short-text tweet and the\ncorresponding sequence of retweet users without text comments, we aim at\npredicting whether the source tweet is fake or not, and generating explanation\nby highlighting the evidences on suspicious retweeters and the words they\nconcern. We develop a novel neural network-based model, Graph-aware\nCo-Attention Networks (GCAN), to achieve the goal. Extensive experiments\nconducted on real tweet datasets exhibit that GCAN can significantly outperform\nstate-of-the-art methods by 16% in accuracy on average. In addition, the case\nstudies also show that GCAN can produce reasonable explanations."}, {"title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer", "authors": "Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, Kai-Wei Chang, Ahmed Hassan Awadallah", "link": "http://arxiv.org/abs/2005.00699", "summary": "Multilingual representations embed words from many languages into a single\nsemantic space such that words with similar meanings are close to each other\nregardless of the language. These embeddings have been widely used in various\nsettings, such as cross-lingual transfer, where a natural language processing\n(NLP) model trained on one language is deployed to another language. While the\ncross-lingual transfer techniques are powerful, they carry gender bias from the\nsource to target languages. In this paper, we study gender bias in multilingual\nembeddings and how it affects transfer learning for NLP applications. We create\na multilingual dataset for bias analysis and propose several ways for\nquantifying bias in multilingual representations from both the intrinsic and\nextrinsic perspectives. Experimental results show that the magnitude of bias in\nthe multilingual representations changes differently when we align the\nembeddings to different target spaces and that the alignment direction can also\nhave an influence on the bias in transfer learning. We further provide\nrecommendations for using the multilingual word representations for downstream\ntasks."}, {"title": "Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations", "authors": "Saif M. Mohammad", "link": "https://arxiv.org/abs/2005.00962", "summary": "Disparities in authorship and citations across genders can have substantial\nadverse consequences not just on the disadvantaged gender, but also on the\nfield of study as a whole. In this work, we examine female first author\npercentages and the citations to their papers in Natural Language Processing.\nWe find that only about 29% of first authors are female and only about 25% of\nlast authors are female. Notably, this percentage has not improved since the\nmid 2000s. We also show that, on average, female first authors are cited less\nthan male first authors, even when controlling for experience and area of\nresearch. We hope that recording citation and participation gaps across\ndemographic groups will improve awareness of gender gaps and encourage more\ninclusiveness and fairness in research."}, {"title": "Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus", "authors": "Luisa Bentivogli, Beatrice Savoldi, Matteo Negri, Mattia A. Di Gangi, Roldano Cattoni, Marco Turchi"}, {"title": "Generalized Entropy Regularization or: There\u2019s Nothing Special about Label Smoothing", "authors": "Clara Meister, Elizabeth Salesky, Ryan Cotterell", "link": "https://arxiv.org/abs/2005.00820", "summary": "Prior work has explored directly regularizing the output distributions of\nprobabilistic models to alleviate peaky (i.e. over-confident) predictions, a\ncommon sign of overfitting. This class of techniques, of which label smoothing\nis one, has a connection to entropy regularization. Despite the consistent\nsuccess of label smoothing across architectures and data sets in language\ngeneration tasks, two problems remain open: (1) there is little understanding\nof the underlying effects entropy regularizers have on models, and (2) the full\nspace of entropy regularization techniques is largely unexplored. We introduce\na parametric family of entropy regularizers, which includes label smoothing as\na special case, and use it to gain a better understanding of the relationship\nbetween the entropy of a model and its performance on language generation\ntasks. We also find that variance in model performance can be explained largely\nby the resulting entropy of the model. Lastly, we find that label smoothing\nprovably does not allow for sparsity in an output distribution, an undesirable\nproperty for language generation models, and therefore advise the use of other\nentropy regularization methods in its place."}, {"title": "Generalizing Natural Language Analysis through Span-relation Representations", "authors": "Zhengbao Jiang, Wei Xu, Jun Araki, Graham Neubig", "link": "https://arxiv.org/abs/1911.03822", "summary": "Natural language processing covers a wide variety of tasks predicting syntax,\nsemantics, and information content, and usually each type of output is\ngenerated with specially designed architectures. In this paper, we provide the\nsimple insight that a great variety of tasks can be represented in a single\nunified format consisting of labeling spans and relations between spans, thus a\nsingle task-independent model can be used across different tasks. We perform\nextensive experiments to test this insight on 10 disparate tasks spanning\ndependency parsing (syntax), semantic role labeling (semantics), relation\nextraction (information content), aspect based sentiment analysis (sentiment),\nand many others, achieving performance comparable to state-of-the-art\nspecialized models. We further demonstrate benefits of multi-task learning, and\nalso show that the proposed method makes it easy to analyze differences and\nsimilarities in how the model handles different tasks. Finally, we convert\nthese datasets into a unified format to build a benchmark, which provides a\nholistic testbed for evaluating future models for generalized natural language\nanalysis."}, {"title": "Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation", "authors": "Haoyu Song, Yan Wang, Wei-Nan Zhang, Xiaojiang Liu, Ting Liu", "link": "https://arxiv.org/abs/2004.07672", "summary": "Maintaining a consistent personality in conversations is quite natural for\nhuman beings, but is still a non-trivial task for machines. The persona-based\ndialogue generation task is thus introduced to tackle the\npersonality-inconsistent problem by incorporating explicit persona text into\ndialogue generation models. Despite the success of existing persona-based\nmodels on generating human-like responses, their one-stage decoding framework\ncan hardly avoid the generation of inconsistent persona words. In this work, we\nintroduce a three-stage framework that employs a generate-delete-rewrite\nmechanism to delete inconsistent words from a generated response prototype and\nfurther rewrite it to a personality-consistent one. We carry out evaluations by\nboth human and automatic metrics. Experiments on the Persona-Chat dataset show\nthat our approach achieves good performance."}, {"title": "Generating Counter Narratives against Online Hate Speech: Data and Strategies", "authors": "Serra Sinem Tekiro\u011flu, Yi-Ling Chung, Marco Guerini", "link": "https://arxiv.org/abs/2004.04216", "summary": "Recently research has started focusing on avoiding undesired effects that\ncome with content moderation, such as censorship and overblocking, when dealing\nwith hatred online. The core idea is to directly intervene in the discussion\nwith textual responses that are meant to counter the hate content and prevent\nit from further spreading. Accordingly, automation strategies, such as natural\nlanguage generation, are beginning to be investigated. Still, they suffer from\nthe lack of sufficient amount of quality data and tend to produce\ngeneric/repetitive responses. Being aware of the aforementioned limitations, we\npresent a study on how to collect responses to hate effectively, employing\nlarge scale unsupervised language models such as GPT-2 for the generation of\nsilver data, and the best annotation strategies/neural architectures that can\nbe used for data filtering before expert validation/post-editing."}, {"title": "Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs", "authors": "Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Donghwan Kim, Sung Ju Hwang", "link": "https://arxiv.org/abs/2005.13837", "summary": "One of the most crucial challenges in question answering (QA) is the scarcity\nof labeled data, since it is costly to obtain question-answer (QA) pairs for a\ntarget text domain with human annotation. An alternative approach to tackle the\nproblem is to use automatically generated QA pairs from either the problem\ncontext or from large amount of unstructured texts (e.g. Wikipedia). In this\nwork, we propose a hierarchical conditional variational autoencoder (HCVAE) for\ngenerating QA pairs given unstructured texts as contexts, while maximizing the\nmutual information between generated QA pairs to ensure their consistency. We\nvalidate our Information Maximizing Hierarchical Conditional Variational\nAutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the\nperformance of the QA model (BERT-base) using only the generated QA pairs\n(QA-based evaluation) or by using both the generated and human-labeled pairs\n(semi-supervised learning) for training, against state-of-the-art baseline\nmodels. The results show that our model obtains impressive performance gains\nover all baselines on both tasks, using only a fraction of data for training."}, {"title": "Generating Fact Checking Explanations", "authors": "Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein", "link": "https://arxiv.org/abs/2004.05773", "summary": "Most existing work on automated fact checking is concerned with predicting\nthe veracity of claims based on metadata, social network spread, language used\nin claims, and, more recently, evidence supporting or denying claims. A crucial\npiece of the puzzle that is still missing is to understand how to automate the\nmost elaborate part of the process -- generating justifications for verdicts on\nclaims. This paper provides the first study of how these explanations can be\ngenerated automatically based on available claim context, and how this task can\nbe modelled jointly with veracity prediction. Our results indicate that\noptimising both objectives at the same time, rather than training them\nseparately, improves the performance of a fact checking system. The results of\na manual evaluation further suggest that the informativeness, coverage and\noverall quality of the generated explanations are also improved in the\nmulti-task model."}, {"title": "Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection", "authors": "Hanjie Chen, Guangtao Zheng, Yangfeng Ji", "link": "https://arxiv.org/abs/2004.02015", "summary": "Generating explanations for neural networks has become crucial for their\napplications in real-world with respect to reliability and trustworthiness. In\nnatural language processing, existing methods usually provide important\nfeatures which are words or phrases selected from an input text as an\nexplanation, but ignore the interactions between them. It poses challenges for\nhumans to interpret an explanation and connect it to model prediction. In this\nwork, we build hierarchical explanations by detecting feature interactions.\nSuch explanations visualize how words and phrases are combined at different\nlevels of the hierarchy, which can help users understand the decision-making of\nblack-box models. The proposed method is evaluated with three neural text\nclassifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic\nand human evaluations. Experiments show the effectiveness of the proposed\nmethod in providing explanations that are both faithful to models and\ninterpretable to humans."}, {"title": "Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy", "authors": "Xiexiong Lin, Weiyu Jian, Jianshan He, Taifeng Wang, Wei Chu"}, {"title": "Generative Semantic Hashing Enhanced via Boltzmann Machines", "authors": "Lin Zheng, Qinliang Su, Dinghan Shen, Changyou Chen"}, {"title": "GLUECoS: An Evaluation Benchmark for Code-Switched NLP", "authors": "Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, Monojit Choudhury", "link": "", "summary": ""}, {"title": "GoEmotions: A Dataset of Fine-Grained Emotions", "authors": "Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, Sujith Ravi", "link": "https://arxiv.org/abs/2005.00547", "summary": "Understanding emotion expressed in language has a wide range of applications,\nfrom building empathetic chatbots to detecting harmful online behavior.\nAdvancement in this area can be improved using large-scale datasets with a\nfine-grained typology, adaptable to multiple downstream tasks. We introduce\nGoEmotions, the largest manually annotated dataset of 58k English Reddit\ncomments, labeled for 27 emotion categories or Neutral. We demonstrate the high\nquality of the annotations via Principal Preserved Component Analysis. We\nconduct transfer learning experiments with existing emotion benchmarks to show\nthat our dataset generalizes well to other domains and different emotion\ntaxonomies. Our BERT-based model achieves an average F1-score of .46 across our\nproposed taxonomy, leaving much room for improvement."}, {"title": "Good-Enough Compositional Data Augmentation", "authors": "Jacob Andreas", "link": "https://arxiv.org/abs/1904.09545", "summary": "We propose a simple data augmentation protocol aimed at providing a\ncompositional inductive bias in conditional and unconditional sequence models.\nUnder this protocol, synthetic training examples are constructed by taking real\ntraining examples and replacing (possibly discontinuous) fragments with other\nfragments that appear in at least one similar environment. The protocol is\nmodel-agnostic and useful for a variety of tasks. Applied to neural\nsequence-to-sequence models, it reduces error rate by as much as 87% on\ndiagnostic tasks from the SCAN dataset and 16% on a semantic parsing task.\nApplied to n-gram language models, it reduces perplexity by roughly 1% on small\ncorpora in several languages."}, {"title": "Graph Neural News Recommendation with Unsupervised Preference Disentanglement", "authors": "Linmei Hu, Siyong Xu, Chen Li, Cheng Yang, Chuan Shi, Nan Duan, Xing Xie, Ming Zhou", "link": "", "summary": ""}, {"title": "Graph-to-Tree Learning for Solving Math Word Problems", "authors": "Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, Ee-Peng Lim"}, {"title": "Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs", "authors": "Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu", "link": "https://arxiv.org/abs/1911.02707", "summary": "Human conversations naturally evolve around related concepts and scatter to\nmulti-hop concepts. This paper presents a new conversation generation model,\nConceptFlow, which leverages commonsense knowledge graphs to explicitly model\nconversation flows. By grounding conversations to the concept space,\nConceptFlow represents the potential conversation flow as traverses in the\nconcept space along commonsense relations. The traverse is guided by graph\nattentions in the concept graph, moving towards more meaningful directions in\nthe concept space, in order to generate more semantic and informative\nresponses. Experiments on Reddit conversations demonstrate ConceptFlow's\neffectiveness over previous knowledge-aware conversation models and GPT-2 based\nmodels while using 70% fewer parameters, confirming the advantage of explicit\nmodeling conversation structures. All source codes of this work are available\nat https://github.com/thunlp/ConceptFlow."}, {"title": "Grounding Conversations with Improvised Dialogues", "authors": "Hyundong Cho, Jonathan May", "link": "https://arxiv.org/abs/2004.09544", "summary": "Effective dialogue involves grounding, the process of establishing mutual\nknowledge that is essential for communication between people. Modern dialogue\nsystems are not explicitly trained to build common ground, and therefore\noverlook this important aspect of communication. Improvisational theater\n(improv) intrinsically contains a high proportion of dialogue focused on\nbuilding common ground, and makes use of the yes-and principle, a strong\ngrounding speech act, to establish coherence and an actionable objective\nreality. We collect a corpus of more than 26,000 yes-and turns, transcribing\nthem from improv dialogues and extracting them from larger, but more sparsely\npopulated movie script dialogue corpora, via a bootstrapped classifier. We\nfine-tune chit-chat dialogue systems with our corpus to encourage more\ngrounded, relevant conversation and confirm these findings with human\nevaluations."}, {"title": "Guiding Variational Response Generator to Exploit Persona", "authors": "Bowen Wu, Mengyuan Li, Zongsheng Wang, Yifu Chen, Derek F. Wong, Qihang Feng, Junhong Huang, Baoxun Wang", "link": "https://arxiv.org/abs/1911.02390", "summary": "Leveraging persona information of users in Neural Response Generators (NRG)\nto perform personalized conversations has been considered as an attractive and\nimportant topic in the research of conversational agents over the past few\nyears. Despite of the promising progresses achieved by recent studies in this\nfield, persona information tends to be incorporated into neural networks in the\nform of user embeddings, with the expectation that the persona can be involved\nvia the End-to-End learning. This paper proposes to adopt the\npersonality-related characteristics of human conversations into variational\nresponse generators, by designing a specific conditional variational\nautoencoder based deep model with two new regularization terms employed to the\nloss function, so as to guide the optimization towards the direction of\ngenerating both persona-aware and relevant responses. Besides, to reasonably\nevaluate the performances of various persona modeling approaches, this paper\nfurther presents three direct persona-oriented metrics from different\nperspectives. The experimental results have shown that our proposed methodology\ncan notably improve the performance of persona-aware response generation, and\nthe metrics are reasonable to evaluate the results."}, {"title": "Handling Rare Entities for Neural Sequence Labeling", "authors": "Yangming Li, Han Li, Kaisheng Yao, Xiaolong Li"}, {"title": "Hard-Coded Gaussian Attention for Neural Machine Translation", "authors": "Weiqiu You, Simeng Sun, Mohit Iyyer", "link": "https://arxiv.org/abs/2005.00742", "summary": "Recent work has questioned the importance of the Transformer's multi-headed\nattention for achieving high translation quality. We push further in this\ndirection by developing a \"hard-coded\" attention variant without any learned\nparameters. Surprisingly, replacing all learned self-attention heads in the\nencoder and decoder with fixed, input-agnostic Gaussian distributions minimally\nimpacts BLEU scores across four different language pairs. However, additionally\nhard-coding cross attention (which connects the decoder to the encoder)\nsignificantly lowers BLEU, suggesting that it is more important than\nself-attention. Much of this BLEU drop can be recovered by adding just a single\nlearned cross attention head to an otherwise hard-coded Transformer. Taken as a\nwhole, our results offer insight into which components of the Transformer are\nactually important, which we hope will guide future work into the development\nof simpler and more efficient attention-based models."}, {"title": "Harnessing the linguistic signal to predict scalar inferences", "authors": "Sebastian Schuster, Yuxing Chen, Judith Degen", "link": "https://arxiv.org/abs/1910.14254", "summary": "Pragmatic inferences often subtly depend on the presence or absence of\nlinguistic features. For example, the presence of a partitive construction (of\nthe) increases the strength of a so-called scalar inference: listeners perceive\nthe inference that Chris did not eat all of the cookies to be stronger after\nhearing \"Chris ate some of the cookies\" than after hearing the same utterance\nwithout a partitive, \"Chris ate some cookies.\" In this work, we explore to what\nextent neural network sentence encoders can learn to predict the strength of\nscalar inferences. We first show that an LSTM-based sentence encoder trained on\nan English dataset of human inference strength ratings is able to predict\nratings with high accuracy (r=0.78). We then probe the model's behavior using\nmanually constructed minimal sentence pairs and corpus data. We find that the\nmodel inferred previously established associations between linguistic features\nand inference strength, suggesting that the model learns to use linguistic\nfeatures to predict pragmatic inferences."}, {"title": "Harvesting and Refining Question-Answer Pairs for Unsupervised QA", "authors": "Zhongli Li, Wenhui Wang, Li Dong, Furu Wei, Ke Xu", "link": "https://arxiv.org/abs/2005.02925", "summary": "Question Answering (QA) has shown great success thanks to the availability of\nlarge-scale datasets and the effectiveness of neural models. Recent research\nworks have attempted to extend these successes to the settings with few or no\nlabeled data available. In this work, we introduce two approaches to improve\nunsupervised QA. First, we harvest lexically and syntactically divergent\nquestions from Wikipedia to automatically construct a corpus of question-answer\npairs (named as RefQA). Second, we take advantage of the QA model to extract\nmore appropriate answers, which iteratively refines data over RefQA. We conduct\nexperiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to\nmanually annotated data. Our approach outperforms previous unsupervised\napproaches by a large margin and is competitive with early supervised models.\nWe also show the effectiveness of our approach in the few-shot learning\nsetting."}, {"title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing", "authors": "Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han", "link": "https://arxiv.org/abs/2005.14187", "summary": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but\nthey are difficult to be deployed on hardware due to the intensive computation.\nTo enable low-latency inference on resource-constrained hardware platforms, we\npropose to design Hardware-Aware Transformers (HAT) with neural architecture\nsearch. We first construct a large design space with $\\textit{arbitrary\nencoder-decoder attention}$ and $\\textit{heterogeneous layers}$. Then we train\na $\\textit{SuperTransformer}$ that covers all candidates in the design space,\nand efficiently produces many $\\textit{SubTransformers}$ with weight sharing.\nFinally, we perform an evolutionary search with a hardware latency constraint\nto find a specialized $\\textit{SubTransformer}$ dedicated to run fast on the\ntarget hardware. Extensive experiments on four machine translation tasks\ndemonstrate that HAT can discover efficient models for different hardware (CPU,\nGPU, IoT device). When running WMT'14 translation task on Raspberry Pi-4, HAT\ncan achieve $\\textbf{3}\\times$ speedup, $\\textbf{3.7}\\times$ smaller size over\nbaseline Transformer; $\\textbf{2.7}\\times$ speedup, $\\textbf{3.6}\\times$\nsmaller size over Evolved Transformer with $\\textbf{12,041}\\times$ less search\ncost and no performance loss. HAT code is\nhttps://github.com/mit-han-lab/hardware-aware-transformers.git"}, {"title": "He said \u201cwho\u2019s gonna take care of your children when you are at ACL?\u201d: Reported Sexist Acts are Not Sexist", "authors": "Patricia Chiril, V\u00e9ronique Moriceau, Farah Benamara, Alda Mari, Gloria Origgi, Marl\u00e8ne Coulomb-Gully"}, {"title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization", "authors": "Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang", "link": "http://arxiv.org/abs/2004.12393", "summary": "As a crucial step in extractive document summarization, learning\ncross-sentence relations has been explored by a plethora of approaches. An\nintuitive way is to put them in the graph-based neural network, which has a\nmore complex structure for capturing inter-sentence relationships. In this\npaper, we present a heterogeneous graph-based neural network for extractive\nsummarization (HeterSumGraph), which contains semantic nodes of different\ngranularity levels apart from sentences. These additional nodes act as the\nintermediary between sentences and enrich the cross-sentence relations.\nBesides, our graph structure is flexible in natural extension from a\nsingle-document setting to multi-document via introducing document nodes. To\nour knowledge, we are the first one to introduce different types of nodes into\ngraph-based neural networks for extractive document summarization and perform a\ncomprehensive qualitative analysis to investigate their benefits. The code will\nbe released on Github"}, {"title": "Heterogeneous Graph Transformer for Graph-to-Sequence Learning", "authors": "Shaowei Yao, Tianming Wang, Xiaojun Wan", "link": "", "summary": ""}, {"title": "Hierarchical Entity Typing via Multi-level Learning to Rank", "authors": "Tongfei Chen, Yunmo Chen, Benjamin Van Durme", "link": "https://arxiv.org/abs/2004.02286", "summary": "We propose a novel method for hierarchical entity classification that\nembraces ontological structure at both training and during prediction. At\ntraining, our novel multi-level learning-to-rank loss compares positive types\nagainst negative siblings according to the type tree. During prediction, we\ndefine a coarse-to-fine decoder that restricts viable candidates at each level\nof the ontology based on already predicted parent type(s). We achieve\nstate-of-the-art across multiple datasets, particularly with respect to strict\naccuracy."}, {"title": "Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention", "authors": "Veronica Lynn, Niranjan Balasubramanian, H. Andrew Schwartz"}, {"title": "Hierarchy-Aware Global Model for Hierarchical Text Classification", "authors": "Jie Zhou, Chunping Ma, Dingkun Long, Guangwei Xu, Ning Ding, Haoyu Zhang, Pengjun Xie, Gongshen Liu"}, {"title": "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks", "authors": "Yekun Chai, Shuo Jin, Xinwen Hou", "link": "http://arxiv.org/abs/2004.08178", "summary": "Self-attention mechanisms have made striking state-of-the-art (SOTA) progress\nin various sequence learning tasks, standing on the multi-headed dot product\nattention by attending to all the global contexts at different locations.\nThrough a pseudo information highway, we introduce a gated component\nself-dependency units (SDU) that incorporates LSTM-styled gating units to\nreplenish internal semantic importance within the multi-dimensional latent\nspace of individual representations. The subsidiary content-based SDU gates\nallow for the information flow of modulated latent embeddings through skipped\nconnections, leading to a clear margin of convergence speed with gradient\ndescent algorithms. We may unveil the role of gating mechanism to aid in the\ncontext-based Transformer modules, with hypothesizing that SDU gates,\nespecially on shallow layers, could push it faster to step towards suboptimal\npoints during the optimization process."}, {"title": "Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation", "authors": "Liting Liu, Jie Liu, Wenzheng Zhang, Ziming Chi, Wenxuan Shi, Yalou Huang"}, {"title": "History for Visual Dialog: Do we really need it?", "authors": "Shubham Agarwal, Trung Bui, Joon-Young Lee, Ioannis Konstas, Verena Rieser", "link": "https://arxiv.org/abs/2005.07493", "summary": "Visual Dialog involves \"understanding\" the dialog history (what has been\ndiscussed previously) and the current question (what is asked), in addition to\ngrounding information in the image, to generate the correct response. In this\npaper, we show that co-attention models which explicitly encode dialog history\noutperform models that don't, achieving state-of-the-art performance (72 % NDCG\non val set). However, we also expose shortcomings of the crowd-sourcing dataset\ncollection procedure by showing that history is indeed only required for a\nsmall amount of the data and that the current evaluation metric encourages\ngeneric replies. To that end, we propose a challenging subset (VisDialConv) of\nthe VisDial val set and provide a benchmark of 63% NDCG."}, {"title": "Hooks in the Headline: Learning to Generate Headlines with Controlled Styles", "authors": "Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, Peter Szolovits", "link": "https://arxiv.org/abs/2004.01980", "summary": "Current summarization systems only produce plain, factual headlines, but do\nnot meet the practical needs of creating memorable titles to increase exposure.\nWe propose a new task, Stylistic Headline Generation (SHG), to enrich the\nheadlines with three style options (humor, romance and clickbait), in order to\nattract more readers. With no style-specific article-headline pair (only a\nstandard headline summarization dataset and mono-style corpora), our method\nTitleStylist generates style-specific headlines by combining the summarization\nand reconstruction tasks into a multitasking framework. We also introduced a\nnovel parameter sharing scheme to further disentangle the style from the text.\nThrough both automatic and human evaluation, we demonstrate that TitleStylist\ncan generate relevant, fluent headlines with three target styles: humor,\nromance, and clickbait. The attraction score of our model generated headlines\nsurpasses that of the state-of-the-art summarization model by 9.68%, and even\noutperforms human-written references."}, {"title": "How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems", "authors": "Archiki Prasad, Preethi Jyothi"}, {"title": "How does BERT\u2019s attention change when you fine-tune? An analysis methodology and a case study in negation scope", "authors": "Yiyun Zhao, Steven Bethard"}, {"title": "How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence", "authors": "Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, Maosong Sun", "link": "https://arxiv.org/abs/2004.12158", "summary": "Legal Artificial Intelligence (LegalAI) focuses on applying the technology of\nartificial intelligence, especially natural language processing, to benefit\ntasks in the legal domain. In recent years, LegalAI has drawn increasing\nattention rapidly from both AI researchers and legal professionals, as LegalAI\nis beneficial to the legal system for liberating legal professionals from a\nmaze of paperwork. Legal professionals often think about how to solve tasks\nfrom rule-based and symbol-based methods, while NLP researchers concentrate\nmore on data-driven and embedding methods. In this paper, we introduce the\nhistory, the current state, and the future directions of research in LegalAI.\nWe illustrate the tasks from the perspectives of legal professionals and NLP\nresearchers and show several representative applications in LegalAI. We conduct\nexperiments and provide an in-depth analysis of the advantages and\ndisadvantages of existing works to explore possible future directions. You can\nfind the implementation of our work from https://github.com/thunlp/CLAIM."}, {"title": "How Does Selective Mechanism Improve Self-Attention Networks?", "authors": "Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, Zhaopeng Tu", "link": "http://arxiv.org/abs/2005.00979", "summary": "Self-attention networks (SANs) with selective mechanism has produced\nsubstantial improvements in various NLP tasks by concentrating on a subset of\ninput words. However, the underlying reasons for their strong performance have\nnot been well explained. In this paper, we bridge the gap by assessing the\nstrengths of selective SANs (SSANs), which are implemented with a flexible and\nuniversal Gumbel-Softmax. Experimental results on several representative NLP\ntasks, including natural language inference, semantic role labelling, and\nmachine translation, show that SSANs consistently outperform the standard SANs.\nThrough well-designed probing experiments, we empirically validate that the\nimprovement of SSANs can be attributed in part to mitigating two commonly-cited\nweaknesses of SANs: word order encoding and structure modeling. Specifically,\nthe selective mechanism improves SANs by paying more attention to content words\nthat contribute to the meaning of the sentence. The code and data are released\nat https://github.com/xwgeng/SSAN."}, {"title": "How to Ask Good Questions? Try to Leverage Paraphrases", "authors": "Xin Jia, Wenjie Zhou, Xu Sun, Yunfang Wu"}, {"title": "Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?", "authors": "Cansu Sen, Thomas Hartvigsen, Biao Yin, Xiangnan Kong, Elke Rundensteiner"}, {"title": "Hyperbolic Capsule Networks for Multi-Label Classification", "authors": "Boli Chen, Xin Huang, Lin Xiao, Liping Jing", "link": "", "summary": ""}, {"title": "HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding", "authors": "Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu, Weifeng Chong"}, {"title": "Image-Chat: Engaging Grounded Conversations", "authors": "Kurt Shuster, Samuel Humeau, Antoine Bordes, Jason Weston", "link": "https://arxiv.org/abs/1811.00945", "summary": "To achieve the long-term goal of machines being able to engage humans in\nconversation, our models should captivate the interest of their speaking\npartners. Communication grounded in images, whereby a dialogue is conducted\nbased on a given photo, is a setup naturally appealing to humans (Hu et al.,\n2014). In this work we study large-scale architectures and datasets for this\ngoal. We test a set of neural architectures using state-of-the-art image and\ntext representations, considering various ways to fuse the components. To test\nsuch models, we collect a dataset of grounded human-human conversations, where\nspeakers are asked to play roles given a provided emotional mood or style, as\nthe use of such traits is also a key factor in engagingness (Guo et al., 2019).\nOur dataset, Image-Chat, consists of 202k dialogues over 202k images using 215\npossible style traits. Automatic metrics and human evaluations of engagingness\nshow the efficacy of our approach; in particular, we obtain state-of-the-art\nperformance on the existing IGC task, and our best performing model is almost\non par with humans on the Image-Chat test set (preferred 47.7% of the time)."}, {"title": "IMoJIE: Iterative Memory-Based Joint Open Information Extraction", "authors": "Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam -, Soumen Chakrabarti", "link": "https://arxiv.org/abs/2005.08178", "summary": "While traditional systems for Open Information Extraction were statistical\nand rule-based, recently neural models have been introduced for the task. Our\nwork builds upon CopyAttention, a sequence generation OpenIE model (Cui et.\nal., 2018). Our analysis reveals that CopyAttention produces a constant number\nof extractions per sentence, and its extracted tuples often express redundant\ninformation.\n  We present IMoJIE, an extension to CopyAttention, which produces the next\nextraction conditioned on all previously extracted tuples. This approach\novercomes both shortcomings of CopyAttention, resulting in a variable number of\ndiverse extractions per sentence. We train IMoJIE on training data bootstrapped\nfrom extractions of several non-neural systems, which have been automatically\nfiltered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by\nabout 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a\nnew state of the art for the task."}, {"title": "Improved Natural Language Generation via Loss Truncation", "authors": "Daniel Kang, Tatsunori Hashimoto", "link": "https://arxiv.org/abs/2004.14589", "summary": "Neural language models are usually trained to match the distributional\nproperties of a large-scale corpus by minimizing the log loss. While\nstraightforward to optimize, this approach forces the model to reproduce all\nvariations in the dataset, including noisy and invalid references (e.g.,\nmisannotation and hallucinated facts). Worse, the commonly used log loss is\noverly sensitive to such phenomena and even a small fraction of noisy data can\ndegrade performance. In this work, we show that the distinguishability of the\nmodels and reference serves as a principled and robust alternative for handling\ninvalid references. To optimize distinguishability, we propose loss truncation,\nwhich adaptively removes high loss examples during training. We show this is as\neasy to optimize as log loss and tightly bounds distinguishability under noise.\nEmpirically, we demonstrate that loss truncation outperforms existing baselines\non distinguishability on a summarization task, and show that samples generated\nby the loss truncation model have factual accuracy ratings that exceed those of\nbaselines and match human references."}, {"title": "Improving Adversarial Text Generation by Modeling the Distant Future", "authors": "Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen, Lawrence Carin", "link": "https://arxiv.org/abs/2005.01279", "summary": "Auto-regressive text generation models usually focus on local fluency, and\nmay cause inconsistent semantic meaning in long text generation. Further,\nautomatically generating words with similar semantics is challenging, and\nhand-crafted linguistic rules are difficult to apply. We consider a text\nplanning scheme and present a model-based imitation-learning approach to\nalleviate the aforementioned issues. Specifically, we propose a novel guider\nnetwork to focus on the generative process over a longer horizon, which can\nassist next-word prediction and provide intermediate rewards for generator\noptimization. Extensive experiments demonstrate that the proposed method leads\nto improved performance."}, {"title": "Improving Chinese Word Segmentation with Wordhood Memory Networks", "authors": "Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang, Yonggang Wang"}, {"title": "Improving Disentangled Text Representation Learning with Information-Theoretic Guidance", "authors": "Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, Lawrence Carin", "link": "https://arxiv.org/abs/2006.00693", "summary": "Learning disentangled representations of natural language is essential for\nmany NLP tasks, e.g., conditional text generation, style transfer, personalized\ndialogue systems, etc. Similar problems have been studied extensively for other\nforms of data, such as images and videos. However, the discrete nature of\nnatural language makes the disentangling of textual representations more\nchallenging (e.g., the manipulation over the data space cannot be easily\nachieved). Inspired by information theory, we propose a novel method that\neffectively manifests disentangled representations of text, without any\nsupervision on semantics. A new mutual information upper bound is derived and\nleveraged to measure dependence between style and content. By minimizing this\nupper bound, the proposed method induces style and content embeddings into two\nindependent low-dimensional spaces. Experiments on both conditional text\ngeneration and text-style transfer demonstrate the high quality of our\ndisentangled representation in terms of content and style preservation."}, {"title": "Improving Disfluency Detection by Self-Training a Self-Attentive Model", "authors": "Paria Jamshid Lou, Mark Johnson", "link": "http://arxiv.org/abs/2004.05323", "summary": "Self-attentive neural syntactic parsers using contextualized word embeddings\n(e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing\nand disfluency detection in speech transcripts. Since the contextualized word\nembeddings are pre-trained on a large amount of unlabeled data, using\nadditional unlabeled data to train a neural model might seem redundant.\nHowever, we show that self-training - a semi-supervised technique for\nincorporating unlabeled data - sets a new state-of-the-art for the\nself-attentive parser on disfluency detection, demonstrating that self-training\nprovides benefits orthogonal to the pre-trained contextualized word\nrepresentations. We also show that ensembling self-trained parsers provides\nfurther gains for disfluency detection."}, {"title": "Improving Event Detection via Open-domain Trigger Knowledge", "authors": "Meihan Tong, Bin Xu, Shuai Wang, Yixin Cao, Lei Hou, Juanzi Li, Jun Xie"}, {"title": "Improving Image Captioning Evaluation by Considering Inter References Variance", "authors": "Yanzhi Yi, Hangyu Deng, Jinglu Hu"}, {"title": "Improving Image Captioning with Better Use of Caption", "authors": "Zhan Shi, Xu Zhou, Xipeng Qiu, Xiaodan Zhu", "link": "", "summary": ""}, {"title": "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation", "authors": "Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich", "link": "https://arxiv.org/abs/2004.11867", "summary": "Massively multilingual models for neural machine translation (NMT) are\ntheoretically attractive, but often underperform bilingual models and deliver\npoor zero-shot translations. In this paper, we explore ways to improve them. We\nargue that multilingual NMT requires stronger modeling capacity to support\nlanguage pairs with varying typological characteristics, and overcome this\nbottleneck via language-specific components and deepening NMT architectures. We\nidentify the off-target translation issue (i.e. translating into a wrong target\nlanguage) as the major source of the inferior zero-shot performance, and\npropose random online backtranslation to enforce the translation of unseen\ntraining language pairs. Experiments on OPUS-100 (a novel multilingual dataset\nwith 100 languages) show that our approach substantially narrows the\nperformance gap with bilingual models in both one-to-many and many-to-many\nsettings, and improves zero-shot performance by ~10 BLEU, approaching\nconventional pivot-based methods."}, {"title": "Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings", "authors": "Apoorv Saxena, Aditay Tripathi, Partha Talukdar"}, {"title": "Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer", "authors": "Jianfei Yu, Jing Jiang, Li Yang, Rui Xia"}, {"title": "Improving Neural Machine Translation with Soft Template Prediction", "authors": "Jian Yang, Shuming Ma, Dongdong Zhang, Zhoujun Li, Ming Zhou"}, {"title": "Improving Segmentation for Technical Support Problems", "authors": "Kushal Chauhan, Abhirut Gupta", "link": "https://arxiv.org/abs/2005.11055", "summary": "Technical support problems are often long and complex. They typically contain\nuser descriptions of the problem, the setup, and steps for attempted\nresolution. Often they also contain various non-natural language text elements\nlike outputs of commands, snippets of code, error messages or stack traces.\nThese elements contain potentially crucial information for problem resolution.\nHowever, they cannot be correctly parsed by tools designed for natural\nlanguage. In this paper, we address the problem of segmentation for technical\nsupport questions. We formulate the problem as a sequence labelling task, and\nstudy the performance of state of the art approaches. We compare this against\nan intuitive contextual sentence-level classification baseline, and a state of\nthe art supervised text-segmentation approach. We also introduce a novel\ncomponent of combining contextual embeddings from multiple language models\npre-trained on different data sources, which achieves a marked improvement over\nusing embeddings from a single pre-trained language model. Finally, we also\ndemonstrate the usefulness of such segmentation with improvements on the\ndownstream task of answer retrieval."}, {"title": "Improving Transformer Models by Reordering their Sublayers", "authors": "Ofir Press, Noah A. Smith, Omer Levy", "link": "https://arxiv.org/abs/1911.03864", "summary": "Multilayer transformer networks consist of interleaved self-attention and\nfeedforward sublayers. Could ordering the sublayers in a different pattern lead\nto better performance? We generate randomly ordered transformers and train them\nwith the language modeling objective. We observe that some of these models are\nable to achieve better performance than the interleaved baseline, and that\nthose successful variants tend to have more self-attention at the bottom and\nmore feedforward sublayers at the top. We propose a new transformer pattern\nthat adheres to this property, the sandwich transformer, and show that it\nimproves perplexity on multiple word-level and character-level language\nmodeling benchmarks, at no cost in parameters, memory, or training time.\nHowever, the sandwich reordering pattern does not guarantee performance gains\nacross every task, as we demonstrate on machine translation models. Instead, we\nsuggest that further exploration of task-specific sublayer reorderings is\nneeded in order to unlock additional gains."}, {"title": "Improving Truthfulness of Headline Generation", "authors": "Kazuki Matsumaru, Sho Takase, Naoaki Okazaki", "link": "https://arxiv.org/abs/2005.00882", "summary": "Most studies on abstractive summarization report ROUGE scores between system\nand reference summaries. However, we have a concern about the truthfulness of\ngenerated summaries: whether all facts of a generated summary are mentioned in\nthe source text. This paper explores improving the truthfulness in headline\ngeneration on two popular datasets. Analyzing headlines generated by the\nstate-of-the-art encoder-decoder model, we show that the model sometimes\ngenerates untruthful headlines. We conjecture that one of the reasons lies in\nuntruthful supervision data used for training the model. In order to quantify\nthe truthfulness of article-headline pairs, we consider the textual entailment\nof whether an article entails its headline. After confirming quite a few\nuntruthful instances in the datasets, this study hypothesizes that removing\nuntruthful instances from the supervision data may remedy the problem of the\nuntruthful behaviors of the model. Building a binary classifier that predicts\nan entailment relation between an article and its headline, we filter out\nuntruthful instances from the supervision data. Experimental results\ndemonstrate that the headline generation model trained on filtered supervision\ndata shows no clear difference in ROUGE scores but remarkable improvements in\nautomatic and manual evaluations of the generated headlines."}, {"title": "In Layman\u2019s Terms: Semi-Open Relation Extraction from Scientific Texts", "authors": "Ruben Kruiper, Julian Vincent, Jessica Chen-Burger, Marc Desmulliez, Ioannis Konstas", "link": "https://arxiv.org/abs/2005.07751", "summary": "Information Extraction (IE) from scientific texts can be used to guide\nreaders to the central information in scientific documents. But narrow IE\nsystems extract only a fraction of the information captured, and Open IE\nsystems do not perform well on the long and complex sentences encountered in\nscientific texts. In this work we combine the output of both types of systems\nto achieve Semi-Open Relation Extraction, a new task that we explore in the\nBiology domain. First, we present the Focused Open Biological Information\nExtraction (FOBIE) dataset and use FOBIE to train a state-of-the-art narrow\nscientific IE system to extract trade-off relations and arguments that are\ncentral to biology texts. We then run both the narrow IE system and a\nstate-of-the-art Open IE system on a corpus of 10k open-access scientific\nbiological texts. We show that a significant amount (65%) of erroneous and\nuninformative Open IE extractions can be filtered using narrow IE extractions.\nFurthermore, we show that the retained extractions are significantly more often\ninformative to a reader."}, {"title": "In Neural Machine Translation, What Does Transfer Learning Transfer?", "authors": "Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield, Rico Sennrich", "link": "", "summary": ""}, {"title": "Inflecting when there\u2019s no majority: Limitations of encoder-decoder neural networks as cognitive models for German plurals", "authors": "Kate McCurdy, Sharon Goldwater, Adam Lopez", "link": "https://arxiv.org/abs/2005.08826", "summary": "Can artificial neural networks learn to represent inflectional morphology and\ngeneralize to new words as human speakers do? Kirov and Cotterell (2018) argue\nthat the answer is yes: modern Encoder-Decoder (ED) architectures learn\nhuman-like behavior when inflecting English verbs, such as extending the\nregular past tense form -(e)d to novel words. However, their work does not\naddress the criticism raised by Marcus et al. (1995): that neural models may\nlearn to extend not the regular, but the most frequent class -- and thus fail\non tasks like German number inflection, where infrequent suffixes like -s can\nstill be productively generalized.\n  To investigate this question, we first collect a new dataset from German\nspeakers (production and ratings of plural forms for novel nouns) that is\ndesigned to avoid sources of information unavailable to the ED model. The\nspeaker data show high variability, and two suffixes evince 'regular' behavior,\nappearing more often with phonologically atypical inputs. Encoder-decoder\nmodels do generalize the most frequently produced plural class, but do not show\nhuman-like variability or 'regular' extension of these other plural markers. We\nconclude that modern neural models may still struggle with minority-class\ngeneralization."}, {"title": "Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models", "authors": "Kaiji Lu, Piotr Mardziel, Klas Leino, Matt Fredrikson, Anupam Datta", "link": "https://arxiv.org/abs/2005.01190", "summary": "LSTM-based recurrent neural networks are the state-of-the-art for many\nnatural language processing (NLP) tasks. Despite their performance, it is\nunclear whether, or how, LSTMs learn structural features of natural languages\nsuch as subject-verb number agreement in English. Lacking this understanding,\nthe generality of LSTM performance on this task and their suitability for\nrelated tasks remains uncertain. Further, errors cannot be properly attributed\nto a lack of structural capability, training data omissions, or other\nexceptional faults. We introduce *influence paths*, a causal account of\nstructural properties as carried by paths across gates and neurons of a\nrecurrent neural network. The approach refines the notion of influence (the\nsubject's grammatical number has influence on the grammatical number of the\nsubsequent verb) into a set of gate or neuron-level paths. The set localizes\nand segments the concept (e.g., subject-verb agreement), its constituent\nelements (e.g., the subject), and related or interfering elements (e.g.,\nattractors). We exemplify the methodology on a widely-studied multi-layer LSTM\nlanguage model, demonstrating its accounting for subject-verb number agreement.\nThe results offer both a finer and a more complete view of an LSTM's handling\nof this structural aspect of the English language than prior results based on\ndiagnostic classifiers and ablation."}, {"title": "Information-Theoretic Probing for Linguistic Structure", "authors": "Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, Ryan Cotterell", "link": "http://arxiv.org/abs/2004.03061", "summary": "The success of neural networks on a diverse set of NLP tasks has led\nresearchers to question how much these networks actually ``know'' about natural\nlanguage. Probes are a natural way of assessing this. When probing, a\nresearcher chooses a linguistic task and trains a supervised model to predict\nannotations in that linguistic task from the network's learned representations.\nIf the probe does well, the researcher may conclude that the representations\nencode knowledge related to the task. A commonly held belief is that using\nsimpler models as probes is better; the logic is that simpler models will\nidentify linguistic structure, but not learn the task itself. We propose an\ninformation-theoretic operationalization of probing as estimating mutual\ninformation that contradicts this received wisdom: one should always select the\nhighest performing probe one can, even if it is more complex, since it will\nresult in a tighter estimate, and thus reveal more of the linguistic\ninformation inherent in the representation. The experimental portion of our\npaper focuses on empirically estimating the mutual information between a\nlinguistic property and BERT, comparing these estimates to several baselines.\nWe evaluate on a set of ten typologically diverse languages often\nunderrepresented in NLP research---plus English---totalling eleven languages."}, {"title": "INFOTABS: Inference on Tables as Semi-structured Data", "authors": "Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar", "link": "", "summary": ""}, {"title": "Injecting Numerical Reasoning Skills into Language Models", "authors": "Mor Geva, Ankit Gupta, Jonathan Berant", "link": "https://arxiv.org/abs/2004.04487", "summary": "Large pre-trained language models (LMs) are known to encode substantial\namounts of linguistic information. However, high-level reasoning skills, such\nas numerical reasoning, are difficult to learn from a language-modeling\nobjective only. Consequently, existing models for numerical reasoning have used\nspecialized architectures with limited flexibility. In this work, we show that\nnumerical reasoning is amenable to automatic data generation, and thus one can\ninject this skill into pre-trained LMs, by generating large amounts of data,\nand training in a multi-task setup. We show that pre-training our model,\nGenBERT, on this data, dramatically improves performance on DROP (49.3\n$\\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art\nmodels of comparable size, while using a simple and general-purpose\nencoder-decoder architecture. Moreover, GenBERT generalizes well to math word\nproblem datasets, while maintaining high performance on standard RC tasks. Our\napproach provides a general recipe for injecting skills into large pre-trained\nLMs, whenever the skill is amenable to automatic data augmentation."}, {"title": "INSET: Sentence Infilling with INter-SEntential Transformer", "authors": "Yichen Huang, Yizhe Zhang, Oussama Elachqar, Yu Cheng", "link": "", "summary": ""}, {"title": "Integrating Multimodal Information in Large Pretrained Transformers", "authors": "Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe Morency, Ehsan Hoque"}, {"title": "Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection", "authors": "Lei Zhong, Juan Cao, Qiang Sheng, Junbo Guo, Ziang Wang", "link": "https://arxiv.org/abs/2005.07886", "summary": "Identifying controversial posts on social media is a fundamental task for\nmining public sentiment, assessing the influence of events, and alleviating the\npolarized views. However, existing methods fail to 1) effectively incorporate\nthe semantic information from content-related posts; 2) preserve the structural\ninformation for reply relationship modeling; 3) properly handle posts from\ntopics dissimilar to those in the training set. To overcome the first two\nlimitations, we propose Topic-Post-Comment Graph Convolutional Network\n(TPC-GCN), which integrates the information from the graph structure and\ncontent of topics, posts, and comments for post-level controversy detection. As\nto the third limitation, we extend our model to Disentangled TPC-GCN\n(DTPC-GCN), to disentangle topic-related and topic-unrelated features and then\nfuse dynamically. Extensive experiments on two real-world datasets demonstrate\nthat our models outperform existing methods. Analysis of the results and cases\nproves that our models can integrate both semantic and structural information\nwith significant generalizability."}, {"title": "Interactive Classification by Asking Informative Questions", "authors": "Lili Yu, Howard Chen, Sida I. Wang, Tao Lei, Yoav Artzi", "link": "https://arxiv.org/abs/1911.03598", "summary": "We study the potential for interaction in natural language classification. We\nadd a limited form of interaction for intent classification, where users\nprovide an initial query using natural language, and the system asks for\nadditional information using binary or multi-choice questions. At each turn,\nour system decides between asking the most informative question or making the\nfinal classification prediction.The simplicity of the model allows for\nbootstrapping of the system without interaction data, instead relying on simple\ncrowdsourcing tasks. We evaluate our approach on two domains, showing the\nbenefit of interaction and the advantage of learning to balance between asking\nadditional questions and making the final prediction."}, {"title": "Interactive Construction of User-Centric Dictionary for Text Analytics", "authors": "Ryosuke Kohita, Issei Yoshida, Hiroshi Kanayama, Tetsuya Nasukawa"}, {"title": "Interactive Machine Comprehension with Information Seeking Agents", "authors": "Xingdi Yuan, Jie Fu, Marc-Alexandre C\u00f4t\u00e9, Yi Tay, Chris Pal, Adam Trischler", "link": "https://arxiv.org/abs/1908.10449", "summary": "Existing machine reading comprehension (MRC) models do not scale effectively\nto real-world applications like web-level information retrieval and question\nanswering (QA). We argue that this stems from the nature of MRC datasets: most\nof these are static environments wherein the supporting documents and all\nnecessary information are fully observed. In this paper, we propose a simple\nmethod that reframes existing MRC datasets as interactive, partially observable\nenvironments. Specifically, we \"occlude\" the majority of a document's text and\nadd context-sensitive commands that reveal \"glimpses\" of the hidden text to a\nmodel. We repurpose SQuAD and NewsQA as an initial case study, and then show\nhow the interactive corpora can be used to train a model that seeks relevant\ninformation through sequential decision making. We believe that this setting\ncan contribute in scaling models to web-level QA scenarios."}, {"title": "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?", "authors": "Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, Samuel R. Bowman", "link": "", "summary": ""}, {"title": "Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings", "authors": "Rishi Bommasani, Kelly Davis, Claire Cardie"}, {"title": "Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions", "authors": "Hannah Craighead, Andrew Caines, Paula Buttery, Helen Yannakoudakis"}, {"title": "Investigating Word-Class Distributions in Word Vector Spaces", "authors": "Ryohei Sasano, Anna Korhonen"}, {"title": "iSarcasm: A Dataset of Intended Sarcasm", "authors": "Silviu Oprea, Walid Magdy", "link": "https://arxiv.org/abs/1911.03123", "summary": "We consider the distinction between intended and perceived sarcasm in the\ncontext of textual sarcasm detection. The former occurs when an utterance is\nsarcastic from the perspective of its author, while the latter occurs when the\nutterance is interpreted as sarcastic by the audience. We show the limitations\nof previous labelling methods in capturing intended sarcasm and introduce the\niSarcasm dataset of tweets labeled for sarcasm directly by their authors.\nExamining the state-of-the-art sarcasm detection models on our dataset showed\nlow performance compared to previously studied datasets, which indicates that\nthese datasets might be biased or obvious and sarcasm could be a phenomenon\nunder-studied computationally thus far. By providing the iSarcasm dataset, we\naim to encourage future NLP research to develop methods for detecting sarcasm\nin text as intended by the authors of the text, not as labeled under\nassumptions that we demonstrate to be sub-optimal."}, {"title": "It Takes Two to Lie: One to Lie, and One to Listen", "authors": "Denis Peskov, Benny Cheng, Ahmed Elgohary, Joe Barrow, Cristian Danescu-Niculescu-Mizil, Jordan Boyd-Graber"}, {"title": "It\u2019s Morphin\u2019 Time! Combating Linguistic Discrimination with Inflectional Perturbations", "authors": "Samson Tan, Shafiq Joty, Min-Yen Kan, Richard Socher", "link": "https://arxiv.org/abs/2005.04364", "summary": "Training on only perfect Standard English corpora predisposes pre-trained\nneural networks to discriminate against minorities from non-standard linguistic\nbackgrounds (e.g., African American Vernacular English, Colloquial Singapore\nEnglish, etc.). We perturb the inflectional morphology of words to craft\nplausible and semantically similar adversarial examples that expose these\nbiases in popular NLP models, e.g., BERT and Transformer, and show that\nadversarially fine-tuning them for a single epoch significantly improves\nrobustness without sacrificing performance on clean data."}, {"title": "Iterative Edit-Based Unsupervised Sentence Simplification", "authors": "Dhruv Kumar, Lili Mou, Lukasz Golab, Olga Vechtomova"}, {"title": "Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge", "authors": "Yuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xiaojun Quan, Tong Zhang, Yonggang Wang"}, {"title": "Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging", "authors": "Nasser Zalmout, Nizar Habash", "link": "https://arxiv.org/abs/1910.02267", "summary": "Semitic languages can be highly ambiguous, having several interpretations of\nthe same surface forms, and morphologically rich, having many morphemes that\nrealize several morphological features. This is further exacerbated for\ndialectal content, which is more prone to noise and lacks a standard\northography. The morphological features can be lexicalized, like lemmas and\ndiacritized forms, or non-lexicalized, like gender, number, and part-of-speech\ntags, among others. Joint modeling of the lexicalized and non-lexicalized\nfeatures can identify more intricate morphological patterns, which provide\nbetter context modeling, and further disambiguate ambiguous lexical choices.\nHowever, the different modeling granularity can make joint modeling more\ndifficult. Our approach models the different features jointly, whether\nlexicalized (on the character-level), where we also model surface form\nnormalization, or non-lexicalized (on the word-level). We use Arabic as a test\ncase, and achieve state-of-the-art results for Modern Standard Arabic, with 20%\nrelative error reduction, and Egyptian Arabic (a dialectal variant of Arabic),\nwith 11% reduction."}, {"title": "Joint Modelling of Emotion and Abusive Language Detection", "authors": "Santhosh Rajamanickam, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova", "link": "https://arxiv.org/abs/2005.14028", "summary": "The rise of online communication platforms has been accompanied by some\nundesirable effects, such as the proliferation of aggressive and abusive\nbehaviour online. Aiming to tackle this problem, the natural language\nprocessing (NLP) community has experimented with a range of techniques for\nabuse detection. While achieving substantial success, these methods have so far\nonly focused on modelling the linguistic properties of the comments and the\nonline communities of users, disregarding the emotional state of the users and\nhow this might affect their language. The latter is, however, inextricably\nlinked to abusive behaviour. In this paper, we present the first joint model of\nemotion and abusive language detection, experimenting in a multi-task learning\nframework that allows one task to inform the other. Our results demonstrate\nthat incorporating affective features leads to significant improvements in\nabuse detection performance across datasets."}, {"title": "Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization", "authors": "Yue Cao, Hui Liu, Xiaojun Wan"}, {"title": "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation", "authors": "Junliang Guo, Linli Xu, Enhong Chen"}, {"title": "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation", "authors": "Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, Xiaoyan Zhu", "link": "https://arxiv.org/abs/2004.04100", "summary": "The research of knowledge-driven conversational systems is largely limited\ndue to the lack of dialog data which consist of multi-turn conversations on\nmultiple topics and with knowledge annotations. In this paper, we propose a\nChinese multi-domain knowledge-driven conversation dataset, KdConv, which\ngrounds the topics in multi-turn conversations to knowledge graphs. Our corpus\ncontains 4.5K conversations from three domains (film, music, and travel), and\n86K utterances with an average turn number of 19.0. These conversations contain\nin-depth discussions on related topics and natural transition between multiple\ntopics. To facilitate the following research on this corpus, we provide several\nbenchmark models. Comparative results show that the models can be enhanced by\nintroducing background knowledge, yet there is still a large space for\nleveraging knowledge to model multi-turn conversations for further research.\nResults also show that there are obvious performance differences between\ndifferent domains, indicating that it is worth to further explore transfer\nlearning and domain adaptation. The corpus and benchmark models are publicly\navailable."}, {"title": "KinGDOM: Knowledge-Guided DOMain adaptation for sentiment analysis", "authors": "Deepanway Ghosal, Devamanyu Hazarika, Abhinaba Roy, Navonil Majumder, Rada Mihalcea, Soujanya Poria", "link": "https://arxiv.org/abs/2005.00791", "summary": "Cross-domain sentiment analysis has received significant attention in recent\nyears, prompted by the need to combat the domain gap between different\napplications that make use of sentiment analysis. In this paper, we take a\nnovel perspective on this task by exploring the role of external commonsense\nknowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet\nknowledge graph to enrich the semantics of a document by providing both\ndomain-specific and domain-general background concepts. These concepts are\nlearned by training a graph convolutional autoencoder that leverages\ninter-domain concepts in a domain-invariant manner. Conditioning a popular\ndomain-adversarial baseline method with these learned concepts helps improve\nits performance over state-of-the-art approaches, demonstrating the efficacy of\nour proposed framework."}, {"title": "KLEJ: Comprehensive Benchmark for Polish Language Understanding", "authors": "Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik", "link": "https://arxiv.org/abs/2005.00630", "summary": "In recent years, a series of Transformer-based models unlocked major\nimprovements in general natural language understanding (NLU) tasks. Such a fast\npace of research would not be possible without general NLU benchmarks, which\nallow for a fair comparison of the proposed methods. However, such benchmarks\nare available only for a handful of languages. To alleviate this issue, we\nintroduce a comprehensive multi-task benchmark for the Polish language\nunderstanding, accompanied by an online leaderboard. It consists of a diverse\nset of tasks, adopted from existing datasets for named entity recognition,\nquestion-answering, textual entailment, and others. We also introduce a new\nsentiment analysis task for the e-commerce domain, named Allegro Reviews (AR).\nTo ensure a common evaluation scheme and promote models that generalize to\ndifferent NLU tasks, the benchmark includes datasets from varying domains and\napplications. Additionally, we release HerBERT, a Transformer-based model\ntrained specifically for the Polish language, which has the best average\nperformance and obtains the best results for three out of nine tasks. Finally,\nwe provide an extensive evaluation, including several standard baselines and\nrecently proposed, multilingual Transformer-based models."}, {"title": "Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation", "authors": "Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao", "link": "https://arxiv.org/abs/2004.10171", "summary": "Unsupervised neural machine translation (UNMT) has recently achieved\nremarkable results for several language pairs. However, it can only translate\nbetween a single language pair and cannot produce translation results for\nmultiple language pairs at the same time. That is, research on multilingual\nUNMT has been limited. In this paper, we empirically introduce a simple method\nto translate between thirteen languages using a single encoder and a single\ndecoder, making use of multilingual data to improve UNMT for all language\npairs. On the basis of the empirical findings, we propose two knowledge\ndistillation methods to further enhance multilingual UNMT performance. Our\nexperiments on a dataset with English translated to and from twelve other\nlanguages (including three language families and six language branches) show\nremarkable results, surpassing strong unsupervised individual baselines while\nachieving promising performance between non-English language pairs in zero-shot\ntranslation scenarios and alleviating poor performance in low-resource language\npairs."}, {"title": "Knowledge Graph Embedding Compression", "authors": "Mrinmaya Sachan"}, {"title": "Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward", "authors": "Luyang Huang, Lingfei Wu, Lu Wang", "link": "https://arxiv.org/abs/2005.01159", "summary": "Sequence-to-sequence models for abstractive summarization have been studied\nextensively, yet the generated summaries commonly suffer from fabricated\ncontent, and are often found to be near-extractive. We argue that, to address\nthese issues, the summarizer should acquire semantic interpretation over input,\ne.g., via structured representation, to allow the generation of more\ninformative summaries. In this paper, we present ASGARD, a novel framework for\nAbstractive Summarization with Graph-Augmentation and semantic-driven RewarD.\nWe propose the use of dual encoders---a sequential document encoder and a\ngraph-structured encoder---to maintain the global context and local\ncharacteristics of entities, complementing each other. We further design a\nreward based on a multiple choice cloze test to drive the model to better\ncapture entity interactions. Results show that our models produce significantly\nhigher ROUGE scores than a variant without knowledge graph as input on both New\nYork Times and CNN/Daily Mail datasets. We also obtain better or comparable\nperformance compared to systems that are fine-tuned from large pretrained\nlanguage models. Human judges further rate our model outputs as more\ninformative and containing fewer unfaithful errors."}, {"title": "Language (Re)modelling: Towards Embodied Language Understanding", "authors": "Ronen Tamari, Chen Shani, Tom Hope, Miriam R L Petruck, Omri Abend, Dafna Shahaf", "link": "https://arxiv.org/abs/2005.00311", "summary": "While natural language understanding (NLU) is advancing rapidly, today's\ntechnology differs from human-like language understanding in fundamental ways,\nnotably in its inferior efficiency, interpretability, and generalization. This\nwork proposes an approach to representation and learning based on the tenets of\nembodied cognitive linguistics (ECL). According to ECL, natural language is\ninherently executable (like programming languages), driven by mental simulation\nand metaphoric mappings over hierarchical compositions of structures and\nschemata learned through embodied interaction. This position paper argues that\nthe use of grounding by metaphoric inference and simulation will greatly\nbenefit NLU systems, and proposes a system architecture along with a roadmap\ntowards realizing this vision."}, {"title": "Language (technology) is power: The need to be explicit about NLP harms", "authors": "Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, Hanna Wallach"}, {"title": "Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese", "authors": "Tatsuki Kuribayashi, Takumi Ito, Jun Suzuki, Kentaro Inui", "link": "http://arxiv.org/abs/2005.00842", "summary": "We examine a methodology using neural language models (LMs) for analyzing the\nword order of language. This LM-based method has the potential to overcome the\ndifficulties existing methods face, such as the propagation of preprocessor\nerrors in count-based methods. In this study, we explore whether the LM-based\nmethod is valid for analyzing the word order. As a case study, this study\nfocuses on Japanese due to its complex and flexible word order. To validate the\nLM-based method, we test (i) parallels between LMs and human word order\npreference, and (ii) consistency of the results obtained using the LM-based\nmethod with previous linguistic studies. Through our experiments, we\ntentatively conclude that LMs display sufficient word order knowledge for usage\nas an analysis tool. Finally, using the LM-based method, we demonstrate the\nrelationship between the canonical word order and topicalization, which had yet\nto be analyzed by large-scale experiments."}, {"title": "Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions", "authors": "Tian Jin, Zhun Liu, Shengjia Yan, Alexandre Eichenberger, Louis-Philippe Morency"}, {"title": "Large Scale Multi-Actor Generative Dialog Modeling", "authors": "Alex Boyd, Raul Puri, Mohammad Shoeybi, Mostofa Patwary, Bryan Catanzaro", "link": "https://arxiv.org/abs/2005.06114", "summary": "Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and\nengaging conversations with a user; however, they typically exhibit either\ninconsistent personality across conversations or the average personality of all\nusers. This paper addresses these issues by controlling an agent's persona upon\ngeneration via conditioning on prior conversations of a target actor. In doing\nso, we are able to utilize more abstract patterns within a person's speech and\nbetter emulate them in generated responses. This work introduces the Generative\nConversation Control model, an augmented and fine-tuned GPT-2 language model\nthat conditions on past reference conversations to probabilistically model\nmulti-turn conversations in the actor's persona. We introduce an accompanying\ndata collection procedure to obtain 10.3M conversations from 6 months worth of\nReddit comments. We demonstrate that scaling model sizes from 117M to 8.3B\nparameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held\nout Reddit conversations. Increasing model scale yielded similar improvements\nin human evaluations that measure preference of model samples to the held out\ntarget distribution in terms of realism (31% increased to 37% preference),\nstyle matching (37% to 42%), grammar and content quality (29% to 42%), and\nconversation coherency (32% to 40%). We find that conditionally modeling past\nconversations improves perplexity by 0.47 in automatic evaluations. Through\nhuman trials we identify positive trends between conditional modeling and style\nmatching and outline steps to further improve persona control."}, {"title": "Learning a Multi-Domain Curriculum for Neural Machine Translation", "authors": "Wei Wang, Ye Tian, Jiquan Ngiam, Yinfei Yang, Isaac Caswell, Zarana Parekh", "link": "https://arxiv.org/abs/1908.10940", "summary": "Most data selection research in machine translation focuses on improving a\nsingle domain. We perform data selection for multiple domains at once. This is\nachieved by carefully introducing instance-level domain-relevance features and\nautomatically constructing a training curriculum to gradually concentrate on\nmulti-domain relevant and noise-reduced data batches. Both the choice of\nfeatures and the use of curriculum are crucial for balancing and improving all\ndomains, including out-of-domain. In large-scale experiments, the multi-domain\ncurriculum simultaneously reaches or outperforms the individual performance and\nbrings solid gains over no-curriculum training."}, {"title": "Learning and Evaluating Emotion Lexicons for 91 Languages", "authors": "Sven Buechel, Susanna R\u00fccker, Udo Hahn", "link": "https://arxiv.org/abs/2005.05672", "summary": "Emotion lexicons describe the affective meaning of words and thus constitute\na centerpiece for advanced sentiment and emotion analysis. Yet, manually\ncurated lexicons are only available for a handful of languages, leaving most\nlanguages of the world without such a precious resource for downstream\napplications. Even worse, their coverage is often limited both in terms of the\nlexical units they contain and the emotional variables they feature. In order\nto break this bottleneck, we here introduce a methodology for creating almost\narbitrarily large emotion lexicons for any target language. Our approach\nrequires nothing but a source language emotion lexicon, a bilingual word\ntranslation model, and a target language embedding model. Fulfilling these\nrequirements for 91 languages, we are able to generate representationally rich\nhigh-coverage lexicons comprising eight emotional variables with more than 100k\nlexical entries each. We evaluated the automatically generated lexicons against\nhuman judgment from 26 datasets, spanning 12 typologically diverse languages,\nand found that our approach produces results in line with state-of-the-art\nmonolingual approaches to lexicon creation and even surpasses human reliability\nfor some languages and variables. Code and data are available at\nhttps://github.com/JULIELab/MEmoLon archived under DOI\nhttps://doi.org/10.5281/zenodo.3779901."}, {"title": "Learning Architectures from an Extended Search Space for Language Modeling", "authors": "Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li", "link": "https://arxiv.org/abs/2005.02593", "summary": "Neural architecture search (NAS) has advanced significantly in recent years\nbut most NAS systems restrict search to learning architectures of a recurrent\nor convolutional cell. In this paper, we extend the search space of NAS. In\nparticular, we present a general approach to learn both intra-cell and\ninter-cell architectures (call it ESS). For a better search result, we design a\njoint learning method to perform intra-cell and inter-cell NAS simultaneously.\nWe implement our model in a differentiable architecture search system. For\nrecurrent neural language modeling, it outperforms a strong baseline\nsignificantly on the PTB and WikiText data, with a new state-of-the-art on PTB.\nMoreover, the learned architectures show good transferability to other systems.\nE.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity\nrecognition (NER) tasks and CoNLL chunking task, indicating a promising line of\nresearch on large-scale pre-learned architectures."}, {"title": "Learning Constraints for Structured Prediction Using Rectifier Networks", "authors": "Xingyuan Pan, Maitrey Mehta, Vivek Srikumar", "link": "https://arxiv.org/abs/2006.01209", "summary": "Various natural language processing tasks are structured prediction problems\nwhere outputs are constructed with multiple interdependent decisions. Past work\nhas shown that domain knowledge, framed as constraints over the output space,\ncan help improve predictive accuracy. However, designing good constraints often\nrelies on domain expertise. In this paper, we study the problem of learning\nsuch constraints. We frame the problem as that of training a two-layer\nrectifier network to identify valid structures or substructures, and show a\nconstruction for converting a trained network into a system of linear\nconstraints over the inference variables. Our experiments on several NLP tasks\nshow that the learned constraints can improve the prediction accuracy,\nespecially when the number of training examples is small."}, {"title": "Learning Dialog Policies from Weak Demonstrations", "authors": "Gabriel Gordon-Hall, Philip John Gorinski, Shay B. Cohen", "link": "https://arxiv.org/abs/2004.11054", "summary": "Deep reinforcement learning is a promising approach to training a dialog\nmanager, but current methods struggle with the large state and action spaces of\nmulti-domain dialog systems. Building upon Deep Q-learning from Demonstrations\n(DQfD), an algorithm that scores highly in difficult Atari games, we leverage\ndialog data to guide the agent to successfully respond to a user's requests. We\nmake progressively fewer assumptions about the data needed, using labeled,\nreduced-labeled, and even unlabeled data to train expert demonstrators. We\nintroduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to\novercome the domain gap between the datasets and the environment. Experiments\nin a challenging multi-domain dialog system framework validate our approaches,\nand get high success rates even when trained on out-of-domain data."}, {"title": "Learning Efficient Dialogue Policy from Demonstrations through Shaping", "authors": "Huimin Wang, Baolin Peng, Kam-Fai Wong"}, {"title": "Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts", "authors": "Jingyuan Zhang, Mingming Sun, Yue Feng, Ping Li"}, {"title": "Learning Source Phrase Representations for Neural Machine Translation", "authors": "Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu, Jingyi Zhang", "link": "https://arxiv.org/abs/1812.10230", "summary": "Neural machine translation (NMT) models generally adopt an encoder-decoder\narchitecture for modeling the entire translation process. The encoder\nsummarizes the representation of input sentence from scratch, which is\npotentially a problem if the sentence is ambiguous. When translating a text,\nhumans often create an initial understanding of the source sentence and then\nincrementally refine it along the translation on the target side. Starting from\nthis intuition, we propose a novel encoder-refiner-decoder framework, which\ndynamically refines the source representations based on the generated\ntarget-side information at each decoding step. Since the refining operations\nare time-consuming, we propose a strategy, leveraging the power of\nreinforcement learning models, to decide when to refine at specific decoding\nsteps. Experimental results on both Chinese-English and English-German\ntranslation tasks show that the proposed approach significantly and\nconsistently improves translation performance over the standard encoder-decoder\nframework. Furthermore, when refining strategy is applied, results still show\nreasonable improvement over the baseline without much decrease in decoding\nspeed."}, {"title": "Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction", "authors": "Zi Chai, Xiaojun Wan", "link": "", "summary": ""}, {"title": "Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling", "authors": "Ouyu Lan, Xiao Huang, Bill Yuchen Lin, He Jiang, Liyuan Liu, Xiang Ren", "link": "https://arxiv.org/abs/1910.04289", "summary": "Sequence labeling is a fundamental framework for various natural language\nprocessing problems. Its performance is largely influenced by the annotation\nquality and quantity in supervised learning scenarios, and obtaining ground\ntruth labels is often costly. In many cases, ground truth labels do not exist,\nbut noisy annotations or annotations from different domains are accessible. In\nthis paper, we propose a novel framework Consensus Network (ConNet) that can be\ntrained on annotations from multiple sources (e.g., crowd annotation,\ncross-domain data...). It learns individual representation for every source and\ndynamically aggregates source-specific knowledge by a context-aware attention\nmodule. Finally, it leads to a model reflecting the agreement (consensus) among\nmultiple sources. We evaluate the proposed framework in two practical settings\nof multi-source learning: learning with crowd annotations and unsupervised\ncross-domain model adaptation. Extensive experimental results show that our\nmodel achieves significant improvements over existing methods in both settings.\nWe also demonstrate that the method can apply to various tasks and cope with\ndifferent encoders."}, {"title": "Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks", "authors": "Yiping Song, Zequn Liu, Wei Bi, Rui Yan, Ming Zhang", "link": "https://arxiv.org/abs/1910.14326", "summary": "Training the generative models with minimal corpus is one of the critical\nchallenges for building open-domain dialogue systems. Existing methods tend to\nuse the meta-learning framework which pre-trains the parameters on all\nnon-target tasks then fine-tunes on the target task. However, fine-tuning\ndistinguishes tasks from the parameter perspective but ignores the\nmodel-structure perspective, resulting in similar dialogue models for different\ntasks. In this paper, we propose an algorithm that can customize a unique\ndialogue model for each task in the few-shot setting. In our approach, each\ndialogue model consists of a shared module, a gating module, and a private\nmodule. The first two modules are shared among all the tasks, while the third\none will differentiate into different network structures to better capture the\ncharacteristics of the corresponding task. The extensive experiments on two\ndatasets show that our method outperforms all the baselines in terms of task\nconsistency, response quality, and diversity."}, {"title": "Learning to Deceive with Attention-Based Explanations", "authors": "Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton", "link": "https://arxiv.org/abs/1909.07913", "summary": "Attention mechanisms are ubiquitous components in neural architectures\napplied to natural language processing. In addition to yielding gains in\npredictive accuracy, attention weights are often claimed to confer\ninterpretability, purportedly useful both for providing insights to\npractitioners and for explaining why a model makes its decisions to\nstakeholders. We call the latter use of attention mechanisms into question by\ndemonstrating a simple method for training models to produce deceptive\nattention masks. Our method diminishes the total weight assigned to designated\nimpermissible tokens, even when the models can be shown to nevertheless rely on\nthese features to drive predictions. Across multiple models and tasks, our\napproach manipulates attention weights while paying surprisingly little cost in\naccuracy. Through a human study, we show that our manipulated attention-based\nexplanations deceive people into thinking that predictions from a model biased\nagainst gender minorities do not rely on the gender. Consequently, our results\ncast doubt on attention's reliability as a tool for auditing algorithms in the\ncontext of fairness and accountability."}, {"title": "Learning to execute instructions in a Minecraft dialogue", "authors": "Prashant Jayannavar, Anjali Narayan-Chen, Julia Hockenmaier"}, {"title": "Learning to Faithfully Rationalize by Construction", "authors": "Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace", "link": "https://arxiv.org/abs/2005.00115", "summary": "In many settings it is important for one to be able to understand why a model\nmade a particular prediction. In NLP this often entails extracting snippets of\nan input text `responsible for' corresponding model output; when such a snippet\ncomprises tokens that indeed informed the model's prediction, it is a faithful\nexplanation. In some settings, faithfulness may be critical to ensure\ntransparency. Lei et al. (2016) proposed a model to produce faithful rationales\nfor neural text classification by defining independent snippet extraction and\nprediction modules. However, the discrete selection over input tokens performed\nby this method complicates training, leading to high variance and requiring\ncareful hyperparameter tuning. We propose a simpler variant of this approach\nthat provides faithful explanations by construction. In our scheme, named\nFRESH, arbitrary feature importance scores (e.g., gradients from a trained\nmodel) are used to induce binary labels over token inputs, which an extractor\ncan be trained to predict. An independent classifier module is then trained\nexclusively on snippets provided by the extractor; these snippets thus\nconstitute faithful explanations, even if the classifier is arbitrarily\ncomplex. In both automatic and manual evaluations we find that variants of this\nsimple framework yield predictive performance superior to `end-to-end'\napproaches, while being more general and easier to train. Code is available at\nhttps://github.com/successar/FRESH"}, {"title": "Learning to Identify Follow-Up Questions in Conversational Question Answering", "authors": "Souvik Kundu, Qian Lin, Hwee Tou Ng"}, {"title": "Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation", "authors": "Qiu Ran, Yankai Lin, Peng Li, Jie Zhou"}, {"title": "Learning to Segment Actions from Observation and Narration", "authors": "Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark, Aida Nematzadeh", "link": "https://arxiv.org/abs/2005.03684", "summary": "We apply a generative segmental model of task structure, guided by narration,\nto action segmentation in video. We focus on unsupervised and weakly-supervised\nsettings where no action labels are known during training. Despite its\nsimplicity, our model performs competitively with previous work on a dataset of\nnaturalistic instructional videos. Our model allows us to vary the sources of\nsupervision used in training, and we find that both task structure and\nnarrative language provide large benefits in segmentation quality."}, {"title": "Learning to Update Natural Language Comments Based on Code Changes", "authors": "Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li, Raymond Mooney", "link": "http://arxiv.org/abs/2004.12169", "summary": "We formulate the novel task of automatically updating an existing natural\nlanguage comment based on changes in the body of code it accompanies. We\npropose an approach that learns to correlate changes across two distinct\nlanguage representations, to generate a sequence of edits that are applied to\nthe existing comment to reflect the source code modifications. We train and\nevaluate our model using a dataset that we collected from commit histories of\nopen-source software projects, with each example consisting of a concurrent\nupdate to a method and its corresponding comment. We compare our approach\nagainst multiple baselines using both automatic metrics and human evaluation.\nResults reflect the challenge of this task and that our model outperforms\nbaselines with respect to making edits."}, {"title": "Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context", "authors": "Shashank Srivastava, Oleksandr Polozov, Nebojsa Jojic, Christopher Meek"}, {"title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "authors": "Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du", "link": "https://arxiv.org/abs/2005.10043", "summary": "Graphs that capture relations between textual units have great benefits for\ndetecting salient information from multiple documents and generating overall\ncoherent summaries. In this paper, we develop a neural abstractive\nmulti-document summarization (MDS) model which can leverage well-known graph\nrepresentations of documents such as similarity graph and discourse graph, to\nmore effectively process multiple input documents and produce abstractive\nsummaries. Our model utilizes graphs to encode documents in order to capture\ncross-document relations, which is crucial to summarizing long documents. Our\nmodel can also take advantage of graphs to guide the summary generation\nprocess, which is beneficial for generating coherent and concise summaries.\nFurthermore, pre-trained language models can be easily combined with our model,\nwhich further improve the summarization performance significantly. Empirical\nresults on the WikiSum and MultiNews dataset show that the proposed\narchitecture brings substantial improvements over several strong baselines."}, {"title": "Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks", "authors": "Yanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, Kai Yu"}, {"title": "Location Attention for Extrapolation to Longer Sequences", "authors": "Yann Dubois, Gautier Dagan, Dieuwke Hupkes, Elia Bruni", "link": "https://arxiv.org/abs/1911.03872", "summary": "Neural networks are surprisingly good at interpolating and perform remarkably\nwell when the training set examples resemble those in the test set. However,\nthey are often unable to extrapolate patterns beyond the seen data, even when\nthe abstractions required for such patterns are simple. In this paper, we first\nreview the notion of extrapolation, why it is important and how one could hope\nto tackle it. We then focus on a specific type of extrapolation which is\nespecially useful for natural language processing: generalization to sequences\nthat are longer than the training ones. We hypothesize that models with a\nseparate content- and location-based attention are more likely to extrapolate\nthan those with common attention mechanisms. We empirically support our claim\nfor recurrent seq2seq models with our proposed attention on variants of the\nLookup Table task. This sheds light on some striking failures of neural models\nfor sequences and on possible methods to approaching such issues."}, {"title": "Logical Natural Language Generation from Open-Domain Tables", "authors": "Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang", "link": "https://arxiv.org/abs/2004.10404", "summary": "Neural natural language generation (NLG) models have recently shown\nremarkable progress in fluency and coherence. However, existing studies on\nneural NLG are primarily focused on surface-level realizations with limited\nemphasis on logical inference, an important aspect of human thinking and\nlanguage. In this paper, we suggest a new NLG task where a model is tasked with\ngenerating natural language statements that can be \\emph{logically entailed} by\nthe facts in an open-domain semi-structured table. To facilitate the study of\nthe proposed logical NLG problem, we use the existing TabFact dataset\n\\cite{chen2019tabfact} featured with a wide range of logical/symbolic\ninferences as our testbed, and propose new automatic metrics to evaluate the\nfidelity of generation models w.r.t.\\ logical inference. The new task poses\nchallenges to the existing monotonic generation frameworks due to the mismatch\nbetween sequence order and logical order. In our experiments, we\ncomprehensively survey different generation architectures (LSTM, Transformer,\nPre-Trained LM) trained with different algorithms (RL, Adversarial Training,\nCoarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained\nLM can significantly boost both the fluency and logical fidelity metrics, 2) RL\nand Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine\ngeneration can help partially alleviate the fidelity issue while maintaining\nhigh language fluency. The code and data are available at\n\\url{https://github.com/wenhuchen/LogicNLG}."}, {"title": "LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network", "authors": "Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, Jian Yin", "link": "http://arxiv.org/abs/2004.13659", "summary": "Verifying the correctness of a textual statement requires not only semantic\nreasoning about the meaning of words, but also symbolic reasoning about logical\noperations like count, superlative, aggregation, etc. In this work, we propose\nLogicalFactChecker, a neural network approach capable of leveraging logical\noperations for fact checking. It achieves the state-of-the-art performance on\nTABFACT, a large-scale, benchmark dataset built for verifying a textual\nstatement with semi-structured tables. This is achieved by a graph module\nnetwork built upon the Transformer-based architecture. With a textual statement\nand a table as the input, LogicalFactChecker automatically derives a program\n(a.k.a. logical form) of the statement in a semantic parsing manner. A\nheterogeneous graph is then constructed to capture not only the structures of\nthe table and the program, but also the connections between inputs with\ndifferent modalities. Such a graph reveals the related contexts of each word in\nthe statement, the table and the program. The graph is used to obtain\ngraph-enhanced contextual representations of words in Transformer-based\narchitecture. After that, a program-driven module network is further introduced\nto exploit the hierarchical structure of the program, where semantic\ncompositionality is dynamically modeled along the program structure with a set\nof function-specific modules. Ablation experiments suggest that both the\nheterogeneous graph and the module network are important to obtain strong\nresults."}, {"title": "Low-Dimensional Hyperbolic Knowledge Graph Embeddings", "authors": "Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, Christopher R\u00e9", "link": "https://arxiv.org/abs/2005.00545", "summary": "Knowledge graph (KG) embeddings learn low-dimensional representations of\nentities and relations to predict missing facts. KGs often exhibit hierarchical\nand logical patterns which must be preserved in the embedding space. For\nhierarchical data, hyperbolic embedding methods have shown promise for\nhigh-fidelity and parsimonious representations. However, existing hyperbolic\nembedding methods do not account for the rich logical patterns in KGs. In this\nwork, we introduce a class of hyperbolic KG embedding models that\nsimultaneously capture hierarchical and logical patterns. Our approach combines\nhyperbolic reflections and rotations with attention to model complex relational\npatterns. Experimental results on standard KG benchmarks show that our method\nimproves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in\nmean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that\ndifferent geometric transformations capture different types of relations while\nattention-based transformations generalize to multiple relations. In high\ndimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR\nand 57.7% on YAGO3-10."}, {"title": "Low-Resource Generation of Multi-hop Reasoning Questions", "authors": "Jianxing Yu, Wei Liu, Shuang Qiu, Qinliang Su, Kai Wang, Xiaojun Quan, Jian Yin"}, {"title": "Machine Reading of Historical Events", "authors": "Or Honovich, Lucas Torroba Hennigen, Omri Abend, Shay B. Cohen"}, {"title": "Mapping Natural Language Instructions to Mobile UI Action Sequences", "authors": "Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge", "link": "https://arxiv.org/abs/2005.03776", "summary": "We present a new problem: grounding natural language instructions to mobile\nuser interface actions, and contribute three new datasets for it. For full task\nevaluation, we create PIXELHELP, a corpus that pairs English instructions with\nactions performed by people on a mobile UI emulator. To scale training, we\ndecouple the language and action data by (a) annotating action phrase spans in\nHowTo instructions and (b) synthesizing grounded descriptions of actions for\nmobile user interfaces. We use a Transformer to extract action phrase tuples\nfrom long-range natural language instructions. A grounding Transformer then\ncontextually represents UI objects using both their content and screen position\nand connects them to object descriptions. Given a starting screen and\ninstruction, our model achieves 70.59% accuracy on predicting complete\nground-truth action sequences in PIXELHELP."}, {"title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning", "authors": "Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara Berg, Mohit Bansal", "link": "https://arxiv.org/abs/2005.05402", "summary": "Generating multi-sentence descriptions for videos is one of the most\nchallenging captioning tasks due to its high requirements for not only visual\nrelevance but also discourse-based coherence across the sentences in the\nparagraph. Towards this goal, we propose a new approach called Memory-Augmented\nRecurrent Transformer (MART), which uses a memory module to augment the\ntransformer architecture. The memory module generates a highly summarized\nmemory state from the video segments and the sentence history so as to help\nbetter prediction of the next sentence (w.r.t. coreference and repetition\naspects), thus encouraging coherent paragraph generation. Extensive\nexperiments, human evaluations, and qualitative analyses on two popular\ndatasets ActivityNet Captions and YouCookII show that MART generates more\ncoherent and less repetitive paragraph captions than baseline methods, while\nmaintaining relevance to the input video events. All code is available\nopen-source at: https://github.com/jayleicn/recurrent-transformer"}, {"title": "Masked Language Model Scoring", "authors": "Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff", "link": "https://arxiv.org/abs/1910.14659", "summary": "Pretrained masked language models (MLMs) require finetuning for most NLP\ntasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood\nscores (PLLs), which are computed by masking tokens one by one. We show that\nPLLs outperform scores from autoregressive language models like GPT-2 in a\nvariety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an\nend-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on\nstate-of-the-art baselines for low-resource translation pairs, with further\ngains from domain adaptation. We attribute this success to PLL's unsupervised\nexpression of linguistic acceptability without a left-to-right bias, greatly\nimproving on scores from GPT-2 (+10 points on island effects, NPI licensing in\nBLiMP). One can finetune MLMs to give scores without masking, enabling\ncomputation in a single inference pass. In all, PLLs and their associated\npseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of\npretrained MLMs; e.g., we use a single cross-lingual model to rerank\ntranslations in multiple languages. We release our library for language model\nscoring at https://github.com/awslabs/mlm-scoring."}, {"title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization", "authors": "Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, Chenliang Li", "link": "https://arxiv.org/abs/2004.12302", "summary": "Recently, large-scale datasets have vastly facilitated the development in\nnearly all domains of Natural Language Processing. However, there is currently\nno cross-task dataset in NLP, which hinders the development of multi-task\nlearning. We propose MATINF, the first jointly labeled large-scale dataset for\nclassification, question answering and summarization. MATINF contains 1.07\nmillion question-answer pairs with human-labeled categories and user-generated\nquestion descriptions. Based on such rich information, MATINF is applicable for\nthree major NLP tasks, including classification, question answering, and\nsummarization. We benchmark existing methods and a novel multi-task baseline\nover MATINF to inspire further research. Our comprehensive comparison and\nexperiments over MATINF and other datasets demonstrate the merits held by\nMATINF."}, {"title": "Max-Margin Incremental CCG Parsing", "authors": "Milo\u0161 Stanojevi\u0107, Mark Steedman"}, {"title": "Measuring Forecasting Skill from Text", "authors": "Shi Zong, Alan Ritter, Eduard Hovy"}, {"title": "Meta-Reinforced Multi-Domain State Generator for Dialogue Systems", "authors": "Yi Huang, Junlan Feng, Min Hu, Xiaoting Wu, Xiaoyu Du, Shuo Ma", "link": "", "summary": ""}, {"title": "MIE: A Medical Information Extractor towards Medical Dialogues", "authors": "Yuanzhe Zhang, Zhongtao Jiang, Tao Zhang, Shiwan Liu, Jiarun Cao, Kang Liu, Shengping Liu, Jun Zhao"}, {"title": "Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance", "authors": "Prasetya Ajie Utama, Nafise Sadat Moosavi, Iryna Gurevych", "link": "http://arxiv.org/abs/2005.00315", "summary": "Models for natural language understanding (NLU) tasks often rely on the\nidiosyncratic biases of the dataset, which make them brittle against test cases\noutside the training distribution. Recently, several proposed debiasing methods\nare shown to be very effective in improving out-of-distribution performance.\nHowever, their improvements come at the expense of performance drop when models\nare evaluated on the in-distribution data, which contain examples with higher\ndiversity. This seemingly inevitable trade-off may not tell us much about the\nchanges in the reasoning and understanding capabilities of the resulting models\non broader types of examples beyond the small subset represented in the\nout-of-distribution data. In this paper, we address this trade-off by\nintroducing a novel debiasing method, called confidence regularization, which\ndiscourage models from exploiting biases while enabling them to receive enough\nincentive to learn from all the training examples. We evaluate our method on\nthree NLU tasks and show that, in contrast to its predecessors, it improves the\nperformance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset)\nwhile maintaining the original in-distribution accuracy."}, {"title": "MIND: A Large-scale Dataset for News Recommendation", "authors": "Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou"}, {"title": "MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification", "authors": "Jiaao Chen, Zichao Yang, Diyi Yang", "link": "https://arxiv.org/abs/2004.12239", "summary": "This paper presents MixText, a semi-supervised learning method for text\nclassification, which uses our newly designed data augmentation method called\nTMix. TMix creates a large amount of augmented training samples by\ninterpolating text in hidden space. Moreover, we leverage recent advances in\ndata augmentation to guess low-entropy labels for unlabeled data, hence making\nthem as easy to use as labeled data.By mixing labeled, unlabeled and augmented\ndata, MixText significantly outperformed current pre-trained and fined-tuned\nmodels and other state-of-the-art semi-supervised learning methods on several\ntext classification benchmarks. The improvement is especially prominent when\nsupervision is extremely limited. We have publicly released our code at\nhttps://github.com/GT-SALT/MixText."}, {"title": "MLQA: Evaluating Cross-lingual Extractive Question Answering", "authors": "Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, Holger Schwenk", "link": "https://arxiv.org/abs/1910.07475", "summary": "Question answering (QA) models have shown rapid progress enabled by the\navailability of large, high-quality benchmark datasets. Such annotated datasets\nare difficult and costly to collect, and rarely exist in languages other than\nEnglish, making training QA systems in other languages challenging. An\nalternative to building large monolingual training datasets is to develop\ncross-lingual systems which can transfer to a target language without requiring\ntraining data in that language. In order to develop such systems, it is crucial\nto invest in high quality multilingual evaluation benchmarks to measure\nprogress. We present MLQA, a multi-way aligned extractive QA evaluation\nbenchmark intended to spur research in this area. MLQA contains QA instances in\n7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and\nSimplified Chinese. It consists of over 12K QA instances in English and 5K in\neach other language, with each QA instance being parallel between 4 languages\non average. MLQA is built using a novel alignment context strategy on Wikipedia\narticles, and serves as a cross-lingual extension to existing extractive QA\ndatasets. We evaluate current state-of-the-art cross-lingual representations on\nMLQA, and also provide machine-translation-based baselines. In all cases,\ntransfer results are shown to be significantly behind training-language\nperformance."}, {"title": "MMPE: A Multi-Modal Interface for Post-Editing Machine Translation", "authors": "Nico Herbig, Tim D\u00fcwel, Santanu Pal, Kalliopi Maria Meladaki, Mahsa Monshizadeh, Antonio Kr\u00fcger, Josef van Genabith"}, {"title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "authors": "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou", "link": "", "summary": ""}, {"title": "Modeling Code-Switch Languages Using Bilingual Parallel Corpus", "authors": "Grandee Lee, Haizhou Li"}, {"title": "Modeling Morphological Typology for Unsupervised Learning of Language Morphology", "authors": "Hongzhi Xu, Jordan Kodner, Mitchell Marcus, Charles Yang"}, {"title": "Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis", "authors": "Minh Hieu Phan, Philip O. Ogunbona"}, {"title": "More Diverse Dialogue Datasets via Diversity-Informed Data Collection", "authors": "Katherine Stasaski, Grace Hui Yang, Marti A. Hearst"}, {"title": "Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders", "authors": "Terra Blevins, Luke Zettlemoyer", "link": "https://arxiv.org/abs/2005.02590", "summary": "A major obstacle in Word Sense Disambiguation (WSD) is that word senses are\nnot uniformly distributed, causing existing models to generally perform poorly\non senses that are either rare or unseen during training. We propose a\nbi-encoder model that independently embeds (1) the target word with its\nsurrounding context and (2) the dictionary definition, or gloss, of each sense.\nThe encoders are jointly optimized in the same representation space, so that\nsense disambiguation can be performed by finding the nearest sense embedding\nfor each target word embedding. Our system outperforms previous\nstate-of-the-art models on English all-words WSD; these gains predominantly\ncome from improved performance on rare senses, leading to a 31.1% error\nreduction on less frequent senses over prior work. This demonstrates that rare\nsenses can be more effectively disambiguated by modeling their definitions."}, {"title": "Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning", "authors": "Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman", "link": "https://arxiv.org/abs/2005.07064", "summary": "We present a method for combining multi-agent communication and traditional\ndata-driven approaches to natural language learning, with an end goal of\nteaching agents to communicate with humans in natural language. Our starting\npoint is a language model that has been trained on generic, not task-specific\nlanguage data. We then place this model in a multi-agent self-play environment\nthat generates task-specific rewards used to adapt or modulate the model,\nturning it into a task-conditional language model. We introduce a new way for\ncombining the two types of learning based on the idea of reranking language\nmodel samples, and show that this method outperforms others in communicating\nwith humans in a visual referential communication task. Finally, we present a\ntaxonomy of different types of language drift that can occur alongside a set of\nmeasures to detect them."}, {"title": "Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition", "authors": "Ryuichi Takanobu, Runze Liang, Minlie Huang", "link": "https://arxiv.org/abs/2004.03809", "summary": "Many studies have applied reinforcement learning to train a dialog policy and\nshow great promise these years. One common approach is to employ a user\nsimulator to obtain a large number of simulated user experiences for\nreinforcement learning algorithms. However, modeling a realistic user simulator\nis challenging. A rule-based simulator requires heavy domain expertise for\ncomplex tasks, and a data-driven simulator requires considerable data and it is\neven unclear how to evaluate a simulator. To avoid explicitly building a user\nsimulator beforehand, we propose Multi-Agent Dialog Policy Learning, which\nregards both the system and the user as the dialog agents. Two agents interact\nwith each other and are jointly learned simultaneously. The method uses the\nactor-critic framework to facilitate pretraining and improve scalability. We\nalso propose Hybrid Value Network for the role-aware reward decomposition to\nintegrate role-specific domain knowledge of each agent in the task-oriented\ndialog. Results show that our method can successfully build a system policy and\na user policy simultaneously, and two agents can achieve a high task success\nrate through conversational interaction."}, {"title": "Multi-Cell Compositional LSTM for NER Domain Adaptation", "authors": "Chen Jia, Yue Zhang"}, {"title": "Multidirectional Associative Optimization of Function-Specific Word Representations", "authors": "Daniela Gerz, Ivan Vuli\u0107, Marek Rei, Roi Reichart, Anna Korhonen", "link": "https://arxiv.org/abs/2005.05264", "summary": "We present a neural framework for learning associations between interrelated\ngroups of words such as the ones found in Subject-Verb-Object (SVO) structures.\nOur model induces a joint function-specific word vector space, where vectors of\ne.g. plausible SVO compositions lie close together. The model retains\ninformation about word group membership even in the joint space, and can\nthereby effectively be applied to a number of tasks reasoning over the SVO\nstructure. We show the robustness and versatility of the proposed framework by\nreporting state-of-the-art results on the tasks of estimating selectional\npreference and event similarity. The results indicate that the combinations of\nrepresentations learned with our task-independent model outperform\ntask-specific architectures from prior work, while reducing the number of\nparameters by up to 95%."}, {"title": "Multi-Domain Dialogue Acts and Response Co-Generation", "authors": "Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu", "link": "https://arxiv.org/abs/2004.12363", "summary": "Generating fluent and informative responses is of critical importance for\ntask-oriented dialogue systems. Existing pipeline approaches generally predict\nmultiple dialogue acts first and use them to assist response generation. There\nare at least two shortcomings with such approaches. First, the inherent\nstructures of multi-domain dialogue acts are neglected. Second, the semantic\nassociations between acts and responses are not taken into account for response\ngeneration. To address these issues, we propose a neural co-generation model\nthat generates dialogue acts and responses concurrently. Unlike those pipeline\napproaches, our act generation module preserves the semantic structures of\nmulti-domain dialogue acts and our response generation module dynamically\nattends to different acts as needed. We train the two modules jointly using an\nuncertainty loss to adjust their task weights adaptively. Extensive experiments\nare conducted on the large-scale MultiWOZ dataset and the results show that our\nmodel achieves very favorable improvement over several state-of-the-art models\nin both automatic and human evaluations."}, {"title": "Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference", "authors": "Jing Wang, Mayank Kulkarni, Daniel Preotiuc-Pietro"}, {"title": "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing", "authors": "Haoming Jiang, Chen Liang, Chong Wang, Tuo Zhao", "link": "https://arxiv.org/abs/1911.02692", "summary": "Many multi-domain neural machine translation (NMT) models achieve knowledge\ntransfer by enforcing one encoder to learn shared embedding across domains.\nHowever, this design lacks adaptation to individual domains. To overcome this\nlimitation, we propose a novel multi-domain NMT model using individual modules\nfor each domain, on which we apply word-level, adaptive and layer-wise domain\nmixing. We first observe that words in a sentence are often related to multiple\ndomains. Hence, we assume each word has a domain proportion, which indicates\nits domain preference. Then word representations are obtained by mixing their\nembedding in individual domains based on their domain proportions. We show this\ncan be achieved by carefully designing multi-head dot-product attention modules\nfor different domains, and eventually taking weighted averages of their\nparameters by word-level layer-wise domain proportions. Through this, we can\nachieve effective domain knowledge sharing, and capture fine-grained\ndomain-specific knowledge as well. Our experiments show that our proposed model\noutperforms existing ones in several NMT tasks."}, {"title": "Multi-Granularity\u00a0Interaction Network for Extractive and Abstractive Multi-Document Summarization", "authors": "Hanqi Jin, Tianming Wang, Xiaojun Wan"}, {"title": "Multi-Hypothesis Machine Translation Evaluation", "authors": "Marina Fomicheva, Lucia Specia, Francisco Guzm\u00e1n"}, {"title": "Multi-Label and Multilingual News Framing Analysis", "authors": "Afra Feyza Aky\u00fcrek, Lei Guo, Randa Elanwar, Prakash Ishwar, Margrit Betke, Derry Tanti Wijaya"}, {"title": "Multimodal Neural Graph Memory Networks for Visual Question Answering", "authors": "Mahmoud Khademi"}, {"title": "MultiQT: Multimodal learning for real-time question tracking in speech", "authors": "Jakob D. Havtorn, Jan Latko, Joakim Edin, Lars Maal\u00f8e, Lasse Borgholt, Lorenzo Belgrano, Nicolai Jacobsen, Regitze Sdun, \u017deljko Agi\u0107", "link": "http://arxiv.org/abs/2005.00812", "summary": "We address a challenging and practical task of labeling questions in speech\nin real time during telephone calls to emergency medical services in English,\nwhich embeds within a broader decision support system for emergency\ncall-takers. We propose a novel multimodal approach to real-time sequence\nlabeling in speech. Our model treats speech and its own textual representation\nas two separate modalities or views, as it jointly learns from streamed audio\nand its noisy transcription into text via automatic speech recognition. Our\nresults show significant gains of jointly learning from the two modalities when\ncompared to text or audio only, under adverse noise and limited volume of\ntraining data. The results generalize to medical symptoms detection where we\nobserve a similar pattern of improvements with multimodal learning."}, {"title": "Multiscale Collaborative Deep Models for Neural Machine Translation", "authors": "Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng, Weihua Luo", "link": "https://arxiv.org/abs/2004.14021", "summary": "Recent evidence reveals that Neural Machine Translation (NMT) models with\ndeeper neural networks can be more effective but are difficult to train. In\nthis paper, we present a MultiScale Collaborative (MSC) framework to ease the\ntraining of NMT models that are substantially deeper than those used\npreviously. We explicitly boost the gradient back-propagation from top to\nbottom levels by introducing a block-scale collaboration mechanism into deep\nNMT models. Then, instead of forcing the whole encoder stack directly learns a\ndesired representation, we let each encoder block learns a fine-grained\nrepresentation and enhance it by encoding spatial dependencies using a\ncontext-scale collaboration. We provide empirical evidence showing that the MSC\nnets are easy to optimize and can obtain improvements of translation quality\nfrom considerably increased depth. On IWSLT translation tasks with three\ntranslation directions, our extremely deep models (with 72-layer encoders)\nsurpass strong baselines by +2.2~+3.1 BLEU points. In addition, our deep MSC\nachieves a BLEU score of 30.56 on WMT14 English-German task that significantly\noutperforms state-of-the-art deep NMT models."}, {"title": "Multi-Sentence Argument Linking", "authors": "Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, Benjamin Van Durme", "link": "https://arxiv.org/abs/1911.03766", "summary": "We present a novel document-level model for finding argument spans that fill\nan event's roles, connecting related ideas in sentence-level semantic role\nlabeling and coreference resolution. Because existing datasets for\ncross-sentence linking are small, development of our neural model is supported\nthrough the creation of a new resource, Roles Across Multiple Sentences (RAMS),\nwhich contains 9,124 annotated events across 139 types. We demonstrate strong\nperformance of our model on RAMS and other event-related datasets."}, {"title": "Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering", "authors": "Ming Yan, Hao Zhang, Di Jin, Joey Tianyi Zhou", "link": "", "summary": ""}, {"title": "MuTual: A Dataset for Multi-Turn Dialogue Reasoning", "authors": "Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, Ming Zhou", "link": "https://arxiv.org/abs/2004.04494", "summary": "Non-task oriented dialogue systems have achieved great success in recent\nyears due to largely accessible conversation data and the development of deep\nlearning techniques. Given a context, current systems are able to yield a\nrelevant and fluent response, but sometimes make logical mistakes because of\nweak reasoning capabilities. To facilitate the conversation reasoning research,\nwe introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning,\nconsisting of 8,860 manually annotated dialogues based on Chinese student\nEnglish listening comprehension exams. Compared to previous benchmarks for\nnon-task oriented dialogue systems, MuTual is much more challenging since it\nrequires a model that can handle various reasoning problems. Empirical results\nshow that state-of-the-art methods only reach 71%, which is far behind the\nhuman performance of 94%, indicating that there is ample room for improving\nreasoning ability. MuTual is available at https://github.com/Nealcly/MuTual."}, {"title": "Named Entity Recognition without Labelled Data: A Weak Supervision Approach", "authors": "Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, Samia Touileb", "link": "https://arxiv.org/abs/2004.14723", "summary": "Named Entity Recognition (NER) performance often degrades rapidly when\napplied to target domains that differ from the texts observed during training.\nWhen in-domain labelled data is available, transfer learning techniques can be\nused to adapt existing NER models to the target domain. But what should one do\nwhen there is no hand-labelled data for the target domain? This paper presents\na simple but powerful approach to learn NER models in the absence of labelled\ndata through weak supervision. The approach relies on a broad spectrum of\nlabelling functions to automatically annotate texts from the target domain.\nThese annotations are then merged together using a hidden Markov model which\ncaptures the varying accuracies and confusions of the labelling functions. A\nsequence labelling model can finally be trained on the basis of this unified\nannotation. We evaluate the approach on two English datasets (CoNLL 2003 and\nnews articles from Reuters and Bloomberg) and demonstrate an improvement of\nabout 7 percentage points in entity-level $F_1$ scores compared to an\nout-of-domain neural NER model."}, {"title": "NAT: Noise-Aware Training for Robust Neural Sequence Labeling", "authors": "Marcin Namysl, Sven Behnke, Joachim K\u00f6hler", "link": "https://arxiv.org/abs/2005.07162", "summary": "Sequence labeling systems should perform reliably not only under ideal\nconditions but also with corrupted inputs - as these systems often process\nuser-generated text or follow an error-prone upstream component. To this end,\nwe formulate the noisy sequence labeling problem, where the input may undergo\nan unknown noising process and propose two Noise-Aware Training (NAT)\nobjectives that improve robustness of sequence labeling performed on perturbed\ninput: Our data augmentation method trains a neural model using a mixture of\nclean and noisy samples, whereas our stability training algorithm encourages\nthe model to create a noise-invariant latent representation. We employ a\nvanilla noise model at training time. For evaluation, we use both the original\ndata and its variants perturbed with real OCR errors and misspellings.\nExtensive experiments on English and German named entity recognition benchmarks\nconfirmed that NAT consistently improved robustness of popular sequence\nlabeling models, preserving accuracy on the original input. We make our code\nand data publicly available for the research community."}, {"title": "Negative Training for Neural Dialogue Response Generation", "authors": "Tianxing He, James Glass", "link": "https://arxiv.org/abs/1903.02134", "summary": "Although deep learning models have brought tremendous advancements to the\nfield of open-domain dialogue response generation, recent research results have\nrevealed that the trained models have undesirable generation behaviors, such as\nmalicious responses and generic (boring) responses. In this work, we propose a\nframework named \"Negative Training\" to minimize such behaviors. Given a trained\nmodel, the framework will first find generated samples that exhibit the\nundesirable behavior, and then use them to feed negative training signals for\nfine-tuning the model. Our experiments show that negative training can\nsignificantly reduce the hit rate of malicious responses, or discourage\nfrequent responses and improve response diversity."}, {"title": "Neighborhood Matching Network for Entity Alignment", "authors": "Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Dongyan Zhao", "link": "http://arxiv.org/abs/2005.05607", "summary": "Structural heterogeneity between knowledge graphs is an outstanding challenge\nfor entity alignment. This paper presents Neighborhood Matching Network (NMN),\na novel entity alignment framework for tackling the structural heterogeneity\nchallenge. NMN estimates the similarities between entities to capture both the\ntopological structure and the neighborhood difference. It provides two\ninnovative components for better learning representations for entity alignment.\nIt first uses a novel graph sampling method to distill a discriminative\nneighborhood for each entity. It then adopts a cross-graph neighborhood\nmatching module to jointly encode the neighborhood difference for a given\nentity pair. Such strategies allow NMN to effectively construct\nmatching-oriented entity representations while ignoring noisy neighbors that\nhave a negative impact on the alignment task. Extensive experiments performed\non three entity alignment datasets show that NMN can well estimate the\nneighborhood similarity in more tough cases and significantly outperforms 12\nprevious state-of-the-art methods."}, {"title": "NeuInfer: Knowledge Inference on N-ary Facts", "authors": "Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng"}, {"title": "Neural CRF Model for Sentence Alignment in Text Simplification", "authors": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu", "link": "http://arxiv.org/abs/2005.02324", "summary": "The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation."}, {"title": "Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence", "authors": "Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, Dietrich Klakow", "link": "https://arxiv.org/abs/2005.01096", "summary": "The neural attention model has achieved great success in data-to-text\ngeneration tasks. Though usually excelling at producing fluent text, it suffers\nfrom the problem of information missing, repetition and \"hallucination\". Due to\nthe black-box nature of the neural attention architecture, avoiding these\nproblems in a systematic way is non-trivial. To address this concern, we\npropose to explicitly segment target text into fragment units and align them\nwith their data correspondences. The segmentation and correspondence are\njointly learned as latent variables without any human annotations. We further\nimpose a soft statistical constraint to regularize the segmental granularity.\nThe resulting architecture maintains the same expressive power as neural\nattention models, while being able to generate fully interpretable outputs with\nseveral times less computational cost. On both E2E and WebNLG benchmarks, we\nshow the proposed model consistently outperforms its neural attention\ncounterparts."}, {"title": "Neural Generation of Dialogue Response Timings", "authors": "Matthew Roddy, Naomi Harte", "link": "https://arxiv.org/abs/2005.09128", "summary": "The timings of spoken response offsets in human dialogue have been shown to\nvary based on contextual elements of the dialogue. We propose neural models\nthat simulate the distributions of these response offsets, taking into account\nthe response turn as well as the preceding turn. The models are designed to be\nintegrated into the pipeline of an incremental spoken dialogue system (SDS). We\nevaluate our models using offline experiments as well as human listening tests.\nWe show that human listeners consider certain response timings to be more\nnatural based on the dialogue context. The introduction of these models into\nSDS pipelines could increase the perceived naturalness of interactions."}, {"title": "Neural Mixed Counting Models for Dispersed Topic Discovery", "authors": "Jiemin Wu, Yanghui Rao, Zusheng Zhang, Haoran Xie, Qing Li, Fu Lee Wang, Ziye Chen"}, {"title": "Neural Reranking for Dependency Parsing: An Evaluation", "authors": "Bich-Ngoc Do, Ines Rehbein"}, {"title": "Neural Syntactic Preordering for Controlled Paraphrase Generation", "authors": "Tanya Goyal, Greg Durrett", "link": "https://arxiv.org/abs/2005.02013", "summary": "Paraphrasing natural language sentences is a multifaceted process: it might\ninvolve replacing individual words or short phrases, local rearrangement of\ncontent, or high-level restructuring like topicalization or passivization. Past\napproaches struggle to cover this space of paraphrase possibilities in an\ninterpretable manner. Our work, inspired by pre-ordering literature in machine\ntranslation, uses syntactic transformations to softly \"reorder'' the source\nsentence and guide our neural paraphrasing model. First, given an input\nsentence, we derive a set of feasible syntactic rearrangements using an\nencoder-decoder model. This model operates over a partially lexical, partially\nsyntactic view of the sentence and can reorder big chunks. Next, we use each\nproposed rearrangement to produce a sequence of position embeddings, which\nencourages our final encoder-decoder paraphrase model to attend to the source\nwords in a particular order. Our evaluation, both automatic and human, shows\nthat the proposed system retains the quality of the baseline approaches while\ngiving a substantial increase in the diversity of the generated paraphrases"}, {"title": "Neural Topic Modeling with Bidirectional Adversarial Training", "authors": "Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye, Haiyang Xu", "link": "https://arxiv.org/abs/2004.12331", "summary": "Recent years have witnessed a surge of interests of using neural topic models\nfor automatic topic extraction from text, since they avoid the complicated\nmathematical derivations for model inference as in traditional topic models\nsuch as Latent Dirichlet Allocation (LDA). However, these models either\ntypically assume improper prior (e.g. Gaussian or Logistic Normal) over latent\ntopic space or could not infer topic distribution for a given document. To\naddress these limitations, we propose a neural topic modeling approach, called\nBidirectional Adversarial Topic (BAT) model, which represents the first attempt\nof applying bidirectional adversarial training for neural topic modeling. The\nproposed BAT builds a two-way projection between the document-topic\ndistribution and the document-word distribution. It uses a generator to capture\nthe semantic patterns from texts and an encoder for topic inference.\nFurthermore, to incorporate word relatedness information, the Bidirectional\nAdversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To\nverify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are\nused in our experiments. The experimental results show that BAT and\nGaussian-BAT obtain more coherent topics, outperforming several competitive\nbaselines. Moreover, when performing text clustering based on the extracted\ntopics, our models outperform all the baselines, with more significant\nimprovements achieved by Gaussian-BAT where an increase of near 6\\% is observed\nin accuracy."}, {"title": "NILE : Natural Language Inference with Faithful Natural Language Explanations", "authors": "Sawan Kumar, Partha Talukdar"}, {"title": "Norm-Based Curriculum Learning for Neural Machine Translation", "authors": "Xuebo Liu, Houtim Lai, Derek F. Wong, Lidia S. Chao", "link": "https://arxiv.org/abs/2006.02014", "summary": "A neural machine translation (NMT) system is expensive to train, especially\nwith high-resource settings. As the NMT architectures become deeper and wider,\nthis issue gets worse and worse. In this paper, we aim to improve the\nefficiency of training an NMT by introducing a novel norm-based curriculum\nlearning method. We use the norm (aka length or module) of a word embedding as\na measure of 1) the difficulty of the sentence, 2) the competence of the model,\nand 3) the weight of the sentence. The norm-based sentence difficulty takes the\nadvantages of both linguistically motivated and model-based sentence\ndifficulties. It is easy to determine and contains learning-dependent features.\nThe norm-based model competence makes NMT learn the curriculum in a fully\nautomated way, while the norm-based sentence weight further enhances the\nlearning of the vector representation of the NMT. Experimental results for the\nWMT'14 English-German and WMT'17 Chinese-English translation tasks demonstrate\nthat the proposed method outperforms strong baselines in terms of BLEU score\n(+1.17/+1.56) and training speedup (2.22x/3.33x)."}, {"title": "Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses", "authors": "Erfan Sadeqi Azer, Daniel Khashabi, Ashish Sabharwal, Dan Roth", "link": "https://arxiv.org/abs/1911.03850", "summary": "Empirical research in Natural Language Processing (NLP) has adopted a narrow\nset of principles for assessing hypotheses, relying mainly on p-value\ncomputation, which suffers from several known issues. While alternative\nproposals have been well-debated and adopted in other fields, they remain\nrarely discussed or used within the NLP community. We address this gap by\ncontrasting various hypothesis assessment techniques, especially those not\ncommonly used in the field (such as evaluations based on Bayesian inference).\nSince these statistical techniques differ in the hypotheses they can support,\nwe argue that practitioners should first decide their target hypothesis before\nchoosing an assessment method. This is crucial because common fallacies,\nmisconceptions, and misinterpretation surrounding hypothesis assessment methods\noften stem from a discrepancy between what one would like to claim versus what\nthe method used actually assesses. Our survey reveals that these issues are\nomnipresent in the NLP research community. As a step forward, we provide best\npractices and guidelines tailored to NLP research, as well as an easy-to-use\npackage called 'HyBayes' for Bayesian assessment of hypotheses, complementing\nexisting tools."}, {"title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection", "authors": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg", "link": "https://arxiv.org/abs/2004.07667", "summary": "The ability to control for the kinds of information encoded in neural\nrepresentation has a variety of use cases, especially in light of the challenge\nof interpreting these models. We present Iterative Null-space Projection\n(INLP), a novel method for removing information from neural representations.\nOur method is based on repeated training of linear classifiers that predict a\ncertain property we aim to remove, followed by projection of the\nrepresentations on their null-space. By doing so, the classifiers become\noblivious to that target property, making it hard to linearly separate the data\naccording to it. While applicable for multiple uses, we evaluate our method on\nbias and fairness use-cases, and show that our method is able to mitigate bias\nin word embeddings, as well as to increase fairness in a setting of multi-class\nclassification."}, {"title": "Obtaining Faithful Interpretations from Compositional Neural Networks", "authors": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner", "link": "https://arxiv.org/abs/2005.00724", "summary": "Neural module networks (NMNs) are a popular approach for modeling\ncompositionality: they achieve high accuracy when applied to problems in\nlanguage and vision, while reflecting the compositional structure of the\nproblem in the network architecture. However, prior work implicitly assumed\nthat the structure of the network modules, describing the abstract reasoning\nprocess, provides a faithful explanation of the model's reasoning; that is,\nthat all modules perform their intended behaviour. In this work, we propose and\nconduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2\nand DROP, two datasets which require composing multiple reasoning steps. We\nfind that the intermediate outputs differ from the expected output,\nillustrating that the network structure does not provide a faithful explanation\nof model behaviour. To remedy that, we train the model with auxiliary\nsupervision and propose particular choices for module architecture that yield\nmuch better faithfulness, at a minimal cost to accuracy."}, {"title": "On Faithfulness and Factuality in Abstractive Summarization", "authors": "Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald", "link": "https://arxiv.org/abs/2005.00661", "summary": "It is well known that the standard likelihood training and approximate\ndecoding objectives in neural text generation models lead to less human-like\nresponses for open-ended tasks such as language modeling and story generation.\nIn this paper we have analyzed limitations of these models for abstractive\ndocument summarization and found that these models are highly prone to\nhallucinate content that is unfaithful to the input document. We conducted a\nlarge scale human evaluation of several neural abstractive summarization\nsystems to better understand the types of hallucinations they produce. Our\nhuman annotators found substantial amounts of hallucinated content in all model\ngenerated summaries. However, our analysis does show that pretrained models are\nbetter summarizers not only in terms of raw metrics, i.e., ROUGE, but also in\ngenerating faithful and factual summaries as evaluated by humans. Furthermore,\nwe show that textual entailment measures better correlate with faithfulness\nthan standard metrics, potentially leading the way to automatic evaluation\nmetrics as well as training and decoding criteria."}, {"title": "On the Cross-lingual Transferability of Monolingual Representations", "authors": "Mikel Artetxe, Sebastian Ruder, Dani Yogatama", "link": "https://arxiv.org/abs/1910.11856", "summary": "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT)\nhave been shown to generalize in a zero-shot cross-lingual setting. This\ngeneralization ability has been attributed to the use of a shared subword\nvocabulary and joint training across multiple languages giving rise to deep\nmultilingual abstractions. We evaluate this hypothesis by designing an\nalternative approach that transfers a monolingual model to new languages at the\nlexical level. More concretely, we first train a transformer-based masked\nlanguage model on one language, and transfer it to a new language by learning a\nnew embedding matrix with the same masked language modeling objective, freezing\nparameters of all other layers. This approach does not rely on a shared\nvocabulary or joint training. However, we show that it is competitive with\nmultilingual BERT on standard cross-lingual classification benchmarks and on a\nnew Cross-lingual Question Answering Dataset (XQuAD). Our results contradict\ncommon beliefs of the basis of the generalization ability of multilingual\nmodels and suggest that deep monolingual models learn some abstractions that\ngeneralize across languages. We also release XQuAD as a more comprehensive\ncross-lingual benchmark, which comprises 240 paragraphs and 1190\nquestion-answer pairs from SQuAD v1.1 translated into ten languages by\nprofessional translators."}, {"title": "On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond", "authors": "Chen Wu, Prince Zizhuang Wang, William Yang Wang", "link": "https://arxiv.org/abs/2004.09189", "summary": "Variational autoencoders (VAEs) combine latent variables with amortized\nvariational inference, whose optimization usually converges into a trivial\nlocal optimum termed posterior collapse, especially in text modeling. By\ntracking the optimization dynamics, we observe the encoder-decoder\nincompatibility that leads to poor parameterizations of the data manifold. We\nargue that the trivial local optimum may be avoided by improving the encoder\nand decoder parameterizations since the posterior network is part of a\ntransition map between them. To this end, we propose Coupled-VAE, which couples\na VAE model with a deterministic autoencoder with the same structure and\nimproves the encoder and decoder parameterizations via encoder weight sharing\nand decoder signal matching. We apply the proposed Coupled-VAE approach to\nvarious VAE models with different regularization, posterior family, decoder\nstructure, and optimization strategy. Experiments on benchmark datasets (i.e.,\nPTB, Yelp, and Yahoo) show consistently improved results in terms of\nprobability estimation and richness of the latent space. We also generalize our\nmethod to conditional language modeling and propose Coupled-CVAE, which largely\nimproves the diversity of dialogue generation on the Switchboard dataset."}, {"title": "On The Evaluation of Machine Translation SystemsTrained With Back-Translation", "authors": "Sergey Edunov, Myle Ott, Marc\u2019Aurelio Ranzato, Michael Auli", "link": "https://arxiv.org/abs/1908.05204", "summary": "Back-translation is a widely used data augmentation technique which leverages\ntarget monolingual data. However, its effectiveness has been challenged since\nautomatic metrics such as BLEU only show significant improvements for test\nexamples where the source itself is a translation, or translationese. This is\nbelieved to be due to translationese inputs better matching the back-translated\ntraining data. In this work, we show that this conjecture is not empirically\nsupported and that back-translation improves translation quality of both\nnaturally occurring text as well as translationese according to professional\nhuman translators. We provide empirical evidence to support the view that\nback-translation is preferred by humans because it produces more fluent\noutputs. BLEU cannot capture human preferences because references are\ntranslationese when source sentences are natural text. We recommend\ncomplementing BLEU with a language model score to measure fluency."}, {"title": "On the Inference Calibration of Neural Machine Translation", "authors": "Shuo Wang, Zhaopeng Tu, Shuming Shi, Yang Liu", "link": "https://arxiv.org/abs/2005.00963", "summary": "Confidence calibration, which aims to make model predictions equal to the\ntrue correctness measures, is important for neural machine translation (NMT)\nbecause it is able to offer useful indicators of translation errors in the\ngenerated output. While prior studies have shown that NMT models trained with\nlabel smoothing are well-calibrated on the ground-truth training data, we find\nthat miscalibration still remains a severe challenge for NMT during inference\ndue to the discrepancy between training and inference. By carefully designing\nexperiments on three language pairs, our work provides in-depth analyses of the\ncorrelation between calibration and translation performance as well as\nlinguistic properties of miscalibration and reports a number of interesting\nfindings that might help humans better analyze, understand and improve NMT\nmodels. Based on these observations, we further propose a new graduated label\nsmoothing method that can improve both inference calibration and translation\nperformance."}, {"title": "On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation", "authors": "Wei Zhao, Goran Glava\u0161, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger", "link": "http://arxiv.org/abs/2005.01196", "summary": "Evaluation of cross-lingual encoders is usually performed either via\nzero-shot cross-lingual transfer in supervised downstream tasks or via\nunsupervised cross-lingual textual similarity. In this paper, we concern\nourselves with reference-free machine translation (MT) evaluation where we\ndirectly compare source texts to (sometimes low-quality) system translations,\nwhich represents a natural adversarial setup for multilingual encoders.\nReference-free evaluation holds the promise of web-scale comparison of MT\nsystems. We systematically investigate a range of metrics based on\nstate-of-the-art cross-lingual semantic representations obtained with\npretrained M-BERT and LASER. We find that they perform poorly as semantic\nencoders for reference-free MT evaluation and identify their two key\nlimitations, namely, (a) a semantic mismatch between representations of mutual\ntranslations and, more prominently, (b) the inability to punish\n\"translationese\", i.e., low-quality literal translations. We propose two\npartial remedies: (1) post-hoc re-alignment of the vector spaces and (2)\ncoupling of semantic-similarity based metrics with target-side language\nmodeling. In segment-level MT evaluation, our best metric surpasses\nreference-based BLEU by 5.7 correlation points."}, {"title": "On the Robustness of Language Encoders against Grammatical Errors", "authors": "Fan Yin, Quanyu Long, Tao Meng, Kai-Wei Chang", "link": "https://arxiv.org/abs/2005.05683", "summary": "We conduct a thorough study to diagnose the behaviors of pre-trained language\nencoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical\nerrors. Specifically, we collect real grammatical errors from non-native\nspeakers and conduct adversarial attacks to simulate these errors on clean text\ndata. We use this approach to facilitate debugging models on downstream\napplications. Results confirm that the performance of all tested models is\naffected but the degree of impact varies. To interpret model behaviors, we\nfurther design a linguistic acceptability task to reveal their abilities in\nidentifying ungrammatical sentences and the position of errors. We find that\nfixed contextual encoders with a simple classifier trained on the prediction of\nsentence correctness are able to locate error positions. We also design a cloze\ntest for BERT and discover that BERT captures the interaction between errors\nand specific tokens in context. Our results shed light on understanding the\nrobustness and behaviors of language encoders against grammatical errors."}, {"title": "One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases", "authors": "Xingdi Yuan, Tong Wang, Rui Meng, Khushboo Thaker, Peter Brusilovsky, Daqing He, Adam Trischler", "link": "https://arxiv.org/abs/1810.05241", "summary": "Different texts shall by nature correspond to different number of keyphrases.\nThis desideratum is largely missing from existing neural keyphrase generation\nmodels. In this study, we address this problem from both modeling and\nevaluation perspectives.\n  We first propose a recurrent generative model that generates multiple\nkeyphrases as delimiter-separated sequences. Generation diversity is further\nenhanced with two novel techniques by manipulating decoder hidden states. In\ncontrast to previous approaches, our model is capable of generating diverse\nkeyphrases and controlling number of outputs.\n  We further propose two evaluation metrics tailored towards the\nvariable-number generation. We also introduce a new dataset StackEx that\nexpands beyond the only existing genre (i.e., academic writing) in keyphrase\ngeneration tasks. With both previous and new evaluation metrics, our model\noutperforms strong baselines on all datasets."}, {"title": "Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports", "authors": "Yuhao Zhang, Derek Merck, Emily Tsai, Christopher D. Manning, Curtis Langlotz", "link": "https://arxiv.org/abs/1911.02541", "summary": "Neural abstractive summarization models are able to generate summaries which\nhave high overlap with human references. However, existing models are not\noptimized for factual correctness, a critical metric in real-world\napplications. In this work, we develop a general framework where we evaluate\nthe factual correctness of a generated summary by fact-checking it\nautomatically against its reference using an information extraction module. We\nfurther propose a training strategy which optimizes a neural summarization\nmodel with a factual correctness reward via reinforcement learning. We apply\nthe proposed method to the summarization of radiology reports, where factual\ncorrectness is a key requirement. On two separate datasets collected from\nhospitals, we show via both automatic and human evaluation that the proposed\napproach substantially improves the factual correctness and overall quality of\noutputs over a competitive neural summarization system, producing radiology\nsummaries that approach the quality of human-authored ones."}, {"title": "Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding", "authors": "Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He, Bowen Zhou", "link": "https://arxiv.org/abs/1911.04910", "summary": "Translational distance-based knowledge graph embedding has shown progressive\nimprovements on the link prediction task, from TransE to the latest\nstate-of-the-art RotatE. However, N-1, 1-N and N-N predictions still remain\nchallenging. In this work, we propose a novel translational distance-based\napproach for knowledge graph link prediction. The proposed method includes\ntwo-folds, first we extend the RotatE from 2D complex domain to high dimension\nspace with orthogonal transforms to model relations for better modeling\ncapacity. Second, the graph context is explicitly modeled via two directed\ncontext representations. These context representations are used as part of the\ndistance scoring function to measure the plausibility of the triples during\ntraining and inference. The proposed approach effectively improves prediction\naccuracy on the difficult N-1, 1-N and N-N cases for knowledge graph link\nprediction task. The experimental results show that it achieves better\nperformance on two benchmark data sets compared to the baseline RotatE,\nespecially on data set (FB15k-237) with many high in-degree connection nodes."}, {"title": "Out of the Echo Chamber: Detecting Countering Debate Speeches", "authors": "Matan Orbach, Yonatan Bilu, Assaf Toledo, Dan Lahav, Michal Jacovi, Ranit Aharonov, Noam Slonim", "link": "https://arxiv.org/abs/2005.01157", "summary": "An educated and informed consumption of media content has become a challenge\nin modern times. With the shift from traditional news outlets to social media\nand similar venues, a major concern is that readers are becoming encapsulated\nin \"echo chambers\" and may fall prey to fake news and disinformation, lacking\neasy access to dissenting views. We suggest a novel task aiming to alleviate\nsome of these concerns -- that of detecting articles that most effectively\ncounter the arguments -- and not just the stance -- made in a given text. We\nstudy this problem in the context of debate speeches. Given such a speech, we\naim to identify, from among a set of speeches on the same topic and with an\nopposing stance, the ones that directly counter it. We provide a large dataset\nof 3,685 such speeches (in English), annotated for this relation, which\nhopefully would be of general interest to the NLP community. We explore several\nalgorithms addressing this task, and while some are successful, all fall short\nof expert human performance, suggesting room for further research. All data\ncollected during this work is freely available for research."}, {"title": "ParaCrawl: Web-Scale Acquisition of Parallel Corpora", "authors": "Marta Ba\u00f1\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl\u00e0-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ram\u00edrez-S\u00e1nchez, Elsa Sarr\u00edas, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, Jaume Zaragoza"}, {"title": "Parallel Corpus Filtering via Pre-trained Language Models", "authors": "Boliang Zhang, Ajay Nagesh, Kevin Knight", "link": "https://arxiv.org/abs/2005.06166", "summary": "Web-crawled data provides a good source of parallel corpora for training\nmachine translation models. It is automatically obtained, but extremely noisy,\nand recent work shows that neural machine translation systems are more\nsensitive to noise than traditional statistical machine translation methods. In\nthis paper, we propose a novel approach to filter out noisy sentence pairs from\nweb-crawled corpora via pre-trained language models. We measure sentence\nparallelism by leveraging the multilingual capability of BERT and use the\nGenerative Pre-training (GPT) language model as a domain filter to balance data\ndomains. We evaluate the proposed method on the WMT 2018 Parallel Corpus\nFiltering shared task, and on our own web-crawled Japanese-Chinese parallel\ncorpus. Our method significantly outperforms baselines and achieves a new\nstate-of-the-art. In an unsupervised setting, our method achieves comparable\nperformance to the top-1 supervised method. We also evaluate on a web-crawled\nJapanese-Chinese parallel corpus that we make publicly available."}, {"title": "Paraphrase Augmented Task-Oriented Dialog Generation", "authors": "Silin Gao, Yichi Zhang, Zhijian Ou, Zhou Yu", "link": "https://arxiv.org/abs/2004.07462", "summary": "Neural generative models have achieved promising performance on dialog\ngeneration tasks if given a huge data set. However, the lack of high-quality\ndialog data and the expensive data annotation process greatly limit their\napplication in real-world settings. We propose a paraphrase augmented response\ngeneration (PARG) framework that jointly trains a paraphrase model and a\nresponse generation model to improve the dialog generation performance. We also\ndesign a method to automatically construct paraphrase training data set based\non dialog state and dialog act labels. PARG is applicable to various dialog\ngeneration models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al.,\n2019). Experimental results show that the proposed framework improves these\nstate-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also\nsignificantly outperforms other data augmentation methods in dialog generation\ntasks, especially under low resource settings."}, {"title": "Paraphrase Generation by Learning How to Edit from Samples", "authors": "Amirhossein Kazemnejad, Mohammadreza Salehi, Mahdieh Soleymani Baghshah"}, {"title": "Parsing into Variable-in-situ Logico-Semantic Graphs", "authors": "Yufei Chen, Weiwei Sun"}, {"title": "Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT", "authors": "Zhiyong Wu, Yun Chen, Ben Kao, Qun Liu", "link": "https://arxiv.org/abs/2004.14786", "summary": "By introducing a small set of additional parameters, a probe learns to solve\nspecific linguistic tasks (e.g., dependency parsing) in a supervised manner\nusing feature representations (e.g., contextualized embeddings). The\neffectiveness of such probing tasks is taken as evidence that the pre-trained\nmodel encodes linguistic knowledge. However, this approach of evaluating a\nlanguage model is undermined by the uncertainty of the amount of knowledge that\nis learned by the probe itself. Complementary to those works, we propose a\nparameter-free probing technique for analyzing pre-trained language models\n(e.g., BERT). Our method does not require direct supervision from the probing\ntasks, nor do we introduce additional parameters to the probing process. Our\nexperiments on BERT show that syntactic trees recovered from BERT using our\nmethod are significantly better than linguistically-uninformed baselines. We\nfurther feed the empirically induced dependency structures into a downstream\nsentiment classification task and find its improvement compatible with or even\nsuperior to a human-designed dependency schema."}, {"title": "PeTra: A Sparsely Supervised Memory Model for People Tracking", "authors": "Shubham Toshniwal, Allyson Ettinger, Kevin Gimpel, Karen Livescu", "link": "http://arxiv.org/abs/2005.02990", "summary": "We propose PeTra, a memory-augmented neural network designed to track\nentities in its memory slots. PeTra is trained using sparse annotation from the\nGAP pronoun resolution dataset and outperforms a prior memory model on the task\nwhile using a simpler architecture. We empirically compare key modeling\nchoices, finding that we can simplify several aspects of the design of the\nmemory module while retaining strong performance. To measure the people\ntracking capability of memory models, we (a) propose a new diagnostic\nevaluation based on counting the number of unique entities in text, and (b)\nconduct a small scale human evaluation to compare evidence of people tracking\nin the memory logs of PeTra relative to a previous approach. PeTra is highly\neffective in both evaluations, demonstrating its ability to track people in its\nmemory despite being trained with limited annotation."}, {"title": "Phone Features Improve Speech Translation", "authors": "Elizabeth Salesky, Alan W Black", "link": "http://arxiv.org/abs/2005.13681"}, {"title": "Phonetic and Visual Priors for Decipherment of Informal Romanization", "authors": "Maria Ryskina, Matthew R. Gormley, Taylor Berg-Kirkpatrick", "link": "https://arxiv.org/abs/2005.02517", "summary": "Informal romanization is an idiosyncratic process used by humans in informal\ndigital communication to encode non-Latin script languages into Latin character\nsets found on common keyboards. Character substitution choices differ between\nusers but have been shown to be governed by the same main principles observed\nacross a variety of languages---namely, character pairs are often associated\nthrough phonetic or visual similarity. We propose a noisy-channel WFST cascade\nmodel for deciphering the original non-Latin script from observed romanized\ntext in an unsupervised fashion. We train our model directly on romanized data\nfrom two languages: Egyptian Arabic and Russian. We demonstrate that adding\ninductive bias through phonetic and visual priors on character mappings\nsubstantially improves the model's performance on both languages, yielding\nresults much closer to the supervised skyline. Finally, we introduce a new\ndataset of romanized Russian, collected from a Russian social network website\nand partially annotated for our experiments."}, {"title": "PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable", "authors": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang", "link": "https://arxiv.org/abs/1910.07931", "summary": "Pre-training models have been proved effective for a wide range of natural\nlanguage processing tasks. Inspired by this, we propose a novel dialogue\ngeneration pre-training framework to support various kinds of conversations,\nincluding chit-chat, knowledge grounded dialogues, and conversational question\nanswering. In this framework, we adopt flexible attention mechanisms to fully\nleverage the bi-directional context and the uni-directional characteristic of\nlanguage generation. We also introduce discrete latent variables to tackle the\ninherent one-to-many mapping problem in response generation. Two reciprocal\ntasks of response generation and latent act recognition are designed and\ncarried out simultaneously within a shared network. Comprehensive experiments\non three publicly available datasets verify the effectiveness and superiority\nof the proposed framework."}, {"title": "Politeness Transfer: A Tag and Generate Approach", "authors": "Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W Black, Shrimai Prabhumoye", "link": "https://arxiv.org/abs/2004.14257", "summary": "This paper introduces a new task of politeness transfer which involves\nconverting non-polite sentences to polite sentences while preserving the\nmeaning. We also provide a dataset of more than 1.39 instances automatically\nlabeled for politeness to encourage benchmark evaluations on this new task. We\ndesign a tag and generate pipeline that identifies stylistic attributes and\nsubsequently generates a sentence in the target style while preserving most of\nthe source content. For politeness as well as five other transfer tasks, our\nmodel outperforms the state-of-the-art methods on automatic metrics for content\npreservation, with a comparable or better performance on style transfer\naccuracy. Additionally, our model surpasses existing methods on human\nevaluations for grammaticality, meaning preservation and transfer accuracy\nacross all the six style transfer tasks. The data and code is located at\nhttps://github.com/tag-and-generate."}, {"title": "Posterior Control of Blackbox Generation", "authors": "Xiang Lisa Li, Alexander Rush", "link": "https://arxiv.org/abs/2005.04560", "summary": "Text generation often requires high-precision output that obeys task-specific\nrules. This fine-grained control is difficult to enforce with off-the-shelf\ndeep learning models. In this work, we consider augmenting neural generation\nmodels with discrete control states learned through a structured\nlatent-variable approach. Under this formulation, task-specific knowledge can\nbe encoded through a range of rich, posterior constraints that are effectively\ntrained into the model. This approach allows users to ground internal model\ndecisions based on prior knowledge, without sacrificing the representational\npower of neural generative models. Experiments consider applications of this\napproach for text generation. We find that this method improves over standard\nbenchmarks, while also providing fine-grained control."}, {"title": "Predicting Declension Class from Form and Meaning", "authors": "Adina Williams, Tiago Pimentel, Arya D. McCarthy, Hagen Blix, Eleanor Chodroff, Ryan Cotterell", "link": "https://arxiv.org/abs/2005.00626", "summary": "The noun lexica of many natural languages are divided into several declension\nclasses with characteristic morphological properties. Class membership is far\nfrom deterministic, but the phonological form of a noun and/or its meaning can\noften provide imperfect clues. Here, we investigate the strength of those\nclues. More specifically, we operationalize this by measuring how much\ninformation, in bits, we can glean about declension class from knowing the form\nand/or meaning of nouns. We know that form and meaning are often also\nindicative of grammatical gender---which, as we quantitatively verify, can\nitself share information with declension class---so we also control for gender.\nWe find for two Indo-European languages (Czech and German) that form and\nmeaning respectively share significant amounts of information with class (and\ncontribute additional information above and beyond gender). The three-way\ninteraction between class, form, and meaning (given gender) is also\nsignificant. Our study is important for two reasons: First, we introduce a new\nmethod that provides additional quantitative support for a classic linguistic\nfinding that form and meaning are relevant for the classification of nouns into\ndeclensions. Secondly, we show not only that individual declensions classes\nvary in the strength of their clues within a language, but also that these\nvariations themselves vary across languages."}, {"title": "Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts", "authors": "Alex Rinaldi, Jean Fox Tree, Snigdha Chaturvedi"}, {"title": "Predicting Performance for Natural Language Processing Tasks", "authors": "Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, Graham Neubig", "link": "https://arxiv.org/abs/2005.00870", "summary": "Given the complexity of combinations of tasks, languages, and domains in\nnatural language processing (NLP) research, it is computationally prohibitive\nto exhaustively test newly proposed models on each possible experimental\nsetting. In this work, we attempt to explore the possibility of gaining\nplausible judgments of how well an NLP model can perform under an experimental\nsetting, without actually training or testing the model. To do so, we build\nregression models to predict the evaluation score of an NLP experiment given\nthe experimental settings as input. Experimenting on 9 different NLP tasks, we\nfind that our predictors can produce meaningful predictions over unseen\nlanguages and different modeling architectures, outperforming reasonable\nbaselines as well as human experts. Going further, we outline how our predictor\ncan be used to find a small subset of representative experiments that should be\nrun in order to obtain plausible predictions for all other experimental\nsettings."}, {"title": "Predicting the Focus of Negation: Model and Error Analysis", "authors": "Md Mosharaf Hossain, Kathleen Hamilton, Alexis Palmer, Eduardo Blanco"}, {"title": "Predicting the Growth of Morphological Families from Social and Linguistic Factors", "authors": "Valentin Hofmann, Janet Pierrehumbert, Hinrich Sch\u00fctze"}, {"title": "Predicting the Topical Stance and Political Leaning of Media using Tweets", "authors": "Peter Stefanov, Kareem Darwish, Atanas Atanasov, Preslav Nakov", "link": "", "summary": ""}, {"title": "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview", "authors": "Deven Santosh Shah, H. Andrew Schwartz, Dirk Hovy", "link": "https://arxiv.org/abs/1912.11078", "summary": "An increasing number of works in natural language processing have addressed\nthe effect of bias on the predicted outcomes, introducing mitigation techniques\nthat act on different parts of the standard NLP pipeline (data and models).\nHowever, these works have been conducted in isolation, without a unifying\nframework to organize efforts within the field. This leads to repetitive\napproaches, and puts an undue focus on the effects of bias, rather than on\ntheir origins. Research focused on bias symptoms rather than the underlying\norigins could limit the development of effective countermeasures. In this\npaper, we propose a unifying conceptualization: the predictive bias framework\nfor NLP. We summarize the NLP literature and propose a general mathematical\ndefinition of predictive bias in NLP along with a conceptual framework,\ndifferentiating four main origins of biases: label bias, selection bias, model\noveramplification, and semantic bias. We discuss how past work has countered\neach bias origin. Our framework serves to guide an introductory overview of\npredictive bias in NLP, integrating existing work into a single structure and\nopening avenues for future research."}, {"title": "Premise Selection in Natural Language Mathematical Texts", "authors": "Deborah Ferreira, Andr\u00e9 Freitas", "link": "", "summary": ""}, {"title": "Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders", "authors": "Yu Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li", "link": "https://arxiv.org/abs/1911.03882", "summary": "Conditional Text Generation has drawn much attention as a topic of Natural\nLanguage Generation (NLG) which provides the possibility for humans to control\nthe properties of generated contents. Current conditional generation models\ncannot handle emerging conditions due to their joint end-to-end learning\nfashion. When a new condition added, these techniques require full retraining.\nIn this paper, we present a new framework named Pre-train and Plug-in\nVariational Auto-Encoder (PPVAE) towards flexible conditional text generation.\nPPVAE decouples the text generation module from the condition representation\nmodule to allow \"one-to-many\" conditional generation. When a fresh condition\nemerges, only a lightweight network needs to be trained and works as a plug-in\nfor PPVAE, which is efficient and desirable for real-world applications.\nExtensive experiments demonstrate the superiority of PPVAE against the existing\nalternatives with better conditionality and diversity but less training effort."}, {"title": "Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning", "authors": "Alexandre Tamborrino, Nicola Pellican\u00f2, Baptiste Pannier, Pascal Voitot, Louise Naudin", "link": "https://arxiv.org/abs/2004.14074", "summary": "Fine-tuning of pre-trained transformer models has become the standard\napproach for solving common NLP tasks. Most of the existing approaches rely on\na randomly initialized classifier on top of such networks. We argue that this\nfine-tuning procedure is sub-optimal as the pre-trained model has no prior on\nthe specific classifier labels, while it might have already learned an\nintrinsic textual representation of the task. In this paper, we introduce a new\nscoring method that casts a plausibility ranking task in a full-text format and\nleverages the masked language modeling head tuned during the pre-training\nphase. We study commonsense reasoning tasks where the model must rank a set of\nhypotheses given a premise, focusing on the COPA, Swag, HellaSwag and\nCommonsenseQA datasets. By exploiting our scoring method without fine-tuning,\nwe are able to produce strong baselines (e.g. 80% test accuracy on COPA) that\nare comparable to supervised approaches. Moreover, when fine-tuning directly on\nthe proposed scoring function, we show that our method provides a much more\nstable training phase across random restarts (e.g $\\times 10$ standard\ndeviation reduction on COPA test accuracy) and requires less annotated data\nthan the standard classifier approach to reach equivalent performances."}, {"title": "Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models", "authors": "Dan Iter, Kelvin Guu, Larry Lansing, Dan Jurafsky", "link": "http://arxiv.org/abs/2005.10389", "summary": "Recent models for unsupervised representation learning of text have employed\na number of techniques to improve contextual word representations but have put\nlittle focus on discourse-level representations. We propose CONPONO, an\ninter-sentence objective for pretraining language models that models discourse\ncoherence and the distance between sentences. Given an anchor sentence, our\nmodel is trained to predict the text k sentences away using a sampled-softmax\nobjective where the candidates consist of neighboring sentences and sentences\nrandomly sampled from the corpus. On the discourse representation benchmark\nDiscoEval, our model improves over the previous state-of-the-art by up to 13%\nand on average 4% absolute across 7 tasks. Our model is the same size as\nBERT-Base, but outperforms the much larger BERT- Large model and other more\nrecent approaches that incorporate discourse. We also show that CONPONO yields\ngains of 2%-6% absolute even for tasks that do not explicitly evaluate\ndiscourse: textual entailment (RTE), common sense reasoning (COPA) and reading\ncomprehension (ReCoRD)."}, {"title": "Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering", "authors": "Hao Cheng, Ming-Wei Chang, Kenton Lee, Kristina Toutanova", "link": "https://arxiv.org/abs/2005.01898", "summary": "We address the problem of extractive question answering using document-level\ndistant super-vision, pairing questions and relevant documents with answer\nstrings. We compare previously used probability space and distant super-vision\nassumptions (assumptions on the correspondence between the weak answer string\nlabels and possible answer mention spans). We show that these assumptions\ninteract, and that different configurations provide complementary benefits. We\ndemonstrate that a multi-objective model can efficiently combine the advantages\nof multiple assumptions and out-perform the best individual formulation. Our\napproach outperforms previous state-of-the-art models by 4.3 points in F1 on\nTriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries."}, {"title": "Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order", "authors": "Yi Liao, Xin Jiang, Qun Liu", "link": "https://arxiv.org/abs/2004.11579", "summary": "Masked language model and autoregressive language model are two types of\nlanguage models. While pretrained masked language models such as BERT overwhelm\nthe line of natural language understanding (NLU) tasks, autoregressive language\nmodels such as GPT are especially capable in natural language generation (NLG).\nIn this paper, we propose a probabilistic masking scheme for the masked\nlanguage model, which we call probabilistically masked language model (PMLM).\nWe implement a specific PMLM with a uniform prior distribution on the masking\nratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive\npermutated language model. One main advantage of the model is that it supports\ntext generation in arbitrary order with surprisingly good quality, which could\npotentially enable new applications over traditional unidirectional generation.\nBesides, the pretrained u-PMLM also outperforms BERT on a set of downstream NLU\ntasks."}, {"title": "Probing for referential information in language models", "authors": "Ionut-Teodor Sorodoc, Kristina Gulordava, Gemma Boleda"}, {"title": "Probing Linguistic Features of Sentence-Level Representations in Relation Extraction", "authors": "Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig", "link": "https://arxiv.org/abs/2004.08134", "summary": "Despite the recent progress, little is known about the features captured by\nstate-of-the-art neural relation extraction (RE) models. Common methods encode\nthe source sentence, conditioned on the entity mentions, before classifying the\nrelation. However, the complexity of the task makes it difficult to understand\nhow encoder architecture and supporting linguistic knowledge affect the\nfeatures learned by the encoder. We introduce 14 probing tasks targeting\nlinguistic properties relevant to RE, and we use them to study representations\nlearned by more than 40 different encoder architecture and linguistic feature\ncombinations trained on two datasets, TACRED and SemEval 2010 Task 8. We find\nthat the bias induced by the architecture and the inclusion of linguistic\nfeatures are clearly expressed in the probing task performance. For example,\nadding contextualized word representations greatly increases performance on\nprobing tasks with a focus on named entity and part-of-speech information, and\nyields better results in RE. In contrast, entity masking improves RE, but\nconsiderably lowers performance on entity type related probing tasks."}, {"title": "Probing Linguistic Systematicity", "authors": "Emily Goodwin, Koustuv Sinha, Timothy J. O\u2019Donnell", "link": "https://arxiv.org/abs/2005.04315", "summary": "Recently, there has been much interest in the question of whether deep\nnatural language understanding models exhibit systematicity; generalizing such\nthat units like words make consistent contributions to the meaning of the\nsentences in which they appear. There is accumulating evidence that neural\nmodels often generalize non-systematically. We examined the notion of\nsystematicity from a linguistic perspective, defining a set of probes and a set\nof metrics to measure systematic behaviour. We also identified ways in which\nnetwork architectures can generalize non-systematically, and discuss why such\nforms of generalization may be unsatisfying. As a case study, we performed a\nseries of experiments in the setting of natural language inference (NLI),\ndemonstrating that some NLU systems achieve high overall performance despite\nbeing non-systematic."}, {"title": "Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding", "authors": "Sebastian Weigelt, Vanessa Steurer, Tobias Hey, Walter F. Tichy"}, {"title": "PuzzLing Machines: A Challenge on Learning From Small Data", "authors": "G\u00f6zde G\u00fcl \u015eahin, Yova Kementchedjhieva, Phillip Rust, Iryna Gurevych", "link": "https://arxiv.org/abs/2004.13161", "summary": "Deep neural models have repeatedly proved excellent at memorizing surface\npatterns from large datasets for various ML and NLP benchmarks. They struggle\nto achieve human-like thinking, however, because they lack the skill of\niterative reasoning upon knowledge. To expose this problem in a new light, we\nintroduce a challenge on learning from small data, PuzzLing Machines, which\nconsists of Rosetta Stone puzzles from Linguistic Olympiads for high school\nstudents. These puzzles are carefully designed to contain only the minimal\namount of parallel text necessary to deduce the form of unseen expressions.\nSolving them does not require external information (e.g., knowledge bases,\nvisual signals) or linguistic expertise, but meta-linguistic awareness and\ndeductive skills. Our challenge contains around 100 puzzles covering a wide\nrange of linguistic phenomena from 81 languages. We show that both simple\nstatistical algorithms and state-of-the-art deep neural models perform\ninadequately on this challenge, as expected. We hope that this benchmark,\navailable at https://ukplab.github.io/PuzzLing-Machines/, inspires further\nefforts towards a new paradigm in NLP---one that is grounded in human-like\nreasoning and understanding."}, {"title": "Pyramid: A Layered Model for Nested Named Entity Recognition", "authors": "Jue Wang, Lidan Shou, Ke Chen, Gang Chen"}, {"title": "QuASE: Question-Answer Driven Sentence Encoding", "authors": "Hangfeng He, Qiang Ning, Dan Roth", "link": "https://arxiv.org/abs/1909.00333", "summary": "Question-answering (QA) data often encodes essential information in many\nfacets. This paper studies a natural question: Can we get supervision from QA\ndata for other tasks (typically, non-QA ones)? For example, {\\em can we use\nQAMR (Michael et al., 2017) to improve named entity recognition?} We suggest\nthat simply further pre-training BERT is often not the best option, and propose\nthe {\\em question-answer driven sentence encoding (QuASE)} framework. QuASE\nlearns representations from QA data, using BERT or other state-of-the-art\ncontextual language models. In particular, we observe the need to distinguish\nbetween two types of sentence encodings, depending on whether the target task\nis a single- or multi-sentence input; in both cases, the resulting encoding is\nshown to be an easy-to-use plugin for many downstream tasks. This work may\npoint out an alternative way to supervise NLP tasks."}, {"title": "R^3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge", "authors": "Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng", "link": "https://arxiv.org/abs/2004.13248", "summary": "We propose an unsupervised approach for sarcasm generation based on a\nnon-sarcastic input sentence. Our method employs a retrieve-and-edit framework\nto instantiate two major characteristics of sarcasm: reversal of valence and\nsemantic incongruity with the context which could include shared commonsense or\nworld knowledge between the speaker and the listener. While prior works on\nsarcasm generation predominantly focus on context incongruity, we show that\ncombining valence reversal and semantic incongruity based on the commonsense\nknowledge generates sarcasm of higher quality. Human evaluation shows that our\nsystem generates sarcasm better than human annotators 34% of the time, and\nbetter than a reinforced hybrid baseline 90% of the time."}, {"title": "Rationalizing Medical Relation Prediction from Corpus-level Statistics", "authors": "Zhen Wang, Jennifer Lee, Simon Lin, Huan Sun", "link": "https://arxiv.org/abs/2005.00889", "summary": "Nowadays, the interpretability of machine learning models is becoming\nincreasingly important, especially in the medical domain. Aiming to shed some\nlight on how to rationalize medical relation prediction, we present a new\ninterpretable framework inspired by existing theories on how human memory\nworks, e.g., theories of recall and recognition. Given the corpus-level\nstatistics, i.e., a global co-occurrence graph of a clinical text corpus, to\npredict the relations between two entities, we first recall rich contexts\nassociated with the target entities, and then recognize relational interactions\nbetween these contexts to form model rationales, which will contribute to the\nfinal prediction. We conduct experiments on a real-world public clinical\ndataset and show that our framework can not only achieve competitive predictive\nperformance against a comprehensive list of neural baseline models, but also\npresent rationales to justify its prediction. We further collaborate with\nmedical experts deeply to verify the usefulness of our model rationales for\nclinical decision making."}, {"title": "Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport", "authors": "Kyle Swanson, Lili Yu, Tao Lei", "link": "https://arxiv.org/abs/2005.13111", "summary": "Selecting input features of top relevance has become a popular method for\nbuilding self-explaining models. In this work, we extend this selective\nrationalization approach to text matching, where the goal is to jointly select\nand align text pieces, such as tokens or sentences, as a justification for the\ndownstream prediction. Our approach employs optimal transport (OT) to find a\nminimal cost alignment between the inputs. However, directly applying OT often\nproduces dense and therefore uninterpretable alignments. To overcome this\nlimitation, we introduce novel constrained variants of the OT problem that\nresult in highly sparse alignments with controllable sparsity. Our model is\nend-to-end differentiable using the Sinkhorn algorithm for OT and can be\ntrained without any alignment annotations. We evaluate our model on the\nStackExchange, MultiNews, e-SNLI, and MultiRC datasets. Our model achieves very\nsparse rationale selections with high fidelity while preserving prediction\naccuracy compared to strong attention baseline models."}, {"title": "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers", "authors": "Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson", "link": "", "summary": ""}, {"title": "Reasoning Over Semantic-Level Graph for Fact Checking", "authors": "Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin", "link": "https://arxiv.org/abs/1909.03745", "summary": "Fact checking is a challenging task because verifying the truthfulness of a\nclaim requires reasoning about multiple retrievable evidence. In this work, we\npresent a method suitable for reasoning about the semantic-level structure of\nevidence. Unlike most previous works, which typically represent evidence\nsentences with either string concatenation or fusing the features of isolated\nevidence sentences, our approach operates on rich semantic structures of\nevidence obtained by semantic role labeling. We propose two mechanisms to\nexploit the structure of evidence while leveraging the advances of pre-trained\nmodels like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we\nfirst utilize the graph structure to re-define the relative distances of words,\nwith the intuition that semantically related words should have short distances.\nThen, we adopt graph convolutional network and graph attention network to\npropagate and aggregate information from neighboring nodes on the graph. We\nevaluate our system on FEVER, a benchmark dataset for fact checking, and find\nthat rich structural information is helpful and both our graph-based mechanisms\nimprove the accuracy. Our model is the state-of-the-art system in terms of both\nofficial evaluation metrics, namely claim verification accuracy and FEVER\nscore."}, {"title": "Reasoning with Latent Structure Refinement for Document-Level Relation Extraction", "authors": "Guoshun Nan, Zhijiang Guo, Ivan Sekulic, Wei Lu", "link": "https://arxiv.org/abs/2005.06312", "summary": "Document-level relation extraction requires integrating information within\nand across multiple sentences of a document and capturing complex interactions\nbetween inter-sentence entities. However, effective aggregation of relevant\ninformation in the document remains a challenging research question. Existing\napproaches construct static document-level graphs based on syntactic trees,\nco-references or heuristics from the unstructured text to model the\ndependencies. Unlike previous methods that may not be able to capture rich\nnon-local interactions for inference, we propose a novel model that empowers\nthe relational reasoning across sentences by automatically inducing the latent\ndocument-level graph. We further develop a refinement strategy, which enables\nthe model to incrementally aggregate relevant information for multi-hop\nreasoning. Specifically, our model achieves an F1 score of 59.05 on a\nlarge-scale document-level dataset (DocRED), significantly improving over the\nprevious results, and also yields new state-of-the-art results on the CDR and\nGDA dataset. Furthermore, extensive analyses show that the model is able to\ndiscover more accurate inter-sentence relations."}, {"title": "Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association", "authors": "Nan Xu, Zhixiong Zeng, Wenji Mao"}, {"title": "(Re)construing Meaning in NLP", "authors": "Sean Trott, Tiago Timponi Torrent, Nancy Chang, Nathan Schneider", "link": "https://arxiv.org/abs/2005.09099", "summary": "Human speakers have an extensive toolkit of ways to express themselves. In\nthis paper, we engage with an idea largely absent from discussions of meaning\nin natural language understanding--namely, that the way something is expressed\nreflects different ways of conceptualizing or construing the information being\nconveyed. We first define this phenomenon more precisely, drawing on\nconsiderable prior work in theoretical cognitive semantics and\npsycholinguistics. We then survey some dimensions of construed meaning and show\nhow insights from construal could inform theoretical and practical work in NLP."}, {"title": "Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension", "authors": "Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen, Dong Yu", "link": "https://arxiv.org/abs/2005.08056", "summary": "In this paper, we study machine reading comprehension (MRC) on long texts,\nwhere a model takes as inputs a lengthy document and a question and then\nextracts a text span from the document as an answer. State-of-the-art models\ntend to use a pretrained transformer model (e.g., BERT) to encode the joint\ncontextual information of document and question. However, these\ntransformer-based models can only take a fixed-length (e.g., 512) text as its\ninput. To deal with even longer text inputs, previous approaches usually chunk\nthem into equally-spaced segments and predict answers based on each segment\nindependently without considering the information from other segments. As a\nresult, they may form segments that fail to cover the correct answer span or\nretain insufficient contexts around it, which significantly degrades the\nperformance. Moreover, they are less capable of answering questions that need\ncross-segment information.\n  We propose to let a model learn to chunk in a more flexible way via\nreinforcement learning: a model can decide the next segment that it wants to\nprocess in either direction. We also employ recurrent mechanisms to enable\ninformation to flow across segments. Experiments on three MRC datasets -- CoQA,\nQuAC, and TriviaQA -- demonstrate the effectiveness of our proposed recurrent\nchunking mechanisms: we can obtain segments that are more likely to contain\ncomplete answers and at the same time provide sufficient contexts around the\nground truth answers for better predictions."}, {"title": "Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment", "authors": "Forrest Davis, Marten van Schijndel", "link": "https://arxiv.org/abs/2005.00165", "summary": "A standard approach to evaluating language models analyzes how models assign\nprobabilities to valid versus invalid syntactic constructions (i.e. is a\ngrammatical sentence more probable than an ungrammatical sentence). Our work\nuses ambiguous relative clause attachment to extend such evaluations to cases\nof multiple simultaneous valid interpretations, where stark grammaticality\ndifferences are absent. We compare model performance in English and Spanish to\nshow that non-linguistic biases in RNN LMs advantageously overlap with\nsyntactic structure in English but not Spanish. Thus, English models may appear\nto acquire human-like syntactic preferences, while models trained on Spanish\nfail to acquire comparable human-like preferences. We conclude by relating\nthese results to broader concerns about the relationship between comprehension\n(i.e. typical language model use cases) and production (which generates the\ntraining data for language models), suggesting that necessary linguistic biases\nare not present in the training signal at all."}, {"title": "Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem", "authors": "Danielle Saunders, Bill Byrne", "link": "https://arxiv.org/abs/2004.04498", "summary": "Training data for NLP tasks often exhibits gender bias in that fewer\nsentences refer to women than to men. In Neural Machine Translation (NMT)\ngender bias has been shown to reduce translation quality, particularly when the\ntarget language has grammatical gender. The recent WinoMT challenge set allows\nus to measure this effect directly (Stanovsky et al, 2019).\n  Ideally we would reduce system bias by simply debiasing all data prior to\ntraining, but achieving this effectively is itself a challenge. Rather than\nattempt to create a `balanced' dataset, we use transfer learning on a small set\nof trusted, gender-balanced examples. This approach gives strong and consistent\nimprovements in gender debiasing with much less computational cost than\ntraining from scratch.\n  A known pitfall of transfer learning on new domains is `catastrophic\nforgetting', which we address both in adaptation and in inference. During\nadaptation we show that Elastic Weight Consolidation allows a performance\ntrade-off between general translation quality and bias reduction. During\ninference we propose a lattice-rescoring scheme which outperforms all systems\nevaluated in Stanovsky et al (2019) on WinoMT with no degradation of general\ntest set BLEU, and we show this scheme can be applied to remove gender bias in\nthe output of `black box` online commercial MT systems. We demonstrate our\napproach translating from English into three languages with varied linguistic\nproperties and data availability."}, {"title": "Refer360\u00b0 : A Referring Expression Recognition Dataset in 360\u00b0 Images", "authors": "Volkan Cirik, Taylor Berg-Kirkpatrick, Louis-Philippe Morency"}, {"title": "ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding", "authors": "Zhiwen Xie, Guangyou Zhou, Jin Liu, Jimmy Xiangji Huang", "link": "", "summary": ""}, {"title": "Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents", "authors": "Daoyuan Chen, Yaliang Li, Kai Lei, Ying Shen", "link": "https://arxiv.org/abs/2004.09930", "summary": "Distant supervision based methods for entity and relation extraction have\nreceived increasing popularity due to the fact that these methods require light\nhuman annotation efforts. In this paper, we consider the problem of\n\\textit{shifted label distribution}, which is caused by the inconsistency\nbetween the noisy-labeled training set subject to external knowledge graph and\nthe human-annotated test set, and exacerbated by the pipelined\nentity-then-relation extraction manner with noise propagation. We propose a\njoint extraction approach to address this problem by re-labeling noisy\ninstances with a group of cooperative multiagents. To handle noisy instances in\na fine-grained manner, each agent in the cooperative group evaluates the\ninstance by calculating a continuous confidence score from its own perspective;\nTo leverage the correlations between these two extraction tasks, a confidence\nconsensus module is designed to gather the wisdom of all agents and\nre-distribute the noisy training set with confidence-scored labels. Further,\nthe confidences are used to adjust the training losses of extractors.\nExperimental results on two real-world datasets verify the benefits of\nre-labeling noisy instance, and show that the proposed model significantly\noutperforms the state-of-the-art entity and relation extraction methods."}, {"title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis", "authors": "Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, Rui Wang", "link": "https://arxiv.org/abs/2004.12362", "summary": "Aspect-based sentiment analysis aims to determine the sentiment polarity\ntowards a specific aspect in online reviews. Most recent efforts adopt\nattention-based neural network models to implicitly connect aspects with\nopinion words. However, due to the complexity of language and the existence of\nmultiple aspects in a single sentence, these models often confuse the\nconnections. In this paper, we address this problem by means of effective\nencoding of syntax information. Firstly, we define a unified aspect-oriented\ndependency tree structure rooted at a target aspect by reshaping and pruning an\nordinary dependency parse tree. Then, we propose a relational graph attention\nnetwork (R-GAT) to encode the new tree structure for sentiment prediction.\nExtensive experiments are conducted on the SemEval 2014 and Twitter datasets,\nand the experimental results confirm that the connections between aspects and\nopinion words can be better established with our approach, and the performance\nof the graph attention network (GAT) is significantly improved as a\nconsequence."}, {"title": "Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis", "authors": "Zhuang Chen, Tieyun Qian", "link": "", "summary": ""}, {"title": "Representation Learning for Information Extraction from Form-like Documents", "authors": "Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, Marc Najork"}, {"title": "Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation", "authors": "Zhiliang Tian, Wei Bi, Dongkyu Lee, Lanqing Xue, Yiping Song, Xiaojiang Liu, Nevin L. Zhang", "link": "https://arxiv.org/abs/2005.06128", "summary": "Neural conversation models are known to generate appropriate but\nnon-informative responses in general. A scenario where informativeness can be\nsignificantly enhanced is Conversing by Reading (CbR), where conversations take\nplace with respect to a given external document. In previous work, the external\ndocument is utilized by (1) creating a context-aware document memory that\nintegrates information from the document and the conversational context, and\nthen (2) generating responses referring to the memory. In this paper, we\npropose to create the document memory with some anticipated responses in mind.\nThis is achieved using a teacher-student framework. The teacher is given the\nexternal document, the context, and the ground-truth response, and learns how\nto build a response-aware document memory from three sources of information.\nThe student learns to construct a response-anticipated document memory from the\nfirst two sources, and the teacher's insight on memory creation. Empirical\nresults show that our model outperforms the previous state-of-the-art for the\nCbR task."}, {"title": "Rethinking Dialogue State Tracking with Reasoning", "authors": "Lizi Liao, Yunshan Ma, Wenqiang Lei, Tat-Seng Chua", "link": "https://arxiv.org/abs/2005.13129", "summary": "Tracking dialogue states to better interpret user goals and feed downstream\npolicy learning is a bottleneck in dialogue management. Common practice has\nbeen to treat it as a problem of classifying dialogue content into a set of\npre-defined slot-value pairs, or generating values for different slots given\nthe dialogue history. Both have limitations on considering dependencies that\noccur on dialogues, and are lacking of reasoning capabilities. This paper\nproposes to track dialogue states gradually with reasoning over dialogue turns\nwith the help of the back-end data. Empirical results demonstrate that our\nmethod significantly outperforms the state-of-the-art methods by 38.6% in terms\nof joint belief accuracy for MultiWOZ 2.1, a large-scale human-human dialogue\ndataset across multiple domains."}, {"title": "Review-based Question Generation with Adaptive Instance Transfer and Augmentation", "authors": "Qian Yu, Lidong Bing, Qiong Zhang, Wai Lam, Luo Si", "link": "https://arxiv.org/abs/1911.01556", "summary": "Online reviews provide rich information about products and service, while it\nremains inefficient for potential consumers to exploit the reviews for\nfulfilling their specific information need. We propose to explore question\ngeneration as a new way of exploiting review information. One major challenge\nof this task is the lack of review-question pairs for training a neural\ngeneration model. We propose an iterative learning framework for handling this\nchallenge via adaptive transfer and augmentation of the training instances with\nthe help of the available user-posed question-answer data. To capture the\naspect characteristics in reviews, the augmentation and generation procedures\nincorporate related features extracted via unsupervised learning. Experiments\non data from 10 categories of a popular E-commerce site demonstrate the\neffectiveness of the framework, as well as the usefulness of the new task."}, {"title": "Revisiting the Context Window for Cross-lingual Word Embeddings", "authors": "Ryokan Ri, Yoshimasa Tsuruoka", "link": "https://arxiv.org/abs/2004.10813", "summary": "Existing approaches to mapping-based cross-lingual word embeddings are based\non the assumption that the source and target embedding spaces are structurally\nsimilar. The structures of embedding spaces largely depend on the co-occurrence\nstatistics of each word, which the choice of context window determines. Despite\nthis obvious connection between the context window and mapping-based\ncross-lingual embeddings, their relationship has been underexplored in prior\nwork. In this work, we provide a thorough evaluation, in various languages,\ndomains, and tasks, of bilingual embeddings trained with different context\nwindows. The highlight of our findings is that increasing the size of both the\nsource and target window sizes improves the performance of bilingual lexicon\ninduction, especially the performance on frequent nouns."}, {"title": "Rigid Formats Controlled Text Generation", "authors": "Piji Li, Haisong Zhang, Xiaojiang Liu, Shuming Shi", "link": "http://arxiv.org/abs/2004.08022", "summary": "Neural text generation has made tremendous progress in various tasks. One\ncommon characteristic of most of the tasks is that the texts are not restricted\nto some rigid formats when generating. However, we may confront some special\ntext paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi\n(classical Chinese poetry of the Song dynasty), etc. The typical\ncharacteristics of these texts are in three folds: (1) They must comply fully\nwith the rigid predefined formats. (2) They must obey some rhyming schemes. (3)\nAlthough they are restricted to some formats, the sentence integrity must be\nguaranteed. To the best of our knowledge, text generation based on the\npredefined rigid formats has not been well investigated. Therefore, we propose\na simple and elegant framework named SongNet to tackle this problem. The\nbackbone of the framework is a Transformer-based auto-regressive language\nmodel. Sets of symbols are tailor-designed to improve the modeling performance\nespecially on format, rhyme, and sentence integrity. We improve the attention\nmechanism to impel the model to capture some future information on the format.\nA pre-training and fine-tuning framework is designed to further improve the\ngeneration quality. Extensive experiments conducted on two collected corpora\ndemonstrate that our proposed framework generates significantly better results\nin terms of both automatic metrics and the human evaluation."}, {"title": "RikiNet: Reading Wikipedia Pages for Natural Question Answering", "authors": "Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Daxin Jiang, Jiancheng Lv, Nan Duan", "link": "https://arxiv.org/abs/2004.14560", "summary": "Reading long documents to answer open-domain questions remains challenging in\nnatural language understanding. In this paper, we introduce a new model, called\nRikiNet, which reads Wikipedia pages for natural question answering. RikiNet\ncontains a dynamic paragraph dual-attention reader and a multi-level cascaded\nanswer predictor. The reader dynamically represents the document and question\nby utilizing a set of complementary attention mechanisms. The representations\nare then fed into the predictor to obtain the span of the short answer, the\nparagraph of the long answer, and the answer type in a cascaded manner. On the\nNatural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1\non long-answer and short-answer tasks. To our best knowledge, it is the first\nsingle model that outperforms the single human performance. Furthermore, an\nensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer\ntasks, achieving the best performance on the official NQ leaderboard"}, {"title": "Robust Encodings: A Framework for Combating Adversarial Typos", "authors": "Erik Jones, Robin Jia, Aditi Raghunathan, Percy Liang", "link": "https://arxiv.org/abs/2005.01229", "summary": "Despite excellent performance on many tasks, NLP systems are easily fooled by\nsmall adversarial perturbations of inputs. Existing procedures to defend\nagainst such perturbations are either (i) heuristic in nature and susceptible\nto stronger attacks or (ii) provide guaranteed robustness to worst-case\nattacks, but are incompatible with state-of-the-art models like BERT. In this\nwork, we introduce robust encodings (RobEn): a simple framework that confers\nguaranteed robustness, without making compromises on model architecture. The\ncore component of RobEn is an encoding function, which maps sentences to a\nsmaller, discrete space of encodings. Systems using these encodings as a\nbottleneck confer guaranteed robustness with standard training, and the same\nencodings can be used across multiple tasks. We identify two desiderata to\nconstruct robust encoding functions: perturbations of a sentence should map to\na small set of encodings (stability), and models using encodings should still\nperform well (fidelity). We instantiate RobEn to defend against a large family\nof adversarial typos. Across six tasks from GLUE, our instantiation of RobEn\npaired with BERT achieves an average robust accuracy of 71.3% against all\nadversarial typos in the family considered, while previous work using a\ntypo-corrector achieves only 35.3% accuracy against a simple greedy attack."}, {"title": "Roles and Utilization of Attention Heads in Transformer-based Neural Language Models", "authors": "Jae-young Jo, Sung-Hyon Myaeng"}, {"title": "S2ORC: The Semantic Scholar Open Research Corpus", "authors": "Kyle Lo, Lucy Wang, Mark Neumann, Rodney Kinney, Daniel Weld", "link": "", "summary": ""}, {"title": "SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing", "authors": "Jiaying Hu, Yan Yang, Chencai Chen, Liang He, Zhou Yu"}, {"title": "SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations", "authors": "Xiang Kong, Varun Gangal, Eduard Hovy", "link": "https://arxiv.org/abs/2004.12934", "summary": "We introduce SCDE, a dataset to evaluate the performance of computational\nmodels through sentence prediction. SCDE is a human-created sentence cloze\ndataset, collected from public school English examinations. Our task requires a\nmodel to fill up multiple blanks in a passage from a shared candidate set with\ndistractors designed by English teachers. Experimental results demonstrate that\nthis task requires the use of non-local, discourse-level context beyond the\nimmediate sentence neighborhood. The blanks require joint solving and\nsignificantly impair each other's context. Furthermore, through ablations, we\nshow that the distractors are of high quality and make the task more\nchallenging. Our experiments show that there is a significant performance gap\nbetween advanced models (72%) and humans (87%), encouraging future models to\nbridge this gap."}, {"title": "schuBERT: Optimizing Elements of BERT", "authors": "Ashish Khetan, Zohar Karnin", "link": "", "summary": ""}, {"title": "SciREX: A Challenge Dataset for Document-Level Information Extraction", "authors": "Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, Iz Beltagy", "link": "https://arxiv.org/abs/2005.00512", "summary": "Extracting information from full documents is an important problem in many\ndomains, but most previous work focus on identifying relationships within a\nsentence or a paragraph. It is challenging to create a large-scale information\nextraction (IE) dataset at the document level since it requires an\nunderstanding of the whole document to annotate entities and their\ndocument-level relationships that usually span beyond sentences or even\nsections. In this paper, we introduce SciREX, a document level IE dataset that\nencompasses multiple IE tasks, including salient entity identification and\ndocument level $N$-ary relation identification from scientific articles. We\nannotate our dataset by integrating automatic and human annotations, leveraging\nexisting scientific knowledge resources. We develop a neural model as a strong\nbaseline that extends previous state-of-the-art IE models to document-level IE.\nAnalyzing the model performance shows a significant gap between human\nperformance and current baselines, inviting the community to use our dataset as\na challenge to develop document-level IE models. Our data and code are publicly\navailable at https://github.com/allenai/SciREX"}, {"title": "Screenplay Summarization Using Latent Narrative Structure", "authors": "Pinelopi Papalampidi, Frank Keller, Lea Frermann, Mirella Lapata", "link": "https://arxiv.org/abs/2004.12727", "summary": "Most general-purpose extractive summarization models are trained on news\narticles, which are short and present all important information upfront. As a\nresult, such models are biased on position and often perform a smart selection\nof sentences from the beginning of the document. When summarizing long\nnarratives, which have complex structure and present information piecemeal,\nsimple position heuristics are not sufficient. In this paper, we propose to\nexplicitly incorporate the underlying structure of narratives into general\nunsupervised and supervised extractive summarization models. We formalize\nnarrative structure in terms of key narrative events (turning points) and treat\nit as latent in order to summarize screenplays (i.e., extract an optimal\nsequence of scenes). Experimental results on the CSI corpus of TV screenplays,\nwhich we augment with scene-level summarization labels, show that latent\nturning points correlate with important aspects of a CSI episode and improve\nsummarization performance over general extractive algorithms leading to more\ncomplete and diverse summaries."}, {"title": "ScriptWriter: Narrative-Guided Script Generation", "authors": "Yutao Zhu, Ruihua Song, Zhicheng Dou, Jian-Yun Nie, Jin Zhou", "link": "https://arxiv.org/abs/2005.10331", "summary": "It is appealing to have a system that generates a story or scripts\nautomatically from a story-line, even though this is still out of our reach. In\ndialogue systems, it would also be useful to drive dialogues by a dialogue\nplan. In this paper, we address a key problem involved in these applications --\nguiding a dialogue by a narrative. The proposed model ScriptWriter selects the\nbest response among the candidates that fit the context as well as the given\nnarrative. It keeps track of what in the narrative has been said and what is to\nbe said. A narrative plays a different role than the context (i.e., previous\nutterances), which is generally used in current dialogue systems. Due to the\nunavailability of data for this new application, we construct a new large-scale\ndata collection GraphMovie from a movie website where end-users can upload\ntheir narratives freely when watching a movie. Experimental results on the\ndataset show that our proposed approach based on narratives significantly\noutperforms the baselines that simply use the narrative as a kind of context."}, {"title": "SEEK: Segmented Embedding of Knowledge Graphs", "authors": "Wentao Xu, Shun Zheng, Liang He, Bin Shao, Jian Yin, Tie-Yan Liu", "link": "https://arxiv.org/abs/2005.00856", "summary": "In recent years, knowledge graph embedding becomes a pretty hot research\ntopic of artificial intelligence and plays increasingly vital roles in various\ndownstream applications, such as recommendation and question answering.\nHowever, existing methods for knowledge graph embedding can not make a proper\ntrade-off between the model complexity and the model expressiveness, which\nmakes them still far from satisfactory. To mitigate this problem, we propose a\nlightweight modeling framework that can achieve highly competitive relational\nexpressiveness without increasing the model complexity. Our framework focuses\non the design of scoring functions and highlights two critical characteristics:\n1) facilitating sufficient feature interactions; 2) preserving both symmetry\nand antisymmetry properties of relations. It is noteworthy that owing to the\ngeneral and elegant design of scoring functions, our framework can incorporate\nmany famous existing methods as special cases. Moreover, extensive experiments\non public benchmarks demonstrate the efficiency and effectiveness of our\nframework. Source codes and data can be found at\n\\url{https://github.com/Wentao-Xu/SEEK}."}, {"title": "Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation", "authors": "Xabier Soto, Dimitar Shterionov, Alberto Poncelas, Andy Way", "link": "http://arxiv.org/abs/2005.00308", "summary": "Machine translation (MT) has benefited from using synthetic training data\noriginating from translating monolingual corpora, a technique known as\nbacktranslation. Combining backtranslated data from different sources has led\nto better results than when using such data in isolation. In this work we\nanalyse the impact that data translated with rule-based, phrase-based\nstatistical and neural MT systems has on new MT systems. We use a real-world\nlow-resource use-case (Basque-to-Spanish in the clinical domain) as well as a\nhigh-resource language pair (German-to-English) to test different scenarios\nwith backtranslation and employ data selection to optimise the synthetic\ncorpora. We exploit different data selection strategies in order to reduce the\namount of data used, while at the same time maintaining high-quality MT\nsystems. We further tune the data selection method by taking into account the\nquality of the MT systems used for backtranslation and lexical diversity of the\nresulting corpora. Our experiments show that incorporating backtranslated data\nfrom different sources can be beneficial, and that availing of data selection\ncan yield improved performance."}, {"title": "Selective Question Answering under Domain Shift", "authors": "Amita Kamath, Robin Jia, Percy Liang"}, {"title": "Semantic Graphs for Generating Deep Questions", "authors": "Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan", "link": "https://arxiv.org/abs/2004.12704", "summary": "This paper proposes the problem of Deep Question Generation (DQG), which aims\nto generate complex questions that require reasoning over multiple pieces of\ninformation of the input passage. In order to capture the global structure of\nthe document and facilitate reasoning, we propose a novel framework which first\nconstructs a semantic-level graph for the input document and then encodes the\nsemantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards,\nwe fuse the document-level and graph-level representations to perform joint\ntraining of content selection and question decoding. On the HotpotQA\ndeep-question centric dataset, our model greatly improves performance over\nquestions requiring reasoning over multiple facts, leading to state-of-the-art\nperformance. The code is publicly available at\nhttps://github.com/WING-NUS/SG-Deep-Question-Generation."}, {"title": "Semantic Parsing for English as a Second Language", "authors": "Yuanyuan Zhao, Weiwei Sun, Junjie Cao, Xiaojun Wan"}, {"title": "Semantic Scaffolds for Pseudocode-to-Code Generation", "authors": "Ruiqi Zhong, Mitchell Stern, Dan Klein", "link": "http://arxiv.org/abs/2005.05927", "summary": "We propose a method for program generation based on semantic scaffolds,\nlightweight structures representing the high-level semantic and syntactic\ncomposition of a program. By first searching over plausible scaffolds then\nusing these as constraints for a beam search over programs, we achieve better\ncoverage of the search space when compared with existing techniques. We apply\nour hierarchical search method to the SPoC dataset for pseudocode-to-code\ngeneration, in which we are given line-level natural language pseudocode\nannotations and aim to produce a program satisfying execution-based test cases.\nBy using semantic scaffolds during inference, we achieve a 10% absolute\nimprovement in top-100 accuracy over the previous state-of-the-art.\nAdditionally, we require only 11 candidates to reach the top-3000 performance\nof the previous best approach when tested against unseen problems,\ndemonstrating a substantial improvement in efficiency."}, {"title": "Semi-supervised Contextual Historical Text Normalization", "authors": "Peter Makarov, Simon Clematide"}, {"title": "Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation", "authors": "Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang", "link": "https://arxiv.org/abs/2005.04379", "summary": "Dialogue policy optimization often obtains feedback until task completion in\ntask-oriented dialogue systems. This is insufficient for training intermediate\ndialogue turns since supervision signals (or rewards) are only provided at the\nend of dialogues. To address this issue, reward learning has been introduced to\nlearn from state-action pairs of an optimal policy to provide turn-by-turn\nrewards. This approach requires complete state-action annotations of\nhuman-to-human dialogues (i.e., expert demonstrations), which is labor\nintensive. To overcome this limitation, we propose a novel reward learning\napproach for semi-supervised policy learning. The proposed approach learns a\ndynamics model as the reward function which models dialogue progress (i.e.,\nstate-action sequences) based on expert demonstrations, either with or without\nannotations. The dynamics model computes rewards by predicting whether the\ndialogue progress is consistent with expert demonstrations. We further propose\nto learn action embeddings for a better generalization of the reward function.\nThe proposed approach outperforms competitive policy learning baselines on\nMultiWOZ, a benchmark multi-domain dataset."}, {"title": "Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders", "authors": "Zixia Jia, Youmi Ma, Jiong Cai, Kewei Tu"}, {"title": "SenseBERT: Driving Some Sense into BERT", "authors": "Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, Yoav Shoham", "link": "", "summary": ""}, {"title": "SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics", "authors": "Da Yin, Tao Meng, Kai-Wei Chang", "link": "https://arxiv.org/abs/2005.04114", "summary": "We propose SentiBERT, a variant of BERT that effectively captures\ncompositional sentiment semantics. The model incorporates contextualized\nrepresentation with binary constituency parse tree to capture semantic\ncomposition. Comprehensive experiments demonstrate that SentiBERT achieves\ncompetitive performance on phrase-level sentiment classification. We further\ndemonstrate that the sentiment composition learned from the phrase-level\nannotations on SST can be transferred to other sentiment analysis tasks as well\nas related tasks, such as emotion classification tasks. Moreover, we conduct\nablation studies and design visualization methods to understand SentiBERT. We\nshow that SentiBERT is better than baseline approaches in capturing negation\nand the contrastive relation and model the compositional sentiment semantics."}, {"title": "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis", "authors": "Dushyant Singh Chauhan, Dhanush S R, Asif Ekbal, Pushpak Bhattacharyya"}, {"title": "SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling", "authors": "Luoxin Chen, Weitong Ruan, Xinyue Liu, Jianhua Lu"}, {"title": "Should All Cross-Lingual Embeddings Speak English?", "authors": "Antonios Anastasopoulos, Graham Neubig", "link": "https://arxiv.org/abs/1911.03058", "summary": "Most of recent work in cross-lingual word embeddings is severely\nAnglocentric. The vast majority of lexicon induction evaluation dictionaries\nare between English and another language, and the English embedding space is\nselected by default as the hub when learning in a multilingual setting. With\nthis work, however, we challenge these practices. First, we show that the\nchoice of hub language can significantly impact downstream lexicon induction\nperformance. Second, we both expand the current evaluation dictionary\ncollection to include all language pairs using triangulation, and also create\nnew dictionaries for under-represented languages. Evaluating established\nmethods over all these language pairs sheds light into their suitability and\npresents new challenges for the field. Finally, in our analysis we identify\ngeneral guidelines for strong cross-lingual embeddings baselines, based on more\nthan just Anglocentric experiments."}, {"title": "Similarity Analysis of Contextual Word Representation Models", "authors": "John Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, James Glass", "link": "https://arxiv.org/abs/2005.01172", "summary": "This paper investigates contextual word representation models from the lens\nof similarity analysis. Given a collection of trained models, we measure the\nsimilarity of their internal representations and attention. Critically, these\nmodels come from vastly different architectures. We use existing and novel\nsimilarity measures that aim to gauge the level of localization of information\nin the deep models, and facilitate the investigation of which design factors\naffect model similarity, without requiring any external linguistic annotation.\nThe analysis reveals that models within the same family are more similar to one\nanother, as may be expected. Surprisingly, different architectures have rather\nsimilar representations, but different individual neurons. We also observed\ndifferences in information localization in lower and higher layers and found\nthat higher layers are more affected by fine-tuning on downstream tasks."}, {"title": "Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora", "authors": "Hila Gonen, Ganesh Jawahar, Djam\u00e9 Seddah, Yoav Goldberg"}, {"title": "Simplify the Usage of Lexicon in Chinese NER", "authors": "Ruotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei, Xuanjing Huang", "link": "https://arxiv.org/abs/1908.05969", "summary": "Recently, many works have tried to utilizing word lexicon to augment the\nperformance of Chinese named entity recognition (NER). As a representative work\nin this line, Lattice-LSTM \\cite{zhang2018chinese} has achieved new\nstate-of-the-art performance on several benchmark Chinese NER datasets.\nHowever, Lattice-LSTM suffers from a complicated model architecture, resulting\nin low computational efficiency. This will heavily limit its application in\nmany industrial areas, which require real-time NER response. In this work, we\nask the question: if we can simplify the usage of lexicon and, at the same\ntime, achieve comparative performance with Lattice-LSTM for Chinese NER?\n  Started with this question and motivated by the idea of Lattice-LSTM, we\npropose a concise but effective method to incorporate the lexicon information\ninto the vector representations of characters. This way, our method can avoid\nintroducing a complicated sequence modeling architecture to model the lexicon\ninformation. Instead, it only needs to subtly adjust the character\nrepresentation layer of the neural sequence model. Experimental study on four\nbenchmark Chinese NER datasets shows that our method can achieve much faster\ninference speed, comparative or better performance over Lattice-LSTM and its\nfollwees. It also shows that our method can be easily transferred across\ndifference neural architectures."}, {"title": "SimulSpeech: End-to-End Simultaneous Speech to Text Translation", "authors": "Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, Tie-Yan Liu"}, {"title": "Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language", "authors": "Qianhui Wu, Zijia Lin, B\u00f6rje Karlsson, Jian-Guang Lou, Biqing Huang", "link": "https://arxiv.org/abs/2004.12440", "summary": "To better tackle the named entity recognition (NER) problem on languages with\nlittle/no labeled data, cross-lingual NER must effectively leverage knowledge\nlearned from source languages with rich labeled data. Previous works on\ncross-lingual NER are mostly based on label projection with pairwise texts or\ndirect model transfer. However, such methods either are not applicable if the\nlabeled data in the source languages is unavailable, or do not leverage\ninformation contained in unlabeled data in the target language. In this paper,\nwe propose a teacher-student learning method to address such limitations, where\nNER models in the source languages are used as teachers to train a student\nmodel on unlabeled data in the target language. The proposed method works for\nboth single-source and multi-source cross-lingual NER. For the latter, we\nfurther propose a similarity measuring method to better weight the supervision\nfrom different teacher models. Extensive experiments for 3 target languages on\nbenchmark datasets well demonstrate that our method outperforms existing\nstate-of-the-art methods for both single-source and multi-source cross-lingual\nNER."}, {"title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis", "authors": "Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, Feng Wu", "link": "http://arxiv.org/abs/2005.05635", "summary": "Recently, sentiment analysis has seen remarkable advance with the help of\npre-training approaches. However, sentiment knowledge, such as sentiment words\nand aspect-sentiment pairs, is ignored in the process of pre-training, despite\nthe fact that they are widely used in traditional sentiment analysis\napproaches. In this paper, we introduce Sentiment Knowledge Enhanced\nPre-training (SKEP) in order to learn a unified sentiment representation for\nmultiple sentiment analysis tasks. With the help of automatically-mined\nknowledge, SKEP conducts sentiment masking and constructs three sentiment\nknowledge prediction objectives, so as to embed sentiment information at the\nword, polarity and aspect level into pre-trained sentiment representation. In\nparticular, the prediction of aspect-sentiment pairs is converted into\nmulti-label classification, aiming to capture the dependency between words in a\npair. Experiments on three kinds of sentiment tasks show that SKEP\nsignificantly outperforms strong pre-training baseline, and achieves new\nstate-of-the-art results on most of the test datasets. We release our code at\nhttps://github.com/baidu/Senta."}, {"title": "Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network", "authors": "Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, Ting Liu"}, {"title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization", "authors": "Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Tuo Zhao", "link": "https://arxiv.org/abs/1911.03437", "summary": "Transfer learning has fundamentally changed the landscape of natural language\nprocessing (NLP) research. Many existing state-of-the-art models are first\npre-trained on a large text corpus and then fine-tuned on downstream tasks.\nHowever, due to limited data resources from downstream tasks and the extremely\nlarge capacity of pre-trained models, aggressive fine-tuning often causes the\nadapted model to overfit the data of downstream tasks and forget the knowledge\nof the pre-trained model. To address the above issue in a more principled\nmanner, we propose a new computational framework for robust and efficient\nfine-tuning for pre-trained language models. Specifically, our proposed\nframework contains two important ingredients: 1. Smoothness-inducing\nregularization, which effectively manages the capacity of the model; 2. Bregman\nproximal point optimization, which is a class of trust-region methods and can\nprevent knowledge forgetting. Our experiments demonstrate that our proposed\nmethod achieves the state-of-the-art performance on multiple NLP benchmarks."}, {"title": "Social Bias Frames: Reasoning about Social and Power Implications of Language", "authors": "Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, Yejin Choi", "link": "https://arxiv.org/abs/1911.03891", "summary": "Warning: this paper contains content that may be offensive or upsetting.\n  Language has the power to reinforce stereotypes and project social biases\nonto others. At the core of the challenge is that it is rarely what is stated\nexplicitly, but rather the implied meanings, that frame people's judgments\nabout others. For example, given a statement that \"we shouldn't lower our\nstandards to hire more women,\" most listeners will infer the implicature\nintended by the speaker -- that \"women (candidates) are less qualified.\" Most\nsemantic formalisms, to date, do not capture such pragmatic implications in\nwhich people express social biases and power differentials in language.\n  We introduce Social Bias Frames, a new conceptual formalism that aims to\nmodel the pragmatic frames in which people project social biases and\nstereotypes onto others. In addition, we introduce the Social Bias Inference\nCorpus to support large-scale modelling and evaluation with 150k structured\nannotations of social media posts, covering over 34k implications about a\nthousand demographic groups.\n  We then establish baseline approaches that learn to recover Social Bias\nFrames from unstructured text. We find that while state-of-the-art neural\nmodels are effective at high-level categorization of whether a given statement\nprojects unwanted social bias (80% F1), they are not effective at spelling out\nmore detailed explanations in terms of Social Bias Frames. Our study motivates\nfuture work that combines structured pragmatic inference with commonsense\nreasoning on social implications."}, {"title": "Sources of Transfer in Multilingual Named Entity Recognition", "authors": "David Mueller, Nicholas Andrews, Mark Dredze", "link": "http://arxiv.org/abs/2005.00847", "summary": "Named-entities are inherently multilingual, and annotations in any given\nlanguage may be limited. This motivates us to consider polyglot named-entity\nrecognition (NER), where one model is trained using annotated data drawn from\nmore than one language. However, a straightforward implementation of this\nsimple idea does not always work in practice: naive training of NER models\nusing annotated data drawn from multiple languages consistently underperforms\nmodels trained on monolingual data alone, despite having access to more\ntraining data. The starting point of this paper is a simple solution to this\nproblem, in which polyglot models are fine-tuned on monolingual data to\nconsistently and significantly outperform their monolingual counterparts. To\nexplain this phenomena, we explore the sources of multilingual transfer in\npolyglot NER models and examine the weight structure of polyglot models\ncompared to their monolingual counterparts. We find that polyglot models\nefficiently share many parameters across languages and that fine-tuning may\nutilize a large number of those parameters."}, {"title": "Span Selection Pre-training for Question Answering", "authors": "Michael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Dinesh Garg, Avi Sil", "link": "https://arxiv.org/abs/1909.04120", "summary": "BERT (Bidirectional Encoder Representations from Transformers) and related\npre-trained Transformers have provided large gains across many language\nunderstanding tasks, achieving a new state-of-the-art (SOTA). BERT is\npre-trained on two auxiliary tasks: Masked Language Model and Next Sentence\nPrediction. In this paper we introduce a new pre-training task inspired by\nreading comprehension and an effort to avoid encoding general knowledge in the\ntransformer network itself. We find significant and consistent improvements\nover both BERT-BASE and BERT-LARGE on multiple reading comprehension (MRC) and\nparaphrasing datasets. Specifically, our proposed model has strong empirical\nevidence as it obtains SOTA results on Natural Questions, a new benchmark MRC\ndataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction. We\nalso establish a new SOTA in HotpotQA, improving answer prediction F1 by 4 F1\npoints and supporting fact prediction by 1 F1 point. Moreover, we show that our\npre-training approach is particularly effective when training data is limited,\nimproving the learning curve by a large amount."}, {"title": "Span-based Localizing Network for Natural Language Video Localization", "authors": "Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou", "link": "https://arxiv.org/abs/2004.13931", "summary": "Given an untrimmed video and a text query, natural language video\nlocalization (NLVL) is to locate a matching span from the video that\nsemantically corresponds to the query. Existing solutions formulate NLVL either\nas a ranking task and apply multimodal matching architecture, or as a\nregression task to directly regress the target video span. In this work, we\naddress NLVL task with a span-based QA approach by treating the input video as\ntext passage. We propose a video span localizing network (VSLNet), on top of\nthe standard span-based QA framework, to address NLVL. The proposed VSLNet\ntackles the differences between NLVL and span-based QA through a simple and yet\neffective query-guided highlighting (QGH) strategy. The QGH guides VSLNet to\nsearch for matching video span within a highlighted region. Through extensive\nexperiments on three benchmark datasets, we show that the proposed VSLNet\noutperforms the state-of-the-art methods; and adopting span-based QA framework\nis a promising direction to solve NLVL."}, {"title": "SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction", "authors": "He Zhao, Longtao Huang, Rong Zhang, Quan Lu, Hui Xue"}, {"title": "Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback", "authors": "Ahmed Elgohary, Saghar Hosseini, Ahmed Hassan Awadallah", "link": "https://arxiv.org/abs/2005.02539", "summary": "We study the task of semantic parse correction with natural language\nfeedback. Given a natural language utterance, most semantic parsing systems\npose the problem as one-shot translation where the utterance is mapped to a\ncorresponding logical form. In this paper, we investigate a more interactive\nscenario where humans can further interact with the system by providing\nfree-form natural language feedback to correct the system when it generates an\ninaccurate interpretation of an initial utterance. We focus on natural language\nto SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL\ninterpretations and the corresponding natural language feedback. We compare\nvarious reference models for the correction task and show that incorporating\nsuch a rich form of feedback can significantly improve the overall semantic\nparsing accuracy while retaining the flexibility of natural language\ninteraction. While we estimated human correction accuracy is 81.5%, our best\nmodel achieves only 25.1%, which leaves a large gap for improvement in future\nresearch. SPLASH is publicly available at https://aka.ms/Splash_dataset."}, {"title": "Speaker Sensitive Response Evaluation Model", "authors": "JinYeong Bak, Alice Oh"}, {"title": "Speakers enhance contextually confusable words", "authors": "Eric Meinhardt, Eric Bakovic, Leon Bergen"}, {"title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers", "authors": "Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel Weld", "link": "https://arxiv.org/abs/2004.07180", "summary": "Representation learning is a critical ingredient for natural language\nprocessing systems. Recent Transformer language models like BERT learn powerful\ntextual representations, but these models are targeted towards token- and\nsentence-level training objectives and do not leverage information on\ninter-document relatedness, which limits their document-level representation\npower. For applications on scientific documents, such as classification and\nrecommendation, the embeddings power strong performance on end tasks. We\npropose SPECTER, a new method to generate document-level embedding of\nscientific documents based on pretraining a Transformer language model on a\npowerful signal of document-level relatedness: the citation graph. Unlike\nexisting pretrained language models, SPECTER can be easily applied to\ndownstream applications without task-specific fine-tuning. Additionally, to\nencourage further research on document-level models, we introduce SciDocs, a\nnew evaluation benchmark consisting of seven document-level tasks ranging from\ncitation prediction, to document classification and recommendation. We show\nthat SPECTER outperforms a variety of competitive baselines on the benchmark."}, {"title": "Speech Translation and the End-to-End Promise: Taking Stock of Where We Are", "authors": "Matthias Sperber, Matthias Paulik", "link": "https://arxiv.org/abs/2004.06358", "summary": "Over its three decade history, speech translation has experienced several\nshifts in its primary research themes; moving from loosely coupled cascades of\nspeech recognition and machine translation, to exploring questions of tight\ncoupling, and finally to end-to-end models that have recently attracted much\nattention. This paper provides a brief survey of these developments, along with\na discussion of the main challenges of traditional approaches which stem from\ncommitting to intermediate representations from the speech recognizer, and from\ntraining cascaded models separately towards different objectives.\n  Recent end-to-end modeling techniques promise a principled way of overcoming\nthese issues by allowing joint training of all model components and removing\nthe need for explicit intermediate representations. However, a closer look\nreveals that many end-to-end models fall short of solving these issues, due to\ncompromises made to address data scarcity. This paper provides a unifying\ncategorization and nomenclature that covers both traditional and recent\napproaches and that may help researchers by highlighting both trade-offs and\nopen research questions."}, {"title": "SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check", "authors": "Xingyi Cheng, Weidi Xu, Kunlong Chen, Shaohua Jiang, Feng Wang, Taifeng Wang, Wei Chu, Yuan Qi", "link": "https://arxiv.org/abs/2004.14166", "summary": "Chinese Spelling Check (CSC) is a task to detect and correct spelling errors\nin Chinese natural language. Existing methods have made attempts to incorporate\nthe similarity knowledge between Chinese characters. However, they take the\nsimilarity knowledge as either an external input resource or just heuristic\nrules. This paper proposes to incorporate phonological and visual similarity\nknowledge into language models for CSC via a specialized graph convolutional\nnetwork (SpellGCN). The model builds a graph over the characters, and SpellGCN\nis learned to map this graph into a set of inter-dependent character\nclassifiers. These classifiers are applied to the representations extracted by\nanother network, such as BERT, enabling the whole network to be end-to-end\ntrainable. Experiments (The dataset and all code for this paper are available\nat https://github.com/ACL2020SpellGCN/SpellGCN) are conducted on three\nhuman-annotated datasets. Our method achieves superior performance against\nprevious models by a large margin."}, {"title": "Spelling Error Correction with Soft-Masked BERT", "authors": "Shaohua Zhang, Haoran Huang, Jicong Liu, Hang Li", "link": "http://arxiv.org/abs/2005.07421", "summary": "Spelling error correction is an important yet challenging task because a\nsatisfactory solution of it essentially needs human-level language\nunderstanding ability. Without loss of generality we consider Chinese spelling\nerror correction (CSC) in this paper. A state-of-the-art method for the task\nselects a character from a list of candidates for correction (including\nnon-correction) at each position of the sentence on the basis of BERT, the\nlanguage representation model. The accuracy of the method can be sub-optimal,\nhowever, because BERT does not have sufficient capability to detect whether\nthere is an error at each position, apparently due to the way of pre-training\nit using mask language modeling. In this work, we propose a novel neural\narchitecture to address the aforementioned issue, which consists of a network\nfor error detection and a network for error correction based on BERT, with the\nformer being connected to the latter with what we call soft-masking technique.\nOur method of using `Soft-Masked BERT' is general, and it may be employed in\nother language detection-correction problems. Experimental results on two\ndatasets demonstrate that the performance of our proposed method is\nsignificantly better than the baselines including the one solely based on BERT."}, {"title": "Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words", "authors": "Josef Klafka, Allyson Ettinger", "link": "https://arxiv.org/abs/2005.01810", "summary": "Although models using contextual word embeddings have achieved\nstate-of-the-art results on a host of NLP tasks, little is known about exactly\nwhat information these embeddings encode about the context words that they are\nunderstood to reflect. To address this question, we introduce a suite of\nprobing tasks that enable fine-grained testing of contextual embeddings for\nencoding of information about surrounding words. We apply these tasks to\nexamine the popular BERT, ELMo and GPT contextual encoders, and find that each\nof our tested information types is indeed encoded as contextual information\nacross tokens, often with near-perfect recoverability-but the encoders vary in\nwhich features they distribute to which tokens, how nuanced their distributions\nare, and how robust the encoding of each feature is to distance. We discuss\nimplications of these results for how different types of models breakdown and\nprioritize word-level context information when constructing token embeddings."}, {"title": "STARC: Structured Annotations for Reading Comprehension", "authors": "Yevgeni Berzak, Jonathan Malmaud, Roger Levy", "link": "https://arxiv.org/abs/2004.14797", "summary": "We present STARC (Structured Annotations for Reading Comprehension), a new\nannotation framework for assessing reading comprehension with multiple choice\nquestions. Our framework introduces a principled structure for the answer\nchoices and ties them to textual span annotations. The framework is implemented\nin OneStopQA, a new high-quality dataset for evaluation and analysis of reading\ncomprehension in English. We use this dataset to demonstrate that STARC can be\nleveraged for a key new application for the development of SAT-like reading\ncomprehension materials: automatic annotation quality probing via span ablation\nexperiments. We further show that it enables in-depth analyses and comparisons\nbetween machine and human reading comprehension behavior, including error\ndistributions and guessing ability. Our experiments also reveal that the\nstandard multiple choice dataset in NLP, RACE, is limited in its ability to\nmeasure reading comprehension. 47% of its questions can be guessed by machines\nwithout accessing the passage, and 18% are unanimously judged by humans as not\nhaving a unique correct answer. OneStopQA provides an alternative test set for\nreading comprehension which alleviates these shortcomings and has a\nsubstantially higher human ceiling performance."}, {"title": "Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio Optimization", "authors": "Xin Du, Kumiko Tanaka-Ishii"}, {"title": "Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset", "authors": "Revanth Rameshkumar, Peter Bailey"}, {"title": "Structural Information Preserving for Graph-to-Text Generation", "authors": "Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, Dong Yu", "link": "", "summary": ""}, {"title": "Structured Tuning for Semantic Role Labeling", "authors": "Tao Li, Parth Anand Jawale, Martha Palmer, Vivek Srikumar", "link": "http://arxiv.org/abs/2005.00496", "summary": "Recent neural network-driven semantic role labeling (SRL) systems have shown\nimpressive improvements in F1 scores. These improvements are due to expressive\ninput representations, which, at least at the surface, are orthogonal to\nknowledge-rich constrained decoding mechanisms that helped linear SRL models.\nIntroducing the benefits of structure to inform neural models presents a\nmethodological challenge. In this paper, we present a structured tuning\nframework to improve models using softened constraints only at training time.\nOur framework leverages the expressiveness of neural networks and provides\nsupervision with structured loss components. We start with a strong baseline\n(RoBERTa) to validate the impact of our approach, and show that our framework\noutperforms the baseline by learning to comply with declarative constraints.\nAdditionally, our experiments with smaller training sizes show that we can\nachieve consistent improvements under low-resource scenarios."}, {"title": "Structure-Level Knowledge Distillation For Multilingual Sequence Labeling", "authors": "Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang, Kewei Tu", "link": "http://arxiv.org/abs/2004.03846", "summary": "Multilingual sequence labeling is a task of predicting label sequences using\na single unified model for multiple languages. Compared with relying on\nmultiple monolingual models, using a multilingual model has the benefit of a\nsmaller model size, easier in online serving, and generalizability to\nlow-resource languages. However, current multilingual models still underperform\nindividual monolingual models significantly due to model capacity limitations.\nIn this paper, we propose to reduce the gap between monolingual models and the\nunified multilingual model by distilling the structural knowledge of several\nmonolingual models (teachers) to the unified multilingual model (student). We\npropose two novel KD methods based on structure-level information: (1)\napproximately minimizes the distance between the student's and the teachers'\nstructure level probability distributions, (2) aggregates the structure-level\nknowledge to local distributions and minimizes the distance between two local\nprobability distributions. Our experiments on 4 multilingual tasks with 25\ndatasets show that our approaches outperform several strong baselines and have\nstronger zero-shot generalizability than both the baseline model and teacher\nmodels."}, {"title": "Suspense in Short Stories is Predicted By Uncertainty Reduction over Neural Story Representation", "authors": "David Wilmot, Frank Keller", "link": "", "summary": ""}, {"title": "Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction", "authors": "Shaowei Chen, Jie Liu, Yu Wang, Wenzheng Zhang, Ziming Chi"}, {"title": "Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation", "authors": "Kaustubh Dhole, Christopher D. Manning", "link": "https://arxiv.org/abs/2004.08694", "summary": "Question Generation (QG) is fundamentally a simple syntactic transformation;\nhowever, many aspects of semantics influence what questions are good to form.\nWe implement this observation by developing Syn-QG, a set of transparent\nsyntactic rules leveraging universal dependencies, shallow semantic parsing,\nlexical resources, and custom rules which transform declarative sentences into\nquestion-answer pairs. We utilize PropBank argument descriptions and VerbNet\nstate predicates to incorporate shallow semantic content, which helps generate\nquestions of a descriptive nature and produce inferential and semantically\nricher questions than existing systems. In order to improve syntactic fluency\nand eliminate grammatically incorrect questions, we employ back-translation\nover the output of these syntactic rules. A set of crowd-sourced evaluations\nshows that our system can generate a larger number of highly grammatical and\nrelevant questions than previous QG systems and that back-translation\ndrastically improves grammaticality at a slight cost of generating irrelevant\nquestions."}, {"title": "Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks", "authors": "Bo Zhang, Yue Zhang, Rui Wang, Zhenghua Li, Min Zhang"}, {"title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data", "authors": "Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel", "link": "https://arxiv.org/abs/2005.08314?context=cs"}, {"title": "TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task", "authors": "Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig", "link": "", "summary": ""}, {"title": "TAG : Type Auxiliary Guiding for Code Comment Generation", "authors": "Ruichu Cai, Zhihao Liang, Boyan Xu, zijian li, Yuexing Hao, Yao Chen", "link": "https://arxiv.org/abs/2005.02835", "summary": "Existing leading code comment generation approaches with the\nstructure-to-sequence framework ignores the type information of the\ninterpretation of the code, e.g., operator, string, etc. However, introducing\nthe type information into the existing framework is non-trivial due to the\nhierarchical dependence among the type information. In order to address the\nissues above, we propose a Type Auxiliary Guiding encoder-decoder framework for\nthe code comment generation task which considers the source code as an N-ary\ntree with type information associated with each node. Specifically, our\nframework is featured with a Type-associated Encoder and a Type-restricted\nDecoder which enables adaptive summarization of the source code. We further\npropose a hierarchical reinforcement learning method to resolve the training\ndifficulties of our proposed framework. Extensive evaluations demonstrate the\nstate-of-the-art performance of our framework with both the auto-evaluated\nmetrics and case studies."}, {"title": "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics", "authors": "Nitika Mathur, Timothy Baldwin, Trevor Cohn"}, {"title": "TaPas: Weakly Supervised Table Parsing via Pre-training", "authors": "Jonathan Herzig, Pawel Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, Julian Eisenschlos", "link": "https://arxiv.org/abs/2004.02349", "summary": "Answering natural language questions over tables is usually seen as a\nsemantic parsing task. To alleviate the collection cost of full logical forms,\none popular approach focuses on weak supervision consisting of denotations\ninstead of logical forms. However, training semantic parsers from weak\nsupervision poses difficulties, and in addition, the generated logical forms\nare only used as an intermediate step prior to retrieving the denotation. In\nthis paper, we present TAPAS, an approach to question answering over tables\nwithout generating logical forms. TAPAS trains from weak supervision, and\npredicts the denotation by selecting table cells and optionally applying a\ncorresponding aggregation operator to such selection. TAPAS extends BERT's\narchitecture to encode tables as input, initializes from an effective joint\npre-training of text segments and tables crawled from Wikipedia, and is trained\nend-to-end. We experiment with three different semantic parsing datasets, and\nfind that TAPAS outperforms or rivals semantic parsing models by improving\nstate-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with\nthe state-of-the-art on WIKISQL and WIKITQ, but with a simpler model\narchitecture. We additionally find that transfer learning, which is trivial in\nour setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the\nstate-of-the-art."}, {"title": "Target Inference in Argument Conclusion Generation", "authors": "Milad Alshomary, Shahbaz Syed, Martin Potthast, Henning Wachsmuth"}, {"title": "Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer", "authors": "Chao Shang, Sarthak Dash, Md Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, Alfio Gliozzo"}, {"title": "Tchebycheff Procedure for Multi-task Text Classification", "authors": "Yuren Mao, Shuang Yun, Weiwei Liu, Bo Du"}, {"title": "Temporal Common Sense Acquisition with Minimal Supervision", "authors": "Ben Zhou, Qiang Ning, Daniel Khashabi, Dan Roth", "link": "https://arxiv.org/abs/2005.04304", "summary": "Temporal common sense (e.g., duration and frequency of events) is crucial for\nunderstanding natural language. However, its acquisition is challenging, partly\nbecause such information is often not expressed explicitly in text, and human\nannotation on such concepts is costly. This work proposes a novel sequence\nmodeling approach that exploits explicit and implicit mentions of temporal\ncommon sense, extracted from a large corpus, to build TACOLM, a temporal common\nsense language model. Our method is shown to give quality predictions of\nvarious dimensions of temporal common sense (on UDST and a newly collected\ndataset from RealNews). It also produces representations of events for relevant\ntasks such as duration comparison, parent-child relations, event coreference\nand temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the\nstandard BERT. Thus, it will be an important component of temporal NLP."}, {"title": "Temporally-Informed Analysis of Named Entity Recognition", "authors": "Shruti Rijhwani, Daniel Preotiuc-Pietro"}, {"title": "Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates", "authors": "Katherine Keith, David Jensen, Brendan O\u2019Connor", "link": "http://arxiv.org/abs/2005.00649", "summary": "Many applications of computational social science aim to infer causal\nconclusions from non-experimental data. Such observational data often contains\nconfounders, variables that influence both potential causes and potential\neffects. Unmeasured or latent confounders can bias causal estimates, and this\nhas motivated interest in measuring potential confounders from observed text.\nFor example, an individual's entire history of social media posts or the\ncontent of a news article could provide a rich measurement of multiple\nconfounders. Yet, methods and applications for this problem are scattered\nacross different communities and evaluation practices are inconsistent. This\nreview is the first to gather and categorize these examples and provide a guide\nto data-processing and evaluation decisions. Despite increased attention on\nadjusting for confounding using text, there are still many open problems, which\nwe highlight in this paper."}, {"title": "Text-Based Ideal Points", "authors": "Keyon Vafa, Suresh Naidu, David Blei", "link": "https://arxiv.org/abs/2005.04232", "summary": "Ideal point models analyze lawmakers' votes to quantify their political\npositions, or ideal points. But votes are not the only way to express a\npolitical position. Lawmakers also give speeches, release press statements, and\npost tweets. In this paper, we introduce the text-based ideal point model\n(TBIP), an unsupervised probabilistic topic model that analyzes texts to\nquantify the political positions of its authors. We demonstrate the TBIP with\ntwo types of politicized text data: U.S. Senate speeches and senator tweets.\nThough the model does not analyze their votes or political affiliations, the\nTBIP separates lawmakers by party, learns interpretable politicized topics, and\ninfers ideal points close to the classical vote-based ideal points. One benefit\nof analyzing texts, as opposed to votes, is that the TBIP can estimate ideal\npoints of anyone who authors political texts, including non-voting actors. To\nthis end, we use it to study tweets from the 2020 Democratic presidential\ncandidates. Using only the texts of their tweets, it identifies them along an\ninterpretable progressive-to-moderate spectrum."}, {"title": "That is a Known Lie: Detecting Previously Fact-Checked Claims", "authors": "Shaden Shaar, Nikolay Babulkov, Giovanni Da San Martino, Preslav Nakov", "link": "https://arxiv.org/abs/2005.06058", "summary": "The recent proliferation of \"fake news\" has triggered a number of responses,\nmost notably the emergence of several manual fact-checking initiatives. As a\nresult and over time, a large number of fact-checked claims have been\naccumulated, which increases the likelihood that a new claim in social media or\na new statement by a politician might have already been fact-checked by some\ntrusted fact-checking organization, as viral claims often come back after a\nwhile in social media, and politicians like to repeat their favorite\nstatements, true or false, over and over again. As manual fact-checking is very\ntime-consuming (and fully automatic fact-checking has credibility issues), it\nis important to try to save this effort and to avoid wasting time on claims\nthat have already been fact-checked. Interestingly, despite the importance of\nthe task, it has been largely ignored by the research community so far. Here,\nwe aim to bridge this gap. In particular, we formulate the task and we discuss\nhow it relates to, but also differs from, previous work. We further create a\nspecialized dataset, which we release to the research community. Finally, we\npresent learning-to-rank experiments that demonstrate sizable improvements over\nstate-of-the-art retrieval and textual similarity approaches."}, {"title": "\u201cThe Boating Store Had Its Best Sail Ever\u201d: Pronunciation-attentive Contextualized Pun Recognition", "authors": "Yichao Zhou, Jyun-Yu Jiang, Jieyu Zhao, Kai-Wei Chang, Wei Wang", "link": "http://arxiv.org/abs/2004.14457", "summary": "Humor plays an important role in human languages and it is essential to model\nhumor when building intelligence systems. Among different forms of humor, puns\nperform wordplay for humorous effects by employing words with double entendre\nand high phonetic similarity. However, identifying and modeling puns are\nchallenging as puns usually involved implicit semantic or phonological tricks.\nIn this paper, we propose Pronunciation-attentive Contextualized Pun\nRecognition (PCPR) to perceive human humor, detect if a sentence contains puns\nand locate them in the sentence. PCPR derives contextualized representation for\neach word in a sentence by capturing the association between the surrounding\ncontext and its corresponding phonetic symbols. Extensive experiments are\nconducted on two benchmark datasets. Results demonstrate that the proposed\napproach significantly outperforms the state-of-the-art methods in pun\ndetection and location tasks. In-depth analyses verify the effectiveness and\nrobustness of PCPR."}, {"title": "The Cascade Transformer: an Application for Efficient Answer Sentence Selection", "authors": "Luca Soldaini, Alessandro Moschitti", "link": "https://arxiv.org/abs/2005.02534", "summary": "Large transformer-based language models have been shown to be very effective\nin many classification tasks. However, their computational complexity prevents\ntheir use in applications requiring the classification of a large set of\ncandidates. While previous works have investigated approaches to reduce model\nsize, relatively little attention has been paid to techniques to improve batch\nthroughput during inference. In this paper, we introduce the Cascade\nTransformer, a simple yet effective technique to adapt transformer-based models\ninto a cascade of rankers. Each ranker is used to prune a subset of candidates\nin a batch, thus dramatically increasing throughput at inference time. Partial\nencodings from the transformer model are shared among rerankers, providing\nfurther speed-up. When compared to a state-of-the-art transformer model, our\napproach reduces computation by 37% with almost no impact on accuracy, as\nmeasured on two English Question Answering datasets."}, {"title": "The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents", "authors": "Kurt Shuster, Da JU, Stephen Roller, Emily Dinan, Y-Lan Boureau, Jason Weston", "link": "https://arxiv.org/abs/1911.03768", "summary": "We introduce dodecaDialogue: a set of 12 tasks that measures if a\nconversational agent can communicate engagingly with personality and empathy,\nask questions, answer questions by utilizing knowledge resources, discuss\ntopics and situations, and perceive and converse about images. By multi-tasking\non such a broad large-scale set of data, we hope to both move towards and\nmeasure progress in producing a single unified agent that can perceive, reason\nand converse with humans in an open-domain setting. We show that such\nmulti-tasking improves over a BERT pre-trained baseline, largely due to\nmulti-tasking with very large dialogue datasets in a similar domain, and that\nthe multi-tasking in general provides gains to both text and image-based tasks\nusing several metrics in both the fine-tune and task transfer settings. We\nobtain state-of-the-art results on many of the tasks, providing a strong\nbaseline for this challenge."}, {"title": "The Paradigm Discovery Problem", "authors": "Alexander Erdmann, Micha Elsner, Shijie Wu, Ryan Cotterell, Nizar Habash", "link": "https://arxiv.org/abs/2005.01630", "summary": "This work treats the paradigm discovery problem (PDP), the task of learning\nan inflectional morphological system from unannotated sentences. We formalize\nthe PDP and develop evaluation metrics for judging systems. Using currently\navailable resources, we construct datasets for the task. We also devise a\nheuristic benchmark for the PDP and report empirical results on five diverse\nlanguages. Our benchmark system first makes use of word embeddings and string\nsimilarity to cluster forms by cell and by paradigm. Then, we bootstrap a\nneural transducer on top of the clustered data to predict words to realize the\nempty paradigm slots. An error analysis of our system suggests clustering by\ncell across different inflection classes is the most pressing challenge for\nfuture work. Our code and data are available for public use."}, {"title": "The Right Tool for the Job: Matching Model and Instance Complexities", "authors": "Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, Noah A. Smith", "link": "https://arxiv.org/abs/2004.07453", "summary": "As NLP models become larger, executing a trained model requires significant\ncomputational resources incurring monetary and environmental costs. To better\nrespect a given inference budget, we propose a modification to contextual\nrepresentation fine-tuning which, during inference, allows for an early (and\nfast) \"exit\" from neural network calculations for simple instances, and late\n(and accurate) exit for hard instances. To achieve this, we add classifiers to\ndifferent layers of BERT and use their calibrated confidence scores to make\nearly exit decisions. We test our proposed modification on five different\ndatasets in two tasks: three text classification datasets and two natural\nlanguage inference benchmarks. Our method presents a favorable speed/accuracy\ntradeoff in almost all cases, producing models which are up to five times\nfaster than the state of the art, while preserving their accuracy. Our method\nalso requires almost no additional training resources (in either time or\nparameters) compared to the baseline BERT model. Finally, our method alleviates\nthe need for costly retraining of multiple models at different levels of\nefficiency; we allow users to control the inference speed/accuracy tradeoff\nusing a single trained model, by setting a single variable at inference time.\nWe publicly release our code."}, {"title": "The Sensitivity of Language Models and Humans to Winograd Schema Perturbations", "authors": "Mostafa Abdou, Vinit Ravishankar, Maria Barrett, Yonatan Belinkov, Desmond Elliott, Anders S\u00f8gaard", "link": "http://arxiv.org/abs/2005.01348", "summary": "Large-scale pretrained language models are the major driving force behind\nrecent improvements in performance on the Winograd Schema Challenge, a widely\nemployed test of common sense reasoning ability. We show, however, with a new\ndiagnostic dataset, that these models are sensitive to linguistic perturbations\nof the Winograd examples that minimally affect human understanding. Our results\nhighlight interesting differences between humans and language models: language\nmodels are more sensitive to number or gender alternations and synonym\nreplacements than humans, and humans are more stable and consistent in their\npredictions, maintain a much higher absolute performance, and perform better on\nnon-associative instances than associative ones. Overall, humans are correct\nmore often than out-of-the-box models, and the models are sometimes right for\nthe wrong reasons. Finally, we show that fine-tuning on a large, task-specific\ndataset can offer a solution to these issues."}, {"title": "The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain", "authors": "Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Marusczyk, Lukas Lange", "link": "http://arxiv.org/abs/2006.03039", "summary": "This paper presents a new challenging information extraction task in the\ndomain of materials science. We develop an annotation scheme for marking\ninformation on experiments related to solid oxide fuel cells in scientific\npublications, such as involved materials and measurement conditions. With this\npaper, we publish our annotation guidelines, as well as our SOFC-Exp corpus\nconsisting of 45 open-access scholarly articles annotated by domain experts. A\ncorpus and an inter-annotator agreement study demonstrate the complexity of the\nsuggested named entity recognition and slot filling tasks as well as high\nannotation quality. We also present strong neural-network based models for a\nvariety of tasks that can be addressed on the basis of our new data set. On all\ntasks, using BERT embeddings leads to large performance gains, but with\nincreasing task complexity, adding a recurrent neural network on top seems\nbeneficial. Our models will serve as competitive baselines in future work, and\nanalysis of their performance highlights difficult cases when modeling the data\nand suggests promising research directions."}, {"title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World", "authors": "Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury", "link": "https://arxiv.org/abs/2004.09095", "summary": "Language technologies contribute to promoting multilingualism and linguistic\ndiversity around the world. However, only a very small number of the over 7000\nlanguages of the world are represented in the rapidly evolving language\ntechnologies and applications. In this paper we look at the relation between\nthe types of languages, resources, and their representation in NLP conferences\nto understand the trajectory that different languages have followed over time.\nOur quantitative investigation underlines the disparity between languages,\nespecially in terms of their resources, and calls into question the \"language\nagnostic\" status of current models and systems. Through this paper, we attempt\nto convince the ACL community to prioritise the resolution of the predicaments\nhighlighted here, so that no language is left behind."}, {"title": "The Summary Loop: Learning to Write Abstractive Summaries Without Examples", "authors": "Philippe Laban, Andrew Hsi, John Canny, Marti A. Hearst"}, {"title": "The TechQA Dataset", "authors": "Vittorio Castelli, Rishav Chakravarti, Saswati Dana, Anthony Ferritto, Radu Florian, Martin Franz, Dinesh Garg, Dinesh Khandelwal, Scott McCarley, Michael McCawley, Mohamed Nasr, Lin Pan, Cezar Pendus, John Pitrelli, Saurabh Pujar, Salim Roukos, Andrzej Sakrajda, Avi Sil, Rosario Uceda-Sosa, Todd Ward, Rong Zhang", "link": "", "summary": ""}, {"title": "The Unstoppable Rise of Computational Linguistics in Deep Learning", "authors": "James Henderson", "link": "https://arxiv.org/abs/2005.06420", "summary": "In this paper, we trace the history of neural networks applied to natural\nlanguage understanding tasks, and identify key contributions which the nature\nof language has made to the development of neural network architectures. We\nfocus on the importance of variable binding and its instantiation in\nattention-based models, and argue that Transformer is not a sequence model but\nan induced-structure model. This perspective leads to predictions of the\nchallenges facing research in deep learning architectures for natural language\nunderstanding."}, {"title": "To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying", "authors": "Markus G\u00e4rtner, Kerstin Jung"}, {"title": "To Test Machine Comprehension, Start by Defining Comprehension", "authors": "Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, Dave Ferrucci", "link": "https://arxiv.org/abs/2005.01525", "summary": "Many tasks aim to measure machine reading comprehension (MRC), often focusing\non question types presumed to be difficult. Rarely, however, do task designers\nstart by considering what systems should in fact comprehend. In this paper we\nmake two key contributions. First, we argue that existing approaches do not\nadequately define comprehension; they are too unsystematic about what content\nis tested. Second, we present a detailed definition of comprehension -- a\n\"Template of Understanding\" -- for a widely useful class of texts, namely short\nnarratives. We then conduct an experiment that strongly suggests existing\nsystems are not up to the task of narrative understanding as we define it."}, {"title": "Toward Gender-Inclusive Coreference Resolution", "authors": "Yang Trista Cao, Hal Daum\u00e9 III", "link": "https://arxiv.org/abs/1910.13913", "summary": "Correctly resolving textual mentions of people fundamentally entails making\ninferences about those people. Such inferences raise the risk of systemic\nbiases in coreference resolution systems, including biases that reinforce\ncis-normativity and can harm binary and non-binary trans (and cis)\nstakeholders. To better understand such biases, we foreground nuanced\nconceptualizations of gender from sociology and sociolinguistics, and\ninvestigate where in the machine learning pipeline such biases can enter a\nsystem. We inspect many existing datasets for trans-exclusionary biases, and\ndevelop two new datasets for interrogating bias in crowd annotations and in\nexisting coreference resolution systems. Through these studies, conducted on\nEnglish text, we confirm that without acknowledging and building systems that\nrecognize the complexity of gender, we will build systems that fail for:\nquality of service, stereotyping, and over- or under-representation."}, {"title": "Towards Conversational Recommendation over Multi-Type Dialogs", "authors": "Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu", "link": "https://arxiv.org/abs/2005.03954", "summary": "We propose a new task of conversational recommendation over multi-type\ndialogs, where the bots can proactively and naturally lead a conversation from\na non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into\naccount user's interests and feedback. To facilitate the study of this task, we\ncreate a human-to-human Chinese dialog dataset \\emph{DuRecDial} (about 10k\ndialogs, 156k utterances), which contains multiple sequential dialogs for every\npair of a recommendation seeker (user) and a recommender (bot). In each dialog,\nthe recommender proactively leads a multi-type dialog to approach\nrecommendation targets and then makes multiple recommendations with rich\ninteraction behavior. This dataset allows us to systematically investigate\ndifferent parts of the overall problem, e.g., how to naturally lead a dialog,\nhow to interact with users for recommendation. Finally we establish baseline\nresults on DuRecDial for future studies. Dataset and codes are publicly\navailable at\nhttps://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/Research/ACL2020-DuRecDial."}, {"title": "Towards Debiasing Sentence Representations", "authors": "Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, Louis-Philippe Morency"}, {"title": "Towards Emotion-aided Multi-modal Dialogue Act Classification", "authors": "Tulika Saha, Aditya Patra, Sriparna Saha, Pushpak Bhattacharyya"}, {"title": "Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints", "authors": "Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen", "link": "http://arxiv.org/abs/2005.00969", "summary": "Text generation from a knowledge base aims to translate knowledge triples to\nnatural language descriptions. Most existing methods ignore the faithfulness\nbetween a generated text description and the original table, leading to\ngenerated information that goes beyond the content of the table. In this paper,\nfor the first time, we propose a novel Transformer-based generation framework\nto achieve the goal. The core techniques in our method to enforce faithfulness\ninclude a new table-text optimal-transport matching loss and a table-text\nembedding similarity loss based on the Transformer model. Furthermore, to\nevaluate faithfulness, we propose a new automatic metric specialized to the\ntable-to-text generation problem. We also provide detailed analysis on each\ncomponent of our model in our experiments. Automatic and human evaluations show\nthat our framework can significantly outperform state-of-the-art by a large\nmargin."}, {"title": "Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation", "authors": "Bo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu, Kewei Tu"}, {"title": "Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs", "authors": "Jun Chen, Xiaoya Dai, Quan Yuan, Chao Lu, Haifeng Huang"}, {"title": "Towards Robustifying NLI Models Against Lexical Dataset Biases", "authors": "Xiang Zhou, Mohit Bansal", "link": "https://arxiv.org/abs/2005.04732", "summary": "While deep learning models are making fast progress on the task of Natural\nLanguage Inference, recent studies have also shown that these models achieve\nhigh accuracy by exploiting several dataset biases, and without deep\nunderstanding of the language semantics. Using contradiction-word bias and\nword-overlapping bias as our two bias examples, this paper explores both\ndata-level and model-level debiasing methods to robustify models against\nlexical dataset biases. First, we debias the dataset through data augmentation\nand enhancement, but show that the model bias cannot be fully removed via this\nmethod. Next, we also compare two ways of directly debiasing the model without\nknowing what the dataset biases are in advance. The first approach aims to\nremove the label bias at the embedding level. The second approach employs a\nbag-of-words sub-model to capture the features that are likely to exploit the\nbias and prevents the original model from learning these biased features by\nforcing orthogonality between these two sub-models. We performed evaluations on\nnew balanced datasets extracted from the original MNLI dataset as well as the\nNLI stress tests, and show that the orthogonality approach is better at\ndebiasing the model while maintaining competitive overall accuracy. Our code\nand data are available at: https://github.com/owenzx/LexicalDebias-ACL2020"}, {"title": "Towards Transparent and Explainable Attention Models", "authors": "Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M. Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran", "link": "https://arxiv.org/abs/2004.14243", "summary": "Recent studies on interpretability of attention distributions have led to\nnotions of faithful and plausible explanations for a model's predictions.\nAttention distributions can be considered a faithful explanation if a higher\nattention weight implies a greater impact on the model's prediction. They can\nbe considered a plausible explanation if they provide a human-understandable\njustification for the model's predictions. In this work, we first explain why\ncurrent attention mechanisms in LSTM based encoders can neither provide a\nfaithful nor a plausible explanation of the model's predictions. We observe\nthat in LSTM based encoders the hidden representations at different time-steps\nare very similar to each other (high conicity) and attention weights in these\nsituations do not carry much meaning because even a random permutation of the\nattention weights does not affect the model's predictions. Based on experiments\non a wide variety of tasks and datasets, we observe attention distributions\noften attribute the model's predictions to unimportant words such as\npunctuation and fail to offer a plausible explanation for the predictions. To\nmake attention mechanisms more faithful and plausible, we propose a modified\nLSTM cell with a diversity-driven training objective that ensures that the\nhidden representations learned at different time steps are diverse. We show\nthat the resulting attention distributions offer more transparency as they (i)\nprovide a more precise importance ranking of the hidden states (ii) are better\nindicative of words important for the model's predictions (iii) correlate\nbetter with gradient-based attribution methods. Human evaluations indicate that\nthe attention distributions learned by our model offer a plausible explanation\nof the model's predictions. Our code has been made publicly available at\nhttps://github.com/akashkm99/Interpretable-Attention"}, {"title": "Towards Understanding Gender Bias in Relation Extraction", "authors": "Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang, Jing Qian, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, William Yang Wang", "link": "https://arxiv.org/abs/1911.03642", "summary": "Recent developments in Neural Relation Extraction (NRE) have made significant\nstrides towards Automated Knowledge Base Construction (AKBC). While much\nattention has been dedicated towards improvements in accuracy, there have been\nno attempts in the literature to our knowledge to evaluate social biases in NRE\nsystems. We create WikiGenderBias, a distantly supervised dataset with a human\nannotated test set. WikiGenderBias has sentences specifically curated to\nanalyze gender bias in relation extraction systems. We use WikiGenderBias to\nevaluate systems for bias and find that NRE systems exhibit gender biased\npredictions and lay groundwork for future evaluation of bias in NRE. We also\nanalyze how name anonymization, hard debiasing for word embeddings, and\ncounterfactual data augmentation affect gender bias in predictions and\nperformance."}, {"title": "Towards Unsupervised Language Understanding and Generation by Joint Dual Learning", "authors": "Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen", "link": "http://arxiv.org/abs/2004.14710", "summary": "In modular dialogue systems, natural language understanding (NLU) and natural\nlanguage generation (NLG) are two critical components, where NLU extracts the\nsemantics from the given texts and NLG is to construct corresponding natural\nlanguage sentences based on the input semantic representations. However, the\ndual property between understanding and generation has been rarely explored.\nThe prior work is the first attempt that utilized the duality between NLU and\nNLG to improve the performance via a dual supervised learning framework.\nHowever, the prior work still learned both components in a supervised manner,\ninstead, this paper introduces a general learning framework to effectively\nexploit such duality, providing flexibility of incorporating both supervised\nand unsupervised learning algorithms to train language understanding and\ngeneration models in a joint fashion. The benchmark experiments demonstrate\nthat the proposed approach is capable of boosting the performance of both NLU\nand NLG."}, {"title": "Toxicity Detection: Does Context Really Matter?", "authors": "John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, Ion Androutsopoulos", "link": "https://arxiv.org/abs/2006.00998", "summary": "Moderation is crucial to promoting healthy on-line discussions. Although\nseveral `toxicity' detection datasets and models have been published, most of\nthem ignore the context of the posts, implicitly assuming that comments maybe\njudged independently. We investigate this assumption by focusing on two\nquestions: (a) does context affect the human judgement, and (b) does\nconditioning on context improve performance of toxicity detection systems? We\nexperiment with Wikipedia conversations, limiting the notion of context to the\nprevious post in the thread and the discussion title. We find that context can\nboth amplify or mitigate the perceived toxicity of posts. Moreover, a small but\nsignificant subset of manually labeled posts (5% in one of our experiments) end\nup having the opposite toxicity labels if the annotators are not provided with\ncontext. Surprisingly, we also find no evidence that context actually improves\nthe performance of toxicity classifiers, having tried a range of classifiers\nand mechanisms to make them context aware. This points to the need for larger\ndatasets of comments annotated in context. We make our code and data publicly\navailable."}, {"title": "Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction", "authors": "Chuang Fan, Chaofa Yuan, Jiachen Du, Lin Gui, Min Yang, Ruifeng Xu"}, {"title": "Transition-based Semantic Dependency Parsing with Pointer Networks", "authors": "Daniel Fern\u00e1ndez-Gonz\u00e1lez, Carlos G\u00f3mez-Rodr\u00edguez", "link": "https://arxiv.org/abs/2005.13344", "summary": "Transition-based parsers implemented with Pointer Networks have become the\nnew state of the art in dependency parsing, excelling in producing labelled\nsyntactic trees and outperforming graph-based models in this task. In order to\nfurther test the capabilities of these powerful neural networks on a harder NLP\nproblem, we propose a transition system that, thanks to Pointer Networks, can\nstraightforwardly produce labelled directed acyclic graphs and perform semantic\ndependency parsing. In addition, we enhance our approach with deep\ncontextualized word embeddings extracted from BERT. The resulting system not\nonly outperforms all existing transition-based models, but also matches the\nbest fully-supervised accuracy to date on the SemEval 2015 Task 18 English\ndatasets among previous state-of-the-art graph-based parsers."}, {"title": "Translationese as a Language in \u201cMultilingual\u201d NMT", "authors": "Parker Riley, Isaac Caswell, Markus Freitag, David Grangier", "link": "https://arxiv.org/abs/1911.03823", "summary": "Machine translation has an undesirable propensity to produce \"translationese\"\nartifacts, which can lead to higher BLEU scores while being liked less by human\nraters. Motivated by this, we model translationese and original (i.e. natural)\ntext as separate languages in a multilingual model, and pose the question: can\nwe perform zero-shot translation between original source text and original\ntarget text? There is no data with original source and original target, so we\ntrain sentence-level classifiers to distinguish translationese from original\ntarget text, and use this classifier to tag the training data for an NMT model.\nUsing this technique we bias the model to produce more natural outputs at test\ntime, yielding gains in human evaluation scores on both accuracy and fluency.\nAdditionally, we demonstrate that it is possible to bias the model to produce\ntranslationese and game the BLEU score, increasing it while decreasing\nhuman-rated quality. We analyze these models using metrics to measure the\ndegree of translationese in the output, and present an analysis of the\ncapriciousness of heuristically-based train-data tagging."}, {"title": "TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition", "authors": "Ruifang He, Jian Wang, Fengyu Guo, Yugui Han"}, {"title": "TVQA+: Spatio-Temporal Grounding for Video Question Answering", "authors": "Jie Lei, Licheng Yu, Tamara Berg, Mohit Bansal", "link": "https://arxiv.org/abs/1904.11574", "summary": "We present the task of Spatio-Temporal Video Question Answering, which\nrequires intelligent systems to simultaneously retrieve relevant moments and\ndetect referenced visual concepts (people and objects) to answer natural\nlanguage questions about videos. We first augment the TVQA dataset with 310.8K\nbounding boxes, linking depicted objects to visual concepts in questions and\nanswers. We name this augmented version as TVQA+. We then propose\nSpatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework\nthat grounds evidence in both spatial and temporal domains to answer questions\nabout videos. Comprehensive experiments and analyses demonstrate the\neffectiveness of our framework and how the rich annotations in our TVQA+\ndataset can contribute to the question answering task. Moreover, by performing\nthis joint task, our model is able to produce insightful and interpretable\nspatio-temporal attention visualizations. Dataset and code are publicly\navailable at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus"}, {"title": "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories", "authors": "Giannis Karamanolakis, Jun Ma, Xin Luna Dong", "link": "https://arxiv.org/abs/2004.13852", "summary": "Extracting structured knowledge from product profiles is crucial for various\napplications in e-Commerce. State-of-the-art approaches for knowledge\nextraction were each designed for a single category of product, and thus do not\napply to real-life e-Commerce scenarios, which often contain thousands of\ndiverse categories. This paper proposes TXtract, a taxonomy-aware knowledge\nextraction model that applies to thousands of product categories organized in a\nhierarchical taxonomy. Through category conditional self-attention and\nmulti-task learning, our approach is both scalable, as it trains a single model\nfor thousands of categories, and effective, as it extracts category-specific\nattribute values. Experiments on products from a taxonomy with 4,000 categories\nshow that TXtract outperforms state-of-the-art approaches by up to 10% in F1\nand 15% in coverage across all categories."}, {"title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "authors": "Yikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan, Lidia S. Chao", "link": "", "summary": ""}, {"title": "Understanding Attention for Text Classification", "authors": "Xiaobing Sun, Wei Lu"}, {"title": "Understanding the Language of Political Agreement and Disagreement in Legislative Texts", "authors": "Maryam Davoodi, Eric Waltenburg, Dan Goldwasser"}, {"title": "Universal Decompositional Semantic Parsing", "authors": "Elias Stengel-Eskin, Aaron Steven White, Sheng Zhang, Benjamin Van Durme", "link": "https://arxiv.org/abs/1910.10138", "summary": "We introduce a transductive model for parsing into Universal Decompositional\nSemantics (UDS) representations, which jointly learns to map natural language\nutterances into UDS graph structures and annotate the graph with\ndecompositional semantic attribute scores. We also introduce a strong pipeline\nmodel for parsing into the UDS graph structure, and show that our transductive\nparser performs comparably while additionally performing attribute prediction.\nBy analyzing the attribute prediction errors, we find the model captures\nnatural relationships between attribute groups."}, {"title": "Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification", "authors": "Guangfeng Yan, Lu Fan, Qimai Li, Han Liu, Xiaotong Zhang, Xiao-Ming Wu, Albert Y.S. Lam"}, {"title": "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering", "authors": "Vikas Yadav, Steven Bethard, Mihai Surdeanu", "link": "https://arxiv.org/abs/2005.01218", "summary": "Evidence retrieval is a critical stage of question answering (QA), necessary\nnot only to improve performance, but also to explain the decisions of the\ncorresponding QA method. We introduce a simple, fast, and unsupervised\niterative evidence retrieval method, which relies on three ideas: (a) an\nunsupervised alignment approach to soft-align questions and answers with\njustification sentences using only GloVe embeddings, (b) an iterative process\nthat reformulates queries focusing on terms that are not covered by existing\njustifications, which (c) a stopping criterion that terminates retrieval when\nthe terms in the given question and candidate answers are covered by the\nretrieved justifications. Despite its simplicity, our approach outperforms all\nthe previous methods (including supervised methods) on the evidence selection\ntask on two datasets: MultiRC and QASC. When these evidence sentences are fed\ninto a RoBERTa answer classification component, we achieve state-of-the-art QA\nperformance on these two datasets."}, {"title": "Unsupervised Cross-lingual Representation Learning at Scale", "authors": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov", "link": "https://arxiv.org/abs/1911.02116", "summary": "This paper shows that pretraining multilingual language models at scale leads\nto significant performance gains for a wide range of cross-lingual transfer\ntasks. We train a Transformer-based masked language model on one hundred\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our\nmodel, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a\nvariety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,\n+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs\nparticularly well on low-resource languages, improving 15.7% in XNLI accuracy\nfor Swahili and 11.4% for Urdu over previous XLM models. We also present a\ndetailed empirical analysis of the key factors that are required to achieve\nthese gains, including the trade-offs between (1) positive transfer and\ncapacity dilution and (2) the performance of high and low resource languages at\nscale. Finally, we show, for the first time, the possibility of multilingual\nmodeling without sacrificing per-language performance; XLM-R is very\ncompetitive with strong monolingual models on the GLUE and XNLI benchmarks. We\nwill make our code, data and models publicly available."}, {"title": "Unsupervised Domain Clusters in Pretrained Language Models", "authors": "Roee Aharoni, Yoav Goldberg", "link": "https://arxiv.org/abs/2004.02105", "summary": "The notion of \"in-domain data\" in NLP is often over-simplistic and vague, as\ntextual data varies in many nuanced linguistic aspects such as topic, style or\nlevel of formality. In addition, domain labels are many times unavailable,\nmaking it challenging to build domain-specific systems. We show that massive\npre-trained language models implicitly learn sentence representations that\ncluster by domains without supervision -- suggesting a simple data-driven\ndefinition of domains in textual data. We harness this property and propose\ndomain data selection methods based on such models, which require only a small\nset of in-domain monolingual data. We evaluate our data selection methods for\nneural machine translation across five diverse domains, where they outperform\nan established approach as measured by both BLEU and by precision and recall of\nsentence selection with respect to an oracle."}, {"title": "Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing", "authors": "Ruisheng Cao, Su Zhu, Chenyu Yang, Chen Liu, Rao Ma, Yanbin Zhao, Lu Chen, Kai Yu", "link": "https://arxiv.org/abs/2005.13485", "summary": "One daunting problem for semantic parsing is the scarcity of annotation.\nAiming to reduce nontrivial human labor, we propose a two-stage semantic\nparsing framework, where the first stage utilizes an unsupervised paraphrase\nmodel to convert an unlabeled natural language utterance into the canonical\nutterance. The downstream naive semantic parser accepts the intermediate output\nand returns the target logical form. Furthermore, the entire training process\nis split into two phases: pre-training and cycle learning. Three tailored\nself-supervised tasks are introduced throughout training to activate the\nunsupervised paraphrase model. Experimental results on benchmarks Overnight and\nGeoGranno demonstrate that our framework is effective and compatible with\nsupervised training."}, {"title": "Unsupervised Morphological Paradigm Completion", "authors": "Huiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya McCarthy, Katharina Kann", "link": "http://arxiv.org/abs/2005.00970", "summary": "We propose the task of unsupervised morphological paradigm completion. Given\nonly raw text and a lemma list, the task consists of generating the\nmorphological paradigms, i.e., all inflected forms, of the lemmas. From a\nnatural language processing (NLP) perspective, this is a challenging\nunsupervised task, and high-performing systems have the potential to improve\ntools for low-resource languages or to assist linguistic annotators. From a\ncognitive science perspective, this can shed light on how children acquire\nmorphological knowledge. We further introduce a system for the task, which\ngenerates morphological paradigms via the following steps: (i) EDIT TREE\nretrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and\n(iv) inflection generation. We perform an evaluation on 14 typologically\ndiverse languages. Our system outperforms trivial baselines with ease and, for\nsome languages, even obtains a higher accuracy than minimally supervised\nsystems."}, {"title": "Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting", "authors": "Po-Yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann", "link": "http://arxiv.org/abs/2005.03119", "summary": "Unsupervised machine translation (MT) has recently achieved impressive\nresults with monolingual corpora only. However, it is still challenging to\nassociate source-target sentences in the latent space. As people speak\ndifferent languages biologically share similar visual systems, the potential of\nachieving better alignment through visual content is promising yet\nunder-explored in unsupervised multimodal MT (MMT). In this paper, we\ninvestigate how to utilize visual content for disambiguation and promoting\nlatent space alignment in unsupervised MMT. Our model employs multimodal\nback-translation and features pseudo visual pivoting in which we learn a shared\nmultilingual visual-semantic embedding space and incorporate visually-pivoted\ncaptioning as additional weak supervision. The experimental results on the\nwidely used Multi30K dataset show that the proposed model significantly\nimproves over the state-of-the-art methods and generalizes well when the images\nare not available at the testing time."}, {"title": "Unsupervised Opinion Summarization as Copycat-Review Generation", "authors": "Arthur Bra\u017einskas, Mirella Lapata, Ivan Titov", "link": "https://arxiv.org/abs/1911.02247", "summary": "Opinion summarization is the task of automatically creating summaries that\nreflect subjective information expressed in multiple documents, such as product\nreviews. While the majority of previous work has focused on the extractive\nsetting, i.e., selecting fragments from input reviews to produce a summary, we\nlet the model generate novel sentences and hence produce abstractive summaries.\nRecent progress in summarization has seen the development of supervised models\nwhich rely on large quantities of document-summary pairs. Since such training\ndata is expensive to acquire, we instead consider the unsupervised setting, in\nother words, we do not use any summaries in training. We define a generative\nmodel for a review collection which capitalizes on the intuition that when\ngenerating a new review given a set of other reviews of a product, we should be\nable to control the \"amount of novelty\" going into the new review or,\nequivalently, vary the extent to which it deviates from the input. At test\ntime, when generating summaries, we force the novelty to be minimal, and\nproduce a text reflecting consensus opinions. We capture this intuition by\ndefining a hierarchical variational autoencoder model. Both individual reviews\nand the products they correspond to are associated with stochastic latent\ncodes, and the review generator (\"decoder\") has direct access to the text of\ninput reviews through the pointer-generator mechanism. Experiments on Amazon\nand Yelp datasets, show that setting at test time the review's latent code to\nits mean, allows the model to produce fluent and coherent summaries reflecting\ncommon opinions."}, {"title": "Unsupervised Opinion Summarization with Noising and Denoising", "authors": "Reinald Kim Amplayo, Mirella Lapata", "link": "https://arxiv.org/abs/2004.10150", "summary": "The supervised training of high-capacity models on large datasets containing\nhundreds of thousands of document-summary pairs is critical to the recent\nsuccess of deep learning techniques for abstractive summarization.\nUnfortunately, in most domains (other than news) such training data is not\navailable and cannot be easily sourced. In this paper we enable the use of\nsupervised learning for the setting where there are only documents available\n(e.g.,~product or business reviews) without ground truth summaries. We create a\nsynthetic dataset from a corpus of user reviews by sampling a review,\npretending it is a summary, and generating noisy versions thereof which we\ntreat as pseudo-review input. We introduce several linguistically motivated\nnoise generation functions and a summarization model which learns to denoise\nthe input and generate the original review. At test time, the model accepts\ngenuine reviews and generates a summary containing salient opinions, treating\nthose that do not reach consensus as noise. Extensive automatic and human\nevaluation shows that our model brings substantial improvements over both\nabstractive and extractive baselines."}, {"title": "Unsupervised Paraphrasing by Simulated Annealing", "authors": "Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song", "link": "https://arxiv.org/abs/1909.03588", "summary": "Unsupervised paraphrase generation is a promising and important research\ntopic in natural language processing. We propose UPSA, a novel approach that\naccomplishes Unsupervised Paraphrasing by Simulated Annealing. We model\nparaphrase generation as an optimization problem and propose a sophisticated\nobjective function, involving semantic similarity, expression diversity, and\nlanguage fluency of paraphrases. Then, UPSA searches the sentence space towards\nthis objective by performing a sequence of local editing. Our method is\nunsupervised and does not require parallel corpora for training, so it could be\neasily applied to different domains. We evaluate our approach on a variety of\nbenchmark datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive\nresults show that UPSA achieves the state-of-the-art performance compared with\nprevious unsupervised methods in terms of both automatic and human evaluations.\nFurther, our approach outperforms most existing domain-adapted supervised\nmodels, showing the generalizability of UPSA."}, {"title": "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation", "authors": "Shikib Mehri, Maxine Eskenazi", "link": "https://arxiv.org/abs/2005.00456", "summary": "The lack of meaningful automatic evaluation metrics for dialog has impeded\nopen-domain dialog research. Standard language generation metrics have been\nshown to be ineffective for evaluating dialog models. To this end, this paper\npresents USR, an UnSupervised and Reference-free evaluation metric for dialog.\nUSR is a reference-free metric that trains unsupervised models to measure\nseveral desirable qualities of dialog. USR is shown to strongly correlate with\nhuman judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and\nPersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces\ninterpretable measures for several desirable properties of dialog."}, {"title": "Weight Poisoning Attacks on Pretrained Models", "authors": "Keita Kurita, Paul Michel, Graham Neubig", "link": "https://arxiv.org/abs/2004.06660", "summary": "Recently, NLP has seen a surge in the usage of large pre-trained models.\nUsers download weights of models pre-trained on large datasets, then fine-tune\nthe weights on a task of their choice. This raises the question of whether\ndownloading untrusted pre-trained weights can pose a security threat. In this\npaper, we show that it is possible to construct ``weight poisoning'' attacks\nwhere pre-trained weights are injected with vulnerabilities that expose\n``backdoors'' after fine-tuning, enabling the attacker to manipulate the model\nprediction simply by injecting an arbitrary keyword. We show that by applying a\nregularization method, which we call RIPPLe, and an initialization procedure,\nwhich we call Embedding Surgery, such attacks are possible even with limited\nknowledge of the dataset and fine-tuning procedure. Our experiments on\nsentiment classification, toxicity detection, and spam detection show that this\nattack is widely applicable and poses a serious threat. Finally, we outline\npractical defenses against such attacks. Code to reproduce our experiments is\navailable at https://github.com/neulab/RIPPLe."}, {"title": "What are the Goals of Distributional Semantics?", "authors": "Guy Emerson", "link": "http://arxiv.org/abs/2005.02982", "summary": "Distributional semantic models have become a mainstay in NLP, providing\nuseful features for downstream tasks. However, assessing long-term progress\nrequires explicit long-term goals. In this paper, I take a broad linguistic\nperspective, looking at how well current models can deal with various semantic\nchallenges. Given stark differences between models proposed in different\nsubfields, a broad perspective is needed to see how we could integrate them. I\nconclude that, while linguistic insights can guide the design of model\narchitectures, future progress will require balancing the often conflicting\ndemands of linguistic expressiveness and computational tractability."}, {"title": "What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks", "authors": "Richard Futrell, William Dyer, Greg Scontras"}, {"title": "What Question Answering can Learn from Trivia Nerds", "authors": "Jordan Boyd-Graber, Benjamin B\u00f6rschinger", "link": "https://arxiv.org/abs/1910.14464", "summary": "In addition to the traditional task of getting machines to answer questions,\na major research question in question answering is to create interesting,\nchallenging questions that can help systems learn how to answer questions and\nalso reveal which systems are the best at answering questions. We argue that\ncreating a question answering dataset -- and the ubiquitous leaderboard that\ngoes with it -- closely resembles running a trivia tournament: you write\nquestions, have agents (either humans or machines) answer the questions, and\ndeclare a winner. However, the research community has ignored the decades of\nhard-learned lessons from decades of the trivia community creating vibrant,\nfair, and effective question answering competitions. After detailing problems\nwith existing QA datasets, we outline the key lessons -- removing ambiguity,\ndiscriminating skill, and adjudicating disputes -- that can transfer to QA\nresearch and how they might be implemented for the QA community."}, {"title": "What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context", "authors": "Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James Glass, Preslav Nakov", "link": "http://arxiv.org/abs/2005.04518", "summary": "Predicting the political bias and the factuality of reporting of entire news\noutlets are critical elements of media profiling, which is an understudied but\nan increasingly important research direction. The present level of\nproliferation of fake, biased, and propagandistic content online, has made it\nimpossible to fact-check every single suspicious claim, either manually or\nautomatically. Alternatively, we can profile entire news outlets and look for\nthose that are likely to publish fake or biased content. This approach makes it\npossible to detect likely \"fake news\" the moment they are published, by simply\nchecking the reliability of their source. From a practical perspective,\npolitical bias and factuality of reporting have a linguistic aspect but also a\nsocial context. Here, we study the impact of both, namely (i) what was written\n(i.e., what was published by the target medium, and how it describes itself on\nTwitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium\non Facebook, Twitter, and YouTube). We further study (iii) what was written\nabout the target medium on Wikipedia. The evaluation results show that what was\nwritten matters most, and that putting all information sources together yields\nhuge improvements over the current state-of-the-art."}, {"title": "When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?", "authors": "Kenneth Joseph, Jonathan Morgan", "link": "http://arxiv.org/abs/2004.12043", "summary": "Social biases are encoded in word embeddings. This presents a unique\nopportunity to study society historically and at scale, and a unique danger\nwhen embeddings are used in downstream applications. Here, we investigate the\nextent to which publicly-available word embeddings accurately reflect beliefs\nabout certain kinds of people as measured via traditional survey methods. We\nfind that biases found in word embeddings do, on average, closely mirror survey\ndata across seventeen dimensions of social meaning. However, we also find that\nbiases in embeddings are much more reflective of survey data for some\ndimensions of meaning (e.g. gender) than others (e.g. race), and that we can be\nhighly confident that embedding-based measures reflect survey data only for the\nmost salient biases."}, {"title": "\u201cWho said it, and Why?\u201d Provenance for Natural Language Claims", "authors": "Yi Zhang, Zachary Ives, Dan Roth"}, {"title": "WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge", "authors": "Hongming Zhang, Xinran Zhao, Yangqiu Song", "link": "https://arxiv.org/abs/2005.05763", "summary": "In this paper, we present the first comprehensive categorization of essential\ncommonsense knowledge for answering the Winograd Schema Challenge (WSC). For\neach of the questions, we invite annotators to first provide reasons for making\ncorrect decisions and then categorize them into six major knowledge categories.\nBy doing so, we better understand the limitation of existing methods (i.e.,\nwhat kind of knowledge cannot be effectively represented or inferred with\nexisting methods) and shed some light on the commonsense knowledge that we need\nto acquire in the future for better commonsense reasoning. Moreover, to\ninvestigate whether current WSC models can understand the commonsense or they\nsimply solve the WSC questions based on the statistical bias of the dataset, we\nleverage the collected reasons to develop a new task called WinoWhy, which\nrequires models to distinguish plausible reasons from very similar but wrong\nreasons for all WSC questions. Experimental results prove that even though\npre-trained language representation models have achieved promising progress on\nthe original WSC dataset, they are still struggling at WinoWhy. Further\nexperiments show that even though supervised models can achieve better\nperformance, the performance of these models can be sensitive to the dataset\ndistribution. WinoWhy and all codes are available at:\nhttps://github.com/HKUST-KnowComp/WinoWhy."}, {"title": "Word-level Textual Adversarial Attacking as Combinatorial Optimization", "authors": "Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun", "link": "https://arxiv.org/abs/1910.12196", "summary": "Adversarial attacks are carried out to reveal the vulnerability of deep\nneural networks. Textual adversarial attacking is challenging because text is\ndiscrete and a small perturbation can bring significant change to the original\ninput. Word-level attacking, which can be regarded as a combinatorial\noptimization problem, is a well-studied class of textual attack methods.\nHowever, existing word-level attack models are far from perfect, largely\nbecause unsuitable search space reduction methods and inefficient optimization\nalgorithms are employed. In this paper, we propose a novel attack model, which\nincorporates the sememe-based word substitution method and particle swarm\noptimization-based search algorithm to solve the two problems separately. We\nconduct exhaustive experiments to evaluate our attack model by attacking BiLSTM\nand BERT on three benchmark datasets. Experimental results demonstrate that our\nmodel consistently achieves much higher attack success rates and crafts more\nhigh-quality adversarial examples as compared to baseline methods. Also,\nfurther experiments show our model has higher transferability and can bring\nmore robustness enhancement to victim models by adversarial training. All the\ncode and data of this paper can be obtained on\nhttps://github.com/thunlp/SememePSO-Attack."}, {"title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models", "authors": "Subhabrata Mukherjee, Ahmed Hassan Awadallah", "link": "https://arxiv.org/abs/2004.05686", "summary": "Deep and large pre-trained language models are the state-of-the-art for\nvarious natural language processing tasks. However, the huge size of these\nmodels could be a deterrent to use them in practice. Some recent and concurrent\nworks use knowledge distillation to compress these huge models into shallow\nones. In this work we study knowledge distillation with a focus on\nmulti-lingual Named Entity Recognition (NER). In particular, we study several\ndistillation strategies and propose a stage-wise optimization scheme leveraging\nteacher internal representations that is agnostic of teacher architecture and\nshow that it outperforms strategies employed in prior works. Additionally, we\ninvestigate the role of several factors like the amount of unlabeled data,\nannotation resources, model architecture and inference latency to name a few.\nWe show that our approach leads to massive compression of MBERT-like teacher\nmodels by upto 35x in terms of parameters and 51x in terms of latency for batch\ninference while retaining 95% of its F1-score for NER over 41 languages."}, {"title": "You Impress Me: Dialogue Generation via Mutual Persona Perception", "authors": "Qian Liu, Yihong Chen, Bei Chen, Jian-Guang Lou, Zixuan Chen, Bin Zhou, Dongmei Zhang", "link": "https://arxiv.org/abs/2004.05388", "summary": "Despite the continuing efforts to improve the engagingness and consistency of\nchit-chat dialogue systems, the majority of current work simply focus on\nmimicking human-like responses, leaving understudied the aspects of modeling\nunderstanding between interlocutors. The research in cognitive science,\ninstead, suggests that understanding is an essential signal for a high-quality\nchit-chat conversation. Motivated by this, we propose P^2 Bot, a\ntransmitter-receiver based framework with the aim of explicitly modeling\nunderstanding. Specifically, P^2 Bot incorporates mutual persona perception to\nenhance the quality of personalized dialogue generation. Experiments on a large\npublic dataset, Persona-Chat, demonstrate the effectiveness of our approach,\nwith a considerable boost over the state-of-the-art baselines across both\nautomatic metrics and human evaluations."}, {"title": "Zero-shot Text Classification via Reinforced Self-training", "authors": "Zhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin Chen, Xiaoxiao Xu, Suhang Zheng, Feng Wang, Jun Zhang, Huajun Chen", "link": "", "summary": ""}, {"title": "Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking", "authors": "Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi, Monica Lam"}, {"title": "ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages", "authors": "Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi", "link": "https://arxiv.org/abs/2005.07105", "summary": "In many documents, such as semi-structured webpages, textual semantics are\naugmented with additional information conveyed using visual elements including\nlayout, font size, and color. Prior work on information extraction from\nsemi-structured websites has required learning an extraction model specific to\na given template via either manually labeled or distantly supervised data from\nthat template. In this work, we propose a solution for \"zero-shot\" open-domain\nrelation extraction from webpages with a previously unseen template, including\nfrom websites with little overlap with existing sources of knowledge for\ndistant supervision and websites in entirely new subject verticals. Our model\nuses a graph neural network-based approach to build a rich representation of\ntext fields on a webpage and the relationships between them, enabling\ngeneralization to new templates. Experiments show this approach provides a 31%\nF1 gain over a baseline for zero-shot extraction in a new subject vertical."}, {"title": "A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle", "authors": "<b>A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle</b>"}, {"title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers", "authors": "Shen-yun Miao, Chao-Chun Liang, Keh-Yih Su"}, {"title": "A Frame-based Sentence Representation for Machine Reading Comprehension", "authors": "Shaoru Guo, Ru Li, Hongye Tan, Xiaoli Li, Yong Guan, Hongyan Zhao, Yueping Zhang"}, {"title": "A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal", "authors": "Demian Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John Glover, Georgiana Ifrim", "link": "https://arxiv.org/abs/2005.10070", "summary": "Multi-document summarization (MDS) aims to compress the content in large\ndocument collections into short summaries and has important applications in\nstory clustering for newsfeeds, presentation of search results, and timeline\ngeneration. However, there is a lack of datasets that realistically address\nsuch use cases at a scale large enough for training supervised models for this\ntask. This work presents a new dataset for MDS that is large both in the total\nnumber of document clusters and in the size of individual clusters. We build\nthis dataset by leveraging the Wikipedia Current Events Portal (WCEP), which\nprovides concise and neutral human-written summaries of news events, with links\nto external source articles. We also automatically extend these source articles\nby looking for related articles in the Common Crawl archive. We provide a\nquantitative analysis of the dataset and empirical results for several\nstate-of-the-art MDS techniques."}, {"title": "A Multi-Perspective Architecture for Semantic Code Search", "authors": "Rajarshi Haldar, Lingfei Wu, JinJun Xiong, Julia Hockenmaier", "link": "https://arxiv.org/abs/2005.06980", "summary": "The ability to match pieces of code to their corresponding natural language\ndescriptions and vice versa is fundamental for natural language search\ninterfaces to software repositories. In this paper, we propose a novel\nmulti-perspective cross-lingual neural framework for code--text matching,\ninspired in part by a previous model for monolingual text-to-text matching, to\ncapture both global and local similarities. Our experiments on the CoNaLa\ndataset show that our proposed model yields better performance on this\ncross-lingual text-to-code matching task than previous approaches that map code\nand text to a single joint embedding space."}, {"title": "A negative case analysis of visual grounding methods for VQA", "authors": "Robik Shrestha, Kushal Kafle, Christopher Kanan", "link": "https://arxiv.org/abs/2004.05704", "summary": "Existing Visual Question Answering (VQA) methods tend to exploit dataset\nbiases and spurious statistical correlations, instead of producing right\nanswers for the right reasons. To address this issue, recent bias mitigation\nmethods for VQA propose to incorporate visual cues (e.g., human attention maps)\nto better ground the VQA models, showcasing impressive gains. However, we show\nthat the performance improvements are not a result of improved visual\ngrounding, but a regularization effect which prevents over-fitting to\nlinguistic priors. For instance, we find that it is not actually necessary to\nprovide proper, human-based cues; random, insensible cues also result in\nsimilar improvements. Based on this observation, we propose a simpler\nregularization scheme that does not require any external annotations and yet\nachieves near state-of-the-art performance on VQA-CPv2."}, {"title": "A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing", "authors": "Kartik Goyal, Chris Dyer, Christopher Warren, Maxwell G\u2019Sell, Taylor Berg-Kirkpatrick", "link": "https://arxiv.org/abs/2005.01646", "summary": "We propose a deep and interpretable probabilistic generative model to analyze\nglyph shapes in printed Early Modern documents. We focus on clustering\nextracted glyph images into underlying templates in the presence of multiple\nconfounding sources of variance. Our approach introduces a neural editor model\nthat first generates well-understood printing phenomena like spatial\nperturbations from template parameters via interpertable latent variables, and\nthen modifies the result by generating a non-interpretable latent vector\nresponsible for inking variations, jitter, noise from the archiving process,\nand other unforeseen phenomena associated with Early Modern printing.\nCritically, by introducing an inference network whose input is restricted to\nthe visual residual between the observation and the interpretably-modified\ntemplate, we are able to control and isolate what the vector-valued latent\nvariable captures. We show that our approach outperforms rigid interpretable\nclustering baselines (Ocular) and overly-flexible deep generative models (VAE)\nalike on the task of completely unsupervised discovery of typefaces in\nmixed-font documents."}, {"title": "A Re-evaluation of Knowledge Graph Completion Methods", "authors": "Zhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha Talukdar, Yiming Yang", "link": "https://arxiv.org/abs/1911.03903", "summary": "Knowledge Graph Completion (KGC) aims at automatically predicting missing\nlinks for large-scale knowledge graphs. A vast number of state-of-the-art KGC\ntechniques have got published at top conferences in several research fields,\nincluding data mining, machine learning, and natural language processing.\nHowever, we notice that several recent papers report very high performance,\nwhich largely outperforms previous state-of-the-art methods. In this paper, we\nfind that this can be attributed to the inappropriate evaluation protocol used\nby them and propose a simple evaluation protocol to address this problem. The\nproposed protocol is robust to handle bias in the model, which can\nsubstantially affect the final results. We conduct extensive experiments and\nreport the performance of several existing methods using our protocol. The\nreproducible code has been made publicly available"}, {"title": "A Relational Memory-based Embedding Model for Triple Classification and Search Personalization", "authors": "Dai Quoc Nguyen, Tu Nguyen, Dinh Phung", "link": "https://arxiv.org/abs/1907.06080", "summary": "Knowledge graph embedding methods often suffer from a limitation of\nmemorizing valid triples to predict new ones for triple classification and\nsearch personalization problems. To this end, we introduce a novel embedding\nmodel, named R-MeN, that explores a relational memory network to encode\npotential dependencies in relationship triples. R-MeN considers each triple as\na sequence of 3 input vectors that recurrently interact with a memory using a\ntransformer self-attention mechanism. Thus R-MeN encodes new information from\ninteractions between the memory and each input vector to return a corresponding\nvector. Consequently, R-MeN feeds these 3 returned vectors to a convolutional\nneural network-based decoder to produce a scalar score for the triple.\nExperimental results show that our proposed R-MeN obtains state-of-the-art\nresults on SEARCH17 for the search personalization task, and on WN11 and FB13\nfor the triple classification task."}, {"title": "A Relaxed Matching Procedure for Unsupervised BLI", "authors": "Xu Zhao, Zihao Wang, Yong Zhang, Hao Wu"}, {"title": "A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation", "authors": "Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, Shuai Ma"}, {"title": "A Simple and Effective Unified Encoder for Document-Level Machine Translation", "authors": "Shuming Ma, Dongdong Zhang, Ming Zhou"}, {"title": "A Tale of a Probe and a Parser", "authors": "Rowan Hall Maudslay, Josef Valvoda, Tiago Pimentel, Adina Williams, Ryan Cotterell", "link": "https://arxiv.org/abs/2005.01641", "summary": "Measuring what linguistic information is encoded in neural models of language\nhas become popular in NLP. Researchers approach this enterprise by training\n\"probes\" - supervised models designed to extract linguistic structure from\nanother model's output. One such probe is the structural probe (Hewitt and\nManning, 2019), designed to quantify the extent to which syntactic information\nis encoded in contextualised word representations. The structural probe has a\nnovel design, unattested in the parsing literature, the precise benefit of\nwhich is not immediately obvious. To explore whether syntactic probes would do\nbetter to make use of existing techniques, we compare the structural probe to a\nmore traditional parser with an identical lightweight parameterisation. The\nparser outperforms structural probe on UUAS in seven of nine analysed\nlanguages, often by a substantial amount (e.g. by 11.1 points in English).\nUnder a second less common metric, however, there is the opposite trend - the\nstructural probe outperforms the parser. This begs the question: which metric\nshould we prefer?"}, {"title": "A Three-Parameter Rank-Frequency Relation in Natural Languages", "authors": "Chenchen Ding, Masao Utiyama, Eiichiro Sumita"}, {"title": "A Transformer-based Approach for Source Code Summarization", "authors": "Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang", "link": "https://arxiv.org/abs/2005.00653", "summary": "Generating a readable summary that describes the functionality of a program\nis known as source code summarization. In this task, learning code\nrepresentation by modeling the pairwise relationship between code tokens to\ncapture their long-range dependencies is crucial. To learn code representation\nfor summarization, we explore the Transformer model that uses a self-attention\nmechanism and has shown to be effective in capturing long-range dependencies.\nIn this work, we show that despite the approach is simple, it outperforms the\nstate-of-the-art techniques by a significant margin. We perform extensive\nanalysis and ablation studies that reveal several important findings, e.g., the\nabsolute encoding of source code tokens' position hinders, while relative\nencoding significantly improves the summarization performance. We have made our\ncode publicly available to facilitate future research."}, {"title": "A Two-Stage Masked LM Method for Term Set Expansion", "authors": "Guy Kushilevitz, Shaul Markovitch, Yoav Goldberg", "link": "https://arxiv.org/abs/2005.01063", "summary": "We tackle the task of Term Set Expansion (TSE): given a small seed set of\nexample terms from a semantic class, finding more members of that class. The\ntask is of great practical utility, and also of theoretical utility as it\nrequires generalization from few examples. Previous approaches to the TSE task\ncan be characterized as either distributional or pattern-based. We harness the\npower of neural masked language models (MLM) and propose a novel TSE algorithm,\nwhich combines the pattern-based and distributional approaches. Due to the\nsmall size of the seed set, fine-tuning methods are not effective, calling for\nmore creative use of the MLM. The gist of the idea is to use the MLM to first\nmine for informative patterns with respect to the seed set, and then to obtain\nmore members of the seed class by generalizing these patterns. Our method\noutperforms state-of-the-art TSE algorithms. Implementation is available at:\nhttps://github.com/ guykush/TermSetExpansion-MPB/"}, {"title": "A Two-Step Approach for Implicit Event Argument Detection", "authors": "Zhisong Zhang, Xiang Kong, Zhengzhong Liu, Xuezhe Ma, Eduard Hovy"}, {"title": "Active Learning for Coreference Resolution using Discrete Annotation", "authors": "Belinda Z. Li, Gabriel Stanovsky, Luke Zettlemoyer", "link": "https://arxiv.org/abs/2004.13671", "summary": "We improve upon pairwise annotation for active learning in coreference\nresolution, by asking annotators to identify mention antecedents if a presented\nmention pair is deemed not coreferent. This simple modification, when combined\nwith a novel mention clustering algorithm for selecting which examples to\nlabel, is much more efficient in terms of the performance obtained per\nannotation budget. In experiments with existing benchmark coreference datasets,\nwe show that the signal from this additional question leads to significant\nperformance gains per human-annotation hour. Future work can use our annotation\nprotocol to effectively develop coreference models for new domains. Our code is\npublicly available at\nhttps://github.com/belindal/discrete-active-learning-coref ."}, {"title": "An Empirical Comparison of Unsupervised Constituency Parsing Methods", "authors": "Jun Li, Yifan Cao, Jiong Cai, Yong Jiang, Kewei Tu"}, {"title": "Analyzing the Persuasive Effect of Style in News Editorial Argumentation", "authors": "Roxanne El Baff, Henning Wachsmuth, Khalid Al Khatib, Benno Stein"}, {"title": "Are we Estimating or Guesstimating Translation Quality?", "authors": "Shuo Sun, Francisco Guzm\u00e1n, Lucia Specia"}, {"title": "Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization", "authors": "Sajad Sotudeh Gharebagh, Nazli Goharian, Ross Filice", "link": "https://arxiv.org/abs/2005.00163", "summary": "Sequence-to-sequence (seq2seq) network is a well-established model for text\nsummarization task. It can learn to produce readable content; however, it falls\nshort in effectively identifying key regions of the source. In this paper, we\napproach the content selection problem for clinical abstractive summarization\nby augmenting salient ontological terms into the summarizer. Our experiments on\ntwo publicly available clinical data sets (107,372 reports of MIMIC-CXR, and\n3,366 reports of OpenI) show that our model statistically significantly boosts\nstate-of-the-art results in terms of Rouge metrics (with improvements: 2.9%\nRG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of\nimprovement impacts patients' welfare."}, {"title": "Autoencoding Keyword Correlation Graph for Document Clustering", "authors": "Billy Chiu, Sunil Kumar Sahu, Derek Thomas, Neha Sengupta, Mohammady Mahdy"}, {"title": "Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring", "authors": "Haoran Zhang, Diane Litman"}, {"title": "Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model", "authors": "Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura"}, {"title": "Bayesian Hierarchical Words Representation Learning", "authors": "Oren Barkan, Idan Rejwan, Avi Caciularu, Noam Koenigstein", "link": "https://arxiv.org/abs/2004.07126", "summary": "This paper presents the Bayesian Hierarchical Words Representation (BHWR)\nlearning algorithm. BHWR facilitates Variational Bayes word representation\nlearning combined with semantic taxonomy modeling via hierarchical priors. By\npropagating relevant information between related words, BHWR utilizes the\ntaxonomy to improve the quality of such representations. Evaluation of several\nlinguistic datasets demonstrates the advantages of BHWR over suitable\nalternatives that facilitate Bayesian modeling with or without semantic priors.\nFinally, we further show that BHWR produces better representations for rare\nwords."}, {"title": "Benefits of Intermediate Annotations in Reading Comprehension", "authors": "Dheeru Dua, Sameer Singh, Matt Gardner"}, {"title": "Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning", "authors": "Zhuoren Jiang, Zhe Gao, Yu Duan, Yangyang Kang, Changlong Sun, Qiong Zhang, Xiaozhong Liu", "link": "", "summary": ""}, {"title": "Character-Level Translation with Self-attention", "authors": "Yingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, Richard H.R. Hahnloser", "link": "https://arxiv.org/abs/2004.14788", "summary": "We explore the suitability of self-attention models for character-level\nneural machine translation. We test the standard transformer model, as well as\na novel variant in which the encoder block combines information from nearby\ncharacters using convolutions. We perform extensive experiments on WMT and UN\ndatasets, testing both bilingual and multilingual translation to English using\nup to three input languages (French, Spanish, and Chinese). Our transformer\nvariant consistently outperforms the standard transformer at the\ncharacter-level and converges faster while learning more robust character-level\nalignments."}, {"title": "ClarQ: A large-scale and diverse dataset for Clarification Question Generation", "authors": "Vaibhav Kumar, Alan W Black"}, {"title": "Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction", "authors": "Mladen Karan, Ivan Vuli\u0107, Anna Korhonen, Goran Glava\u0161"}, {"title": "Clinical Concept Linking with Contextualized Neural Representations", "authors": "Elliot Schumacher, Andriy Mulyar, Mark Dredze"}, {"title": "Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain", "authors": "Lukas Lange, Heike Adel, Jannik Str\u00f6tgen", "link": "http://arxiv.org/abs/2005.09397", "summary": "Exploiting natural language processing in the clinical domain requires\nde-identification, i.e., anonymization of personal information in texts.\nHowever, current research considers de-identification and downstream tasks,\nsuch as concept extraction, only in isolation and does not study the effects of\nde-identification on other tasks. In this paper, we close this gap by reporting\nconcept extraction performance on automatically anonymized data and\ninvestigating joint models for de-identification and concept extraction. In\nparticular, we propose a stacked model with restricted access to\nprivacy-sensitive information and a multitask model. We set the new state of\nthe art on benchmark datasets in English (96.1% F1 for de-identification and\n88.9% F1 for concept extraction) and Spanish (91.4% F1 for concept extraction)."}, {"title": "Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling", "authors": "Zihan Liu, Genta Indra Winata, Peng Xu, Pascale Fung", "link": "http://arxiv.org/abs/2004.11727", "summary": "As an essential task in task-oriented dialog systems, slot filling requires\nextensive training data in a certain domain. However, such data are not always\navailable. Hence, cross-domain slot filling has naturally arisen to cope with\nthis data scarcity problem. In this paper, we propose a Coarse-to-fine approach\n(Coach) for cross-domain slot filling. Our model first learns the general\npattern of slot entities by detecting whether the tokens are slot entities or\nnot. It then predicts the specific types for the slot entities. In addition, we\npropose a template regularization approach to improve the adaptation robustness\nby regularizing the representation of utterances based on utterance templates.\nExperimental results show that our model significantly outperforms\nstate-of-the-art approaches in slot filling. Furthermore, our model can also be\napplied to the cross-domain named entity recognition task, and it achieves\nbetter adaptation performance than other existing baselines. The code is\navailable at https://github.com/zliucr/coach."}, {"title": "Code-switching patterns can be an effective route to improve performance of downstream NLP applications: A case study of humour, sarcasm and hate speech detection", "authors": "Srijan Bansal, Vishal Garimella, Ayush Suhane, Jasabanta Patro, Animesh Mukherjee", "link": "https://arxiv.org/abs/2005.02295", "summary": "In this paper we demonstrate how code-switching patterns can be utilised to\nimprove various downstream NLP applications. In particular, we encode different\nswitching features to improve humour, sarcasm and hate speech detection tasks.\nWe believe that this simple linguistic observation can also be potentially\nhelpful in improving other similar NLP applications."}, {"title": "Composing Elementary Discourse Units in Abstractive Summarization", "authors": "Zhenwen Li, Wenhao Wu, Sujian Li"}, {"title": "Content Word Aware Neural Machine Translation", "authors": "Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita", "link": "", "summary": ""}, {"title": "Contextual Embeddings: When Are They Worth It?", "authors": "Simran Arora, Avner May, Jian Zhang, Christopher R\u00e9", "link": "https://arxiv.org/abs/2005.09117", "summary": "We study the settings for which deep contextual embeddings (e.g., BERT) give\nlarge improvements in performance relative to classic pretrained embeddings\n(e.g., GloVe), and an even simpler baseline---random word embeddings---focusing\non the impact of the training set size and the linguistic properties of the\ntask. Surprisingly, we find that both of these simpler baselines can match\ncontextual embeddings on industry-scale data, and often perform within 5 to 10%\naccuracy (absolute) on benchmark tasks. Furthermore, we identify properties of\ndata for which contextual embeddings give particularly large gains: language\ncontaining complex structure, ambiguous word usage, and words unseen in\ntraining."}, {"title": "Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns", "authors": "KayYen Wong, Sameen Maruf, Gholamreza Haffari", "link": "https://arxiv.org/abs/2004.09894", "summary": "The advent of context-aware NMT has resulted in promising improvements in the\noverall translation quality and specifically in the translation of discourse\nphenomena such as pronouns. Previous works have mainly focused on the use of\npast sentences as context with a focus on anaphora translation. In this work,\nwe investigate the effect of future sentences as context by comparing the\nperformance of a contextual NMT model trained with the future context to the\none trained with the past context. Our experiments and evaluation, using\ngeneric and pronoun-focused automatic metrics, show that the use of future\ncontext not only achieves significant improvements over the context-agnostic\nTransformer, but also demonstrates comparable and in some cases improved\nperformance over its counterpart trained on past context. We also perform an\nevaluation on a targeted cataphora test suite and report significant gains over\nthe context-agnostic Transformer in terms of BLEU."}, {"title": "Contextualized Sparse Representations for Real-Time Open-Domain Question Answering", "authors": "Jinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi, Jaewoo Kang", "link": "https://arxiv.org/abs/1911.02896", "summary": "Open-domain question answering can be formulated as a phrase retrieval\nproblem, in which we can expect huge scalability and speed benefit but often\nsuffer from low accuracy due to the limitation of existing phrase\nrepresentation models. In this paper, we aim to improve the quality of each\nphrase embedding by augmenting it with a contextualized sparse representation\n(Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g.,\ntf-idf) or directly learned (only few thousand dimensions), we leverage\nrectified self-attention to indirectly learn sparse vectors in n-gram\nvocabulary space. By augmenting the previous phrase retrieval model (Seo et\nal., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.\nOur CuratedTREC score is even better than the best known retrieve & read model\nwith at least 45x faster inference speed."}, {"title": "Contextualizing Hate Speech Classifiers with Post-hoc Explanation", "authors": "Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani, Xiang Ren", "link": "https://arxiv.org/abs/2005.02439", "summary": "Hate speech classifiers trained on imbalanced datasets struggle to determine\nif group identifiers like \"gay\" or \"black\" are used in offensive or prejudiced\nways. Such biases manifest in false positives when these identifiers are\npresent, due to models' inability to learn the contexts which constitute a\nhateful usage of identifiers. We extract post-hoc explanations from fine-tuned\nBERT classifiers to detect bias towards identity terms. Then, we propose a\nnovel regularization technique based on these explanations that encourages\nmodels to learn from the context of group identifiers in addition to the\nidentifiers themselves. Our approach improved over baselines in limiting false\npositives on out-of-domain data while maintaining or improving in-domain\nperformance."}, {"title": "Contrastive Self-Supervised Learning for Commonsense Reasoning", "authors": "Tassilo Klein, Moin Nabi", "link": "https://arxiv.org/abs/2005.00669", "summary": "We propose a self-supervised method to solve Pronoun Disambiguation and\nWinograd Schema Challenge problems. Our approach exploits the characteristic\nstructure of training corpora related to so-called \"trigger\" words, which are\nresponsible for flipping the answer in pronoun disambiguation. We achieve such\ncommonsense reasoning by constructing pair-wise contrastive auxiliary\npredictions. To this end, we leverage a mutual exclusive loss regularized by a\ncontrastive margin. Our architecture is based on the recently introduced\ntransformer networks, BERT, that exhibits strong performance on many NLP\nbenchmarks. Empirical results show that our method alleviates the limitation of\ncurrent supervised approaches for commonsense reasoning. This study opens up\navenues for exploiting inexpensive self-supervision to achieve performance gain\nin commonsense reasoning tasks."}, {"title": "Controlled Crowdsourcing for High-Quality QA-SRL Annotation", "authors": "Paul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke Zettlemoyer, Ido Dagan", "link": "https://arxiv.org/abs/1911.03243", "summary": "Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an\nattractive open and natural flavour of SRL, potentially attainable from laymen.\nRecently, a large-scale crowdsourced QA-SRL corpus and a trained parser were\nreleased. Trying to replicate the QA-SRL annotation for new texts, we found\nthat the resulting annotations were lacking in quality, particularly in\ncoverage, making them insufficient for further research and evaluation. In this\npaper, we present an improved crowdsourcing protocol for complex semantic\nannotation, involving worker selection and training, and a data consolidation\nphase. Applying this protocol to QA-SRL yielded high-quality annotation with\ndrastically higher coverage, producing a new gold evaluation dataset. We\nbelieve that our annotation protocol and gold standard will facilitate future\nreplicable research of natural semantic annotations."}, {"title": "Conversational Word Embedding for Retrieval-Based Dialog System", "authors": "Wentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang, Guoping Hu", "link": "https://arxiv.org/abs/2004.13249", "summary": "Human conversations contain many types of information, e.g., knowledge,\ncommon sense, and language habits. In this paper, we propose a conversational\nword embedding method named PR-Embedding, which utilizes the conversation pairs\n$ \\left\\langle{post, reply} \\right\\rangle$ to learn word embedding. Different\nfrom previous works, PR-Embedding uses the vectors from two different semantic\nspaces to represent the words in post and reply. To catch the information among\nthe pair, we first introduce the word alignment model from statistical machine\ntranslation to generate the cross-sentence window, then train the embedding on\nword-level and sentence-level. We evaluate the method on single-turn and\nmulti-turn response selection tasks for retrieval-based dialog systems. The\nexperiment results show that PR-Embedding can improve the quality of the\nselected response. PR-Embedding source code is available at\nhttps://github.com/wtma/PR-Embedding"}, {"title": "Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis", "authors": "Janek Bevendorff, Khalid Al Khatib, Martin Potthast, Benno Stein"}, {"title": "Crossing Variational Autoencoders for Answer Retrieval", "authors": "Wenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng, Meng Jiang", "link": "https://arxiv.org/abs/2005.02557", "summary": "Answer retrieval is to find the most aligned answer from a large set of\ncandidates given a question. Learning vector representations of\nquestions/answers is the key factor. Question-answer alignment and\nquestion/answer semantics are two important signals for learning the\nrepresentations. Existing methods learned semantic representations with dual\nencoders or dual variational auto-encoders. The semantic information was\nlearned from language models or question-to-question (answer-to-answer)\ngenerative processes. However, the alignment and semantics were too separate to\ncapture the aligned semantics between question and answer. In this work, we\npropose to cross variational auto-encoders by generating questions with aligned\nanswers and generating answers with aligned questions. Experiments show that\nour method outperforms the state-of-the-art answer retrieval method on SQuAD."}, {"title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference", "authors": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin", "link": "", "summary": ""}, {"title": "Designing Precise and Robust Dialogue Response Evaluators", "authors": "Tianyu Zhao, Divesh Lala, Tatsuya Kawahara", "link": "https://arxiv.org/abs/2004.04908", "summary": "Automatic dialogue response evaluator has been proposed as an alternative to\nautomated metrics and human evaluation. However, existing automatic evaluators\nachieve only moderate correlation with human judgement and they are not robust.\nIn this work, we propose to build a reference-free evaluator and exploit the\npower of semi-supervised training and pretrained (masked) language models.\nExperimental results demonstrate that the proposed evaluator achieves a strong\ncorrelation (> 0.6) with human judgement and generalizes robustly to diverse\nresponses and corpora. We open-source the code and data in\nhttps://github.com/ZHAOTING/dialog-processing."}, {"title": "Dialogue State Tracking with Explicit Slot Connection Modeling", "authors": "Yawen Ouyang, Moxin Chen, Xinyu Dai, Yinggong Zhao, Shujian Huang, Jiajun Chen"}, {"title": "Do Transformers Need Deep Long-Range Memory?", "authors": "Jack Rae, Ali Razavi"}, {"title": "Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods", "authors": "Ning Miao, Yuxuan Song, Hao Zhou, Lei Li"}, {"title": "Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation", "authors": "Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li", "link": "http://arxiv.org/abs/2005.03393", "summary": "In encoder-decoder neural models, multiple encoders are in general used to\nrepresent the contextual information in addition to the individual sentence. In\nthis paper, we investigate multi-encoder approaches in documentlevel neural\nmachine translation (NMT). Surprisingly, we find that the context encoder does\nnot only encode the surrounding sentences but also behaves as a noise\ngenerator. This makes us rethink the real benefits of multi-encoder in\ncontext-aware translation - some of the improvements come from robust training.\nWe compare several methods that introduce noise and/or well-tuned dropout setup\ninto the training of these encoders. Experimental results show that noisy\ntraining plays an important role in multi-encoder-based NMT, especially when\nthe training data is small. Also, we establish a new state-of-the-art on IWSLT\nFr-En task by careful use of noise generation and dropout methods."}, {"title": "Don\u2019t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction", "authors": "Zhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng, Jianmin Yao"}, {"title": "Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing", "authors": "Jiangming Liu, Shay B. Cohen, Mirella Lapata"}, {"title": "Dynamic Memory Induction Networks for Few-Shot Text Classification", "authors": "Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, Xiaodan Zhu", "link": "https://arxiv.org/abs/2005.05727", "summary": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-shot\ntext classification. The model utilizes dynamic routing to provide more\nflexibility to memory-based few-shot learning in order to better adapt the\nsupport sets, which is a critical capacity of few-shot classification models.\nBased on that, we further develop induction models with query information,\naiming to enhance the generalization ability of meta-learning. The proposed\nmodel achieves new state-of-the-art results on the miniRCV1 and ODIC dataset,\nimproving the best performance (accuracy) by 2~4%. Detailed analysis is further\nperformed to show the effectiveness of each component."}, {"title": "Dynamic Sampling Strategies for Multi-Task Reading Comprehension", "authors": "Ananth Gottumukkala, Dheeru Dua, Sameer Singh, Matt Gardner", "link": "", "summary": ""}, {"title": "Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change", "authors": "Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu", "link": "https://arxiv.org/abs/2005.02008", "summary": "The choice of hyper-parameters affects the performance of neural models.\nWhile much previous research (Sutskever et al., 2013; Duchi et al., 2011;\nKingma and Ba, 2015) focuses on accelerating convergence and reducing the\neffects of the learning rate, comparatively few papers concentrate on the\neffect of batch size. In this paper, we analyze how increasing batch size\naffects gradient direction, and propose to evaluate the stability of gradients\nwith their angle change. Based on our observations, the angle change of\ngradient direction first tends to stabilize (i.e. gradually decrease) while\naccumulating mini-batches, and then starts to fluctuate. We propose to\nautomatically and dynamically determine batch sizes by accumulating gradients\nof mini-batches and performing an optimization step at just the time when the\ndirection of gradients starts to fluctuate. To improve the efficiency of our\napproach for large models, we propose a sampling approach to select gradients\nof parameters sensitive to the batch size. Our approach dynamically determines\nproper and efficient batch sizes during training. In our experiments on the WMT\n14 English to German and English to French tasks, our approach improves the\nTransformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively."}, {"title": "Efficient strategies for hierarchical text classification: external knowledge and auxiliary tasks", "authors": "Kervy Rivas Rojas, Gina Bustamante, Arturo Oncevay, Marco Antonio Sobrevilla Cabezudo", "link": "https://arxiv.org/abs/2005.02473", "summary": "In hierarchical text classification, we perform a sequence of inference steps\nto predict the category of a document from top to bottom of a given class\ntaxonomy. Most of the studies have focused on developing novels neural network\narchitectures to deal with the hierarchical structure, but we prefer to look\nfor efficient ways to strengthen a baseline model. We first define the task as\na sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic\ntask of bottom-up-classification. Then, from external dictionaries, we retrieve\ntextual definitions for the classes of all the hierarchy's layers, and map them\ninto the word vector space. We use the class-definition embeddings as an\nadditional input to condition the prediction of the next layer and in an\nadapted beam search. Whereas the modified search did not provide large gains,\nthe combination of the auxiliary task and the additional input of\nclass-definitions significantly enhance the classification accuracy. With our\nefficient approaches, we outperform previous studies, using a drastically\nreduced number of parameters, in two well-known English datasets."}, {"title": "Embarrassingly Simple Unsupervised Aspect Extraction", "authors": "St\u00e9phan Tulkens, Andreas van Cranenburgh", "link": "https://arxiv.org/abs/2004.13580", "summary": "We present a simple but effective method for aspect identification in\nsentiment analysis. Our unsupervised method only requires word embeddings and a\nPOS tagger, and is therefore straightforward to apply to new domains and\nlanguages. We introduce Contrastive Attention (CAt), a novel single-head\nattention mechanism based on an RBF kernel, which gives a considerable boost in\nperformance and makes the model interpretable. Previous work relied on\nsyntactic features and complex neural models. We show that given the simplicity\nof current benchmark datasets for aspect extraction, such complex models are\nnot needed. The code to reproduce the experiments reported in this paper is\navailable at https://github.com/clips/cat"}, {"title": "Enabling Language Models to Fill in the Blanks", "authors": "Chris Donahue, Mina Lee, Percy Liang", "link": "https://arxiv.org/abs/2005.05339", "summary": "We present a simple approach for text infilling, the task of predicting\nmissing spans of text at any position in a document. While infilling could\nenable rich functionality especially for writing assistance tools, more\nattention has been devoted to language modeling---a special case of infilling\nwhere text is predicted at the end of a document. In this paper, we aim to\nextend the capabilities of language models (LMs) to the more general task of\ninfilling. To this end, we train (or fine-tune) off-the-shelf LMs on sequences\ncontaining the concatenation of artificially-masked text and the text which was\nmasked. We show that this approach, which we call infilling by language\nmodeling, can enable LMs to infill entire sentences effectively on three\ndifferent domains: short stories, scientific abstracts, and lyrics.\nFurthermore, we show that humans have difficulty identifying sentences infilled\nby our approach as machine-generated in the domain of short stories."}, {"title": "Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction", "authors": "Masahiro Kaneko, Masato Mita, Shun Kiyono, Jun Suzuki, Kentaro Inui", "link": "https://arxiv.org/abs/2005.00987", "summary": "This paper investigates how to effectively incorporate a pre-trained masked\nlanguage model (MLM), such as BERT, into an encoder-decoder (EncDec) model for\ngrammatical error correction (GEC). The answer to this question is not as\nstraightforward as one might expect because the previous common methods for\nincorporating a MLM into an EncDec model have potential drawbacks when applied\nto GEC. For example, the distribution of the inputs to a GEC model can be\nconsiderably different (erroneous, clumsy, etc.) from that of the corpora used\nfor pre-training MLMs; however, this issue is not addressed in the previous\nmethods. Our experiments show that our proposed method, where we first\nfine-tune a MLM with a given GEC corpus and then use the output of the\nfine-tuned MLM as additional features in the GEC model, maximizes the benefit\nof the MLM. The best-performing model achieves state-of-the-art performances on\nthe BEA-2019 and CoNLL-2014 benchmarks. Our code is publicly available at:\nhttps://github.com/kanekomasahiro/bert-gec."}, {"title": "ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation", "authors": "Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel", "link": "https://arxiv.org/abs/2005.00850", "summary": "We propose to train a non-autoregressive machine translation model to\nminimize the energy defined by a pretrained autoregressive model. In\nparticular, we view our non-autoregressive translation system as an inference\nnetwork (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher\nenergy. This contrasts with the popular approach of training a\nnon-autoregressive model on a distilled corpus consisting of the beam-searched\noutputs of such a teacher model. Our approach, which we call ENGINE\n(ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive\nresults on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the\nperformance of autoregressive models."}, {"title": "Enhancing Machine Translation with Dependency-Aware Self-Attention", "authors": "Emanuele Bugliarello, Naoaki Okazaki", "link": "https://arxiv.org/abs/1909.03149", "summary": "Most neural machine translation models only rely on pairs of parallel\nsentences, assuming syntactic information is automatically learned by an\nattention mechanism. In this work, we investigate different approaches to\nincorporate syntactic knowledge in the Transformer model and also propose a\nnovel, parameter-free, dependency-aware self-attention mechanism that improves\nits translation quality, especially for long sentences and in low-resource\nscenarios. We show the efficacy of each approach on WMT English-German and\nEnglish-Turkish, and WAT English-Japanese translation tasks."}, {"title": "Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention", "authors": "Yanzeng Li, Bowen Yu, Xue Mengge, Tingwen Liu", "link": "http://arxiv.org/abs/1911.02821", "summary": "Most Chinese pre-trained models take character as the basic unit and learn\nrepresentation according to character's external contexts, ignoring the\nsemantics expressed in the word, which is the smallest meaningful utterance in\nChinese. Hence, we propose a novel word-aligned attention to exploit explicit\nword information, which is complementary to various character-based Chinese\npre-trained language models. Specifically, we devise a pooling mechanism to\nalign the character-level attention to the word level and propose to alleviate\nthe potential issue of segmentation error propagation by multi-source\ninformation fusion. As a result, word and character information are explicitly\nintegrated at the fine-tuning procedure. Experimental results on five Chinese\nNLP benchmark tasks demonstrate that our model could bring another significant\ngain over several pre-trained models."}, {"title": "Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing", "authors": "Daniel Fern\u00e1ndez-Gonz\u00e1lez, Carlos G\u00f3mez-Rodr\u00edguez", "link": "https://arxiv.org/abs/2005.13334", "summary": "Sequence-to-sequence constituent parsing requires a linearization to\nrepresent trees as sequences. Top-down tree linearizations, which can be based\non brackets or shift-reduce actions, have achieved the best accuracy to date.\nIn this paper, we show that these results can be improved by using an in-order\nlinearization instead. Based on this observation, we implement an enriched\nin-order shift-reduce linearization inspired by Vinyals et al. (2015)'s\napproach, achieving the best accuracy to date on the English PTB dataset among\nfully-supervised single-model sequence-to-sequence constituent parsers.\nFinally, we apply deterministic attention mechanisms to match the speed of\nstate-of-the-art transition-based parsers, thus showing that\nsequence-to-sequence models can match them, not only in accuracy, but also in\nspeed."}, {"title": "Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification", "authors": "Nianzu Ma, Sahisnu Mazumder, Hao Wang, Bing Liu", "link": "", "summary": ""}, {"title": "Estimating Mutual Information Between Dense Word Embeddings", "authors": "Vitalii Zhelezniak, Aleksandar Savkov, Nils Hammerla"}, {"title": "Evaluating Dialogue Generation Systems via Response Selection", "authors": "Shiki Sato, Reina Akama, Hiroki Ouchi, Jun Suzuki, Kentaro Inui", "link": "https://arxiv.org/abs/2004.14302", "summary": "Existing automatic evaluation metrics for open-domain dialogue response\ngeneration systems correlate poorly with human evaluation. We focus on\nevaluating response generation systems via response selection. To evaluate\nsystems properly via response selection, we propose the method to construct\nresponse selection test sets with well-chosen false candidates. Specifically,\nwe propose to construct test sets filtering out some types of false candidates:\n(i) those unrelated to the ground-truth response and (ii) those acceptable as\nappropriate responses. Through experiments, we demonstrate that evaluating\nsystems via response selection with the test sets developed by our method\ncorrelates more strongly with human evaluation, compared with widely used\nautomatic evaluation metrics such as BLEU."}, {"title": "Evaluating Robustness to Input Perturbations for Neural Machine Translation", "authors": "Xing Niu, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan", "link": "https://arxiv.org/abs/2005.00580", "summary": "Neural Machine Translation (NMT) models are sensitive to small perturbations\nin the input. Robustness to such perturbations is typically measured using\ntranslation quality metrics such as BLEU on the noisy input. This paper\nproposes additional metrics which measure the relative degradation and changes\nin translation when small perturbations are added to the input. We focus on a\nclass of models employing subword regularization to address robustness and\nperform extensive evaluations of these models using the robustness measures\nproposed. Results show that our proposed metrics reveal a clear trend of\nimproved robustness to perturbations when subword regularization methods are\nused."}, {"title": "Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks", "authors": "Yufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen, Liang Wang", "link": "https://arxiv.org/abs/2004.13826", "summary": "Text classification is fundamental in natural language processing (NLP), and\nGraph Neural Networks (GNN) are recently applied in this task. However, the\nexisting graph-based works can neither capture the contextual word\nrelationships within each document nor fulfil the inductive learning of new\nwords. In this work, to overcome such problems, we propose TextING for\ninductive text classification via GNN. We first build individual graphs for\neach document and then use GNN to learn the fine-grained word representations\nbased on their local structures, which can also effectively produce embeddings\nfor unseen words in the new document. Finally, the word nodes are aggregated as\nthe document embedding. Extensive experiments on four benchmark datasets show\nthat our method outperforms state-of-the-art text classification methods."}, {"title": "ExpBERT: Representation Engineering with Natural Language Explanations", "authors": "Shikhar Murty, Pang Wei Koh, Percy Liang", "link": "https://arxiv.org/abs/2005.01932", "summary": "Suppose we want to specify the inductive bias that married couples typically\ngo on honeymoons for the task of extracting pairs of spouses from text. In this\npaper, we allow model developers to specify these types of inductive biases as\nnatural language explanations. We use BERT fine-tuned on MultiNLI to\n``interpret'' these explanations with respect to the input sentence, producing\nexplanation-guided representations of the input. Across three relation\nextraction tasks, our method, ExpBERT, matches a BERT baseline but with 3--20x\nless labeled data and improves on the baseline by 3--10 F1 points with the same\namount of labeled data."}, {"title": "Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness", "authors": "Khalid Al Khatib, Michael V\u00f6lske, Shahbaz Syed, Nikolay Kolyada, Benno Stein"}, {"title": "Exploring Content Selection in Summarization of Novel Chapters", "authors": "Faisal Ladhak, Bryan Li, Yaser Al-Onaizan, Kathy McKeown", "link": "https://arxiv.org/abs/2005.01840", "summary": "We present a new summarization task, generating summaries of novel chapters\nusing summary/chapter pairs from online study guides. This is a harder task\nthan the news summarization task, given the chapter length as well as the\nextreme paraphrasing and generalization found in the summaries. We focus on\nextractive summarization, which requires the creation of a gold-standard set of\nextractive summaries. We present a new metric for aligning reference summary\nsentences with chapter sentences to create gold extracts and also experiment\nwith different alignment methods. Our experiments demonstrate significant\nimprovement over prior alignment approaches for our task as shown through\nautomatic metrics and a crowd-sourced pyramid analysis."}, {"title": "Fact-based Content Weighting for Evaluating Abstractive Summarisation", "authors": "Xinnuo Xu, Ond\u0159ej Du\u0161ek, Jingyi Li, Verena Rieser, Ioannis Konstas"}, {"title": "Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts", "authors": "Agostina Calabrese, Michele Bevilacqua, Roberto Navigli"}, {"title": "Few-Shot NLG with Pre-Trained Language Model", "authors": "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang", "link": "", "summary": ""}, {"title": "FLAT: Chinese NER Using Flat-Lattice Transformer", "authors": "Xiaonan Li, Hang Yan, Xipeng Qiu, Xuanjing Huang", "link": "", "summary": ""}, {"title": "GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples", "authors": "Danilo Croce, Giuseppe Castellucci, Roberto Basili"}, {"title": "Geometry-aware domain adaptation for unsupervised alignment of word embeddings", "authors": "Pratik Jawanpuria, Mayank Meghwanshi, Bamdev Mishra", "link": "https://arxiv.org/abs/2004.08243", "summary": "We propose a novel manifold based geometric approach for learning\nunsupervised alignment of word embeddings between the source and the target\nlanguages. Our approach formulates the alignment learning problem as a domain\nadaptation problem over the manifold of doubly stochastic matrices. This\nviewpoint arises from the aim to align the second order information of the two\nlanguage spaces. The rich geometry of the doubly stochastic manifold allows to\nemploy efficient Riemannian conjugate gradient algorithm for the proposed\nformulation. Empirically, the proposed approach outperforms state-of-the-art\noptimal transport based approach on the bilingual lexicon induction task across\nseveral language pairs. The performance improvement is more significant for\ndistant language pairs."}, {"title": "Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?", "authors": "Kobi Leins, Jey Han Lau, Timothy Baldwin", "link": "https://arxiv.org/abs/2005.13213", "summary": "As part of growing NLP capabilities, coupled with an awareness of the ethical\ndimensions of research, questions have been raised about whether particular\ndatasets and tasks should be deemed off-limits for NLP research. We examine\nthis question with respect to a paper on automatic legal sentencing from EMNLP\n2019 which was a source of some debate, in asking whether the paper should have\nbeen allowed to be published, who should have been charged with making such a\ndecision, and on what basis. We focus in particular on the role of data\nstatements in ethically assessing research, but also discuss the topic of dual\nuse, and examine the outcomes of similar debates in other scientific\ndisciplines."}, {"title": "Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs", "authors": "Hong-You Chen, SZ-HAN YU, Shou-de Lin"}, {"title": "GPT-too: A language-model-first approach for AMR-to-text generation", "authors": "Manuel Mager, Ram\u00f3n Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, Salim Roukos", "link": "https://arxiv.org/abs/2005.09123", "summary": "Meaning Representations (AMRs) are broad-coverage sentence-level semantic\ngraphs. Existing approaches to generating text from AMR have focused on\ntraining sequence-to-sequence or graph-to-sequence models on AMR annotated data\nonly. In this paper, we propose an alternative approach that combines a strong\npre-trained language model with cycle consistency-based re-scoring. Despite the\nsimplicity of the approach, our experimental results show these models\noutperform all previous techniques on the English LDC2017T10dataset, including\nthe recent use of transformer architectures. In addition to the standard\nevaluation metrics, we provide human evaluation experiments that further\nsubstantiate the strength of our approach."}, {"title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?", "authors": "Tal Linzen", "link": "https://arxiv.org/abs/2005.00955", "summary": "This position paper describes and critiques the Pretraining-Agnostic\nIdentically Distributed (PAID) evaluation paradigm, which has become a central\ntool for measuring progress in natural language understanding. This paradigm\nconsists of three stages: (1) pre-training of a word prediction model on a\ncorpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set\nrepresenting a classification task; (3) evaluation on a test set drawn from the\nsame distribution as that training set. This paradigm favors simple, low-bias\narchitectures, which, first, can be scaled to process vast amounts of data, and\nsecond, can capture the fine-grained statistical properties of a particular\ndata set, regardless of whether those properties are likely to generalize to\nexamples of the task outside the data set. This contrasts with humans, who\nlearn language from several orders of magnitude less data than the systems\nfavored by this evaluation paradigm, and generalize to new tasks in a\nconsistent way. We advocate for supplementing or replacing PAID with paradigms\nthat reward architectures that generalize as quickly and robustly as humans."}, {"title": "Hypernymy Detection for Low-Resource Languages via Meta Learning", "authors": "Changlong Yu, Jialong Han, Haisong Zhang, Wilfred Ng"}, {"title": "Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description", "authors": "Yakun Hu, Zhunchen Luo, Wenhan Chao"}, {"title": "Implicit Discourse Relation Classification: We Need to Talk about Evaluation", "authors": "Najoung Kim, Song Feng, Chulaka Gunasekara, Luis Lastras"}, {"title": "Improved Speech Representations with Multi-Target Autoregressive Predictive Coding", "authors": "Yu-An Chung, James Glass", "link": "http://arxiv.org/abs/2004.05274", "summary": "Training objectives based on predictive coding have recently been shown to be\nvery effective at learning meaningful representations from unlabeled speech.\nOne example is Autoregressive Predictive Coding (Chung et al., 2019), which\ntrains an autoregressive RNN to generate an unseen future frame given a context\nsuch as recent past frames. The basic hypothesis of these approaches is that\nhidden states that can accurately predict future frames are a useful\nrepresentation for many downstream tasks. In this paper we extend this\nhypothesis and aim to enrich the information encoded in the hidden states by\ntraining the model to make more accurate future predictions. We propose an\nauxiliary objective that serves as a regularization to improve generalization\nof the future frame prediction task. Experimental results on phonetic\nclassification, speech recognition, and speech translation not only support the\nhypothesis, but also demonstrate the effectiveness of our approach in learning\nrepresentations that contain richer phonetic content."}, {"title": "Improving Entity Linking through Semantic Reinforced Entity Embeddings", "authors": "Feng Hou, Ruili Wang, Jun He, Yi Zhou"}, {"title": "Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling", "authors": "Canasai Kruengkrai, Thien Hai Nguyen, Sharifah Mahani Aljunied, Lidong Bing"}, {"title": "Improving Non-autoregressive Neural Machine Translation with Monolingual Data", "authors": "Jiawei Zhou, Phillip Keung", "link": "https://arxiv.org/abs/2005.00932", "summary": "Non-autoregressive (NAR) neural machine translation is usually done via\nknowledge distillation from an autoregressive (AR) model. Under this framework,\nwe leverage large monolingual corpora to improve the NAR model's performance,\nwith the goal of transferring the AR model's generalization ability while\npreventing overfitting. On top of a strong NAR baseline, our experimental\nresults on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that\nmonolingual data augmentation consistently improves the performance of the NAR\nmodel to approach the teacher AR model's performance, yields comparable or\nbetter results than the best non-iterative NAR methods in the literature and\nhelps reduce overfitting in the training process."}, {"title": "Incorporating External Knowledge through Pre-training for Natural Language to Code Generation", "authors": "Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, Graham Neubig", "link": "https://arxiv.org/abs/2004.09015", "summary": "Open-domain code generation aims to generate code in a general-purpose\nprogramming language (such as Python) from natural language (NL) intents.\nMotivated by the intuition that developers usually retrieve resources on the\nweb when writing code, we explore the effectiveness of incorporating two\nvarieties of external knowledge into NL-to-code generation: automatically mined\nNL-code pairs from the online programming QA forum StackOverflow and\nprogramming language API documentation. Our evaluations show that combining the\ntwo sources with data augmentation and retrieval-based data re-sampling\nimproves the current state-of-the-art by up to 2.2% absolute BLEU score on the\ncode generation testbed CoNaLa. The code and resources are available at\nhttps://github.com/neulab/external-knowledge-codegen."}, {"title": "Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition", "authors": "Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, Tatsuki Kuribayashi, Ryuto Konno, Kentaro Inui", "link": "https://arxiv.org/abs/2004.14514", "summary": "Interpretable rationales for model predictions play a critical role in\npractical applications. In this study, we develop models possessing\ninterpretable inference process for structured prediction. Specifically, we\npresent a method of instance-based learning that learns similarities between\nspans. At inference time, each span is assigned a class label based on its\nsimilar spans in the training set, where it is easy to understand how much each\ntraining instance contributes to the predictions. Through empirical analysis on\nnamed entity recognition, we demonstrate that our method enables to build\nmodels that have high interpretability without sacrificing performance."}, {"title": "Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder", "authors": "Fan Zhou, Shengming Zhang, Yi Yang"}, {"title": "Interpreting Twitter User Geolocation", "authors": "Ting Zhong, Tianliang Wang, Fan Zhou, Goce Trajcevski, Kunpeng Zhang, Yi Yang"}, {"title": "Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds", "authors": "Kawin Ethayarajh", "link": "https://arxiv.org/abs/2004.12332", "summary": "Most NLP datasets are not annotated with protected attributes such as gender,\nmaking it difficult to measure classification bias using standard measures of\nfairness (e.g., equal opportunity). However, manually annotating a large\ndataset with a protected attribute is slow and expensive. Instead of annotating\nall the examples, can we annotate a subset of them and use that sample to\nestimate the bias? While it is possible to do so, the smaller this annotated\nsample is, the less certain we are that the estimate is close to the true bias.\nIn this work, we propose using Bernstein bounds to represent this uncertainty\nabout the bias estimate as a confidence interval. We provide empirical evidence\nthat a 95% confidence interval derived this way consistently bounds the true\nbias. In quantifying this uncertainty, our method, which we call\nBernstein-bounded unfairness, helps prevent classifiers from being deemed\nbiased or unbiased when there is insufficient evidence to make either claim.\nOur findings suggest that the datasets currently used to measure specific\nbiases are too small to conclusively identify bias except in the most egregious\ncases. For example, consider a co-reference resolution system that is 5% more\naccurate on gender-stereotypical sentences -- to claim it is biased with 95%\nconfidence, we need a bias-specific dataset that is 3.8 times larger than\nWinoBias, the largest available."}, {"title": "It\u2019s Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information", "authors": "Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, Naoaki Okazaki", "link": "http://arxiv.org/abs/2005.02354", "summary": "The performance of neural machine translation systems is commonly evaluated\nin terms of BLEU. However, due to its reliance on target language properties\nand generation, the BLEU metric does not allow an assessment of which\ntranslation directions are more difficult to model. In this paper, we propose\ncross-mutual information (XMI): an asymmetric information-theoretic metric of\nmachine translation difficulty that exploits the probabilistic nature of most\nneural machine translation models. XMI allows us to better evaluate the\ndifficulty of translating text into the target language while controlling for\nthe difficulty of the target-side generation component independent of the\ntranslation task. We then present the first systematic and controlled study of\ncross-lingual translation difficulties using modern neural translation systems.\nCode for replicating our experiments is available online at\nhttps://github.com/e-bug/nmt-difficulty."}, {"title": "Keyphrase Generation for Scientific Document Retrieval", "authors": "Florian Boudin, Ygor Gallina, Akiko Aizawa"}, {"title": "Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms", "authors": "Simeon Sch\u00fcz, Sina Zarrie\u00df"}, {"title": "Language-aware Interlingua for Multilingual Neural Machine Translation", "authors": "Changfeng Zhu, Heng Yu, Shanbo Cheng, Weihua Luo", "link": "", "summary": ""}, {"title": "Learning an Unreferenced Metric for Online Dialogue Evaluation", "authors": "Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William L. Hamilton, Joelle Pineau", "link": "http://arxiv.org/abs/2005.00583", "summary": "Evaluating the quality of a dialogue interaction between two agents is a\ndifficult task, especially in open-domain chit-chat style dialogue. There have\nbeen recent efforts to develop automatic dialogue evaluation metrics, but most\nof them do not generalize to unseen datasets and/or need a human-generated\nreference response during inference, making it infeasible for online\nevaluation. Here, we propose an unreferenced automated evaluation metric that\nuses large pre-trained language models to extract latent representations of\nutterances, and leverages the temporal transitions that exist between them. We\nshow that our model achieves higher correlation with human annotations in an\nonline setting, while not requiring true responses for comparison during\ninference."}, {"title": "Learning Implicit Text Generation via Feature Matching", "authors": "Inkit Padhi, Pierre Dognin, Ke Bai, C\u00edcero Nogueira dos Santos, Vijil Chenthamarakshan, Youssef Mroueh, Payel Das", "link": "https://arxiv.org/abs/2005.03588", "summary": "Generative feature matching network (GFMN) is an approach for training\nimplicit generative models for images by performing moment matching on features\nfrom pre-trained neural networks. In this paper, we present new GFMN\nformulations that are effective for sequential data. Our experimental results\nshow the effectiveness of the proposed method, SeqGFMN, for three distinct\ngeneration tasks in English: unconditional text generation, class-conditional\ntext generation, and unsupervised text style transfer. SeqGFMN is stable to\ntrain and outperforms various adversarial approaches for text generation and\ntext style transfer."}, {"title": "Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment", "authors": "Yinpei Dai, Hangyu Li, Chengguang Tang, Yongbin Li, Jian Sun, Xiaodan Zhu", "link": "", "summary": ""}, {"title": "Learning Robust Models for e-Commerce Product Search", "authors": "Thanh Nguyen, Nikhil Rao, Karthik Subbian", "link": "https://arxiv.org/abs/2005.03624", "summary": "Showing items that do not match search query intent degrades customer\nexperience in e-commerce. These mismatches result from counterfactual biases of\nthe ranking algorithms toward noisy behavioral signals such as clicks and\npurchases in the search logs. Mitigating the problem requires a large labeled\ndataset, which is expensive and time-consuming to obtain. In this paper, we\ndevelop a deep, end-to-end model that learns to effectively classify mismatches\nand to generate hard mismatched examples to improve the classifier. We train\nthe model end-to-end by introducing a latent variable into the cross-entropy\nloss that alternates between using the real and generated samples. This not\nonly makes the classifier more robust but also boosts the overall ranking\nperformance. Our model achieves a relative gain compared to baselines by over\n26% in F-score, and over 17% in Area Under PR curve. On live search traffic,\nour model gains significant improvement in multiple countries."}, {"title": "Learning Spoken Language Representations with Neural Lattice Language Modeling", "authors": "Chao-Wei Huang, Yun-Nung Chen"}, {"title": "Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge", "authors": "Keqing He, Yuanmeng Yan, Weiran XU"}, {"title": "Learning to Understand Child-directed and Adult-directed Speech", "authors": "Lieke Gelderloos, Grzegorz Chrupa\u0142a, Afra Alishahi", "link": "https://arxiv.org/abs/2005.02721", "summary": "Speech directed to children differs from adult-directed speech in linguistic\naspects such as repetition, word choice, and sentence length, as well as in\naspects of the speech signal itself, such as prosodic and phonemic variation.\nHuman language acquisition research indicates that child-directed speech helps\nlanguage learners. This study explores the effect of child-directed speech when\nlearning to extract semantic information from speech directly. We compare the\ntask performance of models trained on adult-directed speech (ADS) and\nchild-directed speech (CDS). We find indications that CDS helps in the initial\nstages of learning, but eventually, models trained on ADS reach comparable task\nperformance, and generalize better. The results suggest that this is at least\npartially due to linguistic rather than acoustic properties of the two\nregisters, as we see the same pattern when looking at models trained on\nacoustically comparable synthetic speech."}, {"title": "Let Me Choose: From Verbal Context to Font Selection", "authors": "Amirreza Shirani, Franck Dernoncourt, Jose Echevarria, Paul Asente, Nedim Lipka, Thamar Solorio", "link": "https://arxiv.org/abs/2005.01151", "summary": "In this paper, we aim to learn associations between visual attributes of\nfonts and the verbal context of the texts they are typically applied to.\nCompared to related work leveraging the surrounding visual context, we choose\nto focus only on the input text as this can enable new applications for which\nthe text is the only visual element in the document. We introduce a new\ndataset, containing examples of different topics in social media posts and ads,\nlabeled through crowd-sourcing. Due to the subjective nature of the task,\nmultiple fonts might be perceived as acceptable for an input text, which makes\nthis problem challenging. To this end, we investigate different end-to-end\nmodels to learn label distributions on crowd-sourced data and capture\ninter-subjectivity across all annotations."}, {"title": "Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation", "authors": "Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Arivazhagan, Yonghui Wu", "link": "https://arxiv.org/abs/2005.04816", "summary": "Over the last few years two promising research directions in low-resource\nneural machine translation (NMT) have emerged. The first focuses on utilizing\nhigh-resource languages to improve the quality of low-resource languages via\nmultilingual NMT. The second direction employs monolingual data with\nself-supervision to pre-train translation models, followed by fine-tuning on\nsmall amounts of supervised data. In this work, we join these two lines of\nresearch and demonstrate the efficacy of monolingual data with self-supervision\nin multilingual NMT. We offer three major results: (i) Using monolingual data\nsignificantly boosts the translation quality of low-resource languages in\nmultilingual models. (ii) Self-supervision improves zero-shot translation\nquality in multilingual models. (iii) Leveraging monolingual data with\nself-supervision provides a viable path towards adding new languages to\nmultilingual models, getting up to 33 BLEU on ro-en translation without any\nparallel data or back-translation."}, {"title": "Lexically Constrained Neural Machine Translation with Levenshtein Transformer", "authors": "Raymond Hendy Susanto, Shamil Chollampatt, Liling Tan", "link": "https://arxiv.org/abs/2004.12681", "summary": "This paper proposes a simple and effective algorithm for incorporating\nlexical constraints in neural machine translation. Previous work either\nrequired re-training existing models with the lexical constraints or\nincorporating them during beam search decoding with significantly higher\ncomputational overheads. Leveraging the flexibility and speed of a recently\nproposed Levenshtein Transformer model (Gu et al., 2019), our method injects\nterminology constraints at inference time without any impact on decoding speed.\nOur method does not require any modification to the training procedure and can\nbe easily applied at runtime with custom dictionaries. Experiments on\nEnglish-German WMT datasets show that our approach improves an unconstrained\nbaseline and previous approaches."}, {"title": "Lipschitz Constrained Parameter Initialization for Deep Transformers", "authors": "Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Jingyi Zhang", "link": "https://arxiv.org/abs/1911.03179", "summary": "The Transformer translation model employs residual connection and layer\nnormalization to ease the optimization difficulties caused by its multi-layer\nencoder/decoder structure. Previous research shows that even with residual\nconnection and layer normalization, deep Transformers still have difficulty in\ntraining, and particularly Transformer models with more than 12 encoder/decoder\nlayers fail to converge. In this paper, we first empirically demonstrate that a\nsimple modification made in the official implementation, which changes the\ncomputation order of residual connection and layer normalization, can\nsignificantly ease the optimization of deep Transformers. We then compare the\nsubtle differences in computation order in considerable detail, and present a\nparameter initialization method that leverages the Lipschitz constraint on the\ninitialization of Transformer parameters that effectively ensures training\nconvergence. In contrast to findings in previous research we further\ndemonstrate that with Lipschitz parameter initialization, deep Transformers\nwith the original computation order can converge, and obtain significant BLEU\nimprovements with up to 24 layers. In contrast to previous research which\nfocuses on deep encoders, our approach additionally enables Transformers to\nalso benefit from deep decoders."}, {"title": "Logic-Guided Data Augmentation and Regularization for Consistent Question Answering", "authors": "Akari Asai, Hannaneh Hajishirzi", "link": "http://arxiv.org/abs/2004.10157", "summary": "Many natural language questions require qualitative, quantitative or logical\ncomparisons between two entities or events. This paper addresses the problem of\nimproving the accuracy and consistency of responses to comparison questions by\nintegrating logic rules and neural models. Our method leverages logical and\nlinguistic knowledge to augment labeled training data and then uses a\nconsistency-based regularizer to train the model. Improving the global\nconsistency of predictions, our approach achieves large improvements over\nprevious methods in a variety of question answering (QA) tasks including\nmultiple-choice qualitative reasoning, cause-effect reasoning, and extractive\nmachine reading comprehension. In particular, our method significantly improves\nthe performance of RoBERTa-based models by 1-5% across datasets. We advance the\nstate of the art by around 5-8% on WIQA and QuaRel and reduce consistency\nviolations by 58% on HotpotQA. We further demonstrate that our approach can\nlearn effectively from limited data."}, {"title": "Low Resource Sequence Tagging using Sentence Reconstruction", "authors": "Tal Perl, Sriram Chaudhury, Raja Giryes"}, {"title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations", "authors": "Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, Phil Blunsom", "link": "https://arxiv.org/abs/1910.03065", "summary": "To increase trust in artificial intelligence systems, a promising research\ndirection consists of designing neural models capable of generating natural\nlanguage explanations for their predictions. In this work, we show that such\nmodels are nonetheless prone to generating mutually inconsistent explanations,\nsuch as \"Because there is a dog in the image\" and \"Because there is no dog in\nthe [same] image\", exposing flaws in either the decision-making process of the\nmodel or in the generation of the explanations. We introduce a simple yet\neffective adversarial framework for sanity checking models against the\ngeneration of inconsistent natural language explanations. Moreover, as part of\nthe framework, we address the problem of adversarial attacks with full target\nsequences, a scenario that was not previously addressed in sequence-to-sequence\nattacks. Finally, we apply our framework on a state-of-the-art neural natural\nlanguage inference model that provides natural language explanations for its\npredictions. Our framework shows that this model is capable of generating a\nsignificant number of inconsistent explanations."}, {"title": "Masking Actor Information Leads to Fairer Political Claims Detection", "authors": "Erenay Dayanik, Sebastian Pad\u00f3"}, {"title": "Meta-Transfer Learning for Code-Switched Speech Recognition", "authors": "Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Peng Xu, Pascale Fung", "link": "https://arxiv.org/abs/2004.14228", "summary": "An increasing number of people in the world today speak a mixed-language as a\nresult of being multilingual. However, building a speech recognition system for\ncode-switching remains difficult due to the availability of limited resources\nand the expense and significant effort required to collect mixed-language data.\nWe therefore propose a new learning method, meta-transfer learning, to transfer\nlearn on a code-switched speech recognition system in a low-resource setting by\njudiciously extracting information from high-resource monolingual datasets. Our\nmodel learns to recognize individual languages, and transfer them so as to\nbetter recognize mixed-language speech by conditioning the optimization on the\ncode-switching data. Based on experimental results, our model outperforms\nexisting baselines on speech recognition and language modeling tasks, and is\nfaster to converge."}, {"title": "Mitigating Gender Bias Amplification in Distribution by Posterior Regularization", "authors": "Shengyu Jia, Tao Meng, Jieyu Zhao, Kai-Wei Chang", "link": "https://arxiv.org/abs/2005.06251", "summary": "Advanced machine learning techniques have boosted the performance of natural\nlanguage processing. Nevertheless, recent studies, e.g., Zhao et al. (2017)\nshow that these techniques inadvertently capture the societal bias hidden in\nthe corpus and further amplify it. However, their analysis is conducted only on\nmodels' top predictions. In this paper, we investigate the gender bias\namplification issue from the distribution perspective and demonstrate that the\nbias is amplified in the view of predicted probability distribution over\nlabels. We further propose a bias mitigation approach based on posterior\nregularization. With little performance loss, our method can almost remove the\nbias amplification in the distribution. Our study sheds the light on\nunderstanding the bias amplification."}, {"title": "Modeling Label Semantics for Predicting Emotional Reactions", "authors": "Radhika Gaonkar, Heeyoung Kwon, Mohaddeseh Bastan, Niranjan Balasubramanian, Nathanael Chambers"}, {"title": "Modeling Long Context for Task-Oriented Dialogue State Generation", "authors": "Jun Quan, Deyi Xiong", "link": "https://arxiv.org/abs/2004.14080", "summary": "Based on the recently proposed transferable dialogue state generator (TRADE)\nthat predicts dialogue states from utterance-concatenated dialogue context, we\npropose a multi-task learning model with a simple yet effective utterance\ntagging technique and a bidirectional language model as an auxiliary task for\ntask-oriented dialogue state generation. By enabling the model to learn a\nbetter representation of the long dialogue context, our approaches attempt to\nsolve the problem that the performance of the baseline significantly drops when\nthe input dialogue context sequence is long. In our experiments, our proposed\nmodel achieves a 7.03% relative improvement over the baseline, establishing a\nnew state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset."}, {"title": "Modeling Word Formation in English\u2013German Neural Machine Translation", "authors": "Marion Weller-Di Marco, Alexander Fraser"}, {"title": "MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs", "authors": "Jifan Yu, Gan Luo, Tong Xiao, Qingyang Zhong, Yuquan Wang, Wenzheng Feng, Junyi Luo, Chenyu Wang, Lei Hou, Juanzi Li, Zhiyuan Liu, Jie Tang"}, {"title": "Multimodal and Multiresolution Speech Recognition with Transformers", "authors": "Georgios Paraskevopoulos, Srinivas Parthasarathy, Aparna Khare, Shiva Sundaram", "link": "", "summary": ""}, {"title": "Multimodal Quality Estimation for Machine Translation", "authors": "Shu Okabe, Fr\u00e9d\u00e9ric Blain, Lucia Specia"}, {"title": "Multimodal Transformer for Multimodal Machine Translation", "authors": "Shaowei Yao, Xiaojun Wan"}, {"title": "Named Entity Recognition as Dependency Parsing", "authors": "Juntao Yu, Bernd Bohnet, Massimo Poesio", "link": "https://arxiv.org/abs/2005.07150", "summary": "Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing, concerned with identifying spans of text expressing references to\nentities. NER research is often focused on flat entities only (flat NER),\nignoring the fact that entity references can be nested, as in [Bank of [China]]\n(Finkel and Manning, 2009). In this paper, we use ideas from graph-based\ndependency parsing to provide our model a global view on the input via a\nbiaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of\nstart and end tokens in a sentence which we use to explore all spans, so that\nthe model is able to predict named entities accurately. We show that the model\nworks well for both nested and flat NER through evaluation on 8 corpora and\nachieving SoTA performance on all of them, with accuracy gains of up to 2.2\npercentage points."}, {"title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly", "authors": "Nora Kassner, Hinrich Sch\u00fctze", "link": "https://arxiv.org/abs/1911.03343", "summary": "Building on Petroni et al. (2019), we propose two new probing tasks analyzing\nfactual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We\nfind that PLMs do not distinguish between negated (\"Birds cannot [MASK]\") and\nnon-negated (\"Birds can [MASK]\") cloze questions. (2) Mispriming. Inspired by\npriming methods in human psychology, we add \"misprimes\" to cloze questions\n(\"Talk? Birds can [MASK]\"). We find that PLMs are easily distracted by\nmisprimes. These results suggest that PLMs still have a long way to go to\nadequately learn human-like factual knowledge."}, {"title": "Neural Graph Matching Networks for Chinese Short Text Matching", "authors": "Lu Chen, Yanbin Zhao, Boer Lv, Lesheng Jin, Zhi Chen, Su Zhu, Kai Yu", "link": "", "summary": ""}, {"title": "Neural Temporal Opinion Modelling for Opinion Prediction on Twitter", "authors": "Lixing Zhu, Yulan He, Deyu Zhou", "link": "https://arxiv.org/abs/2005.13486", "summary": "Opinion prediction on Twitter is challenging due to the transient nature of\ntweet content and neighbourhood context. In this paper, we model users' tweet\nposting behaviour as a temporal point process to jointly predict the posting\ntime and the stance label of the next tweet given a user's historical tweet\nsequence and tweets posted by their neighbours. We design a topic-driven\nattention mechanism to capture the dynamic topic shifts in the neighbourhood\ncontext. Experimental results show that the proposed model predicts both the\nposting time and the stance labels of future tweets more accurately compared to\na number of competitive baselines."}, {"title": "Neural-DINF: A Neural Network based Framework for Measuring Document In\ufb02uence", "authors": "Jie Tan, Changlin Yang, Ying Li, Siliang Tang, Chen Huang, Yueting Zhuang"}, {"title": "Non-Linear Instance-Based Cross-Lingual Mapping for Non-Isomorphic Embedding Spaces", "authors": "Goran Glava\u0161, Ivan Vuli\u0107"}, {"title": "\u201cNone of the Above\u201d: Measure Uncertainty in Dialog Response Retrieval", "authors": "Yulan Feng, Shikib Mehri, Maxine Eskenazi, Tiancheng Zhao", "link": "https://arxiv.org/abs/2004.01926", "summary": "This paper discusses the importance of uncovering uncertainty in end-to-end\ndialog tasks, and presents our experimental results on uncertainty\nclassification on the Ubuntu Dialog Corpus. We show that, instead of retraining\nmodels for this specific purpose, the original retrieval model's underlying\nconfidence concerning the best prediction can be captured with trivial\nadditional computation."}, {"title": "On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation", "authors": "Chaojun Wang, Rico Sennrich", "link": "https://arxiv.org/abs/2005.03642", "summary": "The standard training algorithm in neural machine translation (NMT) suffers\nfrom exposure bias, and alternative algorithms have been proposed to mitigate\nthis. However, the practical impact of exposure bias is under debate. In this\npaper, we link exposure bias to another well-known problem in NMT, namely the\ntendency to generate hallucinations under domain shift. In experiments on three\ndatasets with multiple test domains, we show that exposure bias is partially to\nblame for hallucinations, and that training with Minimum Risk Training, which\navoids exposure bias, can mitigate this. Our analysis explains why exposure\nbias is more problematic under domain shift, and also links exposure bias to\nthe beam search problem, i.e. performance deterioration with increasing beam\nsize. Our results provide a new justification for methods that reduce exposure\nbias: even if they do not increase performance on in-domain test sets, they can\nincrease model robustness to domain shift."}, {"title": "On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology", "authors": "Marcel Bollmann, Desmond Elliott"}, {"title": "On Importance Sampling-Based Evaluation of Latent Language Models", "authors": "Robert L Logan IV, Matt Gardner, Sameer Singh"}, {"title": "On the Importance of Diversity in Question Generation for QA", "authors": "Md Arafat Sultan, Shubham Chandel, Ram\u00f3n Fernandez Astudillo, Vittorio Castelli"}, {"title": "On the Spontaneous Emergence of Discrete and Compositional Signals", "authors": "Nur Geffen Lan, Emmanuel Chemla, Shane Steinert-Threlkeld", "link": "https://arxiv.org/abs/2005.00110", "summary": "We propose a general framework to study language emergence through signaling\ngames with neural agents. Using a continuous latent space, we are able to (i)\ntrain using backpropagation, (ii) show that discrete messages nonetheless\nnaturally emerge. We explore whether categorical perception effects follow and\nshow that the messages are not compositional."}, {"title": "OpinionDigest: A Simple Framework for Opinion Summarization", "authors": "Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis, Wang-Chiew Tan", "link": "https://arxiv.org/abs/2005.01901", "summary": "We present OpinionDigest, an abstractive opinion summarization framework,\nwhich does not rely on gold-standard summaries for training. The framework uses\nan Aspect-based Sentiment Analysis model to extract opinion phrases from\nreviews, and trains a Transformer model to reconstruct the original reviews\nfrom these extractions. At summarization time, we merge extractions from\nmultiple reviews and select the most popular ones. The selected opinions are\nused as input to the trained Transformer model, which verbalizes them into an\nopinion summary. OpinionDigest can also generate customized summaries, tailored\nto specific user needs, by filtering the selected opinions according to their\naspect and/or sentiment. Automatic evaluation on Yelp data shows that our\nframework outperforms competitive baselines. Human studies on two corpora\nverify that OpinionDigest produces informative summaries and shows promising\ncustomization capabilities."}, {"title": "Opportunistic Decoding with Timely Correction for Simultaneous Translation", "authors": "Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, Liang Huang", "link": "https://arxiv.org/abs/2005.00675", "summary": "Simultaneous translation has many important application scenarios and\nattracts much attention from both academia and industry recently. Most existing\nframeworks, however, have difficulties in balancing between the translation\nquality and latency, i.e., the decoding policy is usually either too aggressive\nor too conservative. We propose an opportunistic decoding technique with timely\ncorrection ability, which always (over-)generates a certain mount of extra\nwords at each step to keep the audience on track with the latest information.\nAt the same time, it also corrects, in a timely fashion, the mistakes in the\nformer overgenerated words when observing more source context to ensure high\ntranslation quality. Experiments show our technique achieves substantial\nreduction in latency and up to +3.1 increase in BLEU, with revision rate under\n8% in Chinese-to-English and English-to-Chinese translation."}, {"title": "Overestimation of Syntactic Representation in Neural Language Models", "authors": "Jordan Kodner, Nitish Gupta", "link": "https://arxiv.org/abs/2004.05067", "summary": "With the advent of powerful neural language models over the last few years,\nresearch attention has increasingly focused on what aspects of language they\nrepresent that make them so successful. Several testing methodologies have been\ndeveloped to probe models' syntactic representations. One popular method for\ndetermining a model's ability to induce syntactic structure trains a model on\nstrings generated according to a template then tests the model's ability to\ndistinguish such strings from superficially similar ones with different syntax.\nWe illustrate a fundamental problem with this approach by reproducing positive\nresults from a recent paper with two non-syntactic baseline language models: an\nn-gram model and an LSTM model trained on scrambled inputs."}, {"title": "Parallel Data Augmentation for Formality Style Transfer", "authors": "Yi Zhang, Tao Ge, Xu SUN", "link": "http://arxiv.org/abs/2005.07522", "summary": "The main barrier to progress in the task of Formality Style Transfer is the\ninadequacy of training data. In this paper, we study how to augment parallel\ndata and propose novel and simple data augmentation methods for this task to\nobtain useful sentence pairs with easily accessible models and systems.\nExperiments demonstrate that our augmented parallel data largely helps improve\nformality style transfer when it is used to pre-train the model, leading to the\nstate-of-the-art results in the GYAFC benchmark dataset."}, {"title": "Parallel Sentence Mining by Constrained Decoding", "authors": "Pinzhen Chen, Nikolay Bogoychev, Kenneth Heafield, Faheem Kirefu"}, {"title": "Posterior Calibrated Training on Sentence Classification Tasks", "authors": "Taehee Jung, Dongyeop Kang, Hua Cheng, Lucas Mentch, Thomas Schaaf", "link": "https://arxiv.org/abs/2004.14500", "summary": "Most classification models work by first predicting a posterior probability\ndistribution over all classes and then selecting that class with the largest\nestimated probability. In many settings however, the quality of posterior\nprobability itself (e.g., 65% chance having diabetes), gives more reliable\ninformation than the final predicted class alone. When these methods are shown\nto be poorly calibrated, most fixes to date have relied on posterior\ncalibration, which rescales the predicted probabilities but often has little\nimpact on final classifications. Here we propose an end-to-end training\nprocedure called posterior calibrated (PosCal) training that directly optimizes\nthe objective while minimizing the difference between the predicted and\nempirical posterior probabilities.We show that PosCal not only helps reduce the\ncalibration error but also improve task performance by penalizing drops in\nperformance of both objectives. Our PosCal achieves about 2.5% of task\nperformance gain and 16.1% of calibration error reduction on GLUE (Wang et al.,\n2018) compared to the baseline. We achieved the comparable task performance\nwith 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not\noutperforming the two-stage calibration baseline. PosCal training can be easily\nextendable to any types of classification tasks as a form of regularization\nterm. Also, PosCal has the advantage that it incrementally tracks needed\nstatistics for the calibration objective during the training process, making\nefficient use of large training sets."}, {"title": "Predicting Degrees of Technicality in Automatic Terminology Extraction", "authors": "Anna H\u00e4tty, Dominik Schlechtweg, Michael Dorna, Sabine Schulte im Walde"}, {"title": "Pretrained Transformers Improve Out-of-Distribution Robustness", "authors": "Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song", "link": "https://arxiv.org/abs/2004.06100", "summary": "Although pretrained Transformers such as BERT achieve high accuracy on\nin-distribution examples, do they generalize to new distributions? We\nsystematically measure out-of-distribution (OOD) generalization for seven NLP\ndatasets by constructing a new robustness benchmark with realistic distribution\nshifts. We measure the generalization of previous models including bag-of-words\nmodels, ConvNets, and LSTMs, and we show that pretrained Transformers'\nperformance declines are substantially smaller. Pretrained transformers are\nalso more effective at detecting anomalous or OOD examples, while many previous\nmodels are frequently worse than chance. We examine which factors affect\nrobustness, finding that larger models are not necessarily more robust,\ndistillation can be harmful, and more diverse pretraining data can enhance\nrobustness. Finally, we show where future work can improve OOD robustness."}, {"title": "Quantifying Attention Flow in Transformers", "authors": "Samira Abnar, Willem Zuidema", "link": "https://arxiv.org/abs/2005.00928", "summary": "In the Transformer model, \"self-attention\" combines information from attended\nembeddings into the representation of the focal embedding in the next layer.\nThus, across layers of the Transformer, information originating from different\ntokens gets increasingly mixed. This makes attention weights unreliable as\nexplanations probes. In this paper, we consider the problem of quantifying this\nflow of information through self-attention. We propose two methods for\napproximating the attention to input tokens given attention weights, attention\nrollout and attention flow, as post hoc methods when we use attention weights\nas the relative relevance of the input tokens. We show that these methods give\ncomplementary views on the flow of information, and compared to raw attention,\nboth yield higher correlations with importance scores of input tokens obtained\nusing an ablation method and input gradients."}, {"title": "Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases", "authors": "Yunshi Lan, Jing Jiang"}, {"title": "R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason", "authors": "Naoya Inoue, Pontus Stenetorp, Kentaro Inui", "link": "https://arxiv.org/abs/1910.04601", "summary": "Recent studies have revealed that reading comprehension (RC) systems learn to\nexploit annotation artifacts and other biases in current datasets. This\nprevents the community from reliably measuring the progress of RC systems. To\naddress this issue, we introduce R4C, a new task for evaluating RC systems'\ninternal reasoning. R4C requires giving not only answers but also derivations:\nexplanations that justify predicted answers. We present a reliable,\ncrowdsourced framework for scalably annotating RC datasets with derivations. We\ncreate and publicly release the R4C dataset, the first, quality-assured dataset\nconsisting of 4.6k questions, each of which is annotated with 3 reference\nderivations (i.e. 13.8k derivations). Experiments show that our automatic\nevaluation metrics using multiple reference derivations are reliable, and that\nR4C assesses different skills from an existing benchmark."}, {"title": "Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models", "authors": "Maarten Sap, Eric Horvitz, Yejin Choi, Noah A. Smith, James Pennebaker"}, {"title": "Recursive Template-based Frame Generation for Task Oriented Dialog", "authors": "Rashmi Gangadharaiah, Balakrishnan Narayanaswamy"}, {"title": "Regularized Context Gates on Transformer for Machine Translation", "authors": "Xintong Li, Lemao Liu, Rui Wang, Guoping Huang, Max Meng", "link": "https://arxiv.org/abs/1908.11020", "summary": "Context gates are effective to control the contributions from the source and\ntarget contexts in the recurrent neural network (RNN) based neural machine\ntranslation (NMT). However, it is challenging to extend them into the advanced\nTransformer architecture, which is more complicated than RNN. This paper first\nprovides a method to identify source and target contexts and then introduce a\ngate mechanism to control the source and target contributions in Transformer.\nIn addition, to further reduce the bias problem in the gate mechanism, this\npaper proposes a regularization method to guide the learning of the gates with\nsupervision automatically generated using pointwise mutual information.\nExtensive experiments on 4 translation datasets demonstrate that the proposed\nmodel obtains an averaged gain of 1.0 BLEU score over a strong Transformer\nbaseline."}, {"title": "Relation Extraction with Explanation", "authors": "Hamed Shahbazi, Xiaoli Fern, Reza Ghaeini, Prasad Tadepalli"}, {"title": "Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs", "authors": "Michael Lepori, Tal Linzen, R. Thomas McCoy", "link": "https://arxiv.org/abs/2005.00019", "summary": "Sequence-based neural networks show significant sensitivity to syntactic\nstructure, but they still perform less well on syntactic tasks than tree-based\nnetworks. Such tree-based networks can be provided with a constituency parse, a\ndependency parse, or both. We evaluate which of these two representational\nschemes more effectively introduces biases for syntactic structure that\nincrease performance on the subject-verb agreement prediction task. We find\nthat a constituency-based network generalizes more robustly than a\ndependency-based one, and that combining the two types of structure does not\nyield further improvement. Finally, we show that the syntactic robustness of\nsequential models can be substantially improved by fine-tuning on a small\namount of constructed data, suggesting that data augmentation is a viable\nalternative to explicit constituency structure for imparting the syntactic\nbiases that sequential models are lacking."}, {"title": "Returning the N to NLP: Towards Contextually Personalized Classification Models", "authors": "Lucie Flek"}, {"title": "Reverse Engineering Configurations of Neural Text Generation Models", "authors": "Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, Andrew Tomkins", "link": "https://arxiv.org/abs/2004.06201", "summary": "This paper seeks to develop a deeper understanding of the fundamental\nproperties of neural text generations models. The study of artifacts that\nemerge in machine generated text as a result of modeling choices is a nascent\nresearch area. Previously, the extent and degree to which these artifacts\nsurface in generated text has not been well studied. In the spirit of better\nunderstanding generative text models and their artifacts, we propose the new\ntask of distinguishing which of several variants of a given model generated a\npiece of text, and we conduct an extensive suite of diagnostic tests to observe\nwhether modeling choices (e.g., sampling methods, top-$k$ probabilities, model\narchitectures, etc.) leave detectable artifacts in the text they generate. Our\nkey finding, which is backed by a rigorous set of experiments, is that such\nartifacts are present and that different modeling choices can be inferred by\nobserving the generated text alone. This suggests that neural text generators\nmay be more sensitive to various modeling choices than previously thought."}, {"title": "Revisiting Higher-Order Dependency Parsers", "authors": "Erick Fonseca, Andr\u00e9 F. T. Martins"}, {"title": "Revisiting Unsupervised Relation Extraction", "authors": "Thy Thy Tran, Phong Le, Sophia Ananiadou", "link": "https://arxiv.org/abs/2005.00087", "summary": "Unsupervised relation extraction (URE) extracts relations between named\nentities from raw text without manually-labelled data and existing knowledge\nbases (KBs). URE methods can be categorised into generative and discriminative\napproaches, which rely either on hand-crafted features or surface form.\nHowever, we demonstrate that by using only named entities to induce relation\ntypes, we can outperform existing methods on two popular datasets. We conduct a\ncomparison and evaluation of our findings with other URE techniques, to\nascertain the important features in URE. We conclude that entity types provide\na strong inductive bias for URE."}, {"title": "SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions", "authors": "Mao Ye, Chengyue Gong, Qiang Liu", "link": "https://arxiv.org/abs/2005.14424", "summary": "State-of-the-art NLP models can often be fooled by human-unaware\ntransformations such as synonymous word substitution. For security reasons, it\nis of critical importance to develop models with certified robustness that can\nprovably guarantee that the prediction is can not be altered by any possible\nsynonymous word substitution. In this work, we propose a certified robust\nmethod based on a new randomized smoothing technique, which constructs a\nstochastic ensemble by applying random word substitutions on the input\nsentences, and leverage the statistical properties of the ensemble to provably\ncertify the robustness. Our method is simple and structure-free in that it only\nrequires the black-box queries of the model outputs, and hence can be applied\nto any pre-trained models (such as BERT) and any types of models (world-level\nor subword-level). Our method significantly outperforms recent state-of-the-art\nmethods for certified robustness on both IMDB and Amazon text classification\ntasks. To the best of our knowledge, we are the first work to achieve certified\nrobustness on large systems such as BERT with practically meaningful certified\naccuracy."}, {"title": "Self-Attention Guided Copy Mechanism for Abstractive Summarization", "authors": "Song Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xiaodong He, Bowen Zhou"}, {"title": "Self-Attention with Cross-Lingual Position Representation", "authors": "Liang Ding, Longyue Wang, Dacheng Tao", "link": "https://arxiv.org/abs/2004.13310", "summary": "Position encoding (PE), an essential part of self-attention networks (SANs),\nis used to preserve the word order information for natural language processing\ntasks, generating fixed position indices for input sequences. However, in\ncross-lingual scenarios, e.g. machine translation, the PEs of source and target\nsentences are modeled independently. Due to word order divergences in different\nlanguages, modeling the cross-lingual positional relationships might help SANs\ntackle this problem. In this paper, we augment SANs with \\emph{cross-lingual\nposition representations} to model the bilingually aware latent structure for\nthe input sentence. Specifically, we utilize bracketing transduction grammar\n(BTG)-based reordering information to encourage SANs to learn bilingual\ndiagonal alignments. Experimental results on WMT'14 English$\\Rightarrow$German,\nWAT'17 Japanese$\\Rightarrow$English, and WMT'17 Chinese$\\Leftrightarrow$English\ntranslation tasks demonstrate that our approach significantly and consistently\nimproves translation quality over strong baselines. Extensive analyses confirm\nthat the performance gains come from the cross-lingual information."}, {"title": "Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity", "authors": "Nina Poerner, Ulli Waltinger, Hinrich Sch\u00fctze", "link": "https://arxiv.org/abs/1911.03700", "summary": "We address the task of unsupervised Semantic Textual Similarity (STS) by\nensembling diverse pre-trained sentence encoders into sentence meta-embeddings.\nWe apply, extend and evaluate different meta-embedding methods from the word\nembedding literature at the sentence level, including dimensionality reduction\n(Yin and Sch\\\"utze, 2016), generalized Canonical Correlation Analysis (Rastogi\net al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our\nsentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the\nSTS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and\n6.4% Pearson's r over single-source systems."}, {"title": "Shape of synth to come: Why we should use synthetic data for English surface realization", "authors": "Henry Elder, Robert Burke, Alexander O\u2019Connor, Jennifer Foster", "link": "http://arxiv.org/abs/2005.02693", "summary": "The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language\nGeneration shared tasks with the goal of exploring approaches to surface\nrealization from Universal-Dependency-like trees to surface strings for several\nlanguages. In the 2018 shared task there was very little difference in the\nabsolute performance of systems trained with and without additional,\nsynthetically created data, and a new rule prohibiting the use of synthetic\ndata was introduced for the 2019 shared task. Contrary to the findings of the\n2018 shared task, we show, in experiments on the English 2018 dataset, that the\nuse of synthetic data can have a substantial positive effect - an improvement\nof almost 8 BLEU points for a previously state-of-the-art system. We analyse\nthe effects of synthetic data, and we argue that its use should be encouraged\nrather than prohibited so that future research efforts continue to explore\nsystems that can take advantage of such data."}, {"title": "Shaping Visual Representations with Language for Few-Shot Classification", "authors": "Jesse Mu, Percy Liang, Noah Goodman", "link": "https://arxiv.org/abs/1911.02683", "summary": "Language is designed to convey useful information about the world, thus\nserving as a scaffold for efficient human learning. How can we let language\nguide representation learning in machine learning models? We explore this\nquestion in the setting of few-shot visual classification, proposing models\nwhich learn to perform visual classification while jointly predicting natural\nlanguage task descriptions at train time. At test time, with no language\navailable, we find that these language-influenced visual representations are\nmore generalizable, compared to meta-learning baselines and approaches that\nexplicitly use language as a bottleneck for classification."}, {"title": "Showing Your Work Doesn\u2019t Always Work", "authors": "Raphael Tang, Jaejun Lee, Ji Xin, Xinyu Liu, Yaoliang Yu, Jimmy Lin", "link": "https://arxiv.org/abs/2004.13705", "summary": "In natural language processing, a recently popular line of work explores how\nto best report the experimental results of neural networks. One exemplar\npublication, titled \"Show Your Work: Improved Reporting of Experimental\nResults,\" advocates for reporting the expected validation effectiveness of the\nbest-tuned model, with respect to the computational budget. In the present\nwork, we critically examine this paper. As far as statistical generalizability\nis concerned, we find unspoken pitfalls and caveats with this approach. We\nanalytically show that their estimator is biased and uses error-prone\nassumptions. We find that the estimator favors negative errors and yields poor\nbootstrapped confidence intervals. We derive an unbiased alternative and\nbolster our claims with empirical evidence from statistical simulation. Our\ncodebase is at http://github.com/castorini/meanmax."}, {"title": "Simple and Effective Retrieve-Edit-Rerank Text Generation", "authors": "Nabil Hossain, Marjan Ghazvininejad, Luke Zettlemoyer"}, {"title": "Simultaneous Translation Policies: From Fixed to Adaptive", "authors": "Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma, Hairong Liu, Liang Huang", "link": "http://arxiv.org/abs/2004.13169", "summary": "Adaptive policies are better than fixed policies for simultaneous\ntranslation, since they can flexibly balance the tradeoff between translation\nquality and latency based on the current context information. But previous\nmethods on obtaining adaptive policies either rely on complicated training\nprocess, or underperform simple fixed policies. We design an algorithm to\nachieve adaptive policies via a simple heuristic composition of a set of fixed\npolicies. Experiments on Chinese -> English and German -> English show that our\nadaptive policies can outperform fixed ones by up to 4 BLEU points for the same\nlatency, and more surprisingly, it even surpasses the BLEU score of\nfull-sentence translation in the greedy mode (and very close to beam mode), but\nwith much lower latency."}, {"title": "Single Model Ensemble using Pseudo-Tags and Distinct Vectors", "authors": "Ryosuke Kuwabara, Jun Suzuki, Hideki Nakayama", "link": "https://arxiv.org/abs/2005.00879", "summary": "Model ensemble techniques often increase task performance in neural networks;\nhowever, they require increased time, memory, and management effort. In this\nstudy, we propose a novel method that replicates the effects of a model\nensemble with a single model. Our approach creates K-virtual models within a\nsingle parameter space using K-distinct pseudo-tags and K-distinct vectors.\nExperiments on text classification and sequence labeling tasks on several\ndatasets demonstrate that our method emulates or outperforms a traditional\nmodel ensemble with 1/K-times fewer parameters."}, {"title": "Smart To-Do: Automatic Generation of To-Do Items from Emails", "authors": "Sudipto Mukherjee, Subhabrata Mukherjee, Marcello Hasegawa, Ahmed Hassan Awadallah, Ryen White"}, {"title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "authors": "Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, Stephen Denuyl", "link": "http://arxiv.org/abs/2005.00813", "summary": "Building equitable and inclusive NLP technologies demands consideration of\nwhether and how social attitudes are represented in ML models. In particular,\nrepresentations encoded in models often inadvertently perpetuate undesirable\nsocial biases from the data on which they are trained. In this paper, we\npresent evidence of such undesirable biases towards mentions of disability in\ntwo different English language models: toxicity prediction and sentiment\nanalysis. Next, we demonstrate that the neural embeddings that are the critical\nfirst step in most NLP pipelines similarly contain undesirable biases towards\nmentions of disability. We end by highlighting topical biases in the discourse\nabout disability which may contribute to the observed model biases; for\ninstance, gun violence, homelessness, and drug addiction are over-represented\nin texts discussing mental illness."}, {"title": "Soft Gazetteers for Low-Resource Named Entity Recognition", "authors": "Shruti Rijhwani, Shuyan Zhou, Graham Neubig, Jaime Carbonell", "link": "https://arxiv.org/abs/2005.01866", "summary": "Traditional named entity recognition models use gazetteers (lists of\nentities) as features to improve performance. Although modern neural network\nmodels do not require such hand-crafted features for strong performance, recent\nwork has demonstrated their utility for named entity recognition on English\ndata. However, designing such features for low-resource languages is\nchallenging, because exhaustive entity gazetteers do not exist in these\nlanguages. To address this problem, we propose a method of \"soft gazetteers\"\nthat incorporates ubiquitously available information from English knowledge\nbases, such as Wikipedia, into neural named entity recognition models through\ncross-lingual entity linking. Our experiments on four low-resource languages\nshow an average improvement of 4 points in F1 score. Code and data are\navailable at https://github.com/neulab/soft-gazetteers."}, {"title": "Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations", "authors": "Samuel Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli\u0107, Matthew Henderson", "link": "https://arxiv.org/abs/2005.08866", "summary": "We introduce Span-ConveRT, a light-weight model for dialog slot-filling which\nframes the task as a turn-based span extraction task. This formulation allows\nfor a simple integration of conversational knowledge coded in large pretrained\nconversational models such as ConveRT (Henderson et al., 2019). We show that\nleveraging such knowledge in Span-ConveRT is especially useful for few-shot\nlearning scenarios: we report consistent gains over 1) a span extractor that\ntrains representations from scratch in the target domain, and 2) a BERT-based\nspan extractor. In order to inspire more work on span extraction for the\nslot-filling task, we also release RESTAURANTS-8K, a new challenging data set\nof 8,198 utterances, compiled from actual conversations in the restaurant\nbooking domain."}, {"title": "Stolen Probability: A Structural Weakness of Neural Language Models", "authors": "David Demeter, Gregory Kimmel, Doug Downey", "link": "http://arxiv.org/abs/2005.02433", "summary": "Neural Network Language Models (NNLMs) generate probability distributions by\napplying a softmax function to a distance metric formed by taking the dot\nproduct of a prediction vector with all word vectors in a high-dimensional\nembedding space. The dot-product distance metric forms part of the inductive\nbias of NNLMs. Although NNLMs optimize well with this inductive bias, we show\nthat this results in a sub-optimal ordering of the embedding space that\nstructurally impoverishes some words at the expense of others when assigning\nprobability. We present numerical, theoretical and empirical analyses showing\nthat words on the interior of the convex hull in the embedding space have their\nprobability bounded by the probabilities of the words on the hull."}, {"title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture", "authors": "Christopher Brix, Parnia Bahar, Hermann Ney", "link": "https://arxiv.org/abs/2005.03454", "summary": "Sparse models require less memory for storage and enable a faster inference\nby reducing the necessary number of FLOPs. This is relevant both for\ntime-critical and on-device computations using neural networks. The stabilized\nlottery ticket hypothesis states that networks can be pruned after none or few\ntraining iterations, using a mask computed based on the unpruned converged\nmodel. On the transformer architecture and the WMT 2014 English-to-German and\nEnglish-to-French tasks, we show that stabilized lottery ticket pruning\nperforms similar to magnitude pruning for sparsity levels of up to 85%, and\npropose a new combination of pruning techniques that outperforms all other\ntechniques for even higher levels of sparsity. Furthermore, we confirm that the\nparameter's initial sign and not its specific value is the primary factor for\nsuccessful training, and show that magnitude pruning cannot be used to find\nwinning lottery tickets."}, {"title": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization", "authors": "Yang Gao, Wei Zhao, Steffen Eger", "link": "https://arxiv.org/abs/2005.03724", "summary": "We study unsupervised multi-document summarization evaluation metrics, which\nrequire neither human-written reference summaries nor human annotations (e.g.\npreferences, ratings, etc.). We propose SUPERT, which rates the quality of a\nsummary by measuring its semantic similarity with a pseudo reference summary,\ni.e. selected salient sentences from the source documents, using contextualized\nembeddings and soft token alignment techniques. Compared to the\nstate-of-the-art unsupervised evaluation metrics, SUPERT correlates better with\nhuman ratings by 18-39%. Furthermore, we use SUPERT as rewards to guide a\nneural-based reinforcement learning summarizer, yielding favorable performance\ncompared to the state-of-the-art unsupervised summarizers. All source code is\navailable at https://github.com/yg211/acl20-ref-free-eval."}, {"title": "Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi", "authors": "Aryaman Arora, Luke Gessler, Nathan Schneider", "link": "https://arxiv.org/abs/2004.10353", "summary": "Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one\nexception: whether a schwa represented in the orthography is pronounced or\nunpronounced (deleted). Previous work has attempted to predict schwa deletion\nin a rule-based fashion using prosodic or phonetic analysis. We present the\nfirst statistical schwa deletion classifier for Hindi, which relies solely on\nthe orthography as the input and outperforms previous approaches. We trained\nour model on a newly-compiled pronunciation lexicon extracted from various\nonline dictionaries. Our best Hindi model achieves state of the art\nperformance, and also achieves good performance on a closely related language,\nPunjabi, without modification."}, {"title": "Syntactic Data Augmentation Increases Robustness to Inference Heuristics", "authors": "Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, Tal Linzen", "link": "https://arxiv.org/abs/2004.11999", "summary": "Pretrained neural models such as BERT, when fine-tuned to perform natural\nlanguage inference (NLI), often show high accuracy on standard datasets, but\ndisplay a surprising lack of sensitivity to word order on controlled challenge\nsets. We hypothesize that this issue is not primarily caused by the pretrained\nmodel's limitations, but rather by the paucity of crowdsourced NLI examples\nthat might convey the importance of syntactic structure at the fine-tuning\nstage. We explore several methods to augment standard training sets with\nsyntactically informative examples, generated by applying syntactic\ntransformations to sentences from the MNLI corpus. The best-performing\naugmentation method, subject/object inversion, improved BERT's accuracy on\ncontrolled examples that diagnose sensitivity to word order from 0.28 to 0.73,\nwithout affecting performance on the MNLI test set. This improvement\ngeneralized beyond the particular construction used for data augmentation,\nsuggesting that augmentation causes BERT to recruit abstract syntactic\nrepresentations."}, {"title": "Tagged Back-translation Revisited: Why Does It Really Work?", "authors": "Benjamin Marie, Raphael Rubino, Atsushi Fujita"}, {"title": "tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection", "authors": "Nicole Peinelt, Dong Nguyen, Maria Liakata"}, {"title": "Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering", "authors": "Alexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing Xiang", "link": "https://arxiv.org/abs/2004.11892", "summary": "Question Answering (QA) is in increasing demand as the amount of information\navailable online and the desire for quick access to this content grows. A\ncommon approach to QA has been to fine-tune a pretrained language model on a\ntask-specific labeled dataset. This paradigm, however, relies on scarce, and\ncostly to obtain, large-scale human-labeled data. We propose an unsupervised\napproach to training QA models with generated pseudo-training data. We show\nthat generating questions for QA training by applying a simple template on a\nrelated, retrieved sentence rather than the original context sentence improves\ndownstream QA performance by allowing the model to learn more complex\ncontext-question relationships. Training a QA model on this data gives a\nrelative improvement over a previous unsupervised model in F1 score on the\nSQuAD dataset by about 14%, and 20% when the answer is a named entity,\nachieving state-of-the-art performance on SQuAD for unsupervised QA."}, {"title": "Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference", "authors": "Nikita Kitaev, Dan Klein", "link": "https://arxiv.org/abs/1904.09745", "summary": "We present a constituency parsing algorithm that maps from word-aligned\ncontextualized feature vectors to parse trees. Our algorithm proceeds strictly\nleft-to-right, processing one word at a time by assigning it a label from a\nsmall vocabulary. We show that, with mild assumptions, our inference procedure\nrequires constant computation time per word. Our method gets 95.4 F1 on the WSJ\ntest set."}, {"title": "Text Classification with Negative Supervision", "authors": "Sora Ohashi, Junya Takayama, Tomoyuki Kajiwara, Chenhui Chu, Yuki Arase"}, {"title": "To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks", "authors": "Sinong Wang, Madian Khabsa, Hao Ma"}, {"title": "Topological Sort for Sentence Ordering", "authors": "Shrimai Prabhumoye, Ruslan Salakhutdinov, Alan W Black", "link": "https://arxiv.org/abs/2005.00432", "summary": "Sentence ordering is the task of arranging the sentences of a given text in\nthe correct order. Recent work using deep neural networks for this task has\nframed it as a sequence prediction problem. In this paper, we propose a new\nframing of this task as a constraint solving problem and introduce a new\ntechnique to solve it. Additionally, we propose a human evaluation for this\ntask. The results on both automatic and human metrics across four different\ndatasets show that this new technique is better at capturing coherence in\ndocuments."}, {"title": "Toward Better Storylines with Sentence-Level Language Models", "authors": "Daphne Ippolito, David Grangier, Douglas Eck, Chris Callison-Burch", "link": "https://arxiv.org/abs/2005.05255", "summary": "We propose a sentence-level language model which selects the next sentence in\na story from a finite set of fluent alternatives. Since it does not need to\nmodel fluency, the sentence-level language model can focus on longer range\ndependencies, which are crucial for multi-sentence coherence. Rather than\ndealing with individual words, our method treats the story so far as a list of\npre-trained sentence embeddings and predicts an embedding for the next\nsentence, which is more efficient than predicting word embeddings. Notably this\nallows us to consider a large number of candidates for the next sentence during\ntraining. We demonstrate the effectiveness of our approach with\nstate-of-the-art accuracy on the unsupervised Story Cloze task and with\npromising results on larger-scale next sentence prediction tasks."}, {"title": "Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization", "authors": "Gaku Morio, Hiroaki Ozaki, Terufumi Morishita, Yuta Koreeda, Kohsuke Yanai"}, {"title": "Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations", "authors": "Karan Singla, Zhuohao Chen, David Atkins, Shrikanth Narayanan"}, {"title": "Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "authors": "Alon Jacovi, Yoav Goldberg", "link": "http://arxiv.org/abs/2004.03685", "summary": "With the growing popularity of deep-learning based NLP models, comes a need\nfor interpretable systems. But what is interpretability, and what constitutes a\nhigh-quality interpretation? In this opinion piece we reflect on the current\nstate of interpretability evaluation research. We call for more clearly\ndifferentiating between different desired criteria an interpretation should\nsatisfy, and focus on the faithfulness criteria. We survey the literature with\nrespect to faithfulness evaluation, and arrange the current approaches around\nthree assumptions, providing an explicit form to how faithfulness is \"defined\"\nby the community. We provide concrete guidelines on how evaluation of\ninterpretation methods should and should not be conducted. Finally, we claim\nthat the current binary definition for faithfulness sets a potentially\nunrealistic bar for being considered faithful. We call for discarding the\nbinary notion of faithfulness in favor of a more graded one, which we believe\nwill be of greater practical utility."}, {"title": "Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation", "authors": "Aakanksha Naik, Carolyn Rose"}, {"title": "Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering", "authors": "Changmao Li, Jinho D. Choi", "link": "https://arxiv.org/abs/2004.03561", "summary": "We introduce a novel approach to transformers that learns hierarchical\nrepresentations in multiparty dialogue. First, three language modeling tasks\nare used to pre-train the transformers, token- and utterance-level language\nmodeling and utterance order prediction, that learn both token and utterance\nembeddings for better understanding in dialogue contexts. Then, multi-task\nlearning between the utterance prediction and the token span prediction is\napplied to fine-tune for span-based question answering (QA). Our approach is\nevaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over\nthe two state-of-the-art transformer models, BERT and RoBERTa, respectively."}, {"title": "Treebank Embedding Vectors for Out-of-domain Dependency Parsing", "authors": "Joachim Wagner, James Barry, Jennifer Foster", "link": "https://arxiv.org/abs/2005.00800", "summary": "A recent advance in monolingual dependency parsing is the idea of a treebank\nembedding vector, which allows all treebanks for a particular language to be\nused as training data while at the same time allowing the model to prefer\ntraining data from one treebank over others and to select the preferred\ntreebank at test time. We build on this idea by 1) introducing a method to\npredict a treebank vector for sentences that do not come from a treebank used\nin training, and 2) exploring what happens when we move away from predefined\ntreebank embedding vectors during test time and instead devise tailored\ninterpolations. We show that 1) there are interpolated vectors that are\nsuperior to the predefined ones, and 2) treebank vectors can be predicted with\nsufficient accuracy, for nine out of ten test languages, to match the\nperformance of an oracle approach that knows the most suitable predefined\ntreebank embedding for the test set."}, {"title": "Tree-Structured Neural Topic Model", "authors": "Masaru Isonuma, Junichiro Mori, Danushka Bollegala, Ichiro Sakata", "link": "", "summary": ""}, {"title": "TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition", "authors": "Bill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan Moreno, Xiao Huang, Prashant Shiralkar, Xiang Ren", "link": "https://arxiv.org/abs/2004.07493", "summary": "Training neural models for named entity recognition (NER) in a new domain\noften requires additional human annotations (e.g., tens of thousands of labeled\ninstances) that are usually expensive and time-consuming to collect. Thus, a\ncrucial research question is how to obtain supervision in a cost-effective way.\nIn this paper, we introduce \"entity triggers,\" an effective proxy of human\nexplanations for facilitating label-efficient learning of NER models. An entity\ntrigger is defined as a group of words in a sentence that helps to explain why\nhumans would recognize an entity in the sentence.\n  We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our\nproposed model, Trigger Matching Network, jointly learns trigger\nrepresentations and soft matching module with self-attention such that can\ngeneralize to unseen sentences easily for tagging. Our framework is\nsignificantly more cost-effective than the traditional neural NER frameworks.\nExperiments show that using only 20% of the trigger-annotated sentences results\nin a comparable performance as using 70% of conventional annotated sentences."}, {"title": "Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data", "authors": "Hamidreza Shahidi, Ming Li, Jimmy Lin", "link": "https://arxiv.org/abs/1909.10158", "summary": "A number of researchers have recently questioned the necessity of\nincreasingly complex neural network (NN) architectures. In particular, several\nrecent papers have shown that simpler, properly tuned models are at least\ncompetitive across several NLP tasks. In this work, we show that this is also\nthe case for text generation from structured and unstructured data. We consider\nneural table-to-text generation and neural question generation (NQG) tasks for\ntext generation from structured and unstructured data, respectively.\nTable-to-text generation aims to generate a description based on a given table,\nand NQG is the task of generating a question from a given passage where the\ngenerated question can be answered by a certain sub-span of the passage using\nNN models. Experimental results demonstrate that a basic attention-based\nseq2seq model trained with the exponential moving average technique achieves\nthe state of the art in both tasks. Code is available at\nhttps://github.com/h-shahidi/2birds-gen."}, {"title": "Uncertain Natural Language Inference", "authors": "Tongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke Sakaguchi, Benjamin Van Durme", "link": "https://arxiv.org/abs/1909.03042", "summary": "We introduce Uncertain Natural Language Inference (UNLI), a refinement of\nNatural Language Inference (NLI) that shifts away from categorical labels,\ntargeting instead the direct prediction of subjective probability assessments.\nWe demonstrate the feasibility of collecting annotations for UNLI by relabeling\na portion of the SNLI dataset under a probabilistic scale, where items even\nwith the same categorical label differ in how likely people judge them to be\ntrue given a premise. We describe a direct scalar regression modeling approach,\nand find that existing categorically labeled NLI data can be used in\npre-training. Our best models approach human performance, demonstrating models\nmay be capable of more subtle inferences than the categorical bin assignment\nemployed in current NLI tasks."}, {"title": "Understanding Advertisements with BERT", "authors": "Kanika Kalra, Bhargav Kurma, Silpa Vadakkeeveetil Sreelatha, Manasi Patwardhan, Shirish Karande"}, {"title": "Unsupervised FAQ Retrieval with Question Generation and BERT", "authors": "Yosi Mass, Boaz Carmeli, Haggai Roitman, David Konopnicki", "link": "", "summary": ""}, {"title": "Using Context in Neural Machine Translation Training Objectives", "authors": "Danielle Saunders, Felix Stahlberg, Bill Byrne", "link": "https://arxiv.org/abs/2005.01483", "summary": "We present Neural Machine Translation (NMT) training using document-level\nmetrics with batch-level documents. Previous sequence-objective approaches to\nNMT training focus exclusively on sentence-level metrics like sentence BLEU\nwhich do not correspond to the desired evaluation metric, typically document\nBLEU. Meanwhile research into document-level NMT training focuses on data or\nmodel architecture rather than training procedure. We find that each of these\nlines of research has a clear space in it for the other, and propose merging\nthem with a scheme that allows a document-level evaluation metric to be used in\nthe NMT training objective.\n  We first sample pseudo-documents from sentence samples. We then approximate\nthe expected document BLEU gradient with Monte Carlo sampling for use as a cost\nfunction in Minimum Risk Training (MRT). This two-level sampling procedure\ngives NMT performance gains over sequence MRT and maximum-likelihood training.\nWe demonstrate that training is more robust for document-level metrics than\nwith sequence metrics. We further demonstrate improvements on NMT with TER and\nGrammatical Error Correction (GEC) using GLEU, both metrics used at the\ndocument level for evaluations."}, {"title": "Variational Neural Machine Translation with Normalizing Flows", "authors": "Hendra Setiawan, Matthias Sperber, Udhyakumar Nallasamy, Matthias Paulik", "link": "https://arxiv.org/abs/2005.13978", "summary": "Variational Neural Machine Translation (VNMT) is an attractive framework for\nmodeling the generation of target translations, conditioned not only on the\nsource sentence but also on some latent random variables. The latent variable\nmodeling may introduce useful statistical dependencies that can improve\ntranslation accuracy. Unfortunately, learning informative latent variables is\nnon-trivial, as the latent space can be prohibitively large, and the latent\ncodes are prone to be ignored by many translation models at training time.\nPrevious works impose strong assumptions on the distribution of the latent code\nand limit the choice of the NMT architecture. In this paper, we propose to\napply the VNMT framework to the state-of-the-art Transformer and introduce a\nmore flexible approximate posterior based on normalizing flows. We demonstrate\nthe efficacy of our proposal under both in-domain and out-of-domain conditions,\nsignificantly outperforming strong baselines."}, {"title": "Verbal Multiword Expressions for Identification of Metaphor", "authors": "Omid Rohanian, Marek Rei, Shiva Taslimipoor, Le An Ha"}, {"title": "Video-Grounded Dialogues with Pretrained Generation Language Models", "authors": "Hung Le, Steven C.H. Hoi"}, {"title": "What Does BERT with Vision Look At?", "authors": "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang"}, {"title": "What is Learned in Visually Grounded Neural Syntax Acquisition", "authors": "Noriyuki Kojima, Hadar Averbuch-Elor, Alexander Rush, Yoav Artzi", "link": "", "summary": ""}, {"title": "Why Overfitting Isn\u2019t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries", "authors": "Mozhi Zhang, Yoshinari Fujinuma, Michael J. Paul, Jordan Boyd-Graber", "link": "https://arxiv.org/abs/2005.00524", "summary": "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon\ninduction (BLI). Recent CLWE methods use linear projections, which underfit the\ntraining dictionary, to generalize on BLI. However, underfitting can hinder\ngeneralization to other downstream tasks that rely on words from the training\ndictionary. We address this limitation by retrofitting CLWE to the training\ndictionary, which pulls training translation pairs closer in the embedding\nspace and overfits the training dictionary. This simple post-processing step\noften improves accuracy on two downstream tasks, despite lowering BLI test\naccuracy. We also retrofit to both the training dictionary and a synthetic\ndictionary induced from CLWE, which sometimes generalizes even better on\ndownstream tasks. Our results confirm the importance of fully exploiting\ntraining dictionary in downstream tasks and explains why BLI is a flawed CLWE\nevaluation."}, {"title": "Will-They-Won\u2019t-They: A Very Large Dataset for Stance Detection on Twitter", "authors": "Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, Nigel Collier", "link": "https://arxiv.org/abs/2005.00388", "summary": "We present a new challenging stance detection dataset, called\nWill-They-Won't-They (WT-WT), which contains 51,284 tweets in English, making\nit by far the largest available dataset of the type. All the annotations are\ncarried out by experts; therefore, the dataset constitutes a high-quality and\nreliable benchmark for future research in stance detection. Our experiments\nwith a wide range of recent state-of-the-art stance detection systems show that\nthe dataset poses a strong challenge to existing models in this domain."}, {"title": "Words aren\u2019t enough, their order matters: On the Robustness of Grounding Visual Referring Expressions", "authors": "Arjun Akula, Spandana Gella, Yaser Al-Onaizan, Song-Chun Zhu, Siva Reddy", "link": "https://arxiv.org/abs/2005.01655", "summary": "Visual referring expression recognition is a challenging task that requires\nnatural language understanding in the context of an image. We critically\nexamine RefCOCOg, a standard benchmark for this task, using a human study and\nshow that 83.7% of test instances do not require reasoning on linguistic\nstructure, i.e., words are enough to identify the target object, the word order\ndoesn't matter. To measure the true progress of existing models, we split the\ntest set into two sets, one which requires reasoning on linguistic structure\nand the other which doesn't. Additionally, we create an out-of-distribution\ndataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that\nthe target object changes. Using these datasets, we empirically show that\nexisting methods fail to exploit linguistic structure and are 12% to 23% lower\nin performance than the established progress for this task. We also propose two\nmethods, one based on contrastive learning and the other based on multi-task\nlearning, to increase the robustness of ViLBERT, the current state-of-the-art\nmodel for this task. Our datasets are publicly available at\nhttps://github.com/aws/aws-refcocog-adv"}, {"title": "Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation", "authors": "Shun-Po Chuang, Tzu-Wei Sung, Alexander H. Liu, Hung-yi Lee", "link": "https://arxiv.org/abs/2005.10678", "summary": "Speech translation (ST) aims to learn transformations from speech in the\nsource language to the text in the target language. Previous works show that\nmultitask learning improves the ST performance, in which the recognition\ndecoder generates the text of the source language, and the translation decoder\nobtains the final translations based on the output of the recognition decoder.\nBecause whether the output of the recognition decoder has the correct semantics\nis more critical than its accuracy, we propose to improve the multitask ST\nmodel by utilizing word embedding as the intermediate."}, {"title": "Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences", "authors": "Yi Tay, Donovan Ong, Jie Fu, Alvin Chan, Nancy Chen, Anh Tuan Luu, Chris Pal"}, {"title": "You Don\u2019t Have Time to Read This: An Exploration of Document Reading Time Prediction", "authors": "Orion Weller, Jordan Hildebrandt, Ilya Reznik, Christopher Challis, E. Shannon Tass, Quinn Snell, Kevin Seppi"}, {"title": "``You Sound Just Like Your Father\u2019\u2019 Commercial Machine Translation Systems Include Stylistic Biases", "authors": "Dirk Hovy, Federico Bianchi, Tommaso Fornaciari"}, {"title": "ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT", "authors": "Linfeng Song, Kun Xu, Yue Zhang, Jianshu Chen, Dong Yu"}, {"title": "ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents", "authors": "Chia-Yu Li, Daniel Ortega, Dirk V\u00e4th, Florian Lux, Lindsey Vanderlyn, Maximilian Schmidt, Michael Neumann, Moritz V\u00f6lkel, Pavel Denisov, Sabrina Jenne, Zorica Kacarevic, Ngoc Thang Vu", "link": "http://arxiv.org/abs/2005.01777", "summary": "We present ADVISER - an open-source, multi-domain dialog system toolkit that\nenables the development of multi-modal (incorporating speech, text and vision),\nsocially-engaged (e.g. emotion recognition, engagement level prediction and\nbackchanneling) conversational agents. The final Python-based implementation of\nour toolkit is flexible, easy to use, and easy to extend not only for\ntechnically experienced users, such as machine learning researchers, but also\nfor less technically experienced users, such as linguists or cognitive\nscientists, thereby providing a flexible platform for collaborative research.\nLink to open-source code: https://github.com/DigitalPhonetics/adviser"}, {"title": "BENTO: A Visual Platform for Building Clinical NLP Pipelines Based on CodaLab", "authors": "Yonghao Jin, Fei Li, Hong Yu"}, {"title": "Clinical-Coder: Assigning Interpretable ICD-10 Codes to Chinese Clinical Notes", "authors": "Pengfei Cao, Chenwei Yan, xiangling fu, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu, Weifeng Chong"}, {"title": "CLIReval: Evaluating Machine Translation as a Cross-Lingual Information Retrieval Task", "authors": "Shuo Sun, Suzanna Sia, Kevin Duh"}, {"title": "Conversation Learner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems", "authors": "Swadheen Shukla, Lars Liden, Shahin Shayandeh, Eslam Kamal, Jinchao Li, Matt Mazzola, Thomas Park, Baolin Peng, Jianfeng Gao", "link": "https://arxiv.org/abs/2004.04305", "summary": "Traditionally, industry solutions for building a task-oriented dialog system\nhave relied on helping dialog authors define rule-based dialog managers,\nrepresented as dialog flows. While dialog flows are intuitively interpretable\nand good for simple scenarios, they fall short of performance in terms of the\nflexibility needed to handle complex dialogs. On the other hand, purely\nmachine-learned models can handle complex dialogs, but they are considered to\nbe black boxes and require large amounts of training data. In this\ndemonstration, we showcase Conversation Learner, a machine teaching tool for\nbuilding dialog managers. It combines the best of both approaches by enabling\ndialog authors to create a dialog flow using familiar tools, converting the\ndialog flow into a parametric model (e.g., neural networks), and allowing\ndialog authors to improve the dialog manager (i.e., the parametric model) over\ntime by leveraging user-system dialog logs as training data through a machine\nteaching interface."}, {"title": "ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems", "authors": "Qi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi Takanobu, Jinchao Li, Baolin Peng, Jianfeng Gao, xiaoyan zhu, Minlie Huang", "link": "https://arxiv.org/abs/2002.04793", "summary": "We present ConvLab-2, an open-source toolkit that enables researchers to\nbuild task-oriented dialogue systems with state-of-the-art models, perform an\nend-to-end evaluation, and diagnose the weakness of systems. As the successor\nof ConvLab (Lee et al., 2019b), ConvLab-2 inherits ConvLab's framework but\nintegrates more powerful dialogue models and supports more datasets. Besides,\nwe have developed an analysis tool and an interactive tool to assist\nresearchers in diagnosing dialogue systems. The analysis tool presents rich\nstatistics and summarizes common mistakes from simulated dialogues, which\nfacilitates error analysis and system improvement. The interactive tool\nprovides a user interface that allows developers to diagnose an assembled\ndialogue system by interacting with the system and modifying the output of each\nsystem component."}, {"title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation", "authors": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan", "link": "https://arxiv.org/abs/1911.00536", "summary": "We present a large, tunable neural conversational response generation model,\nDialoGPT (dialogue generative pre-trained transformer). Trained on 147M\nconversation-like exchanges extracted from Reddit comment chains over a period\nspanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch\ntransformer to attain a performance close to human both in terms of automatic\nand human evaluation in single-turn dialogue settings. We show that\nconversational systems that leverage DialoGPT generate more relevant,\ncontentful and context-consistent responses than strong baseline systems. The\npre-trained model and training pipeline are publicly released to facilitate\nresearch into neural response generation and the development of more\nintelligent open-domain dialogue systems."}, {"title": "Embedding-based Scientific Literature Discovery in a Text Editor Application", "authors": "Onur G\u00f6k\u00e7e, Jonathan Prada, Nikola Nikolov, Nianlong Gu, Richard Hahnloser", "link": "https://arxiv.org/abs/2005.04961", "summary": "Each claim in a research paper requires all relevant prior knowledge to be\ndiscovered, assimilated, and appropriately cited. However, despite the\navailability of powerful search engines and sophisticated text editing\nsoftware, discovering relevant papers and integrating the knowledge into a\nmanuscript remain complex tasks associated with high cognitive load. To define\ncomprehensive search queries requires strong motivation from authors,\nirrespective of their familiarity with the research field. Moreover, switching\nbetween independent applications for literature discovery, bibliography\nmanagement, reading papers, and writing text burdens authors further and\ninterrupts their creative process. Here, we present a web application that\ncombines text editing and literature discovery in an interactive user\ninterface. The application is equipped with a search engine that couples\nBoolean keyword filtering with nearest neighbor search over text embeddings,\nproviding a discovery experience tuned to an author's manuscript and his\ninterests. Our application aims to take a step towards more enjoyable and\neffortless academic writing.\n  The demo of the application (https://SciEditorDemo2020.herokuapp.com/) and a\nshort video tutorial (https://youtu.be/pkdVU60IcRc) are available online."}, {"title": "ESPnet-ST: All-in-One Speech Translation Toolkit", "authors": "Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, Shinji Watanabe", "link": "", "summary": ""}, {"title": "EVIDENCEMINER: Textual Evidence Discovery for Life Sciences", "authors": "Xuan Wang, Yingjun Guan, Weili Liu, Aabhas Chauhan, Enyi Jiang, Qi Li, David Liem, Dibakar Sigdel, John Caufield, Peipei Ping, Jiawei Han"}, {"title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models", "authors": "Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann", "link": "https://arxiv.org/abs/1910.05276", "summary": "Large language models can produce powerful contextual representations that\nlead to improvements across many NLP tasks. Since these models are typically\nguided by a sequence of learned self attention mechanisms and may comprise\nundesired inductive biases, it is paramount to be able to explore what the\nattention has learned. While static analyses of these models lead to targeted\ninsights, interactive tools are more dynamic and can help humans better gain an\nintuition for the model-internal reasoning process. We present exBERT, an\ninteractive tool named after the popular BERT language model, that provides\ninsights into the meaning of the contextual representations by matching a\nhuman-specified input to similar contexts in a large annotated dataset. By\naggregating the annotations of the matching similar contexts, exBERT helps\nintuitively explain what each attention-head has learned."}, {"title": "GAIA: A Fine-grained Multimedia Knowledge Extraction System", "authors": "Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, BRIAN CHEN, Bo Wu, Heng Ji, Shih-Fu Chang, Clare Voss, Daniel Napierski, Marjorie Freedman"}, {"title": "Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations", "authors": "Toby Jia-Jun Li, Tom Mitchell, Brad Myers", "link": "", "summary": ""}, {"title": "jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models", "authors": "Yada Pruksachatkun, Phil Yeres, Haokun Liu, Jason Phang, Phu Mon Htut, Alex Wang, Ian Tenney, Samuel R. Bowman"}, {"title": "Label Noise in Context", "authors": "Michael Desmond, Catherine Finegan-Dollak, Jeff Boston, Matt Arnold"}, {"title": "LEAN-LIFE: A Label-Efficient Annotation Framework Towards Learning from Explanation", "authors": "Dong-Ho Lee, Rahul Khanna, Bill Yuchen Lin, Seyeon Lee, Qinyuan Ye, Elizabeth Boschee, Leonardo Neves, Xiang Ren", "link": "https://arxiv.org/abs/2004.07499", "summary": "Successfully training a deep neural network demands a huge corpus of labeled\ndata. However, each label only provides limited information to learn from and\ncollecting the requisite number of labels involves massive human effort. In\nthis work, we introduce LEAN-LIFE, a web-based, Label-Efficient AnnotatioN\nframework for sequence labeling and classification tasks, with an easy-to-use\nUI that not only allows an annotator to provide the needed labels for a task,\nbut also enables LearnIng From Explanations for each labeling decision. Such\nexplanations enable us to generate useful additional labeled data from\nunlabeled instances, bolstering the pool of available training data. On three\npopular NLP tasks (named entity recognition, relation extraction, sentiment\nanalysis), we find that using this enhanced supervision allows our models to\nsurpass competitive baseline F1 scores by more than 5-10 percentage points,\nwhile using 2X times fewer labeled instances. Our framework is the first to\nutilize this enhanced supervision technique and does so for three important\ntasks -- thus providing improved annotation recommendations to users and an\nability to build datasets of (data, label, explanation) triples instead of the\nregular (data, label) pair."}, {"title": "LinggleWrite: a Coaching System for Essay Writing", "authors": "Chung-Ting Tsai, Jhih-Jie Chen, Chingyu Yang, Jason Chang"}, {"title": "MixingBoard: a Knowledgeable Stylized Integrated Text Generation Platform", "authors": "Xiang Gao, Michel Galley, Bill Dolan", "link": "https://arxiv.org/abs/2005.08365", "summary": "We present MixingBoard, a platform for quickly building demos with a focus on\nknowledge grounded stylized text generation. We unify existing text generation\nalgorithms in a shared codebase and further adapt earlier algorithms for\nconstrained generation. To borrow advantages from different models, we\nimplement strategies for cross-model integration, from the token probability\nlevel to the latent space level. An interface to external knowledge is provided\nvia a module that retrieves on-the-fly relevant knowledge from passages on the\nweb or any document collection. A user interface for local development, remote\nwebpage access, and a RESTful API are provided to make it simple for users to\nbuild their own demos."}, {"title": "MMPE: A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation", "authors": "Nico Herbig, Santanu Pal, Tim D\u00fcwel, Kalliopi Maria Meladaki, Mahsa Monshizadeh, Vladislav Hnatovskiy, Antonio Kr\u00fcger, Josef van Genabith"}, {"title": "Multilingual Universal Sentence Encoder for Semantic Retrieval", "authors": "Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope, Ray Kurzweil", "link": "https://arxiv.org/abs/1907.04307", "summary": "We introduce two pre-trained retrieval focused multilingual sentence encoding\nmodels, respectively based on the Transformer and CNN model architectures. The\nmodels embed text from 16 languages into a single semantic space using a\nmulti-task trained dual-encoder that learns tied representations using\ntranslation based bridge tasks (Chidambaram al., 2018). The models provide\nperformance that is competitive with the state-of-the-art on: semantic\nretrieval (SR), translation pair bitext retrieval (BR) and retrieval question\nanswering (ReQA). On English transfer learning tasks, our sentence-level\nembeddings approach, and in some cases exceed, the performance of monolingual,\nEnglish only, sentence embedding models. Our models are made available for\ndownload on TensorFlow Hub."}, {"title": "Nakdan: Professional Hebrew Diacritizer", "authors": "Avi Shmidman, Shaltiel Shmidman, Moshe Koppel, Yoav Goldberg", "link": "https://arxiv.org/abs/2005.03312", "summary": "We present a system for automatic diacritization of Hebrew text. The system\ncombines modern neural models with carefully curated declarative linguistic\nknowledge and comprehensive manually constructed tables and dictionaries.\nBesides providing state of the art diacritization accuracy, the system also\nsupports an interface for manual editing and correction of the automatic\noutput, and has several features which make it particularly useful for\npreparation of scientific editions of Hebrew texts. The system supports Modern\nHebrew, Rabbinic Hebrew and Poetic Hebrew. The system is freely accessible for\nall use at http://nakdanpro.dicta.org.il."}, {"title": "NLP Scholar: An Interactive Visual Explorer for Natural Language Processing Literature", "authors": "Saif Mohammad", "link": "https://arxiv.org/abs/2006.01131", "summary": "As part of the NLP Scholar project, we created a single unified dataset of\nNLP papers and their meta-information (including citation numbers), by\nextracting and aligning information from the ACL Anthology and Google Scholar.\nIn this paper, we describe several interconnected interactive visualizations\n(dashboards) that present various aspects of the data. Clicking on an item\nwithin a visualization or entering query terms in the search boxes filters the\ndata in all visualizations in the dashboard. This allows users to search for\npapers in the area of their interest, published within specific time periods,\npublished by specified authors, etc. The interactive visualizations presented\nhere, and the associated dataset of papers mapped to citations, have additional\nuses as well including understanding how the field is growing (both overall and\nacross sub-areas), as well as quantifying the impact of different types of\npapers on subsequent publications."}, {"title": "NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg", "authors": "Joshua Bambrick, Minjie Xu, Andy Almonte, Igor Malioutov, Guim Perarnau, Vittorio Selo, Iat Chong Chan", "link": "https://arxiv.org/abs/2006.01117", "summary": "Millions of news articles from hundreds of thousands of sources around the\nglobe appear in news aggregators every day. Consuming such a volume of news\npresents an almost insurmountable challenge. For example, a reader searching on\nBloomberg's system for news about the U.K. would find 10,000 articles on a\ntypical day. Apple Inc., the world's most journalistically covered company,\ngarners around 1,800 news articles a day.\n  We realized that a new kind of summarization engine was needed, one that\nwould condense large volumes of news into short, easy to absorb points. The\nsystem would filter out noise and duplicates to identify and summarize key news\nabout companies, countries or markets.\n  When given a user query, Bloomberg's solution, Key News Themes (or NSTM),\nleverages state-of-the-art semantic clustering techniques and novel\nsummarization methods to produce comprehensive, yet concise, digests to\ndramatically simplify the news consumption process.\n  NSTM is available to hundreds of thousands of readers around the world and\nserves thousands of requests daily with sub-second latency. At ACL 2020, we\nwill present a demo of NSTM."}, {"title": "OpusFilter: A Configurable Parallel Corpus Filtering Toolbox", "authors": "Mikko Aulamo, Sami Virpioja, J\u00f6rg Tiedemann"}, {"title": "Penman: An Open-Source Library and Tool for AMR Graphs", "authors": "Michael Wayne Goodman"}, {"title": "Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation", "authors": "Federico Scozzafava, Marco Maru, Fabrizio Brignone, Giovanni Torrisi, Roberto Navigli"}, {"title": "Photon: A Robust Cross-Domain Text-to-SQL System", "authors": "Jichuan Zeng, Xi Victoria Lin, Steven C.H. Hoi, Richard Socher, Caiming Xiong, Michael Lyu, Irwin King"}, {"title": "Prta: A System to Support the Analysis of Propaganda Techniques in the News", "authors": "Giovanni Da San Martino, Shaden Shaar, Yifan Zhang, Seunghak Yu, Alberto Barr\u00f3n-Cede\u00f1o, Preslav Nakov", "link": "https://arxiv.org/abs/2005.05854", "summary": "Recent events, such as the 2016 US Presidential Campaign, Brexit and the\nCOVID-19 \"infodemic\", have brought into the spotlight the dangers of online\ndisinformation. There has been a lot of research focusing on fact-checking and\ndisinformation detection. However, little attention has been paid to the\nspecific rhetorical and psychological techniques used to convey propaganda\nmessages. Revealing the use of such techniques can help promote media literacy\nand critical thinking, and eventually contribute to limiting the impact of\n\"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion\nTechniques Analyzer) allows users to explore the articles crawled on a regular\nbasis by highlighting the spans in which propaganda techniques occur and to\ncompare them on the basis of their use of propaganda techniques. The system\nfurther reports statistics about the use of such techniques, overall and over\ntime, or according to filtering criteria specified by the user based on time\ninterval, keywords, and/or political orientation of the media. Moreover, it\nallows users to analyze any text or URL through a dedicated interface or via an\nAPI. The system is available online: https://www.tanbih.org/prta"}, {"title": "pyBART: Evidence-based Syntactic Transformations for IE", "authors": "Aryeh Tiktinsky, Yoav Goldberg, Reut Tsarfaty", "link": "https://arxiv.org/abs/2005.01306", "summary": "Syntactic dependencies can be predicted with high accuracy, and are useful\nfor both machine-learned and pattern-based information extraction tasks.\nHowever, their utility can be improved. These syntactic dependencies are\ndesigned to accurately reflect syntactic relations, and they do not make\nsemantic relations explicit. Therefore, these representations lack many\nexplicit connections between content words, that would be useful for downstream\napplications. Proposals like English Enhanced UD improve the situation by\nextending universal dependency trees with additional explicit arcs. However,\nthey are not available to Python users, and are also limited in coverage. We\nintroduce a broad-coverage, data-driven and linguistically sound set of\ntransformations, that makes event-structure and many lexical relations\nexplicit. We present pyBART, an easy-to-use open-source Python library for\nconverting English UD trees either to Enhanced UD graphs or to our\nrepresentation. The library can work as a standalone package or be integrated\nwithin a spaCy NLP pipeline. When evaluated in a pattern-based relation\nextraction scenario, our representation results in higher extraction scores\nthan Enhanced UD, while requiring fewer patterns."}, {"title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages", "authors": "Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher D. Manning", "link": "https://arxiv.org/abs/2003.07082", "summary": "We introduce Stanza, an open-source Python natural language processing\ntoolkit supporting 66 human languages. Compared to existing widely used\ntoolkits, Stanza features a language-agnostic fully neural pipeline for text\nanalysis, including tokenization, multi-word token expansion, lemmatization,\npart-of-speech and morphological feature tagging, dependency parsing, and named\nentity recognition. We have trained Stanza on a total of 112 datasets,\nincluding the Universal Dependencies treebanks and other multilingual corpora,\nand show that the same neural architecture generalizes well and achieves\ncompetitive performance on all languages tested. Additionally, Stanza includes\na native Python interface to the widely used Java Stanford CoreNLP software,\nwhich further extends its functionality to cover other tasks such as\ncoreference resolution and relation extraction. Source code, documentation, and\npretrained models for 66 languages are available at\nhttps://stanfordnlp.github.io/stanza."}, {"title": "Stimulating Creativity with FunLines: A Case Study of Humor Generation in Headlines", "authors": "Nabil Hossain, John Krumm, Tanvir Sajed, Henry Kautz", "link": "https://arxiv.org/abs/2002.02031", "summary": "Building datasets of creative text, such as humor, is quite challenging. We\nintroduce FunLines, a competitive game where players edit news headlines to\nmake them funny, and where they rate the funniness of headlines edited by\nothers. FunLines makes the humor generation process fun, interactive,\ncollaborative, rewarding and educational, keeping players engaged and providing\nhumor data at a very low cost compared to traditional crowdsourcing approaches.\nFunLines offers useful performance feedback, assisting players in getting\nbetter over time at generating and assessing humor, as our analysis shows. This\nhelps to further increase the quality of the generated dataset. We show the\neffectiveness of this data by training humor classification models that\noutperform a previous benchmark, and we release this dataset to the public."}, {"title": "SUPP.AI: finding evidence for supplement-drug interactions", "authors": "Lucy Wang, Oyvind Tafjord, Arman Cohan, Sarthak Jain, Sam Skjonsberg, Carissa Schoenick, Nick Botner, Waleed Ammar"}, {"title": "Syntactic Search by Example", "authors": "Micah Shlain, Hillel Taub-Tabib, Shoval Sadde, Yoav Goldberg", "link": "https://arxiv.org/abs/2006.03010", "summary": "We present a system that allows a user to search a large linguistically\nannotated corpus using syntactic patterns over dependency graphs. In contrast\nto previous attempts to this effect, we introduce a light-weight query language\nthat does not require the user to know the details of the underlying syntactic\nrepresentations, and instead to query the corpus by providing an example\nsentence coupled with simple markup. Search is performed at an interactive\nspeed due to an efficient linguistic graph-indexing and retrieval engine. This\nallows for rapid exploration, development and refinement of syntax-based\nqueries. We demonstrate the system using queries over two corpora: the English\nwikipedia, and a collection of English pubmed abstracts. A demo of the\nwikipedia system is available at: https://allenai.github.io/spike"}, {"title": "SyntaxGym: An Online Platform for Targeted Evaluation of Language Models", "authors": "Jon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian, Roger Levy"}, {"title": "Tabouid: a Wikipedia-based word guessing game", "authors": "Timoth\u00e9e Bernard"}, {"title": "Talk to Papers: Bringing Neural Question Answering to Academic Search", "authors": "Tiancheng Zhao, Kyusong Lee", "link": "https://arxiv.org/abs/2004.02002", "summary": "We introduce Talk to Papers, which exploits the recent open-domain question\nanswering (QA) techniques to improve the current experience of academic search.\nIt's designed to enable researchers to use natural language queries to find\nprecise answers and extract insights from a massive amount of academic papers.\nWe present a large improvement over classic search engine baseline on several\nstandard QA datasets and provide the community a collaborative data collection\ntool to curate the first natural language processing research QA dataset via a\ncommunity effort."}, {"title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing", "authors": "Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting Liu, Shijin Wang, Guoping Hu", "link": "https://arxiv.org/abs/2002.12620", "summary": "In this paper, we introduce TextBrewer, an open-source knowledge distillation\ntoolkit designed for natural language processing. It works with different\nneural network models and supports various kinds of supervised learning tasks,\nsuch as text classification, reading comprehension, sequence labeling.\nTextBrewer provides a simple and uniform workflow that enables quick setting up\nof distillation experiments with highly flexible configurations. It offers a\nset of predefined distillation methods and can be extended with custom code. As\na case study, we use TextBrewer to distill BERT on several typical NLP tasks.\nWith simple configurations, we achieve results that are comparable with or even\nhigher than the public distilled BERT models with similar numbers of\nparameters. Our toolkit is available through: http://textbrewer.hfl-rc.com"}, {"title": "The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding", "authors": "Xiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He, Weizhu Chen, Hoifung Poon, Guihong Cao, Jianfeng Gao", "link": "https://arxiv.org/abs/2002.07972", "summary": "We present MT-DNN, an open-source natural language understanding (NLU)\ntoolkit that makes it easy for researchers and developers to train customized\ndeep learning models. Built upon PyTorch and Transformers, MT-DNN is designed\nto facilitate rapid customization for a broad spectrum of NLU tasks, using a\nvariety of objectives (classification, regression, structured prediction) and\ntext encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is\nits built-in support for robust and transferable learning using the adversarial\nmulti-task learning paradigm. To enable efficient production deployment, MT-DNN\nsupports multi-task knowledge distillation, which can substantially compress a\ndeep neural model without significant performance drop. We demonstrate the\neffectiveness of MT-DNN on a wide range of NLU applications across general and\nbiomedical domains. The software and pre-trained models will be publicly\navailable at https://github.com/namisan/mt-dnn."}, {"title": "Torch-Struct: Deep Structured Prediction Library", "authors": "Alexander Rush", "link": "https://arxiv.org/abs/2002.00876", "summary": "The literature on structured prediction for NLP describes a rich collection\nof distributions and algorithms over sequences, segmentations, alignments, and\ntrees; however, these algorithms are difficult to utilize in deep learning\nframeworks. We introduce Torch-Struct, a library for structured prediction\ndesigned to take advantage of and integrate with vectorized,\nauto-differentiation based frameworks. Torch-Struct includes a broad collection\nof probabilistic structures accessed through a simple and flexible\ndistribution-based API that connects to any deep learning model. The library\nutilizes batched, vectorized operations and exploits auto-differentiation to\nproduce readable, fast, and testable code. Internally, we also include a number\nof general-purpose optimizations to provide cross-algorithm efficiency.\nExperiments show significant performance gains over fast baselines and\ncase-studies demonstrate the benefits of the library. Torch-Struct is available\nat https://github.com/harvardnlp/pytorch-struct."}, {"title": "Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time", "authors": "Benjamin Nye, Ani Nenkova, Iain Marshall, Byron C. Wallace", "link": "https://arxiv.org/abs/2005.10865", "summary": "We introduce Trialstreamer, a living database of clinical trial reports. Here\nwe mainly describe the evidence extraction component; this extracts from\nbiomedical abstracts key pieces of information that clinicians need when\nappraising the literature, and also the relations between these. Specifically,\nthe system extracts descriptions of trial participants, the treatments compared\nin each arm (the interventions), and which outcomes were measured. The system\nthen attempts to infer which interventions were reported to work best by\ndetermining their relationship with identified trial outcome measures. In\naddition to summarizing individual trials, these extracted data elements allow\nautomatic synthesis of results across many trials on the same topic. We apply\nthe system at scale to all reports of randomized controlled trials indexed in\nMEDLINE, powering the automatic generation of evidence maps, which provide a\nglobal view of the efficacy of different interventions combining data from all\nrelevant clinical trials on a topic. We make all code and models freely\navailable alongside a demonstration of the web interface."}, {"title": "Usnea: An Authorship Tool for Interactive Fiction using Retrieval Based Semantic Parsing", "authors": "Ben Swanson, Boris Smus"}, {"title": "What\u2019s The Latest? A Question-driven News Chatbot", "authors": "Philippe Laban, John Canny, Marti A. Hearst"}, {"title": "Xiaomingbot: A Multilingual Robot News Reporter", "authors": "Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou, Ying Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan Wang, Lei Li"}, {"title": "#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media", "authors": "Ashima Suvarna, Grusha Bhalla"}, {"title": "A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples", "authors": "Zhao Meng, Roger Wattenhofer", "link": "", "summary": ""}, {"title": "A Simple and Effective Dependency parser for Telugu", "authors": "Sneha Nallani, Manish Shrivastava, Dipti Sharma"}, {"title": "Adaptive Transformers for Learning Multimodal Representations", "authors": "Prajjwal Bhargava", "link": "https://arxiv.org/abs/2005.07486", "summary": "The usage of transformers has grown from learning about language semantics to\nforming meaningful visiolinguistic representations. These architectures are\noften over-parametrized, requiring large amounts of computation. In this work,\nwe extend adaptive approaches to learn more about model interpretability and\ncomputational efficiency. Specifically, we study attention spans, sparse, and\nstructured dropout methods to help understand how their attention mechanism\nextends for vision and language tasks. We further show that these approaches\ncan help us learn more about how the network perceives the complexity of input\nsequences, sparsity preferences for different modalities, and other related\nphenomena."}, {"title": "AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss", "authors": "Mahmoud Daif, Shunsuke Kitada, Hitoshi Iyatomi"}, {"title": "Building a Japanese Typo Dataset from Wikipedia\u2019s Revision History", "authors": "Yu Tanaka, Yugo Murawaki, Daisuke Kawahara, Sadao Kurohashi"}, {"title": "Checkpoint Reranking: An Approach To Select Better Hypothesis For Neural Machine Translation Systems", "authors": "Vinay Pandramish, Dipti Misra Sharma"}, {"title": "Combining Subword Representations into Word-level Representations in the Transformer Architecture", "authors": "Noe Casas, Marta R. Costa-juss\u00e0, Jos\u00e9 A. R. Fonollosa"}, {"title": "Compositional generalization by factorizing alignment and translation", "authors": "Jacob Russin, Jason Jo, Randall O\u2019Reilly, Yoshua Bengio"}, {"title": "Considering Likelihood in NLP Classi\ufb01cation Explanations with Occlusion and Language Modeling", "authors": "David Harbecke, Christoph Alt", "link": "https://arxiv.org/abs/2004.09890", "summary": "Recently, state-of-the-art NLP models gained an increasing syntactic and\nsemantic understanding of language, and explanation methods are crucial to\nunderstand their decisions. Occlusion is a well established method that\nprovides explanations on discrete language data, e.g. by removing a language\nunit from an input and measuring the impact on a model's decision. We argue\nthat current occlusion-based methods often produce invalid or syntactically\nincorrect language data, neglecting the improved abilities of recent NLP\nmodels. Furthermore, gradient-based explanation methods disregard the discrete\ndistribution of data in NLP. Thus, we propose OLM: a novel explanation method\nthat combines occlusion and language models to sample valid and syntactically\ncorrect replacements with high likelihood, given the context of the original\ninput. We lay out a theoretical foundation that alleviates these weaknesses of\nother explanation methods in NLP and provide results that underline the\nimportance of considering data likelihood in occlusion-based explanation."}, {"title": "Crossing the Line: Where do Demographic Variables Fit into Humor Detection?", "authors": "J. A. Meaney"}, {"title": "Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup", "authors": "Jishnu Ray Chowdhury, Cornelia Caragea, Doina Caragea"}, {"title": "Dominance as an Indicator of Rapport and Learning in Human-Agent Communication", "authors": "Amanda Buddemeyer, Xiaoyi Tian, Erin Walker"}, {"title": "Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions", "authors": "Stein\u00fe\u00f3r Steingr\u00edmsson"}, {"title": "Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages", "authors": "Vikrant Goyal, Sourav Kumar, Dipti Misra Sharma"}, {"title": "Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition", "authors": "Takuma Kato, Kaori Abe, Hiroki Ouchi, Shumpei Miyawaki, Jun Suzuki, Kentaro Inui", "link": "http://arxiv.org/abs/2006.01372", "summary": "In general, the labels used in sequence labeling consist of different types\nof elements. For example, IOB-format entity labels, such as B-Person and\nI-Person, can be decomposed into span (B and I) and type information (Person).\nHowever, while most sequence labeling models do not consider such label\ncomponents, the shared components across labels, such as Person, can be\nbeneficial for label prediction. In this work, we propose to integrate label\ncomponent information as embeddings into models. Through experiments on English\nand Japanese fine-grained named entity recognition, we demonstrate that the\nproposed method improves performance, especially for instances with\nlow-frequency labels."}, {"title": "Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources", "authors": "Magdalena Biesialska, bardia rafieian, Marta R. Costa-juss\u00e0", "link": "https://arxiv.org/abs/2005.10048", "summary": "In this work, we present an effective method for semantic specialization of\nword vector representations. To this end, we use traditional word embeddings\nand apply specialization methods to better capture semantic relations between\nwords. In our approach, we leverage external knowledge from rich lexical\nresources such as BabelNet. We also show that our proposed post-specialization\nmethod based on an adversarial neural network with the Wasserstein distance\nallows to gain improvements over state-of-the-art methods on two tasks: word\nsimilarity and dialog state tracking."}, {"title": "Exploring Interpretability in Event Extraction: Multitask Learning of a Neural Event Classifier and an Explanation Decoder", "authors": "Zheng Tang, Gus Hahn-Powell, Mihai Surdeanu"}, {"title": "Exploring the Role of Context to Distinguish Rhetorical and Information-Seeking Questions", "authors": "Yuan Zhuang, Ellen Riloff"}, {"title": "Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information", "authors": "Hyeryun Park, Kyungmo Kim, Jooyoung Yoon, Seongkeun Park, Jinwook Choi"}, {"title": "Grammatical Error Correction Using Pseudo Learner Corpus Considering Learner\u2019s Error Tendency", "authors": "Yujin Takahashi, Satoru Katsumata, Mamoru Komachi"}, {"title": "HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing", "authors": "Miaomiao Yu, Yujiu Yang, Chenhui Li"}, {"title": "How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?", "authors": "Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal", "link": "https://arxiv.org/abs/2005.08199", "summary": "Long short-term memory (LSTM) networks and their variants are capable of\nencapsulating long-range dependencies, which is evident from their performance\non a variety of linguistic tasks. On the other hand, simple recurrent networks\n(SRNs), which appear more biologically grounded in terms of synaptic\nconnections, have generally been less successful at capturing long-range\ndependencies as well as the loci of grammatical errors in an unsupervised\nsetting. In this paper, we seek to develop models that bridge the gap between\nbiological plausibility and linguistic competence. We propose a new\narchitecture, the Decay RNN, which incorporates the decaying nature of neuronal\nactivations and models the excitatory and inhibitory connections in a\npopulation of neurons. Besides its biological inspiration, our model also shows\ncompetitive performance relative to LSTMs on subject-verb agreement, sentence\ngrammaticality, and language modeling tasks. These results provide some\npointers towards probing the nature of the inductive biases required for RNN\narchitectures to model linguistic phenomena successfully."}, {"title": "\u03c5BLEU: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems", "authors": "Tsuta Yuma, Naoki Yoshinaga, Masashi Toyoda", "link": "", "summary": ""}, {"title": "Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition", "authors": "Yuhui Zhang, Allen Nie"}, {"title": "Let\u2019s be Humorous: Knowledge Enhanced Humor Generation", "authors": "Hang Zhang, Dayiheng Liu, Jiancheng Lv, Luo Cheng", "link": "https://arxiv.org/abs/2004.13317", "summary": "The generation of humor is an under-explored and challenging problem.\nPrevious works mainly utilize templates or replace phrases to generate humor.\nHowever, few works focus on freer forms and the background knowledge of humor.\nThe linguistic theory of humor defines the structure of a humor sentence as\nset-up and punchline. In this paper, we explore how to generate a punchline\ngiven the set-up with the relevant knowledge. We propose a framework that can\nfuse the knowledge to end-to-end models. To our knowledge, this is the first\nattempt to generate punchlines with knowledge enhanced model. Furthermore, we\ncreate the first humor-knowledge dataset. The experimental results demonstrate\nthat our method can make use of knowledge to generate fluent, funny punchlines,\nwhich outperforms several baselines."}, {"title": "Logical Inferences with Comparatives and Generalized Quantifiers", "authors": "Izumi Haruta, Koji Mineshima, Daisuke Bekki", "link": "https://arxiv.org/abs/2005.07954", "summary": "Comparative constructions pose a challenge in Natural Language Inference\n(NLI), which is the task of determining whether a text entails a hypothesis.\nComparatives are structurally complex in that they interact with other\nlinguistic phenomena such as quantifiers, numerals, and lexical antonyms. In\nformal semantics, there is a rich body of work on comparatives and gradable\nexpressions using the notion of degree. However, a logical inference system for\ncomparatives has not been sufficiently developed for use in the NLI task. In\nthis paper, we present a compositional semantics that maps various comparative\nconstructions in English to semantic representations via Combinatory Categorial\nGrammar (CCG) parsers and combine it with an inference system based on\nautomated theorem proving. We evaluate our system on three NLI datasets that\ncontain complex logical inferences with comparatives, generalized quantifiers,\nand numerals. We show that the system outperforms previous logic-based systems\nas well as recent deep learning-based models."}, {"title": "Media Bias, the Social Sciences, and NLP: Automating Frame Analyses to Identify Bias by Word Choice and Labeling", "authors": "Felix Hamborg"}, {"title": "Multi-Task Neural Model for Agglutinative Language Translation", "authors": "Yirong Pan, Xiao Li, Yating Yang, Rui Dong"}, {"title": "Noise-Based Augmentation Techniques for Emotion Datasets: What do we Recommend?", "authors": "Mimansa Jaiswal, Emily Mower Provost"}, {"title": "Non-Topical Coherence in Social Talk: A Call for Dialogue Model Enrichment", "authors": "Alex L\u01b0u, Sophia A. Malamud"}, {"title": "Pointwise Paraphrase Appraisal is Potentially Problematic", "authors": "Hannah Chen, Yangfeng Ji, David Evans", "link": "https://arxiv.org/abs/2005.11996", "summary": "The prevailing approach for training and evaluating paraphrase identification\nmodels is constructed as a binary classification problem: the model is given a\npair of sentences, and is judged by how accurately it classifies pairs as\neither paraphrases or non-paraphrases. This pointwise-based evaluation method\ndoes not match well the objective of most real world applications, so the goal\nof our work is to understand how models which perform well under pointwise\nevaluation may fail in practice and find better methods for evaluating\nparaphrase identification models. As a first step towards that goal, we show\nthat although the standard way of fine-tuning BERT for paraphrase\nidentification by pairing two sentences as one sequence results in a model with\nstate-of-the-art performance, that model may perform poorly on simple tasks\nlike identifying pairs with two identical sentences. Moreover, we show that\nthese models may even predict a pair of randomly-selected sentences with higher\nparaphrase score than a pair of identical ones."}, {"title": "Pre-training via Leveraging Assisting Languages for Neural Machine Translation", "authors": "Haiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi, Eiichiro Sumita", "link": "https://arxiv.org/abs/2001.08353", "summary": "Sequence-to-sequence (S2S) pre-training using large monolingual data is known\nto improve performance for various S2S NLP tasks in low-resource settings.\nHowever, large monolingual corpora might not always be available for the\nlanguages of interest (LOI). To this end, we propose to exploit monolingual\ncorpora of other languages to complement the scarcity of monolingual corpora\nfor the LOI. A case study of low-resource Japanese-English neural machine\ntranslation (NMT) reveals that leveraging large Chinese and French monolingual\ncorpora can help overcome the shortage of Japanese and English monolingual\ncorpora, respectively, for S2S pre-training. We further show how to utilize\nscript mapping (Chinese to Japanese) to increase the similarity between the two\nmonolingual corpora leading to further improvements in translation quality.\nAdditionally, we propose simple data-selection techniques to be used prior to\npre-training that significantly impact the quality of S2S pre-training. An\nempirical comparison of our proposed methods reveals that leveraging assisting\nlanguage monolingual corpora, data selection and script mapping are extremely\nimportant for NMT pre-training in low-resource scenarios."}, {"title": "Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation", "authors": "Hiroaki Funayama, Shota Sasaki, Yuichiroh Matsubayashi, Tomoya Mizumoto, Jun Suzuki, Masato Mita, Kentaro Inui"}, {"title": "Reflection-based Word Attribute Transfer", "authors": "Yoichi Ishibashi, Katsuhito Sudoh, Koichiro Yoshino, Satoshi Nakamura"}, {"title": "Research on Task Discovery for Transfer Learning in Deep Neural Networks", "authors": "Arda Akdemir"}, {"title": "Research Replication Prediction Using Weakly Supervised Learning", "authors": "Tianyi Luo, Xingyu Li, Hainan Wang, Yang Liu"}, {"title": "RPD: A Distance Function Between Word Embeddings", "authors": "Xuhui Zhou, Shujian Huang, Zaixiang Zheng", "link": "http://arxiv.org/abs/2005.08113", "summary": "It is well-understood that different algorithms, training processes, and\ncorpora produce different word embeddings. However, less is known about the\nrelation between different embedding spaces, i.e. how far different sets of\nembeddings deviate from each other. In this paper, we propose a novel metric\ncalled Relative pairwise inner Product Distance (RPD) to quantify the distance\nbetween different sets of word embeddings. This metric has a unified scale for\ncomparing different sets of word embeddings. Based on the properties of RPD, we\nstudy the relations of word embeddings of different algorithms systematically\nand investigate the influence of different training processes and corpora. The\nresults shed light on the poorly understood word embeddings and justify RPD as\na measure of the distance of embedding spaces."}, {"title": "SCAR: Sentence Compression using Autoencoders for Reconstruction", "authors": "Chanakya Malireddy, Tirth Maniar, Manish Shrivastava", "link": "", "summary": ""}, {"title": "Self-Attention is Not Only a Weight: Analyzing BERT with Vector Norms", "authors": "Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui", "link": "", "summary": ""}, {"title": "Story-level Text Style Transfer: A Proposal", "authors": "Yusu Qian"}, {"title": "To compress or not to compress? A Finite-State approach to Nen verbal morphology", "authors": "Saliha Muradoglu, Nicholas Evans, Hanna Suominen"}, {"title": "Topic balancing with additive regularization of topic models", "authors": "Eugeniia Veselova, Konstantin Vorontsov"}, {"title": "Transferring Monolingual Model to Low-Resource Language: The Case of Tigrinya", "authors": "Abrhalei Frezghi Tela, Abraham Woubie Zewoudie, Ville Hautam\u00e4ki"}, {"title": "Understanding Points of Correspondence between Sentences for Abstractive Summarization", "authors": "Logan Lebanoff, John Muchovej, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang, Fei Liu"}, {"title": "Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining", "authors": "Ivana Kvapil\u00edkov\u00e1, Mikel Artetxe, Gorka Labaka, Eneko Agirre, Ond\u0159ej Bojar", "link": "", "summary": ""}, {"title": "Unsupervised Paraphasia Classification in Aphasic Speech", "authors": "Sharan Pai, Nikhil Sachdeva, Prince Sachdeva, Rajiv Ratn Shah"}, {"title": "Why is penguin more similar to polar bear than to sea gull? Analyzing conceptual knowledge in distributional models", "authors": "Pia Sommerauer"}, {"title": "Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition", "authors": "Hwichan Kim, Tosho Hirasawa, Mamoru Komachi"}]